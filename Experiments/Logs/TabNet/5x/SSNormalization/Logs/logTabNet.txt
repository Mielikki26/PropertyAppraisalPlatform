TabNet Logs:

Saving copy of script...
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:53:09
epoch 0  | loss: 0.75628 | val_0_rmse: 0.7603  | val_1_rmse: 0.7683  |  0:00:04s
epoch 1  | loss: 0.46993 | val_0_rmse: 0.66871 | val_1_rmse: 0.68096 |  0:00:06s
epoch 2  | loss: 0.41409 | val_0_rmse: 0.60894 | val_1_rmse: 0.62209 |  0:00:07s
epoch 3  | loss: 0.38161 | val_0_rmse: 0.59109 | val_1_rmse: 0.60295 |  0:00:08s
epoch 4  | loss: 0.36346 | val_0_rmse: 0.57746 | val_1_rmse: 0.5938  |  0:00:10s
epoch 5  | loss: 0.35579 | val_0_rmse: 0.56431 | val_1_rmse: 0.58016 |  0:00:11s
epoch 6  | loss: 0.34472 | val_0_rmse: 0.56136 | val_1_rmse: 0.57813 |  0:00:13s
epoch 7  | loss: 0.34415 | val_0_rmse: 0.56703 | val_1_rmse: 0.58551 |  0:00:14s
epoch 8  | loss: 0.34625 | val_0_rmse: 0.55684 | val_1_rmse: 0.57294 |  0:00:16s
epoch 9  | loss: 0.33851 | val_0_rmse: 0.57895 | val_1_rmse: 0.58928 |  0:00:17s
epoch 10 | loss: 0.33856 | val_0_rmse: 0.57499 | val_1_rmse: 0.59155 |  0:00:19s
epoch 11 | loss: 0.33799 | val_0_rmse: 0.56262 | val_1_rmse: 0.57971 |  0:00:20s
epoch 12 | loss: 0.33499 | val_0_rmse: 0.56714 | val_1_rmse: 0.57781 |  0:00:21s
epoch 13 | loss: 0.32857 | val_0_rmse: 0.57344 | val_1_rmse: 0.58841 |  0:00:23s
epoch 14 | loss: 0.33167 | val_0_rmse: 0.54708 | val_1_rmse: 0.56099 |  0:00:24s
epoch 15 | loss: 0.32133 | val_0_rmse: 0.54195 | val_1_rmse: 0.55942 |  0:00:26s
epoch 16 | loss: 0.3188  | val_0_rmse: 0.59635 | val_1_rmse: 0.61133 |  0:00:27s
epoch 17 | loss: 0.31872 | val_0_rmse: 0.53341 | val_1_rmse: 0.55161 |  0:00:29s
epoch 18 | loss: 0.31613 | val_0_rmse: 0.55674 | val_1_rmse: 0.57399 |  0:00:30s
epoch 19 | loss: 0.31877 | val_0_rmse: 0.57891 | val_1_rmse: 0.5982  |  0:00:32s
epoch 20 | loss: 0.31441 | val_0_rmse: 0.5394  | val_1_rmse: 0.55677 |  0:00:33s
epoch 21 | loss: 0.31563 | val_0_rmse: 0.56993 | val_1_rmse: 0.58276 |  0:00:34s
epoch 22 | loss: 0.32015 | val_0_rmse: 0.54918 | val_1_rmse: 0.56479 |  0:00:36s
epoch 23 | loss: 0.30984 | val_0_rmse: 0.58899 | val_1_rmse: 0.59894 |  0:00:37s
epoch 24 | loss: 0.30838 | val_0_rmse: 0.53077 | val_1_rmse: 0.54977 |  0:00:39s
epoch 25 | loss: 0.32489 | val_0_rmse: 0.56359 | val_1_rmse: 0.58024 |  0:00:40s
epoch 26 | loss: 0.34704 | val_0_rmse: 0.56588 | val_1_rmse: 0.58543 |  0:00:42s
epoch 27 | loss: 0.33687 | val_0_rmse: 0.55176 | val_1_rmse: 0.56439 |  0:00:43s
epoch 28 | loss: 0.32794 | val_0_rmse: 0.55379 | val_1_rmse: 0.5736  |  0:00:44s
epoch 29 | loss: 0.31886 | val_0_rmse: 0.54896 | val_1_rmse: 0.56645 |  0:00:46s
epoch 30 | loss: 0.32348 | val_0_rmse: 0.61104 | val_1_rmse: 0.62615 |  0:00:47s
epoch 31 | loss: 0.32034 | val_0_rmse: 0.5588  | val_1_rmse: 0.57777 |  0:00:49s
epoch 32 | loss: 0.32011 | val_0_rmse: 0.56151 | val_1_rmse: 0.58142 |  0:00:50s
epoch 33 | loss: 0.31786 | val_0_rmse: 0.54218 | val_1_rmse: 0.56054 |  0:00:52s
epoch 34 | loss: 0.32403 | val_0_rmse: 0.61016 | val_1_rmse: 0.63111 |  0:00:53s
epoch 35 | loss: 0.31941 | val_0_rmse: 0.55855 | val_1_rmse: 0.573   |  0:00:54s
epoch 36 | loss: 0.31728 | val_0_rmse: 0.53771 | val_1_rmse: 0.55827 |  0:00:56s
epoch 37 | loss: 0.31066 | val_0_rmse: 0.5888  | val_1_rmse: 0.59944 |  0:00:57s
epoch 38 | loss: 0.3162  | val_0_rmse: 0.53759 | val_1_rmse: 0.55875 |  0:00:59s
epoch 39 | loss: 0.30752 | val_0_rmse: 0.53688 | val_1_rmse: 0.55772 |  0:01:00s
epoch 40 | loss: 0.30608 | val_0_rmse: 0.53234 | val_1_rmse: 0.55013 |  0:01:02s
epoch 41 | loss: 0.3076  | val_0_rmse: 0.54962 | val_1_rmse: 0.56545 |  0:01:03s
epoch 42 | loss: 0.31674 | val_0_rmse: 0.54757 | val_1_rmse: 0.56796 |  0:01:04s
epoch 43 | loss: 0.30949 | val_0_rmse: 0.54568 | val_1_rmse: 0.56685 |  0:01:06s
epoch 44 | loss: 0.30623 | val_0_rmse: 0.55954 | val_1_rmse: 0.58263 |  0:01:07s
epoch 45 | loss: 0.31537 | val_0_rmse: 0.53148 | val_1_rmse: 0.54937 |  0:01:09s
epoch 46 | loss: 0.30878 | val_0_rmse: 0.53853 | val_1_rmse: 0.55459 |  0:01:10s
epoch 47 | loss: 0.30605 | val_0_rmse: 0.54255 | val_1_rmse: 0.55496 |  0:01:12s
epoch 48 | loss: 0.30611 | val_0_rmse: 0.54564 | val_1_rmse: 0.56403 |  0:01:13s
epoch 49 | loss: 0.29767 | val_0_rmse: 0.52946 | val_1_rmse: 0.54189 |  0:01:14s
epoch 50 | loss: 0.29832 | val_0_rmse: 0.52777 | val_1_rmse: 0.54651 |  0:01:16s
epoch 51 | loss: 0.30007 | val_0_rmse: 0.52801 | val_1_rmse: 0.54675 |  0:01:17s
epoch 52 | loss: 0.3081  | val_0_rmse: 0.53275 | val_1_rmse: 0.54471 |  0:01:19s
epoch 53 | loss: 0.3062  | val_0_rmse: 0.52882 | val_1_rmse: 0.54485 |  0:01:20s
epoch 54 | loss: 0.30116 | val_0_rmse: 0.53455 | val_1_rmse: 0.55478 |  0:01:22s
epoch 55 | loss: 0.29594 | val_0_rmse: 0.52403 | val_1_rmse: 0.53987 |  0:01:23s
epoch 56 | loss: 0.3001  | val_0_rmse: 0.52614 | val_1_rmse: 0.54024 |  0:01:24s
epoch 57 | loss: 0.29611 | val_0_rmse: 0.5309  | val_1_rmse: 0.54997 |  0:01:26s
epoch 58 | loss: 0.30571 | val_0_rmse: 0.51983 | val_1_rmse: 0.54131 |  0:01:27s
epoch 59 | loss: 0.29006 | val_0_rmse: 0.55462 | val_1_rmse: 0.56827 |  0:01:29s
epoch 60 | loss: 0.29617 | val_0_rmse: 0.5493  | val_1_rmse: 0.56277 |  0:01:30s
epoch 61 | loss: 0.29894 | val_0_rmse: 0.54112 | val_1_rmse: 0.56    |  0:01:32s
epoch 62 | loss: 0.29557 | val_0_rmse: 0.52417 | val_1_rmse: 0.53996 |  0:01:33s
epoch 63 | loss: 0.29745 | val_0_rmse: 0.52424 | val_1_rmse: 0.5389  |  0:01:34s
epoch 64 | loss: 0.30198 | val_0_rmse: 0.53444 | val_1_rmse: 0.55382 |  0:01:36s
epoch 65 | loss: 0.30258 | val_0_rmse: 0.51486 | val_1_rmse: 0.53652 |  0:01:37s
epoch 66 | loss: 0.28891 | val_0_rmse: 0.53682 | val_1_rmse: 0.55205 |  0:01:39s
epoch 67 | loss: 0.30086 | val_0_rmse: 0.55033 | val_1_rmse: 0.57365 |  0:01:40s
epoch 68 | loss: 0.30511 | val_0_rmse: 0.52738 | val_1_rmse: 0.54177 |  0:01:42s
epoch 69 | loss: 0.29674 | val_0_rmse: 0.53836 | val_1_rmse: 0.56353 |  0:01:43s
epoch 70 | loss: 0.29374 | val_0_rmse: 0.51432 | val_1_rmse: 0.53288 |  0:01:45s
epoch 71 | loss: 0.28997 | val_0_rmse: 0.52541 | val_1_rmse: 0.54819 |  0:01:46s
epoch 72 | loss: 0.2952  | val_0_rmse: 0.53356 | val_1_rmse: 0.55105 |  0:01:47s
epoch 73 | loss: 0.29074 | val_0_rmse: 0.51065 | val_1_rmse: 0.53054 |  0:01:49s
epoch 74 | loss: 0.28757 | val_0_rmse: 0.52887 | val_1_rmse: 0.54519 |  0:01:50s
epoch 75 | loss: 0.29916 | val_0_rmse: 0.53059 | val_1_rmse: 0.55321 |  0:01:52s
epoch 76 | loss: 0.29065 | val_0_rmse: 0.53992 | val_1_rmse: 0.55452 |  0:01:53s
epoch 77 | loss: 0.29868 | val_0_rmse: 0.52931 | val_1_rmse: 0.55092 |  0:01:55s
epoch 78 | loss: 0.29358 | val_0_rmse: 0.52514 | val_1_rmse: 0.54576 |  0:01:56s
epoch 79 | loss: 0.2962  | val_0_rmse: 0.56624 | val_1_rmse: 0.57708 |  0:01:58s
epoch 80 | loss: 0.29462 | val_0_rmse: 0.53722 | val_1_rmse: 0.56313 |  0:01:59s
epoch 81 | loss: 0.29776 | val_0_rmse: 0.51808 | val_1_rmse: 0.54025 |  0:02:00s
epoch 82 | loss: 0.29096 | val_0_rmse: 0.52731 | val_1_rmse: 0.54825 |  0:02:02s
epoch 83 | loss: 0.29375 | val_0_rmse: 0.5244  | val_1_rmse: 0.54237 |  0:02:03s
epoch 84 | loss: 0.29466 | val_0_rmse: 0.50926 | val_1_rmse: 0.52742 |  0:02:05s
epoch 85 | loss: 0.2869  | val_0_rmse: 0.52871 | val_1_rmse: 0.551   |  0:02:06s
epoch 86 | loss: 0.2849  | val_0_rmse: 0.51448 | val_1_rmse: 0.53641 |  0:02:08s
epoch 87 | loss: 0.28258 | val_0_rmse: 0.55352 | val_1_rmse: 0.57445 |  0:02:09s
epoch 88 | loss: 0.29374 | val_0_rmse: 0.51701 | val_1_rmse: 0.53618 |  0:02:10s
epoch 89 | loss: 0.29182 | val_0_rmse: 0.52243 | val_1_rmse: 0.54283 |  0:02:12s
epoch 90 | loss: 0.29423 | val_0_rmse: 0.52616 | val_1_rmse: 0.54936 |  0:02:13s
epoch 91 | loss: 0.29715 | val_0_rmse: 0.5273  | val_1_rmse: 0.54783 |  0:02:15s
epoch 92 | loss: 0.28901 | val_0_rmse: 0.51087 | val_1_rmse: 0.53015 |  0:02:16s
epoch 93 | loss: 0.29612 | val_0_rmse: 0.53901 | val_1_rmse: 0.55758 |  0:02:18s
epoch 94 | loss: 0.29884 | val_0_rmse: 0.54215 | val_1_rmse: 0.56489 |  0:02:19s
epoch 95 | loss: 0.29334 | val_0_rmse: 0.53825 | val_1_rmse: 0.55534 |  0:02:20s
epoch 96 | loss: 0.29568 | val_0_rmse: 0.5151  | val_1_rmse: 0.5323  |  0:02:22s
epoch 97 | loss: 0.29311 | val_0_rmse: 0.50781 | val_1_rmse: 0.52917 |  0:02:23s
epoch 98 | loss: 0.29165 | val_0_rmse: 0.51292 | val_1_rmse: 0.53563 |  0:02:25s
epoch 99 | loss: 0.28246 | val_0_rmse: 0.51175 | val_1_rmse: 0.5363  |  0:02:26s
epoch 100| loss: 0.28556 | val_0_rmse: 0.53971 | val_1_rmse: 0.568   |  0:02:28s
epoch 101| loss: 0.28364 | val_0_rmse: 0.5342  | val_1_rmse: 0.56198 |  0:02:29s
epoch 102| loss: 0.28586 | val_0_rmse: 0.51731 | val_1_rmse: 0.54001 |  0:02:31s
epoch 103| loss: 0.28717 | val_0_rmse: 0.51111 | val_1_rmse: 0.53397 |  0:02:32s
epoch 104| loss: 0.2827  | val_0_rmse: 0.50702 | val_1_rmse: 0.52911 |  0:02:33s
epoch 105| loss: 0.28238 | val_0_rmse: 0.50425 | val_1_rmse: 0.52879 |  0:02:35s
epoch 106| loss: 0.28352 | val_0_rmse: 0.5331  | val_1_rmse: 0.55415 |  0:02:36s
epoch 107| loss: 0.28525 | val_0_rmse: 0.51444 | val_1_rmse: 0.53888 |  0:02:38s
epoch 108| loss: 0.28754 | val_0_rmse: 0.51783 | val_1_rmse: 0.53705 |  0:02:39s
epoch 109| loss: 0.28137 | val_0_rmse: 0.50492 | val_1_rmse: 0.53102 |  0:02:41s
epoch 110| loss: 0.2866  | val_0_rmse: 0.50382 | val_1_rmse: 0.52734 |  0:02:42s
epoch 111| loss: 0.27958 | val_0_rmse: 0.50508 | val_1_rmse: 0.53078 |  0:02:43s
epoch 112| loss: 0.27761 | val_0_rmse: 0.50562 | val_1_rmse: 0.52858 |  0:02:45s
epoch 113| loss: 0.27849 | val_0_rmse: 0.5015  | val_1_rmse: 0.52355 |  0:02:46s
epoch 114| loss: 0.27876 | val_0_rmse: 0.516   | val_1_rmse: 0.53752 |  0:02:48s
epoch 115| loss: 0.28507 | val_0_rmse: 0.53031 | val_1_rmse: 0.54782 |  0:02:49s
epoch 116| loss: 0.28119 | val_0_rmse: 0.52597 | val_1_rmse: 0.55159 |  0:02:51s
epoch 117| loss: 0.28625 | val_0_rmse: 0.51831 | val_1_rmse: 0.53895 |  0:02:52s
epoch 118| loss: 0.27944 | val_0_rmse: 0.50519 | val_1_rmse: 0.53178 |  0:02:53s
epoch 119| loss: 0.27478 | val_0_rmse: 0.52589 | val_1_rmse: 0.55388 |  0:02:55s
epoch 120| loss: 0.27015 | val_0_rmse: 0.50312 | val_1_rmse: 0.52839 |  0:02:56s
epoch 121| loss: 0.2735  | val_0_rmse: 0.5229  | val_1_rmse: 0.54611 |  0:02:58s
epoch 122| loss: 0.2836  | val_0_rmse: 0.49855 | val_1_rmse: 0.52375 |  0:02:59s
epoch 123| loss: 0.278   | val_0_rmse: 0.49769 | val_1_rmse: 0.52408 |  0:03:01s
epoch 124| loss: 0.27972 | val_0_rmse: 0.51589 | val_1_rmse: 0.54888 |  0:03:02s
epoch 125| loss: 0.27324 | val_0_rmse: 0.50479 | val_1_rmse: 0.52846 |  0:03:04s
epoch 126| loss: 0.29292 | val_0_rmse: 0.5249  | val_1_rmse: 0.5549  |  0:03:05s
epoch 127| loss: 0.2844  | val_0_rmse: 0.5038  | val_1_rmse: 0.53013 |  0:03:06s
epoch 128| loss: 0.28263 | val_0_rmse: 0.53532 | val_1_rmse: 0.552   |  0:03:08s
epoch 129| loss: 0.27916 | val_0_rmse: 0.49788 | val_1_rmse: 0.52146 |  0:03:09s
epoch 130| loss: 0.27642 | val_0_rmse: 0.49807 | val_1_rmse: 0.5202  |  0:03:11s
epoch 131| loss: 0.28231 | val_0_rmse: 0.50322 | val_1_rmse: 0.52585 |  0:03:12s
epoch 132| loss: 0.28113 | val_0_rmse: 0.53343 | val_1_rmse: 0.55087 |  0:03:14s
epoch 133| loss: 0.28081 | val_0_rmse: 0.50729 | val_1_rmse: 0.52988 |  0:03:15s
epoch 134| loss: 0.28267 | val_0_rmse: 0.53594 | val_1_rmse: 0.56017 |  0:03:16s
epoch 135| loss: 0.28258 | val_0_rmse: 0.53515 | val_1_rmse: 0.55821 |  0:03:18s
epoch 136| loss: 0.27553 | val_0_rmse: 0.49982 | val_1_rmse: 0.52655 |  0:03:19s
epoch 137| loss: 0.27581 | val_0_rmse: 0.52871 | val_1_rmse: 0.55178 |  0:03:21s
epoch 138| loss: 0.27107 | val_0_rmse: 0.48979 | val_1_rmse: 0.51504 |  0:03:22s
epoch 139| loss: 0.27503 | val_0_rmse: 0.51436 | val_1_rmse: 0.54011 |  0:03:24s
epoch 140| loss: 0.27293 | val_0_rmse: 0.51452 | val_1_rmse: 0.53585 |  0:03:25s
epoch 141| loss: 0.27809 | val_0_rmse: 0.53253 | val_1_rmse: 0.55192 |  0:03:26s
epoch 142| loss: 0.27424 | val_0_rmse: 0.496   | val_1_rmse: 0.52717 |  0:03:28s
epoch 143| loss: 0.27968 | val_0_rmse: 0.5131  | val_1_rmse: 0.53618 |  0:03:29s
epoch 144| loss: 0.27836 | val_0_rmse: 0.50377 | val_1_rmse: 0.52954 |  0:03:31s
epoch 145| loss: 0.28929 | val_0_rmse: 0.50075 | val_1_rmse: 0.5259  |  0:03:32s
epoch 146| loss: 0.28013 | val_0_rmse: 0.50756 | val_1_rmse: 0.53119 |  0:03:34s
epoch 147| loss: 0.28007 | val_0_rmse: 0.49935 | val_1_rmse: 0.5231  |  0:03:35s
epoch 148| loss: 0.28309 | val_0_rmse: 0.50045 | val_1_rmse: 0.52705 |  0:03:36s
epoch 149| loss: 0.27218 | val_0_rmse: 0.50983 | val_1_rmse: 0.53571 |  0:03:38s
Stop training because you reached max_epochs = 150 with best_epoch = 138 and best_val_1_rmse = 0.51504
Best weights from best epoch are automatically used!
ended training at: 23:56:48
Feature importance:
[('Area', 0.3017088174123514), ('Baths', 0.0), ('Beds', 0.0), ('Latitude', 0.23587441801079653), ('Longitude', 0.27013974960766274), ('Month', 0.04176555131801874), ('Year', 0.15051146365117057)]
Mean squared error is of 6104409000.272222
Mean absolute error:53131.4945889466
MAPE:0.16717257988732928
R2 score:0.7292972800484627
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:56:49
epoch 0  | loss: 0.75078 | val_0_rmse: 0.81353 | val_1_rmse: 0.82665 |  0:00:01s
epoch 1  | loss: 0.50526 | val_0_rmse: 0.70806 | val_1_rmse: 0.70465 |  0:00:02s
epoch 2  | loss: 0.45051 | val_0_rmse: 0.72965 | val_1_rmse: 0.72345 |  0:00:04s
epoch 3  | loss: 0.43802 | val_0_rmse: 0.63472 | val_1_rmse: 0.62651 |  0:00:05s
epoch 4  | loss: 0.42559 | val_0_rmse: 0.62813 | val_1_rmse: 0.62484 |  0:00:07s
epoch 5  | loss: 0.40543 | val_0_rmse: 0.62697 | val_1_rmse: 0.61774 |  0:00:08s
epoch 6  | loss: 0.41801 | val_0_rmse: 0.6639  | val_1_rmse: 0.65055 |  0:00:10s
epoch 7  | loss: 0.4036  | val_0_rmse: 0.60874 | val_1_rmse: 0.60342 |  0:00:11s
epoch 8  | loss: 0.38561 | val_0_rmse: 0.59881 | val_1_rmse: 0.59117 |  0:00:13s
epoch 9  | loss: 0.38586 | val_0_rmse: 0.61839 | val_1_rmse: 0.60394 |  0:00:14s
epoch 10 | loss: 0.37822 | val_0_rmse: 0.65994 | val_1_rmse: 0.66157 |  0:00:15s
epoch 11 | loss: 0.37842 | val_0_rmse: 0.59294 | val_1_rmse: 0.59224 |  0:00:17s
epoch 12 | loss: 0.36117 | val_0_rmse: 0.57147 | val_1_rmse: 0.56771 |  0:00:18s
epoch 13 | loss: 0.35549 | val_0_rmse: 0.57296 | val_1_rmse: 0.56997 |  0:00:20s
epoch 14 | loss: 0.3568  | val_0_rmse: 0.58501 | val_1_rmse: 0.57975 |  0:00:21s
epoch 15 | loss: 0.35288 | val_0_rmse: 0.57583 | val_1_rmse: 0.5696  |  0:00:23s
epoch 16 | loss: 0.36344 | val_0_rmse: 0.57066 | val_1_rmse: 0.5636  |  0:00:24s
epoch 17 | loss: 0.36723 | val_0_rmse: 0.57582 | val_1_rmse: 0.56663 |  0:00:26s
epoch 18 | loss: 0.34935 | val_0_rmse: 0.58042 | val_1_rmse: 0.57441 |  0:00:27s
epoch 19 | loss: 0.35833 | val_0_rmse: 0.56268 | val_1_rmse: 0.55808 |  0:00:29s
epoch 20 | loss: 0.34557 | val_0_rmse: 0.559   | val_1_rmse: 0.5524  |  0:00:30s
epoch 21 | loss: 0.34013 | val_0_rmse: 0.54944 | val_1_rmse: 0.5514  |  0:00:32s
epoch 22 | loss: 0.34051 | val_0_rmse: 0.56395 | val_1_rmse: 0.56045 |  0:00:33s
epoch 23 | loss: 0.34103 | val_0_rmse: 0.5731  | val_1_rmse: 0.56746 |  0:00:35s
epoch 24 | loss: 0.33378 | val_0_rmse: 0.55405 | val_1_rmse: 0.55799 |  0:00:36s
epoch 25 | loss: 0.33361 | val_0_rmse: 0.57656 | val_1_rmse: 0.57523 |  0:00:37s
epoch 26 | loss: 0.33694 | val_0_rmse: 0.54983 | val_1_rmse: 0.54701 |  0:00:39s
epoch 27 | loss: 0.33294 | val_0_rmse: 0.56862 | val_1_rmse: 0.56995 |  0:00:40s
epoch 28 | loss: 0.32591 | val_0_rmse: 0.55091 | val_1_rmse: 0.5549  |  0:00:42s
epoch 29 | loss: 0.33796 | val_0_rmse: 0.55568 | val_1_rmse: 0.55715 |  0:00:43s
epoch 30 | loss: 0.33432 | val_0_rmse: 0.54885 | val_1_rmse: 0.54683 |  0:00:45s
epoch 31 | loss: 0.34868 | val_0_rmse: 0.57105 | val_1_rmse: 0.56665 |  0:00:46s
epoch 32 | loss: 0.34073 | val_0_rmse: 0.55274 | val_1_rmse: 0.55112 |  0:00:48s
epoch 33 | loss: 0.32208 | val_0_rmse: 0.55721 | val_1_rmse: 0.55492 |  0:00:49s
epoch 34 | loss: 0.33338 | val_0_rmse: 0.56655 | val_1_rmse: 0.56242 |  0:00:50s
epoch 35 | loss: 0.33491 | val_0_rmse: 0.57682 | val_1_rmse: 0.58416 |  0:00:52s
epoch 36 | loss: 0.32996 | val_0_rmse: 0.55294 | val_1_rmse: 0.55615 |  0:00:53s
epoch 37 | loss: 0.32579 | val_0_rmse: 0.54932 | val_1_rmse: 0.55494 |  0:00:55s
epoch 38 | loss: 0.32714 | val_0_rmse: 0.59488 | val_1_rmse: 0.59473 |  0:00:56s
epoch 39 | loss: 0.34627 | val_0_rmse: 0.55489 | val_1_rmse: 0.55365 |  0:00:58s
epoch 40 | loss: 0.32604 | val_0_rmse: 0.54024 | val_1_rmse: 0.54028 |  0:00:59s
epoch 41 | loss: 0.31941 | val_0_rmse: 0.54239 | val_1_rmse: 0.54817 |  0:01:01s
epoch 42 | loss: 0.33014 | val_0_rmse: 0.5601  | val_1_rmse: 0.57001 |  0:01:02s
epoch 43 | loss: 0.3276  | val_0_rmse: 0.55126 | val_1_rmse: 0.54952 |  0:01:03s
epoch 44 | loss: 0.31721 | val_0_rmse: 0.5366  | val_1_rmse: 0.54242 |  0:01:05s
epoch 45 | loss: 0.31023 | val_0_rmse: 0.53737 | val_1_rmse: 0.54344 |  0:01:06s
epoch 46 | loss: 0.32071 | val_0_rmse: 0.55222 | val_1_rmse: 0.56134 |  0:01:08s
epoch 47 | loss: 0.3235  | val_0_rmse: 0.53456 | val_1_rmse: 0.54165 |  0:01:09s
epoch 48 | loss: 0.31304 | val_0_rmse: 0.54346 | val_1_rmse: 0.54944 |  0:01:11s
epoch 49 | loss: 0.31781 | val_0_rmse: 0.554   | val_1_rmse: 0.55213 |  0:01:12s
epoch 50 | loss: 0.33634 | val_0_rmse: 0.55247 | val_1_rmse: 0.54878 |  0:01:14s
epoch 51 | loss: 0.32304 | val_0_rmse: 0.53775 | val_1_rmse: 0.54155 |  0:01:15s
epoch 52 | loss: 0.31464 | val_0_rmse: 0.53311 | val_1_rmse: 0.53969 |  0:01:16s
epoch 53 | loss: 0.31046 | val_0_rmse: 0.53721 | val_1_rmse: 0.53883 |  0:01:18s
epoch 54 | loss: 0.30773 | val_0_rmse: 0.5312  | val_1_rmse: 0.53318 |  0:01:19s
epoch 55 | loss: 0.30682 | val_0_rmse: 0.53005 | val_1_rmse: 0.53827 |  0:01:21s
epoch 56 | loss: 0.30598 | val_0_rmse: 0.53438 | val_1_rmse: 0.54143 |  0:01:22s
epoch 57 | loss: 0.30858 | val_0_rmse: 0.5304  | val_1_rmse: 0.53532 |  0:01:24s
epoch 58 | loss: 0.3069  | val_0_rmse: 0.53026 | val_1_rmse: 0.53694 |  0:01:25s
epoch 59 | loss: 0.30375 | val_0_rmse: 0.52548 | val_1_rmse: 0.5289  |  0:01:27s
epoch 60 | loss: 0.30183 | val_0_rmse: 0.53867 | val_1_rmse: 0.54307 |  0:01:28s
epoch 61 | loss: 0.30686 | val_0_rmse: 0.53615 | val_1_rmse: 0.53798 |  0:01:29s
epoch 62 | loss: 0.31752 | val_0_rmse: 0.54234 | val_1_rmse: 0.54247 |  0:01:31s
epoch 63 | loss: 0.31526 | val_0_rmse: 0.53359 | val_1_rmse: 0.53393 |  0:01:32s
epoch 64 | loss: 0.31355 | val_0_rmse: 0.56048 | val_1_rmse: 0.56742 |  0:01:34s
epoch 65 | loss: 0.30565 | val_0_rmse: 0.52674 | val_1_rmse: 0.53118 |  0:01:35s
epoch 66 | loss: 0.30437 | val_0_rmse: 0.5355  | val_1_rmse: 0.53975 |  0:01:37s
epoch 67 | loss: 0.3116  | val_0_rmse: 0.59656 | val_1_rmse: 0.601   |  0:01:38s
epoch 68 | loss: 0.31744 | val_0_rmse: 0.53134 | val_1_rmse: 0.54049 |  0:01:40s
epoch 69 | loss: 0.29929 | val_0_rmse: 0.52649 | val_1_rmse: 0.53426 |  0:01:41s
epoch 70 | loss: 0.29913 | val_0_rmse: 0.53371 | val_1_rmse: 0.54258 |  0:01:42s
epoch 71 | loss: 0.30905 | val_0_rmse: 0.52074 | val_1_rmse: 0.52552 |  0:01:44s
epoch 72 | loss: 0.30628 | val_0_rmse: 0.54817 | val_1_rmse: 0.55077 |  0:01:45s
epoch 73 | loss: 0.30496 | val_0_rmse: 0.52002 | val_1_rmse: 0.52934 |  0:01:47s
epoch 74 | loss: 0.30608 | val_0_rmse: 0.53102 | val_1_rmse: 0.53167 |  0:01:48s
epoch 75 | loss: 0.29626 | val_0_rmse: 0.51983 | val_1_rmse: 0.52596 |  0:01:50s
epoch 76 | loss: 0.30466 | val_0_rmse: 0.53328 | val_1_rmse: 0.54228 |  0:01:51s
epoch 77 | loss: 0.2939  | val_0_rmse: 0.52096 | val_1_rmse: 0.52261 |  0:01:53s
epoch 78 | loss: 0.31622 | val_0_rmse: 0.53649 | val_1_rmse: 0.53851 |  0:01:54s
epoch 79 | loss: 0.31124 | val_0_rmse: 0.54178 | val_1_rmse: 0.54463 |  0:01:56s
epoch 80 | loss: 0.30929 | val_0_rmse: 0.53558 | val_1_rmse: 0.53437 |  0:01:57s
epoch 81 | loss: 0.30804 | val_0_rmse: 0.52564 | val_1_rmse: 0.53188 |  0:01:58s
epoch 82 | loss: 0.30299 | val_0_rmse: 0.56157 | val_1_rmse: 0.57678 |  0:02:00s
epoch 83 | loss: 0.30118 | val_0_rmse: 0.53887 | val_1_rmse: 0.55246 |  0:02:01s
epoch 84 | loss: 0.31039 | val_0_rmse: 0.53336 | val_1_rmse: 0.53686 |  0:02:03s
epoch 85 | loss: 0.30414 | val_0_rmse: 0.51779 | val_1_rmse: 0.52388 |  0:02:04s
epoch 86 | loss: 0.30242 | val_0_rmse: 0.51918 | val_1_rmse: 0.53176 |  0:02:06s
epoch 87 | loss: 0.2973  | val_0_rmse: 0.51391 | val_1_rmse: 0.5247  |  0:02:07s
epoch 88 | loss: 0.29553 | val_0_rmse: 0.52204 | val_1_rmse: 0.52983 |  0:02:08s
epoch 89 | loss: 0.29782 | val_0_rmse: 0.51934 | val_1_rmse: 0.52696 |  0:02:10s
epoch 90 | loss: 0.29516 | val_0_rmse: 0.53311 | val_1_rmse: 0.53916 |  0:02:11s
epoch 91 | loss: 0.29653 | val_0_rmse: 0.51815 | val_1_rmse: 0.52246 |  0:02:13s
epoch 92 | loss: 0.29799 | val_0_rmse: 0.5343  | val_1_rmse: 0.5394  |  0:02:14s
epoch 93 | loss: 0.2995  | val_0_rmse: 0.55027 | val_1_rmse: 0.56031 |  0:02:16s
epoch 94 | loss: 0.29365 | val_0_rmse: 0.53646 | val_1_rmse: 0.54618 |  0:02:17s
epoch 95 | loss: 0.29364 | val_0_rmse: 0.51748 | val_1_rmse: 0.51936 |  0:02:19s
epoch 96 | loss: 0.29383 | val_0_rmse: 0.50909 | val_1_rmse: 0.52044 |  0:02:20s
epoch 97 | loss: 0.29197 | val_0_rmse: 0.51879 | val_1_rmse: 0.52758 |  0:02:22s
epoch 98 | loss: 0.2922  | val_0_rmse: 0.52089 | val_1_rmse: 0.52696 |  0:02:23s
epoch 99 | loss: 0.2952  | val_0_rmse: 0.55618 | val_1_rmse: 0.55746 |  0:02:24s
epoch 100| loss: 0.29058 | val_0_rmse: 0.51273 | val_1_rmse: 0.52097 |  0:02:26s
epoch 101| loss: 0.2916  | val_0_rmse: 0.51458 | val_1_rmse: 0.52856 |  0:02:27s
epoch 102| loss: 0.29774 | val_0_rmse: 0.52051 | val_1_rmse: 0.52935 |  0:02:29s
epoch 103| loss: 0.2973  | val_0_rmse: 0.51788 | val_1_rmse: 0.52671 |  0:02:31s
epoch 104| loss: 0.29179 | val_0_rmse: 0.51589 | val_1_rmse: 0.53269 |  0:02:32s
epoch 105| loss: 0.28922 | val_0_rmse: 0.51332 | val_1_rmse: 0.52763 |  0:02:34s
epoch 106| loss: 0.29462 | val_0_rmse: 0.51385 | val_1_rmse: 0.52528 |  0:02:35s
epoch 107| loss: 0.29192 | val_0_rmse: 0.50983 | val_1_rmse: 0.5233  |  0:02:37s
epoch 108| loss: 0.28968 | val_0_rmse: 0.51413 | val_1_rmse: 0.52271 |  0:02:38s
epoch 109| loss: 0.28779 | val_0_rmse: 0.51186 | val_1_rmse: 0.51941 |  0:02:40s
epoch 110| loss: 0.28804 | val_0_rmse: 0.50651 | val_1_rmse: 0.51741 |  0:02:41s
epoch 111| loss: 0.28813 | val_0_rmse: 0.51898 | val_1_rmse: 0.52667 |  0:02:43s
epoch 112| loss: 0.29008 | val_0_rmse: 0.5195  | val_1_rmse: 0.52571 |  0:02:44s
epoch 113| loss: 0.29092 | val_0_rmse: 0.50862 | val_1_rmse: 0.52144 |  0:02:46s
epoch 114| loss: 0.29397 | val_0_rmse: 0.51316 | val_1_rmse: 0.5188  |  0:02:48s
epoch 115| loss: 0.29515 | val_0_rmse: 0.5106  | val_1_rmse: 0.51716 |  0:02:49s
epoch 116| loss: 0.29104 | val_0_rmse: 0.50671 | val_1_rmse: 0.5142  |  0:02:51s
epoch 117| loss: 0.28655 | val_0_rmse: 0.50903 | val_1_rmse: 0.51444 |  0:02:53s
epoch 118| loss: 0.28734 | val_0_rmse: 0.50109 | val_1_rmse: 0.51177 |  0:02:54s
epoch 119| loss: 0.28283 | val_0_rmse: 0.50774 | val_1_rmse: 0.52589 |  0:02:56s
epoch 120| loss: 0.31073 | val_0_rmse: 0.5315  | val_1_rmse: 0.53659 |  0:02:57s
epoch 121| loss: 0.30778 | val_0_rmse: 0.53609 | val_1_rmse: 0.53702 |  0:02:59s
epoch 122| loss: 0.30038 | val_0_rmse: 0.51081 | val_1_rmse: 0.52057 |  0:03:01s
epoch 123| loss: 0.28693 | val_0_rmse: 0.50684 | val_1_rmse: 0.51602 |  0:03:02s
epoch 124| loss: 0.29087 | val_0_rmse: 0.51064 | val_1_rmse: 0.51887 |  0:03:04s
epoch 125| loss: 0.29147 | val_0_rmse: 0.51133 | val_1_rmse: 0.51863 |  0:03:05s
epoch 126| loss: 0.28904 | val_0_rmse: 0.50237 | val_1_rmse: 0.51664 |  0:03:07s
epoch 127| loss: 0.28208 | val_0_rmse: 0.50181 | val_1_rmse: 0.51301 |  0:03:08s
epoch 128| loss: 0.28977 | val_0_rmse: 0.52826 | val_1_rmse: 0.53146 |  0:03:10s
epoch 129| loss: 0.28923 | val_0_rmse: 0.50797 | val_1_rmse: 0.5175  |  0:03:11s
epoch 130| loss: 0.28474 | val_0_rmse: 0.50408 | val_1_rmse: 0.51812 |  0:03:13s
epoch 131| loss: 0.28373 | val_0_rmse: 0.50368 | val_1_rmse: 0.51354 |  0:03:14s
epoch 132| loss: 0.27638 | val_0_rmse: 0.50597 | val_1_rmse: 0.51337 |  0:03:16s
epoch 133| loss: 0.28027 | val_0_rmse: 0.5081  | val_1_rmse: 0.51632 |  0:03:17s
epoch 134| loss: 0.27568 | val_0_rmse: 0.50467 | val_1_rmse: 0.51235 |  0:03:19s
epoch 135| loss: 0.28793 | val_0_rmse: 0.5153  | val_1_rmse: 0.53009 |  0:03:20s
epoch 136| loss: 0.28401 | val_0_rmse: 0.5144  | val_1_rmse: 0.52794 |  0:03:22s
epoch 137| loss: 0.29234 | val_0_rmse: 0.52847 | val_1_rmse: 0.5365  |  0:03:23s
epoch 138| loss: 0.29085 | val_0_rmse: 0.50564 | val_1_rmse: 0.51796 |  0:03:24s
epoch 139| loss: 0.27955 | val_0_rmse: 0.49727 | val_1_rmse: 0.51054 |  0:03:26s
epoch 140| loss: 0.29213 | val_0_rmse: 0.51496 | val_1_rmse: 0.52523 |  0:03:27s
epoch 141| loss: 0.28962 | val_0_rmse: 0.5074  | val_1_rmse: 0.52202 |  0:03:29s
epoch 142| loss: 0.2802  | val_0_rmse: 0.50023 | val_1_rmse: 0.51276 |  0:03:30s
epoch 143| loss: 0.27764 | val_0_rmse: 0.51922 | val_1_rmse: 0.54057 |  0:03:32s
epoch 144| loss: 0.28482 | val_0_rmse: 0.52299 | val_1_rmse: 0.53629 |  0:03:33s
epoch 145| loss: 0.27839 | val_0_rmse: 0.50267 | val_1_rmse: 0.52116 |  0:03:35s
epoch 146| loss: 0.28861 | val_0_rmse: 0.51186 | val_1_rmse: 0.52327 |  0:03:36s
epoch 147| loss: 0.29669 | val_0_rmse: 0.51717 | val_1_rmse: 0.52739 |  0:03:38s
epoch 148| loss: 0.29326 | val_0_rmse: 0.50903 | val_1_rmse: 0.5192  |  0:03:39s
epoch 149| loss: 0.28714 | val_0_rmse: 0.51243 | val_1_rmse: 0.52671 |  0:03:40s
Stop training because you reached max_epochs = 150 with best_epoch = 139 and best_val_1_rmse = 0.51054
Best weights from best epoch are automatically used!
ended training at: 00:00:30
Feature importance:
[('Area', 0.31026641265018734), ('Baths', 0.07707996592278737), ('Beds', 0.02128721740894665), ('Latitude', 0.20563133918574356), ('Longitude', 0.35403088524030774), ('Month', 0.0), ('Year', 0.031704179592027315)]
Mean squared error is of 6390031108.709841
Mean absolute error:55779.186507039936
MAPE:0.18598223795771765
R2 score:0.7253367559335887
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:00:30
epoch 0  | loss: 0.79008 | val_0_rmse: 0.80014 | val_1_rmse: 0.78697 |  0:00:01s
epoch 1  | loss: 0.52719 | val_0_rmse: 0.6921  | val_1_rmse: 0.70061 |  0:00:02s
epoch 2  | loss: 0.47808 | val_0_rmse: 0.7161  | val_1_rmse: 0.71149 |  0:00:04s
epoch 3  | loss: 0.44001 | val_0_rmse: 0.63109 | val_1_rmse: 0.63871 |  0:00:05s
epoch 4  | loss: 0.44735 | val_0_rmse: 0.63716 | val_1_rmse: 0.63357 |  0:00:07s
epoch 5  | loss: 0.42024 | val_0_rmse: 0.61389 | val_1_rmse: 0.60263 |  0:00:08s
epoch 6  | loss: 0.40511 | val_0_rmse: 0.63419 | val_1_rmse: 0.6287  |  0:00:10s
epoch 7  | loss: 0.40162 | val_0_rmse: 0.61471 | val_1_rmse: 0.61331 |  0:00:11s
epoch 8  | loss: 0.38407 | val_0_rmse: 0.60542 | val_1_rmse: 0.5957  |  0:00:13s
epoch 9  | loss: 0.38166 | val_0_rmse: 0.5945  | val_1_rmse: 0.58595 |  0:00:14s
epoch 10 | loss: 0.37168 | val_0_rmse: 0.59097 | val_1_rmse: 0.58563 |  0:00:16s
epoch 11 | loss: 0.37497 | val_0_rmse: 0.59208 | val_1_rmse: 0.58608 |  0:00:17s
epoch 12 | loss: 0.36159 | val_0_rmse: 0.5696  | val_1_rmse: 0.5611  |  0:00:19s
epoch 13 | loss: 0.36611 | val_0_rmse: 0.58867 | val_1_rmse: 0.58313 |  0:00:20s
epoch 14 | loss: 0.35842 | val_0_rmse: 0.57263 | val_1_rmse: 0.56979 |  0:00:21s
epoch 15 | loss: 0.34424 | val_0_rmse: 0.57922 | val_1_rmse: 0.57426 |  0:00:23s
epoch 16 | loss: 0.34348 | val_0_rmse: 0.57167 | val_1_rmse: 0.57248 |  0:00:24s
epoch 17 | loss: 0.33951 | val_0_rmse: 0.55735 | val_1_rmse: 0.55714 |  0:00:26s
epoch 18 | loss: 0.34418 | val_0_rmse: 0.55888 | val_1_rmse: 0.56085 |  0:00:27s
epoch 19 | loss: 0.35007 | val_0_rmse: 0.56398 | val_1_rmse: 0.56765 |  0:00:29s
epoch 20 | loss: 0.35305 | val_0_rmse: 0.56343 | val_1_rmse: 0.56066 |  0:00:30s
epoch 21 | loss: 0.33698 | val_0_rmse: 0.55614 | val_1_rmse: 0.55527 |  0:00:32s
epoch 22 | loss: 0.33247 | val_0_rmse: 0.56042 | val_1_rmse: 0.55862 |  0:00:33s
epoch 23 | loss: 0.33079 | val_0_rmse: 0.56114 | val_1_rmse: 0.56087 |  0:00:35s
epoch 24 | loss: 0.32596 | val_0_rmse: 0.55298 | val_1_rmse: 0.55424 |  0:00:36s
epoch 25 | loss: 0.33498 | val_0_rmse: 0.55276 | val_1_rmse: 0.5491  |  0:00:38s
epoch 26 | loss: 0.3225  | val_0_rmse: 0.56039 | val_1_rmse: 0.5589  |  0:00:39s
epoch 27 | loss: 0.32913 | val_0_rmse: 0.54233 | val_1_rmse: 0.54376 |  0:00:40s
epoch 28 | loss: 0.32588 | val_0_rmse: 0.54603 | val_1_rmse: 0.54835 |  0:00:42s
epoch 29 | loss: 0.32793 | val_0_rmse: 0.5506  | val_1_rmse: 0.5509  |  0:00:43s
epoch 30 | loss: 0.32397 | val_0_rmse: 0.55617 | val_1_rmse: 0.55316 |  0:00:45s
epoch 31 | loss: 0.31844 | val_0_rmse: 0.56326 | val_1_rmse: 0.5643  |  0:00:46s
epoch 32 | loss: 0.32175 | val_0_rmse: 0.55412 | val_1_rmse: 0.55613 |  0:00:48s
epoch 33 | loss: 0.33579 | val_0_rmse: 0.57653 | val_1_rmse: 0.58037 |  0:00:49s
epoch 34 | loss: 0.3252  | val_0_rmse: 0.55514 | val_1_rmse: 0.55441 |  0:00:51s
epoch 35 | loss: 0.32106 | val_0_rmse: 0.54141 | val_1_rmse: 0.54262 |  0:00:52s
epoch 36 | loss: 0.31869 | val_0_rmse: 0.54555 | val_1_rmse: 0.54844 |  0:00:54s
epoch 37 | loss: 0.31882 | val_0_rmse: 0.53559 | val_1_rmse: 0.54046 |  0:00:55s
epoch 38 | loss: 0.31977 | val_0_rmse: 0.54306 | val_1_rmse: 0.54762 |  0:00:57s
epoch 39 | loss: 0.327   | val_0_rmse: 0.55123 | val_1_rmse: 0.55037 |  0:00:58s
epoch 40 | loss: 0.31824 | val_0_rmse: 0.59447 | val_1_rmse: 0.59565 |  0:00:59s
epoch 41 | loss: 0.32308 | val_0_rmse: 0.53493 | val_1_rmse: 0.54154 |  0:01:01s
epoch 42 | loss: 0.31949 | val_0_rmse: 0.554   | val_1_rmse: 0.55597 |  0:01:02s
epoch 43 | loss: 0.33318 | val_0_rmse: 0.55445 | val_1_rmse: 0.56243 |  0:01:04s
epoch 44 | loss: 0.32432 | val_0_rmse: 0.56505 | val_1_rmse: 0.5677  |  0:01:05s
epoch 45 | loss: 0.31236 | val_0_rmse: 0.54326 | val_1_rmse: 0.55124 |  0:01:07s
epoch 46 | loss: 0.30988 | val_0_rmse: 0.5637  | val_1_rmse: 0.56889 |  0:01:08s
epoch 47 | loss: 0.31385 | val_0_rmse: 0.53209 | val_1_rmse: 0.53638 |  0:01:10s
epoch 48 | loss: 0.31188 | val_0_rmse: 0.53416 | val_1_rmse: 0.54225 |  0:01:11s
epoch 49 | loss: 0.31024 | val_0_rmse: 0.5393  | val_1_rmse: 0.54557 |  0:01:12s
epoch 50 | loss: 0.30701 | val_0_rmse: 0.53129 | val_1_rmse: 0.53864 |  0:01:14s
epoch 51 | loss: 0.31017 | val_0_rmse: 0.54154 | val_1_rmse: 0.54833 |  0:01:15s
epoch 52 | loss: 0.3074  | val_0_rmse: 0.5455  | val_1_rmse: 0.55408 |  0:01:17s
epoch 53 | loss: 0.31057 | val_0_rmse: 0.52815 | val_1_rmse: 0.5354  |  0:01:18s
epoch 54 | loss: 0.30371 | val_0_rmse: 0.53391 | val_1_rmse: 0.54086 |  0:01:20s
epoch 55 | loss: 0.31233 | val_0_rmse: 0.53553 | val_1_rmse: 0.54246 |  0:01:21s
epoch 56 | loss: 0.31646 | val_0_rmse: 0.53515 | val_1_rmse: 0.54588 |  0:01:23s
epoch 57 | loss: 0.31332 | val_0_rmse: 0.56248 | val_1_rmse: 0.57047 |  0:01:24s
epoch 58 | loss: 0.30899 | val_0_rmse: 0.53921 | val_1_rmse: 0.54671 |  0:01:25s
epoch 59 | loss: 0.30352 | val_0_rmse: 0.53293 | val_1_rmse: 0.53879 |  0:01:27s
epoch 60 | loss: 0.30462 | val_0_rmse: 0.56354 | val_1_rmse: 0.57187 |  0:01:28s
epoch 61 | loss: 0.31372 | val_0_rmse: 0.5605  | val_1_rmse: 0.5653  |  0:01:30s
epoch 62 | loss: 0.30658 | val_0_rmse: 0.52545 | val_1_rmse: 0.53395 |  0:01:31s
epoch 63 | loss: 0.30755 | val_0_rmse: 0.53382 | val_1_rmse: 0.53579 |  0:01:33s
epoch 64 | loss: 0.30768 | val_0_rmse: 0.53763 | val_1_rmse: 0.54645 |  0:01:34s
epoch 65 | loss: 0.3052  | val_0_rmse: 0.53691 | val_1_rmse: 0.5443  |  0:01:36s
epoch 66 | loss: 0.30408 | val_0_rmse: 0.53679 | val_1_rmse: 0.54757 |  0:01:37s
epoch 67 | loss: 0.30177 | val_0_rmse: 0.53757 | val_1_rmse: 0.54511 |  0:01:39s
epoch 68 | loss: 0.29991 | val_0_rmse: 0.53273 | val_1_rmse: 0.54857 |  0:01:40s
epoch 69 | loss: 0.30395 | val_0_rmse: 0.53955 | val_1_rmse: 0.55151 |  0:01:41s
epoch 70 | loss: 0.30762 | val_0_rmse: 0.5337  | val_1_rmse: 0.54239 |  0:01:43s
epoch 71 | loss: 0.30242 | val_0_rmse: 0.51826 | val_1_rmse: 0.53467 |  0:01:44s
epoch 72 | loss: 0.30465 | val_0_rmse: 0.54117 | val_1_rmse: 0.55133 |  0:01:46s
epoch 73 | loss: 0.30284 | val_0_rmse: 0.55321 | val_1_rmse: 0.56642 |  0:01:47s
epoch 74 | loss: 0.30932 | val_0_rmse: 0.54597 | val_1_rmse: 0.55443 |  0:01:49s
epoch 75 | loss: 0.30103 | val_0_rmse: 0.51592 | val_1_rmse: 0.53124 |  0:01:50s
epoch 76 | loss: 0.30191 | val_0_rmse: 0.52502 | val_1_rmse: 0.53616 |  0:01:52s
epoch 77 | loss: 0.30801 | val_0_rmse: 0.52605 | val_1_rmse: 0.53436 |  0:01:53s
epoch 78 | loss: 0.29982 | val_0_rmse: 0.53476 | val_1_rmse: 0.54198 |  0:01:54s
epoch 79 | loss: 0.3018  | val_0_rmse: 0.56276 | val_1_rmse: 0.56939 |  0:01:56s
epoch 80 | loss: 0.31155 | val_0_rmse: 0.53397 | val_1_rmse: 0.54729 |  0:01:57s
epoch 81 | loss: 0.29762 | val_0_rmse: 0.52328 | val_1_rmse: 0.5342  |  0:01:59s
epoch 82 | loss: 0.29656 | val_0_rmse: 0.5261  | val_1_rmse: 0.53813 |  0:02:00s
epoch 83 | loss: 0.30422 | val_0_rmse: 0.52778 | val_1_rmse: 0.53653 |  0:02:02s
epoch 84 | loss: 0.30884 | val_0_rmse: 0.53365 | val_1_rmse: 0.54845 |  0:02:03s
epoch 85 | loss: 0.30514 | val_0_rmse: 0.52387 | val_1_rmse: 0.53755 |  0:02:05s
epoch 86 | loss: 0.30301 | val_0_rmse: 0.52982 | val_1_rmse: 0.54293 |  0:02:06s
epoch 87 | loss: 0.30106 | val_0_rmse: 0.54316 | val_1_rmse: 0.5576  |  0:02:07s
epoch 88 | loss: 0.30372 | val_0_rmse: 0.52391 | val_1_rmse: 0.53965 |  0:02:09s
epoch 89 | loss: 0.30412 | val_0_rmse: 0.53958 | val_1_rmse: 0.54922 |  0:02:10s
epoch 90 | loss: 0.2993  | val_0_rmse: 0.51914 | val_1_rmse: 0.53141 |  0:02:12s
epoch 91 | loss: 0.29821 | val_0_rmse: 0.51881 | val_1_rmse: 0.5321  |  0:02:13s
epoch 92 | loss: 0.30316 | val_0_rmse: 0.53972 | val_1_rmse: 0.55291 |  0:02:15s
epoch 93 | loss: 0.29736 | val_0_rmse: 0.51706 | val_1_rmse: 0.52925 |  0:02:16s
epoch 94 | loss: 0.29532 | val_0_rmse: 0.52611 | val_1_rmse: 0.54103 |  0:02:18s
epoch 95 | loss: 0.29618 | val_0_rmse: 0.52747 | val_1_rmse: 0.54097 |  0:02:19s
epoch 96 | loss: 0.29255 | val_0_rmse: 0.51428 | val_1_rmse: 0.52908 |  0:02:20s
epoch 97 | loss: 0.29753 | val_0_rmse: 0.54815 | val_1_rmse: 0.55765 |  0:02:22s
epoch 98 | loss: 0.29992 | val_0_rmse: 0.5191  | val_1_rmse: 0.53943 |  0:02:23s
epoch 99 | loss: 0.29324 | val_0_rmse: 0.52062 | val_1_rmse: 0.53159 |  0:02:25s
epoch 100| loss: 0.29229 | val_0_rmse: 0.52416 | val_1_rmse: 0.53929 |  0:02:26s
epoch 101| loss: 0.29699 | val_0_rmse: 0.52595 | val_1_rmse: 0.53607 |  0:02:28s
epoch 102| loss: 0.29415 | val_0_rmse: 0.51423 | val_1_rmse: 0.52866 |  0:02:29s
epoch 103| loss: 0.28948 | val_0_rmse: 0.5157  | val_1_rmse: 0.53056 |  0:02:31s
epoch 104| loss: 0.29552 | val_0_rmse: 0.53214 | val_1_rmse: 0.54971 |  0:02:32s
epoch 105| loss: 0.2983  | val_0_rmse: 0.52394 | val_1_rmse: 0.53627 |  0:02:33s
epoch 106| loss: 0.29631 | val_0_rmse: 0.51931 | val_1_rmse: 0.53571 |  0:02:35s
epoch 107| loss: 0.30072 | val_0_rmse: 0.52068 | val_1_rmse: 0.53815 |  0:02:36s
epoch 108| loss: 0.29162 | val_0_rmse: 0.51458 | val_1_rmse: 0.53147 |  0:02:38s
epoch 109| loss: 0.29556 | val_0_rmse: 0.52841 | val_1_rmse: 0.54032 |  0:02:39s
epoch 110| loss: 0.29546 | val_0_rmse: 0.58907 | val_1_rmse: 0.59607 |  0:02:41s
epoch 111| loss: 0.30621 | val_0_rmse: 0.51048 | val_1_rmse: 0.52747 |  0:02:42s
epoch 112| loss: 0.29466 | val_0_rmse: 0.53557 | val_1_rmse: 0.55023 |  0:02:44s
epoch 113| loss: 0.29791 | val_0_rmse: 0.54437 | val_1_rmse: 0.55941 |  0:02:45s
epoch 114| loss: 0.29207 | val_0_rmse: 0.52203 | val_1_rmse: 0.53666 |  0:02:46s
epoch 115| loss: 0.29392 | val_0_rmse: 0.51877 | val_1_rmse: 0.53218 |  0:02:48s
epoch 116| loss: 0.29443 | val_0_rmse: 0.53982 | val_1_rmse: 0.55687 |  0:02:49s
epoch 117| loss: 0.29487 | val_0_rmse: 0.52831 | val_1_rmse: 0.5421  |  0:02:51s
epoch 118| loss: 0.29954 | val_0_rmse: 0.53577 | val_1_rmse: 0.54921 |  0:02:52s
epoch 119| loss: 0.30029 | val_0_rmse: 0.5117  | val_1_rmse: 0.53097 |  0:02:54s
epoch 120| loss: 0.28754 | val_0_rmse: 0.51791 | val_1_rmse: 0.5356  |  0:02:55s
epoch 121| loss: 0.29625 | val_0_rmse: 0.52361 | val_1_rmse: 0.53949 |  0:02:57s
epoch 122| loss: 0.28827 | val_0_rmse: 0.52239 | val_1_rmse: 0.54191 |  0:02:58s
epoch 123| loss: 0.28588 | val_0_rmse: 0.52062 | val_1_rmse: 0.53538 |  0:02:59s
epoch 124| loss: 0.29886 | val_0_rmse: 0.55108 | val_1_rmse: 0.56192 |  0:03:01s
epoch 125| loss: 0.30186 | val_0_rmse: 0.52389 | val_1_rmse: 0.5417  |  0:03:02s
epoch 126| loss: 0.29978 | val_0_rmse: 0.52141 | val_1_rmse: 0.53401 |  0:03:04s
epoch 127| loss: 0.29333 | val_0_rmse: 0.51359 | val_1_rmse: 0.53279 |  0:03:05s
epoch 128| loss: 0.29105 | val_0_rmse: 0.5206  | val_1_rmse: 0.53586 |  0:03:07s
epoch 129| loss: 0.30087 | val_0_rmse: 0.53982 | val_1_rmse: 0.55003 |  0:03:08s
epoch 130| loss: 0.31518 | val_0_rmse: 0.52657 | val_1_rmse: 0.54149 |  0:03:10s
epoch 131| loss: 0.302   | val_0_rmse: 0.52233 | val_1_rmse: 0.53939 |  0:03:11s
epoch 132| loss: 0.29741 | val_0_rmse: 0.51626 | val_1_rmse: 0.5318  |  0:03:12s
epoch 133| loss: 0.29647 | val_0_rmse: 0.52815 | val_1_rmse: 0.53987 |  0:03:14s
epoch 134| loss: 0.29983 | val_0_rmse: 0.52135 | val_1_rmse: 0.53282 |  0:03:15s
epoch 135| loss: 0.29196 | val_0_rmse: 0.54515 | val_1_rmse: 0.55927 |  0:03:17s
epoch 136| loss: 0.29589 | val_0_rmse: 0.52253 | val_1_rmse: 0.54554 |  0:03:18s
epoch 137| loss: 0.29106 | val_0_rmse: 0.5102  | val_1_rmse: 0.52773 |  0:03:20s
epoch 138| loss: 0.29208 | val_0_rmse: 0.54226 | val_1_rmse: 0.55946 |  0:03:21s
epoch 139| loss: 0.29007 | val_0_rmse: 0.51489 | val_1_rmse: 0.52994 |  0:03:23s
epoch 140| loss: 0.28883 | val_0_rmse: 0.50994 | val_1_rmse: 0.52859 |  0:03:24s
epoch 141| loss: 0.28562 | val_0_rmse: 0.54941 | val_1_rmse: 0.56679 |  0:03:26s

Early stopping occured at epoch 141 with best_epoch = 111 and best_val_1_rmse = 0.52747
Best weights from best epoch are automatically used!
ended training at: 00:03:57
Feature importance:
[('Area', 0.28214462621078434), ('Baths', 0.059156736347619386), ('Beds', 0.0), ('Latitude', 0.2584307914951809), ('Longitude', 0.20677479976606758), ('Month', 0.013044770089663009), ('Year', 0.1804482760906848)]
Mean squared error is of 5982068089.061389
Mean absolute error:53624.09772955321
MAPE:0.1789937055948901
R2 score:0.7255967802774339
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:03:57
epoch 0  | loss: 0.76561 | val_0_rmse: 0.79954 | val_1_rmse: 0.79904 |  0:00:01s
epoch 1  | loss: 0.54971 | val_0_rmse: 0.69697 | val_1_rmse: 0.69606 |  0:00:03s
epoch 2  | loss: 0.46238 | val_0_rmse: 0.65183 | val_1_rmse: 0.65673 |  0:00:04s
epoch 3  | loss: 0.45113 | val_0_rmse: 0.67197 | val_1_rmse: 0.67283 |  0:00:06s
epoch 4  | loss: 0.43609 | val_0_rmse: 0.64358 | val_1_rmse: 0.64784 |  0:00:07s
epoch 5  | loss: 0.42273 | val_0_rmse: 0.652   | val_1_rmse: 0.65336 |  0:00:09s
epoch 6  | loss: 0.41001 | val_0_rmse: 0.61315 | val_1_rmse: 0.6195  |  0:00:10s
epoch 7  | loss: 0.39913 | val_0_rmse: 0.60465 | val_1_rmse: 0.60875 |  0:00:12s
epoch 8  | loss: 0.38946 | val_0_rmse: 0.6068  | val_1_rmse: 0.61292 |  0:00:13s
epoch 9  | loss: 0.38424 | val_0_rmse: 0.60352 | val_1_rmse: 0.60234 |  0:00:15s
epoch 10 | loss: 0.39044 | val_0_rmse: 0.59529 | val_1_rmse: 0.60339 |  0:00:16s
epoch 11 | loss: 0.36921 | val_0_rmse: 0.60372 | val_1_rmse: 0.61075 |  0:00:18s
epoch 12 | loss: 0.37547 | val_0_rmse: 0.59299 | val_1_rmse: 0.59751 |  0:00:19s
epoch 13 | loss: 0.37109 | val_0_rmse: 0.59781 | val_1_rmse: 0.60646 |  0:00:21s
epoch 14 | loss: 0.36423 | val_0_rmse: 0.6044  | val_1_rmse: 0.61018 |  0:00:22s
epoch 15 | loss: 0.3691  | val_0_rmse: 0.58346 | val_1_rmse: 0.58932 |  0:00:24s
epoch 16 | loss: 0.36254 | val_0_rmse: 0.5838  | val_1_rmse: 0.58471 |  0:00:25s
epoch 17 | loss: 0.35919 | val_0_rmse: 0.57627 | val_1_rmse: 0.58041 |  0:00:27s
epoch 18 | loss: 0.35924 | val_0_rmse: 0.61548 | val_1_rmse: 0.62031 |  0:00:28s
epoch 19 | loss: 0.3599  | val_0_rmse: 0.57864 | val_1_rmse: 0.58391 |  0:00:30s
epoch 20 | loss: 0.35934 | val_0_rmse: 0.58701 | val_1_rmse: 0.59382 |  0:00:32s
epoch 21 | loss: 0.35099 | val_0_rmse: 0.58061 | val_1_rmse: 0.59301 |  0:00:33s
epoch 22 | loss: 0.34565 | val_0_rmse: 0.56731 | val_1_rmse: 0.57835 |  0:00:35s
epoch 23 | loss: 0.34128 | val_0_rmse: 0.55878 | val_1_rmse: 0.56819 |  0:00:36s
epoch 24 | loss: 0.34394 | val_0_rmse: 0.56877 | val_1_rmse: 0.57987 |  0:00:38s
epoch 25 | loss: 0.35304 | val_0_rmse: 0.5711  | val_1_rmse: 0.5841  |  0:00:39s
epoch 26 | loss: 0.35671 | val_0_rmse: 0.5785  | val_1_rmse: 0.58922 |  0:00:41s
epoch 27 | loss: 0.3466  | val_0_rmse: 0.56327 | val_1_rmse: 0.57825 |  0:00:42s
epoch 28 | loss: 0.33979 | val_0_rmse: 0.56405 | val_1_rmse: 0.57263 |  0:00:44s
epoch 29 | loss: 0.33895 | val_0_rmse: 0.56961 | val_1_rmse: 0.57801 |  0:00:45s
epoch 30 | loss: 0.3412  | val_0_rmse: 0.55827 | val_1_rmse: 0.56708 |  0:00:46s
epoch 31 | loss: 0.34103 | val_0_rmse: 0.56723 | val_1_rmse: 0.57946 |  0:00:48s
epoch 32 | loss: 0.34947 | val_0_rmse: 0.58103 | val_1_rmse: 0.59522 |  0:00:49s
epoch 33 | loss: 0.33472 | val_0_rmse: 0.56168 | val_1_rmse: 0.57474 |  0:00:51s
epoch 34 | loss: 0.32951 | val_0_rmse: 0.56225 | val_1_rmse: 0.57795 |  0:00:52s
epoch 35 | loss: 0.32816 | val_0_rmse: 0.55526 | val_1_rmse: 0.56979 |  0:00:54s
epoch 36 | loss: 0.332   | val_0_rmse: 0.5623  | val_1_rmse: 0.57459 |  0:00:55s
epoch 37 | loss: 0.32824 | val_0_rmse: 0.55609 | val_1_rmse: 0.56976 |  0:00:57s
epoch 38 | loss: 0.34221 | val_0_rmse: 0.55601 | val_1_rmse: 0.57211 |  0:00:58s
epoch 39 | loss: 0.32911 | val_0_rmse: 0.55386 | val_1_rmse: 0.56429 |  0:00:59s
epoch 40 | loss: 0.33451 | val_0_rmse: 0.55981 | val_1_rmse: 0.56876 |  0:01:01s
epoch 41 | loss: 0.33659 | val_0_rmse: 0.60197 | val_1_rmse: 0.6124  |  0:01:02s
epoch 42 | loss: 0.34082 | val_0_rmse: 0.54891 | val_1_rmse: 0.56169 |  0:01:04s
epoch 43 | loss: 0.33715 | val_0_rmse: 0.55689 | val_1_rmse: 0.57445 |  0:01:05s
epoch 44 | loss: 0.32748 | val_0_rmse: 0.55879 | val_1_rmse: 0.56958 |  0:01:07s
epoch 45 | loss: 0.33226 | val_0_rmse: 0.55127 | val_1_rmse: 0.56416 |  0:01:08s
epoch 46 | loss: 0.32689 | val_0_rmse: 0.56316 | val_1_rmse: 0.57176 |  0:01:10s
epoch 47 | loss: 0.33549 | val_0_rmse: 0.5638  | val_1_rmse: 0.57095 |  0:01:11s
epoch 48 | loss: 0.3236  | val_0_rmse: 0.57359 | val_1_rmse: 0.58785 |  0:01:12s
epoch 49 | loss: 0.33635 | val_0_rmse: 0.56566 | val_1_rmse: 0.57421 |  0:01:14s
epoch 50 | loss: 0.32636 | val_0_rmse: 0.54784 | val_1_rmse: 0.56148 |  0:01:15s
epoch 51 | loss: 0.32248 | val_0_rmse: 0.53845 | val_1_rmse: 0.55305 |  0:01:17s
epoch 52 | loss: 0.31461 | val_0_rmse: 0.54606 | val_1_rmse: 0.55971 |  0:01:18s
epoch 53 | loss: 0.32096 | val_0_rmse: 0.54518 | val_1_rmse: 0.55604 |  0:01:20s
epoch 54 | loss: 0.32306 | val_0_rmse: 0.54253 | val_1_rmse: 0.55705 |  0:01:21s
epoch 55 | loss: 0.31782 | val_0_rmse: 0.56247 | val_1_rmse: 0.56992 |  0:01:23s
epoch 56 | loss: 0.32051 | val_0_rmse: 0.55258 | val_1_rmse: 0.57016 |  0:01:24s
epoch 57 | loss: 0.31983 | val_0_rmse: 0.53965 | val_1_rmse: 0.55307 |  0:01:25s
epoch 58 | loss: 0.32235 | val_0_rmse: 0.55758 | val_1_rmse: 0.57259 |  0:01:27s
epoch 59 | loss: 0.3248  | val_0_rmse: 0.54954 | val_1_rmse: 0.56045 |  0:01:28s
epoch 60 | loss: 0.31446 | val_0_rmse: 0.52453 | val_1_rmse: 0.54034 |  0:01:30s
epoch 61 | loss: 0.32018 | val_0_rmse: 0.55079 | val_1_rmse: 0.56115 |  0:01:31s
epoch 62 | loss: 0.31648 | val_0_rmse: 0.54558 | val_1_rmse: 0.56361 |  0:01:33s
epoch 63 | loss: 0.32077 | val_0_rmse: 0.54047 | val_1_rmse: 0.55729 |  0:01:34s
epoch 64 | loss: 0.30794 | val_0_rmse: 0.53288 | val_1_rmse: 0.54985 |  0:01:36s
epoch 65 | loss: 0.31302 | val_0_rmse: 0.56986 | val_1_rmse: 0.58478 |  0:01:37s
epoch 66 | loss: 0.31857 | val_0_rmse: 0.55713 | val_1_rmse: 0.57517 |  0:01:38s
epoch 67 | loss: 0.32242 | val_0_rmse: 0.54689 | val_1_rmse: 0.56375 |  0:01:40s
epoch 68 | loss: 0.32204 | val_0_rmse: 0.54771 | val_1_rmse: 0.56532 |  0:01:41s
epoch 69 | loss: 0.31252 | val_0_rmse: 0.56657 | val_1_rmse: 0.57752 |  0:01:43s
epoch 70 | loss: 0.31307 | val_0_rmse: 0.53571 | val_1_rmse: 0.55144 |  0:01:44s
epoch 71 | loss: 0.31387 | val_0_rmse: 0.53506 | val_1_rmse: 0.54557 |  0:01:46s
epoch 72 | loss: 0.31911 | val_0_rmse: 0.54899 | val_1_rmse: 0.56501 |  0:01:47s
epoch 73 | loss: 0.32367 | val_0_rmse: 0.57363 | val_1_rmse: 0.59315 |  0:01:49s
epoch 74 | loss: 0.31763 | val_0_rmse: 0.53752 | val_1_rmse: 0.55235 |  0:01:50s
epoch 75 | loss: 0.31506 | val_0_rmse: 0.5417  | val_1_rmse: 0.55912 |  0:01:51s
epoch 76 | loss: 0.30798 | val_0_rmse: 0.55281 | val_1_rmse: 0.56305 |  0:01:53s
epoch 77 | loss: 0.3058  | val_0_rmse: 0.5269  | val_1_rmse: 0.54658 |  0:01:54s
epoch 78 | loss: 0.31454 | val_0_rmse: 0.53476 | val_1_rmse: 0.54811 |  0:01:56s
epoch 79 | loss: 0.32127 | val_0_rmse: 0.55605 | val_1_rmse: 0.5752  |  0:01:57s
epoch 80 | loss: 0.33462 | val_0_rmse: 0.57075 | val_1_rmse: 0.57932 |  0:01:59s
epoch 81 | loss: 0.33303 | val_0_rmse: 0.54455 | val_1_rmse: 0.56219 |  0:02:00s
epoch 82 | loss: 0.33874 | val_0_rmse: 0.56547 | val_1_rmse: 0.57414 |  0:02:02s
epoch 83 | loss: 0.3268  | val_0_rmse: 0.54416 | val_1_rmse: 0.5602  |  0:02:03s
epoch 84 | loss: 0.31798 | val_0_rmse: 0.54257 | val_1_rmse: 0.55823 |  0:02:05s
epoch 85 | loss: 0.31513 | val_0_rmse: 0.55129 | val_1_rmse: 0.56289 |  0:02:06s
epoch 86 | loss: 0.31665 | val_0_rmse: 0.56497 | val_1_rmse: 0.5732  |  0:02:07s
epoch 87 | loss: 0.32483 | val_0_rmse: 0.54179 | val_1_rmse: 0.55504 |  0:02:09s
epoch 88 | loss: 0.31711 | val_0_rmse: 0.53854 | val_1_rmse: 0.54918 |  0:02:10s
epoch 89 | loss: 0.31651 | val_0_rmse: 0.52788 | val_1_rmse: 0.5411  |  0:02:12s
epoch 90 | loss: 0.30836 | val_0_rmse: 0.53008 | val_1_rmse: 0.5434  |  0:02:13s

Early stopping occured at epoch 90 with best_epoch = 60 and best_val_1_rmse = 0.54034
Best weights from best epoch are automatically used!
ended training at: 00:06:11
Feature importance:
[('Area', 0.31366868899149425), ('Baths', 0.11768497662828058), ('Beds', 0.01328075725994294), ('Latitude', 0.3121440155504044), ('Longitude', 0.15166670330297646), ('Month', 0.0036552590752283753), ('Year', 0.08789959919167299)]
Mean squared error is of 6432230822.988122
Mean absolute error:55973.34367782219
MAPE:0.17932419810345204
R2 score:0.7114179744321689
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:06:11
epoch 0  | loss: 0.77419 | val_0_rmse: 0.75243 | val_1_rmse: 0.75598 |  0:00:01s
epoch 1  | loss: 0.45758 | val_0_rmse: 0.65678 | val_1_rmse: 0.65921 |  0:00:02s
epoch 2  | loss: 0.4194  | val_0_rmse: 0.68355 | val_1_rmse: 0.68821 |  0:00:04s
epoch 3  | loss: 0.4122  | val_0_rmse: 0.60604 | val_1_rmse: 0.61314 |  0:00:05s
epoch 4  | loss: 0.38743 | val_0_rmse: 0.59639 | val_1_rmse: 0.59731 |  0:00:07s
epoch 5  | loss: 0.3701  | val_0_rmse: 0.61147 | val_1_rmse: 0.61053 |  0:00:08s
epoch 6  | loss: 0.36627 | val_0_rmse: 0.60836 | val_1_rmse: 0.62048 |  0:00:10s
epoch 7  | loss: 0.36651 | val_0_rmse: 0.60627 | val_1_rmse: 0.61312 |  0:00:11s
epoch 8  | loss: 0.36122 | val_0_rmse: 0.59761 | val_1_rmse: 0.60943 |  0:00:13s
epoch 9  | loss: 0.3415  | val_0_rmse: 0.59151 | val_1_rmse: 0.59378 |  0:00:14s
epoch 10 | loss: 0.34519 | val_0_rmse: 0.56081 | val_1_rmse: 0.56481 |  0:00:15s
epoch 11 | loss: 0.33267 | val_0_rmse: 0.55129 | val_1_rmse: 0.5532  |  0:00:17s
epoch 12 | loss: 0.33396 | val_0_rmse: 0.55979 | val_1_rmse: 0.55967 |  0:00:18s
epoch 13 | loss: 0.33813 | val_0_rmse: 0.5713  | val_1_rmse: 0.57348 |  0:00:20s
epoch 14 | loss: 0.33798 | val_0_rmse: 0.58274 | val_1_rmse: 0.58511 |  0:00:21s
epoch 15 | loss: 0.34317 | val_0_rmse: 0.56353 | val_1_rmse: 0.57155 |  0:00:23s
epoch 16 | loss: 0.33721 | val_0_rmse: 0.57111 | val_1_rmse: 0.57716 |  0:00:24s
epoch 17 | loss: 0.3365  | val_0_rmse: 0.55239 | val_1_rmse: 0.55747 |  0:00:26s
epoch 18 | loss: 0.32663 | val_0_rmse: 0.54981 | val_1_rmse: 0.55183 |  0:00:27s
epoch 19 | loss: 0.33419 | val_0_rmse: 0.5661  | val_1_rmse: 0.56961 |  0:00:29s
epoch 20 | loss: 0.33598 | val_0_rmse: 0.5712  | val_1_rmse: 0.57295 |  0:00:30s
epoch 21 | loss: 0.33164 | val_0_rmse: 0.56584 | val_1_rmse: 0.56604 |  0:00:32s
epoch 22 | loss: 0.32679 | val_0_rmse: 0.55739 | val_1_rmse: 0.56509 |  0:00:33s
epoch 23 | loss: 0.33262 | val_0_rmse: 0.5532  | val_1_rmse: 0.56429 |  0:00:34s
epoch 24 | loss: 0.32691 | val_0_rmse: 0.56689 | val_1_rmse: 0.57256 |  0:00:36s
epoch 25 | loss: 0.32658 | val_0_rmse: 0.54925 | val_1_rmse: 0.55582 |  0:00:37s
epoch 26 | loss: 0.32934 | val_0_rmse: 0.55325 | val_1_rmse: 0.5582  |  0:00:39s
epoch 27 | loss: 0.32307 | val_0_rmse: 0.54531 | val_1_rmse: 0.5494  |  0:00:40s
epoch 28 | loss: 0.31789 | val_0_rmse: 0.54055 | val_1_rmse: 0.5464  |  0:00:42s
epoch 29 | loss: 0.31975 | val_0_rmse: 0.53868 | val_1_rmse: 0.54235 |  0:00:43s
epoch 30 | loss: 0.31207 | val_0_rmse: 0.55708 | val_1_rmse: 0.56356 |  0:00:45s
epoch 31 | loss: 0.33211 | val_0_rmse: 0.54601 | val_1_rmse: 0.54967 |  0:00:46s
epoch 32 | loss: 0.32675 | val_0_rmse: 0.54287 | val_1_rmse: 0.549   |  0:00:47s
epoch 33 | loss: 0.31414 | val_0_rmse: 0.54084 | val_1_rmse: 0.54679 |  0:00:49s
epoch 34 | loss: 0.31395 | val_0_rmse: 0.54897 | val_1_rmse: 0.55278 |  0:00:50s
epoch 35 | loss: 0.31394 | val_0_rmse: 0.54281 | val_1_rmse: 0.54609 |  0:00:52s
epoch 36 | loss: 0.32403 | val_0_rmse: 0.53742 | val_1_rmse: 0.54241 |  0:00:53s
epoch 37 | loss: 0.31492 | val_0_rmse: 0.53801 | val_1_rmse: 0.54125 |  0:00:55s
epoch 38 | loss: 0.30806 | val_0_rmse: 0.53634 | val_1_rmse: 0.54859 |  0:00:56s
epoch 39 | loss: 0.31911 | val_0_rmse: 0.5418  | val_1_rmse: 0.54969 |  0:00:58s
epoch 40 | loss: 0.30568 | val_0_rmse: 0.53815 | val_1_rmse: 0.54438 |  0:00:59s
epoch 41 | loss: 0.30961 | val_0_rmse: 0.53476 | val_1_rmse: 0.54404 |  0:01:01s
epoch 42 | loss: 0.30868 | val_0_rmse: 0.53929 | val_1_rmse: 0.54872 |  0:01:02s
epoch 43 | loss: 0.30084 | val_0_rmse: 0.52961 | val_1_rmse: 0.53179 |  0:01:03s
epoch 44 | loss: 0.30208 | val_0_rmse: 0.52705 | val_1_rmse: 0.52999 |  0:01:05s
epoch 45 | loss: 0.30892 | val_0_rmse: 0.53075 | val_1_rmse: 0.53733 |  0:01:06s
epoch 46 | loss: 0.30465 | val_0_rmse: 0.532   | val_1_rmse: 0.5374  |  0:01:08s
epoch 47 | loss: 0.30784 | val_0_rmse: 0.54203 | val_1_rmse: 0.54535 |  0:01:09s
epoch 48 | loss: 0.30841 | val_0_rmse: 0.53831 | val_1_rmse: 0.54667 |  0:01:11s
epoch 49 | loss: 0.30992 | val_0_rmse: 0.57725 | val_1_rmse: 0.57743 |  0:01:12s
epoch 50 | loss: 0.31469 | val_0_rmse: 0.54167 | val_1_rmse: 0.54996 |  0:01:14s
epoch 51 | loss: 0.32085 | val_0_rmse: 0.5624  | val_1_rmse: 0.56192 |  0:01:15s
epoch 52 | loss: 0.32129 | val_0_rmse: 0.58095 | val_1_rmse: 0.59055 |  0:01:17s
epoch 53 | loss: 0.32339 | val_0_rmse: 0.53866 | val_1_rmse: 0.54548 |  0:01:18s
epoch 54 | loss: 0.31269 | val_0_rmse: 0.53395 | val_1_rmse: 0.53753 |  0:01:19s
epoch 55 | loss: 0.30545 | val_0_rmse: 0.52467 | val_1_rmse: 0.5301  |  0:01:21s
epoch 56 | loss: 0.31414 | val_0_rmse: 0.53802 | val_1_rmse: 0.54215 |  0:01:22s
epoch 57 | loss: 0.31407 | val_0_rmse: 0.54248 | val_1_rmse: 0.55202 |  0:01:24s
epoch 58 | loss: 0.30565 | val_0_rmse: 0.55226 | val_1_rmse: 0.55855 |  0:01:25s
epoch 59 | loss: 0.31726 | val_0_rmse: 0.5616  | val_1_rmse: 0.56549 |  0:01:27s
epoch 60 | loss: 0.30703 | val_0_rmse: 0.54091 | val_1_rmse: 0.54665 |  0:01:28s
epoch 61 | loss: 0.30951 | val_0_rmse: 0.54467 | val_1_rmse: 0.54888 |  0:01:30s
epoch 62 | loss: 0.31412 | val_0_rmse: 0.53457 | val_1_rmse: 0.54272 |  0:01:31s
epoch 63 | loss: 0.31    | val_0_rmse: 0.56801 | val_1_rmse: 0.56399 |  0:01:33s
epoch 64 | loss: 0.30932 | val_0_rmse: 0.53363 | val_1_rmse: 0.53818 |  0:01:34s
epoch 65 | loss: 0.30531 | val_0_rmse: 0.53897 | val_1_rmse: 0.54041 |  0:01:35s
epoch 66 | loss: 0.30637 | val_0_rmse: 0.53132 | val_1_rmse: 0.53589 |  0:01:37s
epoch 67 | loss: 0.30108 | val_0_rmse: 0.54651 | val_1_rmse: 0.55157 |  0:01:38s
epoch 68 | loss: 0.30977 | val_0_rmse: 0.52598 | val_1_rmse: 0.53221 |  0:01:40s
epoch 69 | loss: 0.30275 | val_0_rmse: 0.53712 | val_1_rmse: 0.53987 |  0:01:41s
epoch 70 | loss: 0.30327 | val_0_rmse: 0.52048 | val_1_rmse: 0.53292 |  0:01:43s
epoch 71 | loss: 0.29708 | val_0_rmse: 0.52829 | val_1_rmse: 0.53231 |  0:01:44s
epoch 72 | loss: 0.30318 | val_0_rmse: 0.53572 | val_1_rmse: 0.53972 |  0:01:46s
epoch 73 | loss: 0.3031  | val_0_rmse: 0.53017 | val_1_rmse: 0.53791 |  0:01:47s
epoch 74 | loss: 0.30301 | val_0_rmse: 0.53364 | val_1_rmse: 0.54243 |  0:01:48s

Early stopping occured at epoch 74 with best_epoch = 44 and best_val_1_rmse = 0.52999
Best weights from best epoch are automatically used!
ended training at: 00:08:01
Feature importance:
[('Area', 0.31627090995052587), ('Baths', 0.11131147311594111), ('Beds', 0.0), ('Latitude', 0.23653357282409776), ('Longitude', 0.27604228009682136), ('Month', 0.0), ('Year', 0.05984176401261393)]
Mean squared error is of 6752519540.815094
Mean absolute error:56416.76106815744
MAPE:0.18437496307904816
R2 score:0.6998649720394914
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:08:01
epoch 0  | loss: 0.61042 | val_0_rmse: 0.72287 | val_1_rmse: 0.72003 |  0:00:05s
epoch 1  | loss: 0.52383 | val_0_rmse: 0.69926 | val_1_rmse: 0.69677 |  0:00:10s
epoch 2  | loss: 0.44117 | val_0_rmse: 0.6302  | val_1_rmse: 0.62585 |  0:00:16s
epoch 3  | loss: 0.39419 | val_0_rmse: 0.60616 | val_1_rmse: 0.6056  |  0:00:21s
epoch 4  | loss: 0.38329 | val_0_rmse: 0.61665 | val_1_rmse: 0.6153  |  0:00:27s
epoch 5  | loss: 0.374   | val_0_rmse: 0.61927 | val_1_rmse: 0.61584 |  0:00:32s
epoch 6  | loss: 0.37289 | val_0_rmse: 0.67867 | val_1_rmse: 0.6742  |  0:00:38s
epoch 7  | loss: 0.36492 | val_0_rmse: 0.6314  | val_1_rmse: 0.63252 |  0:00:43s
epoch 8  | loss: 0.35898 | val_0_rmse: 0.59444 | val_1_rmse: 0.59366 |  0:00:49s
epoch 9  | loss: 0.35762 | val_0_rmse: 0.6298  | val_1_rmse: 0.62518 |  0:00:54s
epoch 10 | loss: 0.35464 | val_0_rmse: 0.63864 | val_1_rmse: 0.63178 |  0:01:00s
epoch 11 | loss: 0.35456 | val_0_rmse: 0.5884  | val_1_rmse: 0.58513 |  0:01:05s
epoch 12 | loss: 0.34992 | val_0_rmse: 0.61259 | val_1_rmse: 0.61282 |  0:01:11s
epoch 13 | loss: 0.34939 | val_0_rmse: 0.79068 | val_1_rmse: 0.79809 |  0:01:16s
epoch 14 | loss: 0.35082 | val_0_rmse: 0.59299 | val_1_rmse: 0.59261 |  0:01:21s
epoch 15 | loss: 0.34634 | val_0_rmse: 0.61958 | val_1_rmse: 0.61261 |  0:01:27s
epoch 16 | loss: 0.34648 | val_0_rmse: 0.67967 | val_1_rmse: 0.67243 |  0:01:32s
epoch 17 | loss: 0.34283 | val_0_rmse: 0.63895 | val_1_rmse: 0.64114 |  0:01:38s
epoch 18 | loss: 0.34378 | val_0_rmse: 0.62694 | val_1_rmse: 0.62152 |  0:01:43s
epoch 19 | loss: 0.341   | val_0_rmse: 0.57118 | val_1_rmse: 0.56971 |  0:01:49s
epoch 20 | loss: 0.34007 | val_0_rmse: 0.73676 | val_1_rmse: 0.73505 |  0:01:54s
epoch 21 | loss: 0.33849 | val_0_rmse: 0.71475 | val_1_rmse: 0.72005 |  0:02:00s
epoch 22 | loss: 0.33893 | val_0_rmse: 0.64478 | val_1_rmse: 0.63987 |  0:02:05s
epoch 23 | loss: 0.33524 | val_0_rmse: 0.59617 | val_1_rmse: 0.59486 |  0:02:11s
epoch 24 | loss: 0.33513 | val_0_rmse: 0.68628 | val_1_rmse: 0.67999 |  0:02:16s
epoch 25 | loss: 0.33869 | val_0_rmse: 0.69414 | val_1_rmse: 0.68252 |  0:02:22s
epoch 26 | loss: 0.33553 | val_0_rmse: 0.61094 | val_1_rmse: 0.60284 |  0:02:27s
epoch 27 | loss: 0.33247 | val_0_rmse: 0.61241 | val_1_rmse: 0.61369 |  0:02:33s
epoch 28 | loss: 0.33256 | val_0_rmse: 0.59215 | val_1_rmse: 0.59119 |  0:02:38s
epoch 29 | loss: 0.33224 | val_0_rmse: 0.64547 | val_1_rmse: 0.64534 |  0:02:43s
epoch 30 | loss: 0.33444 | val_0_rmse: 0.66971 | val_1_rmse: 0.66664 |  0:02:49s
epoch 31 | loss: 0.33265 | val_0_rmse: 0.61921 | val_1_rmse: 0.61988 |  0:02:54s
epoch 32 | loss: 0.33389 | val_0_rmse: 0.65592 | val_1_rmse: 0.65243 |  0:03:00s
epoch 33 | loss: 0.33131 | val_0_rmse: 0.58369 | val_1_rmse: 0.58354 |  0:03:05s
epoch 34 | loss: 0.33272 | val_0_rmse: 0.7044  | val_1_rmse: 0.69425 |  0:03:11s
epoch 35 | loss: 0.33025 | val_0_rmse: 0.5871  | val_1_rmse: 0.58596 |  0:03:16s
epoch 36 | loss: 0.33117 | val_0_rmse: 0.58005 | val_1_rmse: 0.58045 |  0:03:22s
epoch 37 | loss: 0.33238 | val_0_rmse: 0.5747  | val_1_rmse: 0.57035 |  0:03:27s
epoch 38 | loss: 0.32847 | val_0_rmse: 0.61841 | val_1_rmse: 0.61601 |  0:03:33s
epoch 39 | loss: 0.32936 | val_0_rmse: 0.61913 | val_1_rmse: 0.61204 |  0:03:38s
epoch 40 | loss: 0.32741 | val_0_rmse: 0.58887 | val_1_rmse: 0.58732 |  0:03:43s
epoch 41 | loss: 0.3293  | val_0_rmse: 0.6222  | val_1_rmse: 0.62427 |  0:03:49s
epoch 42 | loss: 0.32794 | val_0_rmse: 0.58035 | val_1_rmse: 0.57494 |  0:03:54s
epoch 43 | loss: 0.32889 | val_0_rmse: 0.63066 | val_1_rmse: 0.6259  |  0:04:00s
epoch 44 | loss: 0.32654 | val_0_rmse: 0.55921 | val_1_rmse: 0.55634 |  0:04:05s
epoch 45 | loss: 0.32742 | val_0_rmse: 0.63249 | val_1_rmse: 0.63626 |  0:04:11s
epoch 46 | loss: 0.32539 | val_0_rmse: 0.62925 | val_1_rmse: 0.62873 |  0:04:16s
epoch 47 | loss: 0.3284  | val_0_rmse: 0.65137 | val_1_rmse: 0.64875 |  0:04:22s
epoch 48 | loss: 0.32939 | val_0_rmse: 0.57747 | val_1_rmse: 0.57306 |  0:04:27s
epoch 49 | loss: 0.3274  | val_0_rmse: 0.63861 | val_1_rmse: 0.63517 |  0:04:33s
epoch 50 | loss: 0.3294  | val_0_rmse: 0.68782 | val_1_rmse: 0.68964 |  0:04:38s
epoch 51 | loss: 0.32928 | val_0_rmse: 0.59678 | val_1_rmse: 0.59018 |  0:04:44s
epoch 52 | loss: 0.32675 | val_0_rmse: 0.71774 | val_1_rmse: 0.71259 |  0:04:49s
epoch 53 | loss: 0.32547 | val_0_rmse: 0.60554 | val_1_rmse: 0.60585 |  0:04:55s
epoch 54 | loss: 0.32411 | val_0_rmse: 0.59046 | val_1_rmse: 0.59152 |  0:05:00s
epoch 55 | loss: 0.32574 | val_0_rmse: 0.59736 | val_1_rmse: 0.59256 |  0:05:06s
epoch 56 | loss: 0.32605 | val_0_rmse: 0.57015 | val_1_rmse: 0.56821 |  0:05:11s
epoch 57 | loss: 0.32237 | val_0_rmse: 0.58396 | val_1_rmse: 0.57816 |  0:05:16s
epoch 58 | loss: 0.32685 | val_0_rmse: 0.62363 | val_1_rmse: 0.6181  |  0:05:22s
epoch 59 | loss: 0.325   | val_0_rmse: 0.58565 | val_1_rmse: 0.58388 |  0:05:27s
epoch 60 | loss: 0.32576 | val_0_rmse: 0.64604 | val_1_rmse: 0.64151 |  0:05:33s
epoch 61 | loss: 0.32636 | val_0_rmse: 0.60241 | val_1_rmse: 0.60241 |  0:05:38s
epoch 62 | loss: 0.32116 | val_0_rmse: 0.60959 | val_1_rmse: 0.60249 |  0:05:44s
epoch 63 | loss: 0.32383 | val_0_rmse: 0.59894 | val_1_rmse: 0.60159 |  0:05:49s
epoch 64 | loss: 0.32256 | val_0_rmse: 0.65138 | val_1_rmse: 0.65088 |  0:05:55s
epoch 65 | loss: 0.32033 | val_0_rmse: 0.57708 | val_1_rmse: 0.57363 |  0:06:00s
epoch 66 | loss: 0.32397 | val_0_rmse: 0.63205 | val_1_rmse: 0.63457 |  0:06:05s
epoch 67 | loss: 0.3252  | val_0_rmse: 0.67491 | val_1_rmse: 0.66581 |  0:06:11s
epoch 68 | loss: 0.32283 | val_0_rmse: 0.72208 | val_1_rmse: 0.71355 |  0:06:16s
epoch 69 | loss: 0.32153 | val_0_rmse: 0.5862  | val_1_rmse: 0.58242 |  0:06:22s
epoch 70 | loss: 0.32273 | val_0_rmse: 0.79548 | val_1_rmse: 0.78742 |  0:06:27s
epoch 71 | loss: 0.3232  | val_0_rmse: 0.57404 | val_1_rmse: 0.57267 |  0:06:33s
epoch 72 | loss: 0.32221 | val_0_rmse: 0.77624 | val_1_rmse: 0.77856 |  0:06:38s
epoch 73 | loss: 0.32199 | val_0_rmse: 0.68073 | val_1_rmse: 0.68064 |  0:06:44s
epoch 74 | loss: 0.32225 | val_0_rmse: 0.60555 | val_1_rmse: 0.60779 |  0:06:49s

Early stopping occured at epoch 74 with best_epoch = 44 and best_val_1_rmse = 0.55634
Best weights from best epoch are automatically used!
ended training at: 00:14:52
Feature importance:
[('Area', 0.20923835515154332), ('Baths', 0.2183354885302968), ('Beds', 0.04075377666502059), ('Latitude', 0.2930503219295439), ('Longitude', 0.21375739005921257), ('Month', 0.0), ('Year', 0.024864667664382847)]
Mean squared error is of 2129523267.988867
Mean absolute error:33323.17030602454
MAPE:0.3229168902247088
R2 score:0.6847170600953896
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:14:54
epoch 0  | loss: 0.58699 | val_0_rmse: 0.74092 | val_1_rmse: 0.73939 |  0:00:05s
epoch 1  | loss: 0.50316 | val_0_rmse: 0.68841 | val_1_rmse: 0.69264 |  0:00:10s
epoch 2  | loss: 0.47794 | val_0_rmse: 0.67585 | val_1_rmse: 0.67964 |  0:00:16s
epoch 3  | loss: 0.47042 | val_0_rmse: 0.67985 | val_1_rmse: 0.68309 |  0:00:21s
epoch 4  | loss: 0.46847 | val_0_rmse: 0.68954 | val_1_rmse: 0.69266 |  0:00:27s
epoch 5  | loss: 0.46913 | val_0_rmse: 0.71772 | val_1_rmse: 0.71785 |  0:00:32s
epoch 6  | loss: 0.44495 | val_0_rmse: 0.71    | val_1_rmse: 0.71118 |  0:00:38s
epoch 7  | loss: 0.41261 | val_0_rmse: 0.63647 | val_1_rmse: 0.63916 |  0:00:43s
epoch 8  | loss: 0.39931 | val_0_rmse: 0.61831 | val_1_rmse: 0.62201 |  0:00:49s
epoch 9  | loss: 0.38921 | val_0_rmse: 0.63327 | val_1_rmse: 0.63935 |  0:00:54s
epoch 10 | loss: 0.39086 | val_0_rmse: 0.62689 | val_1_rmse: 0.6316  |  0:01:00s
epoch 11 | loss: 0.3822  | val_0_rmse: 0.61829 | val_1_rmse: 0.62333 |  0:01:05s
epoch 12 | loss: 0.37748 | val_0_rmse: 0.61607 | val_1_rmse: 0.61924 |  0:01:11s
epoch 13 | loss: 0.3767  | val_0_rmse: 0.61283 | val_1_rmse: 0.61874 |  0:01:16s
epoch 14 | loss: 0.37028 | val_0_rmse: 0.63877 | val_1_rmse: 0.64006 |  0:01:22s
epoch 15 | loss: 0.36807 | val_0_rmse: 0.60389 | val_1_rmse: 0.60767 |  0:01:27s
epoch 16 | loss: 0.36678 | val_0_rmse: 0.59462 | val_1_rmse: 0.59999 |  0:01:33s
epoch 17 | loss: 0.36505 | val_0_rmse: 0.61378 | val_1_rmse: 0.61595 |  0:01:38s
epoch 18 | loss: 0.36199 | val_0_rmse: 0.61244 | val_1_rmse: 0.61408 |  0:01:44s
epoch 19 | loss: 0.36304 | val_0_rmse: 0.61182 | val_1_rmse: 0.61477 |  0:01:49s
epoch 20 | loss: 0.36038 | val_0_rmse: 0.6077  | val_1_rmse: 0.61013 |  0:01:54s
epoch 21 | loss: 0.35894 | val_0_rmse: 0.60181 | val_1_rmse: 0.60486 |  0:02:00s
epoch 22 | loss: 0.36173 | val_0_rmse: 0.64819 | val_1_rmse: 0.64826 |  0:02:05s
epoch 23 | loss: 0.35763 | val_0_rmse: 0.61278 | val_1_rmse: 0.62007 |  0:02:11s
epoch 24 | loss: 0.35814 | val_0_rmse: 0.64493 | val_1_rmse: 0.64502 |  0:02:16s
epoch 25 | loss: 0.35628 | val_0_rmse: 0.65468 | val_1_rmse: 0.65544 |  0:02:22s
epoch 26 | loss: 0.35949 | val_0_rmse: 0.63363 | val_1_rmse: 0.63413 |  0:02:27s
epoch 27 | loss: 0.35685 | val_0_rmse: 0.61782 | val_1_rmse: 0.62033 |  0:02:33s
epoch 28 | loss: 0.35272 | val_0_rmse: 0.59081 | val_1_rmse: 0.59657 |  0:02:38s
epoch 29 | loss: 0.3538  | val_0_rmse: 0.57965 | val_1_rmse: 0.58364 |  0:02:44s
epoch 30 | loss: 0.35004 | val_0_rmse: 0.61291 | val_1_rmse: 0.6175  |  0:02:49s
epoch 31 | loss: 0.35073 | val_0_rmse: 0.62152 | val_1_rmse: 0.62546 |  0:02:55s
epoch 32 | loss: 0.34888 | val_0_rmse: 0.62145 | val_1_rmse: 0.62336 |  0:03:00s
epoch 33 | loss: 0.34877 | val_0_rmse: 0.58454 | val_1_rmse: 0.58895 |  0:03:06s
epoch 34 | loss: 0.34686 | val_0_rmse: 0.59881 | val_1_rmse: 0.60038 |  0:03:11s
epoch 35 | loss: 0.34733 | val_0_rmse: 0.58151 | val_1_rmse: 0.58556 |  0:03:16s
epoch 36 | loss: 0.34622 | val_0_rmse: 0.58912 | val_1_rmse: 0.59409 |  0:03:22s
epoch 37 | loss: 0.34708 | val_0_rmse: 0.67119 | val_1_rmse: 0.67091 |  0:03:27s
epoch 38 | loss: 0.34457 | val_0_rmse: 0.61221 | val_1_rmse: 0.61428 |  0:03:33s
epoch 39 | loss: 0.34786 | val_0_rmse: 0.58194 | val_1_rmse: 0.58513 |  0:03:38s
epoch 40 | loss: 0.34637 | val_0_rmse: 0.58428 | val_1_rmse: 0.58796 |  0:03:44s
epoch 41 | loss: 0.34398 | val_0_rmse: 0.59636 | val_1_rmse: 0.5987  |  0:03:49s
epoch 42 | loss: 0.34409 | val_0_rmse: 0.58862 | val_1_rmse: 0.59301 |  0:03:55s
epoch 43 | loss: 0.34092 | val_0_rmse: 0.62587 | val_1_rmse: 0.63165 |  0:04:00s
epoch 44 | loss: 0.34108 | val_0_rmse: 0.58576 | val_1_rmse: 0.58864 |  0:04:06s
epoch 45 | loss: 0.33837 | val_0_rmse: 0.59866 | val_1_rmse: 0.59897 |  0:04:11s
epoch 46 | loss: 0.34009 | val_0_rmse: 0.61761 | val_1_rmse: 0.61801 |  0:04:17s
epoch 47 | loss: 0.33979 | val_0_rmse: 0.64715 | val_1_rmse: 0.64694 |  0:04:22s
epoch 48 | loss: 0.33986 | val_0_rmse: 0.56946 | val_1_rmse: 0.57288 |  0:04:28s
epoch 49 | loss: 0.33809 | val_0_rmse: 0.61858 | val_1_rmse: 0.62266 |  0:04:33s
epoch 50 | loss: 0.33821 | val_0_rmse: 0.69693 | val_1_rmse: 0.69619 |  0:04:39s
epoch 51 | loss: 0.34004 | val_0_rmse: 0.57581 | val_1_rmse: 0.58138 |  0:04:44s
epoch 52 | loss: 0.35489 | val_0_rmse: 0.62003 | val_1_rmse: 0.62156 |  0:04:50s
epoch 53 | loss: 0.34601 | val_0_rmse: 0.74899 | val_1_rmse: 0.74873 |  0:04:55s
epoch 54 | loss: 0.34407 | val_0_rmse: 0.5989  | val_1_rmse: 0.60614 |  0:05:01s
epoch 55 | loss: 0.34245 | val_0_rmse: 0.6448  | val_1_rmse: 0.6499  |  0:05:06s
epoch 56 | loss: 0.3406  | val_0_rmse: 0.62627 | val_1_rmse: 0.63373 |  0:05:12s
epoch 57 | loss: 0.34466 | val_0_rmse: 0.65846 | val_1_rmse: 0.66555 |  0:05:17s
epoch 58 | loss: 0.34211 | val_0_rmse: 0.59922 | val_1_rmse: 0.60589 |  0:05:23s
epoch 59 | loss: 0.33787 | val_0_rmse: 0.57855 | val_1_rmse: 0.58415 |  0:05:28s
epoch 60 | loss: 0.33938 | val_0_rmse: 0.5849  | val_1_rmse: 0.58844 |  0:05:34s
epoch 61 | loss: 0.3409  | val_0_rmse: 0.64205 | val_1_rmse: 0.64287 |  0:05:39s
epoch 62 | loss: 0.34009 | val_0_rmse: 0.6547  | val_1_rmse: 0.65402 |  0:05:44s
epoch 63 | loss: 0.33768 | val_0_rmse: 0.6376  | val_1_rmse: 0.63973 |  0:05:50s
epoch 64 | loss: 0.33728 | val_0_rmse: 0.581   | val_1_rmse: 0.58483 |  0:05:55s
epoch 65 | loss: 0.33774 | val_0_rmse: 0.58794 | val_1_rmse: 0.59081 |  0:06:01s
epoch 66 | loss: 0.3369  | val_0_rmse: 0.61113 | val_1_rmse: 0.61211 |  0:06:06s
epoch 67 | loss: 0.33606 | val_0_rmse: 0.61521 | val_1_rmse: 0.62374 |  0:06:12s
epoch 68 | loss: 0.33616 | val_0_rmse: 0.64747 | val_1_rmse: 0.64841 |  0:06:17s
epoch 69 | loss: 0.33607 | val_0_rmse: 0.59055 | val_1_rmse: 0.59455 |  0:06:23s
epoch 70 | loss: 0.33314 | val_0_rmse: 0.60652 | val_1_rmse: 0.61097 |  0:06:28s
epoch 71 | loss: 0.33317 | val_0_rmse: 0.57878 | val_1_rmse: 0.58186 |  0:06:34s
epoch 72 | loss: 0.3322  | val_0_rmse: 0.59444 | val_1_rmse: 0.59712 |  0:06:39s
epoch 73 | loss: 0.33221 | val_0_rmse: 0.58438 | val_1_rmse: 0.58775 |  0:06:45s
epoch 74 | loss: 0.32988 | val_0_rmse: 0.5984  | val_1_rmse: 0.60032 |  0:06:50s
epoch 75 | loss: 0.33082 | val_0_rmse: 0.57641 | val_1_rmse: 0.57924 |  0:06:56s
epoch 76 | loss: 0.32962 | val_0_rmse: 0.59064 | val_1_rmse: 0.5934  |  0:07:01s
epoch 77 | loss: 0.33172 | val_0_rmse: 0.57295 | val_1_rmse: 0.57805 |  0:07:07s
epoch 78 | loss: 0.32939 | val_0_rmse: 0.58391 | val_1_rmse: 0.58669 |  0:07:12s

Early stopping occured at epoch 78 with best_epoch = 48 and best_val_1_rmse = 0.57288
Best weights from best epoch are automatically used!
ended training at: 00:22:08
Feature importance:
[('Area', 0.5081111697628746), ('Baths', 0.31775395001975226), ('Beds', 0.0), ('Latitude', 0.03003604176147754), ('Longitude', 0.14403106233390378), ('Month', 0.0), ('Year', 6.777612199175532e-05)]
Mean squared error is of 2221861792.2715936
Mean absolute error:34439.92875960576
MAPE:0.34552970404506556
R2 score:0.6760145700505289
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:22:09
epoch 0  | loss: 0.6135  | val_0_rmse: 0.71316 | val_1_rmse: 0.71145 |  0:00:05s
epoch 1  | loss: 0.46551 | val_0_rmse: 0.68583 | val_1_rmse: 0.68798 |  0:00:11s
epoch 2  | loss: 0.42386 | val_0_rmse: 0.64639 | val_1_rmse: 0.64475 |  0:00:16s
epoch 3  | loss: 0.40759 | val_0_rmse: 0.68797 | val_1_rmse: 0.68579 |  0:00:21s
epoch 4  | loss: 0.38422 | val_0_rmse: 0.65055 | val_1_rmse: 0.65097 |  0:00:27s
epoch 5  | loss: 0.37909 | val_0_rmse: 0.59973 | val_1_rmse: 0.60041 |  0:00:32s
epoch 6  | loss: 0.37345 | val_0_rmse: 0.6218  | val_1_rmse: 0.6215  |  0:00:38s
epoch 7  | loss: 0.36939 | val_0_rmse: 0.59438 | val_1_rmse: 0.59418 |  0:00:43s
epoch 8  | loss: 0.36735 | val_0_rmse: 0.60295 | val_1_rmse: 0.60269 |  0:00:49s
epoch 9  | loss: 0.36478 | val_0_rmse: 0.60411 | val_1_rmse: 0.60292 |  0:00:54s
epoch 10 | loss: 0.36577 | val_0_rmse: 0.65399 | val_1_rmse: 0.65452 |  0:01:00s
epoch 11 | loss: 0.3634  | val_0_rmse: 0.63443 | val_1_rmse: 0.63538 |  0:01:05s
epoch 12 | loss: 0.35742 | val_0_rmse: 0.61973 | val_1_rmse: 0.61651 |  0:01:11s
epoch 13 | loss: 0.35555 | val_0_rmse: 0.71115 | val_1_rmse: 0.71042 |  0:01:16s
epoch 14 | loss: 0.35672 | val_0_rmse: 0.72031 | val_1_rmse: 0.72323 |  0:01:21s
epoch 15 | loss: 0.35202 | val_0_rmse: 0.60143 | val_1_rmse: 0.60079 |  0:01:27s
epoch 16 | loss: 0.35228 | val_0_rmse: 0.68814 | val_1_rmse: 0.6874  |  0:01:32s
epoch 17 | loss: 0.34771 | val_0_rmse: 0.66274 | val_1_rmse: 0.66601 |  0:01:38s
epoch 18 | loss: 0.34722 | val_0_rmse: 0.61994 | val_1_rmse: 0.61708 |  0:01:43s
epoch 19 | loss: 0.34449 | val_0_rmse: 0.58541 | val_1_rmse: 0.58285 |  0:01:49s
epoch 20 | loss: 0.34215 | val_0_rmse: 0.66558 | val_1_rmse: 0.66513 |  0:01:54s
epoch 21 | loss: 0.34251 | val_0_rmse: 0.73031 | val_1_rmse: 0.7287  |  0:02:00s
epoch 22 | loss: 0.34511 | val_0_rmse: 0.65873 | val_1_rmse: 0.65674 |  0:02:05s
epoch 23 | loss: 0.34476 | val_0_rmse: 0.61256 | val_1_rmse: 0.61465 |  0:02:10s
epoch 24 | loss: 0.34231 | val_0_rmse: 0.65127 | val_1_rmse: 0.65038 |  0:02:16s
epoch 25 | loss: 0.34091 | val_0_rmse: 0.70679 | val_1_rmse: 0.70945 |  0:02:21s
epoch 26 | loss: 0.33818 | val_0_rmse: 0.64354 | val_1_rmse: 0.64261 |  0:02:27s
epoch 27 | loss: 0.34155 | val_0_rmse: 0.70572 | val_1_rmse: 0.70554 |  0:02:32s
epoch 28 | loss: 0.34013 | val_0_rmse: 0.6912  | val_1_rmse: 0.69097 |  0:02:38s
epoch 29 | loss: 0.34096 | val_0_rmse: 0.67955 | val_1_rmse: 0.68018 |  0:02:43s
epoch 30 | loss: 0.34008 | val_0_rmse: 0.63336 | val_1_rmse: 0.63602 |  0:02:49s
epoch 31 | loss: 0.33786 | val_0_rmse: 0.64428 | val_1_rmse: 0.64737 |  0:02:54s
epoch 32 | loss: 0.33229 | val_0_rmse: 0.78826 | val_1_rmse: 0.787   |  0:03:00s
epoch 33 | loss: 0.33524 | val_0_rmse: 0.75544 | val_1_rmse: 0.75803 |  0:03:05s
epoch 34 | loss: 0.33457 | val_0_rmse: 0.63819 | val_1_rmse: 0.64071 |  0:03:11s
epoch 35 | loss: 0.33056 | val_0_rmse: 0.77728 | val_1_rmse: 0.77612 |  0:03:16s
epoch 36 | loss: 0.33353 | val_0_rmse: 0.57719 | val_1_rmse: 0.57703 |  0:03:22s
epoch 37 | loss: 0.33159 | val_0_rmse: 0.60168 | val_1_rmse: 0.60594 |  0:03:27s
epoch 38 | loss: 0.33085 | val_0_rmse: 0.59315 | val_1_rmse: 0.59625 |  0:03:32s
epoch 39 | loss: 0.32887 | val_0_rmse: 0.65041 | val_1_rmse: 0.64909 |  0:03:38s
epoch 40 | loss: 0.3293  | val_0_rmse: 0.57145 | val_1_rmse: 0.57208 |  0:03:43s
epoch 41 | loss: 0.32719 | val_0_rmse: 0.67975 | val_1_rmse: 0.67946 |  0:03:49s
epoch 42 | loss: 0.33143 | val_0_rmse: 0.67966 | val_1_rmse: 0.67801 |  0:03:54s
epoch 43 | loss: 0.32838 | val_0_rmse: 0.6223  | val_1_rmse: 0.62526 |  0:04:00s
epoch 44 | loss: 0.32828 | val_0_rmse: 0.67264 | val_1_rmse: 0.671   |  0:04:05s
epoch 45 | loss: 0.32883 | val_0_rmse: 0.66881 | val_1_rmse: 0.66891 |  0:04:11s
epoch 46 | loss: 0.33195 | val_0_rmse: 0.69837 | val_1_rmse: 0.70211 |  0:04:16s
epoch 47 | loss: 0.33127 | val_0_rmse: 0.59921 | val_1_rmse: 0.59776 |  0:04:21s
epoch 48 | loss: 0.329   | val_0_rmse: 0.63634 | val_1_rmse: 0.63885 |  0:04:27s
epoch 49 | loss: 0.33289 | val_0_rmse: 0.67022 | val_1_rmse: 0.67109 |  0:04:32s
epoch 50 | loss: 0.33601 | val_0_rmse: 0.69633 | val_1_rmse: 0.69442 |  0:04:38s
epoch 51 | loss: 0.33236 | val_0_rmse: 0.8605  | val_1_rmse: 0.85926 |  0:04:43s
epoch 52 | loss: 0.32903 | val_0_rmse: 0.6283  | val_1_rmse: 0.62991 |  0:04:49s
epoch 53 | loss: 0.32713 | val_0_rmse: 0.58941 | val_1_rmse: 0.58839 |  0:04:54s
epoch 54 | loss: 0.33    | val_0_rmse: 0.82376 | val_1_rmse: 0.82292 |  0:05:00s
epoch 55 | loss: 0.33266 | val_0_rmse: 0.59375 | val_1_rmse: 0.59502 |  0:05:05s
epoch 56 | loss: 0.33213 | val_0_rmse: 0.76727 | val_1_rmse: 0.768   |  0:05:11s
epoch 57 | loss: 0.33315 | val_0_rmse: 0.6268  | val_1_rmse: 0.62868 |  0:05:16s
epoch 58 | loss: 0.33026 | val_0_rmse: 0.59599 | val_1_rmse: 0.59381 |  0:05:22s
epoch 59 | loss: 0.32499 | val_0_rmse: 0.58781 | val_1_rmse: 0.58521 |  0:05:27s
epoch 60 | loss: 0.32401 | val_0_rmse: 0.74757 | val_1_rmse: 0.74834 |  0:05:32s
epoch 61 | loss: 0.32578 | val_0_rmse: 0.66774 | val_1_rmse: 0.67318 |  0:05:38s
epoch 62 | loss: 0.33151 | val_0_rmse: 0.60033 | val_1_rmse: 0.59953 |  0:05:43s
epoch 63 | loss: 0.32159 | val_0_rmse: 0.68891 | val_1_rmse: 0.68819 |  0:05:49s
epoch 64 | loss: 0.32235 | val_0_rmse: 0.64656 | val_1_rmse: 0.64875 |  0:05:54s
epoch 65 | loss: 0.3232  | val_0_rmse: 0.6131  | val_1_rmse: 0.61875 |  0:06:00s
epoch 66 | loss: 0.32115 | val_0_rmse: 0.60963 | val_1_rmse: 0.6087  |  0:06:05s
epoch 67 | loss: 0.32286 | val_0_rmse: 0.72658 | val_1_rmse: 0.72586 |  0:06:11s
epoch 68 | loss: 0.32186 | val_0_rmse: 0.58077 | val_1_rmse: 0.58244 |  0:06:16s
epoch 69 | loss: 0.32147 | val_0_rmse: 0.6927  | val_1_rmse: 0.69342 |  0:06:22s
epoch 70 | loss: 0.32058 | val_0_rmse: 0.66308 | val_1_rmse: 0.66503 |  0:06:27s

Early stopping occured at epoch 70 with best_epoch = 40 and best_val_1_rmse = 0.57208
Best weights from best epoch are automatically used!
ended training at: 00:28:38
Feature importance:
[('Area', 0.22145049234557057), ('Baths', 0.1962279630203691), ('Beds', 0.037424723164158506), ('Latitude', 0.19846683633660978), ('Longitude', 0.29305741309888395), ('Month', 0.0), ('Year', 0.05337257203440806)]
Mean squared error is of 2217135689.089158
Mean absolute error:33685.328280316106
MAPE:0.3156058891888503
R2 score:0.6732251306043786
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:28:39
epoch 0  | loss: 0.62565 | val_0_rmse: 0.74691 | val_1_rmse: 0.75006 |  0:00:05s
epoch 1  | loss: 0.50477 | val_0_rmse: 0.69016 | val_1_rmse: 0.69138 |  0:00:11s
epoch 2  | loss: 0.48892 | val_0_rmse: 0.69241 | val_1_rmse: 0.69164 |  0:00:16s
epoch 3  | loss: 0.47266 | val_0_rmse: 0.67561 | val_1_rmse: 0.67514 |  0:00:22s
epoch 4  | loss: 0.44827 | val_0_rmse: 0.67203 | val_1_rmse: 0.66699 |  0:00:27s
epoch 5  | loss: 0.44224 | val_0_rmse: 0.72599 | val_1_rmse: 0.72089 |  0:00:33s
epoch 6  | loss: 0.43452 | val_0_rmse: 0.63405 | val_1_rmse: 0.63153 |  0:00:38s
epoch 7  | loss: 0.41886 | val_0_rmse: 0.68551 | val_1_rmse: 0.68335 |  0:00:43s
epoch 8  | loss: 0.4062  | val_0_rmse: 0.67781 | val_1_rmse: 0.67687 |  0:00:49s
epoch 9  | loss: 0.40605 | val_0_rmse: 0.62523 | val_1_rmse: 0.62462 |  0:00:54s
epoch 10 | loss: 0.39296 | val_0_rmse: 0.63177 | val_1_rmse: 0.62886 |  0:01:00s
epoch 11 | loss: 0.39173 | val_0_rmse: 0.62829 | val_1_rmse: 0.62762 |  0:01:05s
epoch 12 | loss: 0.3901  | val_0_rmse: 0.61887 | val_1_rmse: 0.61605 |  0:01:11s
epoch 13 | loss: 0.3864  | val_0_rmse: 0.6385  | val_1_rmse: 0.63601 |  0:01:16s
epoch 14 | loss: 0.38439 | val_0_rmse: 0.61243 | val_1_rmse: 0.61058 |  0:01:22s
epoch 15 | loss: 0.3819  | val_0_rmse: 0.65247 | val_1_rmse: 0.65016 |  0:01:27s
epoch 16 | loss: 0.38274 | val_0_rmse: 0.60823 | val_1_rmse: 0.60575 |  0:01:33s
epoch 17 | loss: 0.37805 | val_0_rmse: 0.63773 | val_1_rmse: 0.63912 |  0:01:38s
epoch 18 | loss: 0.37685 | val_0_rmse: 0.62942 | val_1_rmse: 0.6256  |  0:01:44s
epoch 19 | loss: 0.37138 | val_0_rmse: 0.60562 | val_1_rmse: 0.60316 |  0:01:49s
epoch 20 | loss: 0.36804 | val_0_rmse: 0.69545 | val_1_rmse: 0.69626 |  0:01:55s
epoch 21 | loss: 0.36356 | val_0_rmse: 0.60019 | val_1_rmse: 0.59899 |  0:02:00s
epoch 22 | loss: 0.36893 | val_0_rmse: 0.65254 | val_1_rmse: 0.6474  |  0:02:05s
epoch 23 | loss: 0.35779 | val_0_rmse: 0.58639 | val_1_rmse: 0.5848  |  0:02:11s
epoch 24 | loss: 0.35589 | val_0_rmse: 0.6243  | val_1_rmse: 0.62081 |  0:02:16s
epoch 25 | loss: 0.35524 | val_0_rmse: 0.72598 | val_1_rmse: 0.72411 |  0:02:22s
epoch 26 | loss: 0.35471 | val_0_rmse: 0.58259 | val_1_rmse: 0.57892 |  0:02:27s
epoch 27 | loss: 0.3528  | val_0_rmse: 0.60666 | val_1_rmse: 0.60354 |  0:02:33s
epoch 28 | loss: 0.35207 | val_0_rmse: 0.60201 | val_1_rmse: 0.60073 |  0:02:38s
epoch 29 | loss: 0.3524  | val_0_rmse: 0.59004 | val_1_rmse: 0.58815 |  0:02:44s
epoch 30 | loss: 0.34676 | val_0_rmse: 0.57705 | val_1_rmse: 0.57384 |  0:02:49s
epoch 31 | loss: 0.34507 | val_0_rmse: 0.61349 | val_1_rmse: 0.60947 |  0:02:55s
epoch 32 | loss: 0.34568 | val_0_rmse: 0.75899 | val_1_rmse: 0.76043 |  0:03:00s
epoch 33 | loss: 0.34917 | val_0_rmse: 0.77378 | val_1_rmse: 0.77125 |  0:03:06s
epoch 34 | loss: 0.36739 | val_0_rmse: 0.65421 | val_1_rmse: 0.65111 |  0:03:11s
epoch 35 | loss: 0.35602 | val_0_rmse: 0.98411 | val_1_rmse: 0.98216 |  0:03:17s
epoch 36 | loss: 0.38021 | val_0_rmse: 0.60351 | val_1_rmse: 0.60135 |  0:03:22s
epoch 37 | loss: 0.35472 | val_0_rmse: 0.57977 | val_1_rmse: 0.57527 |  0:03:28s
epoch 38 | loss: 0.35003 | val_0_rmse: 0.58851 | val_1_rmse: 0.58489 |  0:03:33s
epoch 39 | loss: 0.34529 | val_0_rmse: 0.57713 | val_1_rmse: 0.57523 |  0:03:39s
epoch 40 | loss: 0.34668 | val_0_rmse: 0.61798 | val_1_rmse: 0.61348 |  0:03:44s
epoch 41 | loss: 0.34288 | val_0_rmse: 0.67594 | val_1_rmse: 0.67153 |  0:03:49s
epoch 42 | loss: 0.33931 | val_0_rmse: 0.61731 | val_1_rmse: 0.61682 |  0:03:55s
epoch 43 | loss: 0.33528 | val_0_rmse: 0.71381 | val_1_rmse: 0.71523 |  0:04:00s
epoch 44 | loss: 0.34013 | val_0_rmse: 0.58877 | val_1_rmse: 0.58626 |  0:04:06s
epoch 45 | loss: 0.33927 | val_0_rmse: 0.63581 | val_1_rmse: 0.6323  |  0:04:11s
epoch 46 | loss: 0.33773 | val_0_rmse: 0.57399 | val_1_rmse: 0.57007 |  0:04:17s
epoch 47 | loss: 0.33648 | val_0_rmse: 0.64401 | val_1_rmse: 0.64316 |  0:04:22s
epoch 48 | loss: 0.33491 | val_0_rmse: 0.57801 | val_1_rmse: 0.57715 |  0:04:28s
epoch 49 | loss: 0.33488 | val_0_rmse: 0.6106  | val_1_rmse: 0.61099 |  0:04:33s
epoch 50 | loss: 0.33844 | val_0_rmse: 0.6417  | val_1_rmse: 0.63744 |  0:04:39s
epoch 51 | loss: 0.3327  | val_0_rmse: 0.58086 | val_1_rmse: 0.57627 |  0:04:44s
epoch 52 | loss: 0.3313  | val_0_rmse: 0.72821 | val_1_rmse: 0.73063 |  0:04:49s
epoch 53 | loss: 0.33335 | val_0_rmse: 0.56877 | val_1_rmse: 0.56635 |  0:04:55s
epoch 54 | loss: 0.33081 | val_0_rmse: 0.60758 | val_1_rmse: 0.60588 |  0:05:00s
epoch 55 | loss: 0.33059 | val_0_rmse: 0.57842 | val_1_rmse: 0.57395 |  0:05:06s
epoch 56 | loss: 0.32826 | val_0_rmse: 0.65295 | val_1_rmse: 0.64939 |  0:05:11s
epoch 57 | loss: 0.33076 | val_0_rmse: 0.743   | val_1_rmse: 0.73749 |  0:05:17s
epoch 58 | loss: 0.33083 | val_0_rmse: 0.75071 | val_1_rmse: 0.75594 |  0:05:22s
epoch 59 | loss: 0.33018 | val_0_rmse: 0.57874 | val_1_rmse: 0.577   |  0:05:28s
epoch 60 | loss: 0.3285  | val_0_rmse: 0.56802 | val_1_rmse: 0.56606 |  0:05:33s
epoch 61 | loss: 0.33303 | val_0_rmse: 0.78975 | val_1_rmse: 0.78654 |  0:05:39s
epoch 62 | loss: 0.33389 | val_0_rmse: 0.8422  | val_1_rmse: 0.8466  |  0:05:44s
epoch 63 | loss: 0.335   | val_0_rmse: 0.64613 | val_1_rmse: 0.64447 |  0:05:50s
epoch 64 | loss: 0.34142 | val_0_rmse: 0.60761 | val_1_rmse: 0.60448 |  0:05:55s
epoch 65 | loss: 0.33024 | val_0_rmse: 0.63245 | val_1_rmse: 0.63142 |  0:06:01s
epoch 66 | loss: 0.32943 | val_0_rmse: 0.56684 | val_1_rmse: 0.56417 |  0:06:06s
epoch 67 | loss: 0.32949 | val_0_rmse: 0.69592 | val_1_rmse: 0.69202 |  0:06:12s
epoch 68 | loss: 0.33058 | val_0_rmse: 0.77503 | val_1_rmse: 0.77494 |  0:06:17s
epoch 69 | loss: 0.32852 | val_0_rmse: 0.58934 | val_1_rmse: 0.58756 |  0:06:23s
epoch 70 | loss: 0.32841 | val_0_rmse: 0.59842 | val_1_rmse: 0.5956  |  0:06:28s
epoch 71 | loss: 0.32724 | val_0_rmse: 0.65202 | val_1_rmse: 0.65126 |  0:06:34s
epoch 72 | loss: 0.32433 | val_0_rmse: 0.56659 | val_1_rmse: 0.56367 |  0:06:39s
epoch 73 | loss: 0.32858 | val_0_rmse: 0.61738 | val_1_rmse: 0.61718 |  0:06:45s
epoch 74 | loss: 0.32627 | val_0_rmse: 0.62524 | val_1_rmse: 0.62048 |  0:06:50s
epoch 75 | loss: 0.33003 | val_0_rmse: 0.62541 | val_1_rmse: 0.62503 |  0:06:55s
epoch 76 | loss: 0.32533 | val_0_rmse: 0.57585 | val_1_rmse: 0.57314 |  0:07:01s
epoch 77 | loss: 0.32509 | val_0_rmse: 0.59205 | val_1_rmse: 0.5925  |  0:07:06s
epoch 78 | loss: 0.32501 | val_0_rmse: 0.72642 | val_1_rmse: 0.67136 |  0:07:12s
epoch 79 | loss: 0.32659 | val_0_rmse: 0.60825 | val_1_rmse: 0.60484 |  0:07:17s
epoch 80 | loss: 0.32625 | val_0_rmse: 0.58036 | val_1_rmse: 0.58048 |  0:07:23s
epoch 81 | loss: 0.32651 | val_0_rmse: 0.56082 | val_1_rmse: 0.55772 |  0:07:28s
epoch 82 | loss: 0.32536 | val_0_rmse: 0.626   | val_1_rmse: 0.62641 |  0:07:34s
epoch 83 | loss: 0.32748 | val_0_rmse: 0.64038 | val_1_rmse: 0.64127 |  0:07:39s
epoch 84 | loss: 0.32399 | val_0_rmse: 0.56566 | val_1_rmse: 0.56182 |  0:07:45s
epoch 85 | loss: 0.32279 | val_0_rmse: 0.57023 | val_1_rmse: 0.56638 |  0:07:50s
epoch 86 | loss: 0.32132 | val_0_rmse: 0.64641 | val_1_rmse: 0.64302 |  0:07:55s
epoch 87 | loss: 0.32596 | val_0_rmse: 0.67007 | val_1_rmse: 0.6715  |  0:08:01s
epoch 88 | loss: 0.32275 | val_0_rmse: 0.62139 | val_1_rmse: 0.61866 |  0:08:06s
epoch 89 | loss: 0.32957 | val_0_rmse: 0.7092  | val_1_rmse: 0.70652 |  0:08:12s
epoch 90 | loss: 0.35805 | val_0_rmse: 0.61979 | val_1_rmse: 0.61892 |  0:08:17s
epoch 91 | loss: 0.33655 | val_0_rmse: 0.57476 | val_1_rmse: 0.57009 |  0:08:23s
epoch 92 | loss: 0.33503 | val_0_rmse: 0.65297 | val_1_rmse: 0.64852 |  0:08:28s
epoch 93 | loss: 0.34971 | val_0_rmse: 0.71443 | val_1_rmse: 0.71735 |  0:08:34s
epoch 94 | loss: 0.33933 | val_0_rmse: 0.64512 | val_1_rmse: 0.63917 |  0:08:39s
epoch 95 | loss: 0.33815 | val_0_rmse: 0.60364 | val_1_rmse: 0.6013  |  0:08:45s
epoch 96 | loss: 0.33631 | val_0_rmse: 0.56379 | val_1_rmse: 0.56082 |  0:08:50s
epoch 97 | loss: 0.33708 | val_0_rmse: 0.5882  | val_1_rmse: 0.58357 |  0:08:56s
epoch 98 | loss: 0.34373 | val_0_rmse: 0.98396 | val_1_rmse: 0.98722 |  0:09:01s
epoch 99 | loss: 0.36541 | val_0_rmse: 0.63066 | val_1_rmse: 0.63004 |  0:09:06s
epoch 100| loss: 0.35264 | val_0_rmse: 0.59617 | val_1_rmse: 0.59375 |  0:09:12s
epoch 101| loss: 0.35112 | val_0_rmse: 0.69437 | val_1_rmse: 0.69101 |  0:09:17s
epoch 102| loss: 0.34861 | val_0_rmse: 0.63992 | val_1_rmse: 0.63752 |  0:09:23s
epoch 103| loss: 0.35126 | val_0_rmse: 0.64061 | val_1_rmse: 0.64027 |  0:09:28s
epoch 104| loss: 0.3481  | val_0_rmse: 0.58691 | val_1_rmse: 0.5844  |  0:09:34s
epoch 105| loss: 0.3519  | val_0_rmse: 0.64269 | val_1_rmse: 0.63686 |  0:09:39s
epoch 106| loss: 0.34691 | val_0_rmse: 0.58912 | val_1_rmse: 0.58636 |  0:09:45s
epoch 107| loss: 0.34172 | val_0_rmse: 0.60712 | val_1_rmse: 0.60689 |  0:09:50s
epoch 108| loss: 0.33957 | val_0_rmse: 0.63426 | val_1_rmse: 0.62932 |  0:09:56s
epoch 109| loss: 0.34145 | val_0_rmse: 0.75829 | val_1_rmse: 0.75909 |  0:10:01s
epoch 110| loss: 0.35428 | val_0_rmse: 0.61298 | val_1_rmse: 0.61001 |  0:10:07s
epoch 111| loss: 0.34659 | val_0_rmse: 0.6688  | val_1_rmse: 0.66452 |  0:10:12s

Early stopping occured at epoch 111 with best_epoch = 81 and best_val_1_rmse = 0.55772
Best weights from best epoch are automatically used!
ended training at: 00:38:53
Feature importance:
[('Area', 0.3216077631000918), ('Baths', 0.273968581287757), ('Beds', 0.0), ('Latitude', 0.21111657714792234), ('Longitude', 0.11603523062420104), ('Month', 0.0), ('Year', 0.0772718478400278)]
Mean squared error is of 2160342306.4709067
Mean absolute error:33655.03253308222
MAPE:0.3240626610220527
R2 score:0.6811859426759694
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:38:53
epoch 0  | loss: 0.59826 | val_0_rmse: 0.71788 | val_1_rmse: 0.72351 |  0:00:05s
epoch 1  | loss: 0.50353 | val_0_rmse: 0.68192 | val_1_rmse: 0.68785 |  0:00:11s
epoch 2  | loss: 0.45593 | val_0_rmse: 0.64714 | val_1_rmse: 0.64968 |  0:00:16s
epoch 3  | loss: 0.42563 | val_0_rmse: 0.77309 | val_1_rmse: 0.78075 |  0:00:22s
epoch 4  | loss: 0.4123  | val_0_rmse: 0.67064 | val_1_rmse: 0.67494 |  0:00:27s
epoch 5  | loss: 0.40616 | val_0_rmse: 0.67946 | val_1_rmse: 0.67556 |  0:00:32s
epoch 6  | loss: 0.40731 | val_0_rmse: 0.62416 | val_1_rmse: 0.62518 |  0:00:38s
epoch 7  | loss: 0.40698 | val_0_rmse: 0.65906 | val_1_rmse: 0.66279 |  0:00:43s
epoch 8  | loss: 0.40187 | val_0_rmse: 0.64394 | val_1_rmse: 0.64239 |  0:00:49s
epoch 9  | loss: 0.40403 | val_0_rmse: 0.64349 | val_1_rmse: 0.64113 |  0:00:54s
epoch 10 | loss: 0.39638 | val_0_rmse: 0.63872 | val_1_rmse: 0.63491 |  0:01:00s
epoch 11 | loss: 0.39238 | val_0_rmse: 0.61269 | val_1_rmse: 0.61325 |  0:01:05s
epoch 12 | loss: 0.38956 | val_0_rmse: 0.66357 | val_1_rmse: 0.65799 |  0:01:11s
epoch 13 | loss: 0.38589 | val_0_rmse: 0.60463 | val_1_rmse: 0.60305 |  0:01:16s
epoch 14 | loss: 0.38142 | val_0_rmse: 0.60287 | val_1_rmse: 0.60096 |  0:01:22s
epoch 15 | loss: 0.37912 | val_0_rmse: 0.6257  | val_1_rmse: 0.62422 |  0:01:27s
epoch 16 | loss: 0.37705 | val_0_rmse: 0.61188 | val_1_rmse: 0.61326 |  0:01:33s
epoch 17 | loss: 0.37837 | val_0_rmse: 0.67722 | val_1_rmse: 0.67495 |  0:01:38s
epoch 18 | loss: 0.37867 | val_0_rmse: 0.65028 | val_1_rmse: 0.65479 |  0:01:44s
epoch 19 | loss: 0.37372 | val_0_rmse: 0.6556  | val_1_rmse: 0.64929 |  0:01:49s
epoch 20 | loss: 0.37192 | val_0_rmse: 0.61064 | val_1_rmse: 0.61058 |  0:01:55s
epoch 21 | loss: 0.36912 | val_0_rmse: 0.69444 | val_1_rmse: 0.68894 |  0:02:00s
epoch 22 | loss: 0.36611 | val_0_rmse: 0.72771 | val_1_rmse: 0.72247 |  0:02:06s
epoch 23 | loss: 0.36756 | val_0_rmse: 0.68438 | val_1_rmse: 0.69088 |  0:02:11s
epoch 24 | loss: 0.36802 | val_0_rmse: 0.6301  | val_1_rmse: 0.63286 |  0:02:16s
epoch 25 | loss: 0.36655 | val_0_rmse: 0.58394 | val_1_rmse: 0.58536 |  0:02:22s
epoch 26 | loss: 0.36566 | val_0_rmse: 0.64257 | val_1_rmse: 0.64922 |  0:02:27s
epoch 27 | loss: 0.36314 | val_0_rmse: 0.6179  | val_1_rmse: 0.61506 |  0:02:33s
epoch 28 | loss: 0.36068 | val_0_rmse: 0.60321 | val_1_rmse: 0.60616 |  0:02:38s
epoch 29 | loss: 0.35723 | val_0_rmse: 0.74028 | val_1_rmse: 0.74916 |  0:02:44s
epoch 30 | loss: 0.36023 | val_0_rmse: 0.73235 | val_1_rmse: 0.74017 |  0:02:49s
epoch 31 | loss: 0.35541 | val_0_rmse: 0.72339 | val_1_rmse: 0.73335 |  0:02:55s
epoch 32 | loss: 0.35207 | val_0_rmse: 0.87702 | val_1_rmse: 0.87121 |  0:03:00s
epoch 33 | loss: 0.35364 | val_0_rmse: 0.58624 | val_1_rmse: 0.58719 |  0:03:06s
epoch 34 | loss: 0.35728 | val_0_rmse: 0.59723 | val_1_rmse: 0.60079 |  0:03:11s
epoch 35 | loss: 0.34965 | val_0_rmse: 0.6678  | val_1_rmse: 0.66235 |  0:03:17s
epoch 36 | loss: 0.35099 | val_0_rmse: 0.59225 | val_1_rmse: 0.59581 |  0:03:22s
epoch 37 | loss: 0.34919 | val_0_rmse: 0.60858 | val_1_rmse: 0.60709 |  0:03:28s
epoch 38 | loss: 0.35078 | val_0_rmse: 0.65593 | val_1_rmse: 0.65254 |  0:03:33s
epoch 39 | loss: 0.3509  | val_0_rmse: 0.65124 | val_1_rmse: 0.65853 |  0:03:39s
epoch 40 | loss: 0.34945 | val_0_rmse: 0.60845 | val_1_rmse: 0.60824 |  0:03:44s
epoch 41 | loss: 0.34468 | val_0_rmse: 0.64751 | val_1_rmse: 0.64374 |  0:03:50s
epoch 42 | loss: 0.3459  | val_0_rmse: 0.69944 | val_1_rmse: 0.70822 |  0:03:55s
epoch 43 | loss: 0.34527 | val_0_rmse: 0.61956 | val_1_rmse: 0.61925 |  0:04:01s
epoch 44 | loss: 0.3459  | val_0_rmse: 0.62569 | val_1_rmse: 0.62926 |  0:04:06s
epoch 45 | loss: 0.36659 | val_0_rmse: 0.66044 | val_1_rmse: 0.65674 |  0:04:12s
epoch 46 | loss: 0.37866 | val_0_rmse: 0.61404 | val_1_rmse: 0.61888 |  0:04:17s
epoch 47 | loss: 0.35975 | val_0_rmse: 0.62711 | val_1_rmse: 0.63037 |  0:04:23s
epoch 48 | loss: 0.3561  | val_0_rmse: 0.62472 | val_1_rmse: 0.62051 |  0:04:28s
epoch 49 | loss: 0.3619  | val_0_rmse: 0.71242 | val_1_rmse: 0.70586 |  0:04:34s
epoch 50 | loss: 0.36367 | val_0_rmse: 0.61852 | val_1_rmse: 0.62271 |  0:04:39s
epoch 51 | loss: 0.35408 | val_0_rmse: 0.64216 | val_1_rmse: 0.64741 |  0:04:45s
epoch 52 | loss: 0.35089 | val_0_rmse: 0.60784 | val_1_rmse: 0.61107 |  0:04:50s
epoch 53 | loss: 0.35342 | val_0_rmse: 0.5937  | val_1_rmse: 0.59596 |  0:04:56s
epoch 54 | loss: 0.35163 | val_0_rmse: 0.8     | val_1_rmse: 0.79533 |  0:05:01s
epoch 55 | loss: 0.34643 | val_0_rmse: 0.60057 | val_1_rmse: 0.5969  |  0:05:06s

Early stopping occured at epoch 55 with best_epoch = 25 and best_val_1_rmse = 0.58536
Best weights from best epoch are automatically used!
ended training at: 00:44:02
Feature importance:
[('Area', 0.27876810994572554), ('Baths', 0.23692940964809106), ('Beds', 0.0), ('Latitude', 0.2811639057544069), ('Longitude', 0.20313857465177648), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 2344815644.5096755
Mean absolute error:35003.02335560841
MAPE:0.34044058037707453
R2 score:0.6530422442872141
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:44:03
epoch 0  | loss: 0.56216 | val_0_rmse: 0.60965 | val_1_rmse: 0.6269  |  0:00:02s
epoch 1  | loss: 0.33276 | val_0_rmse: 0.56486 | val_1_rmse: 0.57871 |  0:00:04s
epoch 2  | loss: 0.32666 | val_0_rmse: 0.55614 | val_1_rmse: 0.57436 |  0:00:06s
epoch 3  | loss: 0.30561 | val_0_rmse: 0.54712 | val_1_rmse: 0.56482 |  0:00:08s
epoch 4  | loss: 0.31425 | val_0_rmse: 0.56609 | val_1_rmse: 0.58226 |  0:00:10s
epoch 5  | loss: 0.3148  | val_0_rmse: 0.54202 | val_1_rmse: 0.55993 |  0:00:12s
epoch 6  | loss: 0.30456 | val_0_rmse: 0.54248 | val_1_rmse: 0.56021 |  0:00:14s
epoch 7  | loss: 0.29765 | val_0_rmse: 0.53333 | val_1_rmse: 0.55406 |  0:00:16s
epoch 8  | loss: 0.29204 | val_0_rmse: 0.56073 | val_1_rmse: 0.57724 |  0:00:18s
epoch 9  | loss: 0.28846 | val_0_rmse: 0.52656 | val_1_rmse: 0.54612 |  0:00:20s
epoch 10 | loss: 0.2829  | val_0_rmse: 0.51372 | val_1_rmse: 0.53496 |  0:00:22s
epoch 11 | loss: 0.28271 | val_0_rmse: 0.53506 | val_1_rmse: 0.55314 |  0:00:24s
epoch 12 | loss: 0.2773  | val_0_rmse: 0.51397 | val_1_rmse: 0.53188 |  0:00:26s
epoch 13 | loss: 0.27456 | val_0_rmse: 0.54174 | val_1_rmse: 0.5605  |  0:00:28s
epoch 14 | loss: 0.28023 | val_0_rmse: 0.54924 | val_1_rmse: 0.56641 |  0:00:30s
epoch 15 | loss: 0.28278 | val_0_rmse: 0.52168 | val_1_rmse: 0.54083 |  0:00:32s
epoch 16 | loss: 0.27781 | val_0_rmse: 0.52786 | val_1_rmse: 0.5464  |  0:00:34s
epoch 17 | loss: 0.28501 | val_0_rmse: 0.51783 | val_1_rmse: 0.53756 |  0:00:36s
epoch 18 | loss: 0.27722 | val_0_rmse: 0.51193 | val_1_rmse: 0.52939 |  0:00:38s
epoch 19 | loss: 0.27451 | val_0_rmse: 0.51594 | val_1_rmse: 0.53488 |  0:00:40s
epoch 20 | loss: 0.2735  | val_0_rmse: 0.51908 | val_1_rmse: 0.53555 |  0:00:42s
epoch 21 | loss: 0.26981 | val_0_rmse: 0.5173  | val_1_rmse: 0.5317  |  0:00:44s
epoch 22 | loss: 0.26433 | val_0_rmse: 0.51901 | val_1_rmse: 0.5379  |  0:00:46s
epoch 23 | loss: 0.26893 | val_0_rmse: 0.556   | val_1_rmse: 0.56757 |  0:00:48s
epoch 24 | loss: 0.26856 | val_0_rmse: 0.55565 | val_1_rmse: 0.56893 |  0:00:49s
epoch 25 | loss: 0.26935 | val_0_rmse: 0.56503 | val_1_rmse: 0.58176 |  0:00:51s
epoch 26 | loss: 0.26545 | val_0_rmse: 0.50444 | val_1_rmse: 0.52242 |  0:00:53s
epoch 27 | loss: 0.26393 | val_0_rmse: 0.50065 | val_1_rmse: 0.52015 |  0:00:55s
epoch 28 | loss: 0.25834 | val_0_rmse: 0.54862 | val_1_rmse: 0.56378 |  0:00:57s
epoch 29 | loss: 0.26069 | val_0_rmse: 0.50074 | val_1_rmse: 0.52002 |  0:00:59s
epoch 30 | loss: 0.25832 | val_0_rmse: 0.49861 | val_1_rmse: 0.51588 |  0:01:01s
epoch 31 | loss: 0.25931 | val_0_rmse: 0.4979  | val_1_rmse: 0.51729 |  0:01:03s
epoch 32 | loss: 0.25962 | val_0_rmse: 0.51087 | val_1_rmse: 0.5267  |  0:01:05s
epoch 33 | loss: 0.25597 | val_0_rmse: 0.52206 | val_1_rmse: 0.53569 |  0:01:07s
epoch 34 | loss: 0.25645 | val_0_rmse: 0.55766 | val_1_rmse: 0.57443 |  0:01:09s
epoch 35 | loss: 0.25821 | val_0_rmse: 0.5026  | val_1_rmse: 0.52354 |  0:01:11s
epoch 36 | loss: 0.26046 | val_0_rmse: 0.53747 | val_1_rmse: 0.55282 |  0:01:13s
epoch 37 | loss: 0.25877 | val_0_rmse: 0.5007  | val_1_rmse: 0.5183  |  0:01:15s
epoch 38 | loss: 0.2564  | val_0_rmse: 0.49261 | val_1_rmse: 0.51039 |  0:01:17s
epoch 39 | loss: 0.25541 | val_0_rmse: 0.54141 | val_1_rmse: 0.5581  |  0:01:19s
epoch 40 | loss: 0.26066 | val_0_rmse: 0.51002 | val_1_rmse: 0.52753 |  0:01:21s
epoch 41 | loss: 0.25532 | val_0_rmse: 0.51855 | val_1_rmse: 0.53258 |  0:01:23s
epoch 42 | loss: 0.25629 | val_0_rmse: 0.52892 | val_1_rmse: 0.54627 |  0:01:25s
epoch 43 | loss: 0.25381 | val_0_rmse: 0.56991 | val_1_rmse: 0.58369 |  0:01:27s
epoch 44 | loss: 0.25897 | val_0_rmse: 0.53341 | val_1_rmse: 0.54923 |  0:01:29s
epoch 45 | loss: 0.26012 | val_0_rmse: 0.49668 | val_1_rmse: 0.51449 |  0:01:31s
epoch 46 | loss: 0.26217 | val_0_rmse: 0.54012 | val_1_rmse: 0.55424 |  0:01:33s
epoch 47 | loss: 0.25503 | val_0_rmse: 0.54761 | val_1_rmse: 0.56207 |  0:01:35s
epoch 48 | loss: 0.25156 | val_0_rmse: 0.50516 | val_1_rmse: 0.52503 |  0:01:37s
epoch 49 | loss: 0.2544  | val_0_rmse: 0.50785 | val_1_rmse: 0.5263  |  0:01:39s
epoch 50 | loss: 0.25522 | val_0_rmse: 0.50187 | val_1_rmse: 0.51933 |  0:01:41s
epoch 51 | loss: 0.26568 | val_0_rmse: 0.55068 | val_1_rmse: 0.56793 |  0:01:43s
epoch 52 | loss: 0.26439 | val_0_rmse: 0.5503  | val_1_rmse: 0.56361 |  0:01:45s
epoch 53 | loss: 0.25807 | val_0_rmse: 0.53003 | val_1_rmse: 0.54783 |  0:01:47s
epoch 54 | loss: 0.25549 | val_0_rmse: 0.65479 | val_1_rmse: 0.66384 |  0:01:49s
epoch 55 | loss: 0.25762 | val_0_rmse: 0.50495 | val_1_rmse: 0.52339 |  0:01:51s
epoch 56 | loss: 0.25872 | val_0_rmse: 0.49833 | val_1_rmse: 0.51577 |  0:01:53s
epoch 57 | loss: 0.25659 | val_0_rmse: 0.59135 | val_1_rmse: 0.60825 |  0:01:55s
epoch 58 | loss: 0.25529 | val_0_rmse: 0.51655 | val_1_rmse: 0.5311  |  0:01:57s
epoch 59 | loss: 0.25548 | val_0_rmse: 0.50295 | val_1_rmse: 0.51873 |  0:01:59s
epoch 60 | loss: 0.24984 | val_0_rmse: 0.50015 | val_1_rmse: 0.51809 |  0:02:01s
epoch 61 | loss: 0.25162 | val_0_rmse: 0.49621 | val_1_rmse: 0.51467 |  0:02:03s
epoch 62 | loss: 0.26194 | val_0_rmse: 0.61484 | val_1_rmse: 0.63044 |  0:02:05s
epoch 63 | loss: 0.25587 | val_0_rmse: 0.52006 | val_1_rmse: 0.53597 |  0:02:07s
epoch 64 | loss: 0.25229 | val_0_rmse: 0.50891 | val_1_rmse: 0.52564 |  0:02:09s
epoch 65 | loss: 0.25836 | val_0_rmse: 0.50014 | val_1_rmse: 0.51716 |  0:02:11s
epoch 66 | loss: 0.25195 | val_0_rmse: 0.49462 | val_1_rmse: 0.51266 |  0:02:13s
epoch 67 | loss: 0.25181 | val_0_rmse: 0.49862 | val_1_rmse: 0.51564 |  0:02:15s
epoch 68 | loss: 0.25023 | val_0_rmse: 0.49432 | val_1_rmse: 0.51075 |  0:02:17s

Early stopping occured at epoch 68 with best_epoch = 38 and best_val_1_rmse = 0.51039
Best weights from best epoch are automatically used!
ended training at: 00:46:21
Feature importance:
[('Area', 0.3971211304624995), ('Baths', 0.0882754046428164), ('Beds', 0.0), ('Latitude', 0.11651148949819971), ('Longitude', 0.2337875135457622), ('Month', 0.11115904913808548), ('Year', 0.053145412712636717)]
Mean squared error is of 997898051.6112118
Mean absolute error:21432.898678188336
MAPE:0.26803157604513095
R2 score:0.7500140768577084
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:46:22
epoch 0  | loss: 0.54866 | val_0_rmse: 0.63992 | val_1_rmse: 0.63433 |  0:00:01s
epoch 1  | loss: 0.35696 | val_0_rmse: 0.5826  | val_1_rmse: 0.58577 |  0:00:04s
epoch 2  | loss: 0.33053 | val_0_rmse: 0.56268 | val_1_rmse: 0.56111 |  0:00:06s
epoch 3  | loss: 0.32699 | val_0_rmse: 0.55712 | val_1_rmse: 0.55567 |  0:00:08s
epoch 4  | loss: 0.31318 | val_0_rmse: 0.54872 | val_1_rmse: 0.55436 |  0:00:10s
epoch 5  | loss: 0.30729 | val_0_rmse: 0.54728 | val_1_rmse: 0.553   |  0:00:12s
epoch 6  | loss: 0.30074 | val_0_rmse: 0.54436 | val_1_rmse: 0.54782 |  0:00:14s
epoch 7  | loss: 0.29741 | val_0_rmse: 0.52818 | val_1_rmse: 0.52937 |  0:00:16s
epoch 8  | loss: 0.29335 | val_0_rmse: 0.52823 | val_1_rmse: 0.53053 |  0:00:18s
epoch 9  | loss: 0.29077 | val_0_rmse: 0.5273  | val_1_rmse: 0.52968 |  0:00:20s
epoch 10 | loss: 0.29429 | val_0_rmse: 0.53837 | val_1_rmse: 0.53801 |  0:00:22s
epoch 11 | loss: 0.28982 | val_0_rmse: 0.55905 | val_1_rmse: 0.56315 |  0:00:24s
epoch 12 | loss: 0.28356 | val_0_rmse: 0.51279 | val_1_rmse: 0.51716 |  0:00:26s
epoch 13 | loss: 0.2778  | val_0_rmse: 0.53486 | val_1_rmse: 0.53325 |  0:00:28s
epoch 14 | loss: 0.27329 | val_0_rmse: 0.51911 | val_1_rmse: 0.52056 |  0:00:30s
epoch 15 | loss: 0.26742 | val_0_rmse: 0.8433  | val_1_rmse: 0.85488 |  0:00:31s
epoch 16 | loss: 0.28996 | val_0_rmse: 0.57295 | val_1_rmse: 0.56969 |  0:00:33s
epoch 17 | loss: 0.2787  | val_0_rmse: 0.55413 | val_1_rmse: 0.55145 |  0:00:35s
epoch 18 | loss: 0.26687 | val_0_rmse: 0.54241 | val_1_rmse: 0.53974 |  0:00:37s
epoch 19 | loss: 0.2683  | val_0_rmse: 0.50807 | val_1_rmse: 0.50744 |  0:00:39s
epoch 20 | loss: 0.26653 | val_0_rmse: 0.50362 | val_1_rmse: 0.50395 |  0:00:41s
epoch 21 | loss: 0.26716 | val_0_rmse: 0.56988 | val_1_rmse: 0.56907 |  0:00:43s
epoch 22 | loss: 0.26868 | val_0_rmse: 0.51049 | val_1_rmse: 0.50799 |  0:00:45s
epoch 23 | loss: 0.26225 | val_0_rmse: 0.49953 | val_1_rmse: 0.49652 |  0:00:47s
epoch 24 | loss: 0.26592 | val_0_rmse: 0.52803 | val_1_rmse: 0.52551 |  0:00:49s
epoch 25 | loss: 0.26714 | val_0_rmse: 0.51651 | val_1_rmse: 0.51957 |  0:00:51s
epoch 26 | loss: 0.26326 | val_0_rmse: 0.54938 | val_1_rmse: 0.55448 |  0:00:53s
epoch 27 | loss: 0.26411 | val_0_rmse: 0.58771 | val_1_rmse: 0.58582 |  0:00:55s
epoch 28 | loss: 0.26165 | val_0_rmse: 0.60839 | val_1_rmse: 0.6068  |  0:00:57s
epoch 29 | loss: 0.26145 | val_0_rmse: 0.56999 | val_1_rmse: 0.57209 |  0:00:59s
epoch 30 | loss: 0.26142 | val_0_rmse: 0.56543 | val_1_rmse: 0.56903 |  0:01:01s
epoch 31 | loss: 0.26203 | val_0_rmse: 0.61744 | val_1_rmse: 0.61745 |  0:01:03s
epoch 32 | loss: 0.26177 | val_0_rmse: 0.50129 | val_1_rmse: 0.49903 |  0:01:05s
epoch 33 | loss: 0.25805 | val_0_rmse: 0.6483  | val_1_rmse: 0.64851 |  0:01:07s
epoch 34 | loss: 0.2728  | val_0_rmse: 0.57503 | val_1_rmse: 0.57794 |  0:01:09s
epoch 35 | loss: 0.26887 | val_0_rmse: 0.74186 | val_1_rmse: 0.74377 |  0:01:11s
epoch 36 | loss: 0.27092 | val_0_rmse: 0.55142 | val_1_rmse: 0.55713 |  0:01:13s
epoch 37 | loss: 0.2681  | val_0_rmse: 0.50684 | val_1_rmse: 0.50782 |  0:01:15s
epoch 38 | loss: 0.25586 | val_0_rmse: 0.5348  | val_1_rmse: 0.53137 |  0:01:17s
epoch 39 | loss: 0.25783 | val_0_rmse: 0.51844 | val_1_rmse: 0.51611 |  0:01:19s
epoch 40 | loss: 0.25542 | val_0_rmse: 0.51952 | val_1_rmse: 0.51669 |  0:01:21s
epoch 41 | loss: 0.25469 | val_0_rmse: 0.68533 | val_1_rmse: 0.68879 |  0:01:23s
epoch 42 | loss: 0.2546  | val_0_rmse: 0.5761  | val_1_rmse: 0.57887 |  0:01:25s
epoch 43 | loss: 0.25558 | val_0_rmse: 0.49595 | val_1_rmse: 0.49346 |  0:01:27s
epoch 44 | loss: 0.25593 | val_0_rmse: 0.53795 | val_1_rmse: 0.53643 |  0:01:29s
epoch 45 | loss: 0.25276 | val_0_rmse: 0.5077  | val_1_rmse: 0.51377 |  0:01:31s
epoch 46 | loss: 0.25612 | val_0_rmse: 0.51103 | val_1_rmse: 0.50846 |  0:01:33s
epoch 47 | loss: 0.25134 | val_0_rmse: 0.49552 | val_1_rmse: 0.49349 |  0:01:35s
epoch 48 | loss: 0.25003 | val_0_rmse: 0.5361  | val_1_rmse: 0.53736 |  0:01:37s
epoch 49 | loss: 0.25328 | val_0_rmse: 0.5475  | val_1_rmse: 0.5497  |  0:01:39s
epoch 50 | loss: 0.24913 | val_0_rmse: 0.49496 | val_1_rmse: 0.49777 |  0:01:41s
epoch 51 | loss: 0.25788 | val_0_rmse: 0.53445 | val_1_rmse: 0.52808 |  0:01:43s
epoch 52 | loss: 0.25541 | val_0_rmse: 0.53487 | val_1_rmse: 0.53724 |  0:01:45s
epoch 53 | loss: 0.25628 | val_0_rmse: 0.5583  | val_1_rmse: 0.55715 |  0:01:47s
epoch 54 | loss: 0.2533  | val_0_rmse: 0.52813 | val_1_rmse: 0.53111 |  0:01:49s
epoch 55 | loss: 0.25454 | val_0_rmse: 0.51877 | val_1_rmse: 0.51759 |  0:01:51s
epoch 56 | loss: 0.24724 | val_0_rmse: 0.51136 | val_1_rmse: 0.51077 |  0:01:53s
epoch 57 | loss: 0.24934 | val_0_rmse: 0.86741 | val_1_rmse: 0.88126 |  0:01:55s
epoch 58 | loss: 0.24983 | val_0_rmse: 0.56309 | val_1_rmse: 0.5648  |  0:01:57s
epoch 59 | loss: 0.25123 | val_0_rmse: 0.50558 | val_1_rmse: 0.50442 |  0:01:59s
epoch 60 | loss: 0.24347 | val_0_rmse: 0.51897 | val_1_rmse: 0.51904 |  0:02:01s
epoch 61 | loss: 0.25292 | val_0_rmse: 0.60906 | val_1_rmse: 0.61245 |  0:02:03s
epoch 62 | loss: 0.24799 | val_0_rmse: 0.53181 | val_1_rmse: 0.53127 |  0:02:05s
epoch 63 | loss: 0.25235 | val_0_rmse: 0.52128 | val_1_rmse: 0.52166 |  0:02:07s
epoch 64 | loss: 0.24876 | val_0_rmse: 0.4999  | val_1_rmse: 0.49891 |  0:02:09s
epoch 65 | loss: 0.24298 | val_0_rmse: 0.509   | val_1_rmse: 0.51062 |  0:02:11s
epoch 66 | loss: 0.2453  | val_0_rmse: 0.5529  | val_1_rmse: 0.55676 |  0:02:13s
epoch 67 | loss: 0.24827 | val_0_rmse: 0.49295 | val_1_rmse: 0.49288 |  0:02:15s
epoch 68 | loss: 0.2447  | val_0_rmse: 0.56701 | val_1_rmse: 0.56763 |  0:02:17s
epoch 69 | loss: 0.24716 | val_0_rmse: 0.51419 | val_1_rmse: 0.5157  |  0:02:19s
epoch 70 | loss: 0.24675 | val_0_rmse: 0.51449 | val_1_rmse: 0.51857 |  0:02:21s
epoch 71 | loss: 0.2406  | val_0_rmse: 0.54623 | val_1_rmse: 0.5475  |  0:02:23s
epoch 72 | loss: 0.24399 | val_0_rmse: 0.54871 | val_1_rmse: 0.54975 |  0:02:25s
epoch 73 | loss: 0.24149 | val_0_rmse: 0.52504 | val_1_rmse: 0.52877 |  0:02:27s
epoch 74 | loss: 0.23856 | val_0_rmse: 0.4895  | val_1_rmse: 0.4887  |  0:02:29s
epoch 75 | loss: 0.24158 | val_0_rmse: 0.47844 | val_1_rmse: 0.47967 |  0:02:31s
epoch 76 | loss: 0.24068 | val_0_rmse: 0.49335 | val_1_rmse: 0.49695 |  0:02:33s
epoch 77 | loss: 0.2429  | val_0_rmse: 0.52681 | val_1_rmse: 0.52633 |  0:02:35s
epoch 78 | loss: 0.24147 | val_0_rmse: 0.53255 | val_1_rmse: 0.53178 |  0:02:37s
epoch 79 | loss: 0.2426  | val_0_rmse: 0.51837 | val_1_rmse: 0.52083 |  0:02:39s
epoch 80 | loss: 0.24252 | val_0_rmse: 0.55927 | val_1_rmse: 0.56074 |  0:02:41s
epoch 81 | loss: 0.2422  | val_0_rmse: 0.50484 | val_1_rmse: 0.50837 |  0:02:43s
epoch 82 | loss: 0.23855 | val_0_rmse: 0.51624 | val_1_rmse: 0.51623 |  0:02:45s
epoch 83 | loss: 0.24125 | val_0_rmse: 0.5352  | val_1_rmse: 0.53782 |  0:02:47s
epoch 84 | loss: 0.23662 | val_0_rmse: 0.48528 | val_1_rmse: 0.48553 |  0:02:49s
epoch 85 | loss: 0.24043 | val_0_rmse: 0.57586 | val_1_rmse: 0.58045 |  0:02:51s
epoch 86 | loss: 0.24119 | val_0_rmse: 0.58302 | val_1_rmse: 0.58484 |  0:02:53s
epoch 87 | loss: 0.24847 | val_0_rmse: 0.51357 | val_1_rmse: 0.51569 |  0:02:55s
epoch 88 | loss: 0.24136 | val_0_rmse: 0.78832 | val_1_rmse: 0.80013 |  0:02:57s
epoch 89 | loss: 0.23934 | val_0_rmse: 0.50128 | val_1_rmse: 0.50292 |  0:02:59s
epoch 90 | loss: 0.24124 | val_0_rmse: 0.48368 | val_1_rmse: 0.48602 |  0:03:01s
epoch 91 | loss: 0.24546 | val_0_rmse: 0.54895 | val_1_rmse: 0.54966 |  0:03:03s
epoch 92 | loss: 0.24099 | val_0_rmse: 0.57024 | val_1_rmse: 0.57131 |  0:03:05s
epoch 93 | loss: 0.23972 | val_0_rmse: 0.58662 | val_1_rmse: 0.58693 |  0:03:07s
epoch 94 | loss: 0.24402 | val_0_rmse: 0.84841 | val_1_rmse: 0.85926 |  0:03:09s
epoch 95 | loss: 0.24245 | val_0_rmse: 0.56089 | val_1_rmse: 0.56832 |  0:03:11s
epoch 96 | loss: 0.23983 | val_0_rmse: 0.50998 | val_1_rmse: 0.50979 |  0:03:13s
epoch 97 | loss: 0.2392  | val_0_rmse: 0.48896 | val_1_rmse: 0.4908  |  0:03:15s
epoch 98 | loss: 0.23803 | val_0_rmse: 0.50946 | val_1_rmse: 0.51241 |  0:03:17s
epoch 99 | loss: 0.23716 | val_0_rmse: 0.4821  | val_1_rmse: 0.48086 |  0:03:19s
epoch 100| loss: 0.23749 | val_0_rmse: 0.4889  | val_1_rmse: 0.49513 |  0:03:21s
epoch 101| loss: 0.23626 | val_0_rmse: 0.65571 | val_1_rmse: 0.65988 |  0:03:23s
epoch 102| loss: 0.23969 | val_0_rmse: 0.49875 | val_1_rmse: 0.50298 |  0:03:25s
epoch 103| loss: 0.23352 | val_0_rmse: 0.51662 | val_1_rmse: 0.52171 |  0:03:27s
epoch 104| loss: 0.23848 | val_0_rmse: 0.50373 | val_1_rmse: 0.50582 |  0:03:29s
epoch 105| loss: 0.23776 | val_0_rmse: 0.5063  | val_1_rmse: 0.50727 |  0:03:31s

Early stopping occured at epoch 105 with best_epoch = 75 and best_val_1_rmse = 0.47967
Best weights from best epoch are automatically used!
ended training at: 00:49:54
Feature importance:
[('Area', 0.4388388570656011), ('Baths', 0.0), ('Beds', 0.0), ('Latitude', 0.21473017721407162), ('Longitude', 0.16389943959163186), ('Month', 0.055770475907047916), ('Year', 0.1267610502216475)]
Mean squared error is of 938618602.8270607
Mean absolute error:20914.577716103857
MAPE:0.2591100662788727
R2 score:0.764633185071957
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:49:54
epoch 0  | loss: 0.55853 | val_0_rmse: 0.62042 | val_1_rmse: 0.62561 |  0:00:01s
epoch 1  | loss: 0.35024 | val_0_rmse: 0.56813 | val_1_rmse: 0.577   |  0:00:04s
epoch 2  | loss: 0.32246 | val_0_rmse: 0.55857 | val_1_rmse: 0.56564 |  0:00:05s
epoch 3  | loss: 0.31446 | val_0_rmse: 0.55909 | val_1_rmse: 0.56946 |  0:00:08s
epoch 4  | loss: 0.31292 | val_0_rmse: 0.55536 | val_1_rmse: 0.56203 |  0:00:09s
epoch 5  | loss: 0.31958 | val_0_rmse: 0.54949 | val_1_rmse: 0.55737 |  0:00:12s
epoch 6  | loss: 0.30647 | val_0_rmse: 0.53858 | val_1_rmse: 0.55036 |  0:00:13s
epoch 7  | loss: 0.29995 | val_0_rmse: 0.53338 | val_1_rmse: 0.54414 |  0:00:15s
epoch 8  | loss: 0.29464 | val_0_rmse: 0.53719 | val_1_rmse: 0.54732 |  0:00:18s
epoch 9  | loss: 0.30011 | val_0_rmse: 0.54333 | val_1_rmse: 0.55332 |  0:00:19s
epoch 10 | loss: 0.29577 | val_0_rmse: 0.53047 | val_1_rmse: 0.54099 |  0:00:21s
epoch 11 | loss: 0.29059 | val_0_rmse: 0.5337  | val_1_rmse: 0.54373 |  0:00:23s
epoch 12 | loss: 0.29495 | val_0_rmse: 0.53474 | val_1_rmse: 0.54679 |  0:00:25s
epoch 13 | loss: 0.29004 | val_0_rmse: 0.53347 | val_1_rmse: 0.54347 |  0:00:27s
epoch 14 | loss: 0.28825 | val_0_rmse: 0.53627 | val_1_rmse: 0.54778 |  0:00:29s
epoch 15 | loss: 0.28565 | val_0_rmse: 0.52148 | val_1_rmse: 0.53366 |  0:00:31s
epoch 16 | loss: 0.28513 | val_0_rmse: 0.5306  | val_1_rmse: 0.54286 |  0:00:33s
epoch 17 | loss: 0.28884 | val_0_rmse: 0.52056 | val_1_rmse: 0.53217 |  0:00:35s
epoch 18 | loss: 0.28385 | val_0_rmse: 0.60527 | val_1_rmse: 0.61654 |  0:00:37s
epoch 19 | loss: 0.28712 | val_0_rmse: 0.52554 | val_1_rmse: 0.53519 |  0:00:39s
epoch 20 | loss: 0.28202 | val_0_rmse: 0.51669 | val_1_rmse: 0.52692 |  0:00:41s
epoch 21 | loss: 0.28212 | val_0_rmse: 0.52099 | val_1_rmse: 0.53173 |  0:00:43s
epoch 22 | loss: 0.28255 | val_0_rmse: 0.52733 | val_1_rmse: 0.53732 |  0:00:45s
epoch 23 | loss: 0.28215 | val_0_rmse: 0.52401 | val_1_rmse: 0.53625 |  0:00:47s
epoch 24 | loss: 0.28328 | val_0_rmse: 0.55089 | val_1_rmse: 0.55752 |  0:00:49s
epoch 25 | loss: 0.27921 | val_0_rmse: 0.52884 | val_1_rmse: 0.53872 |  0:00:51s
epoch 26 | loss: 0.27612 | val_0_rmse: 0.52509 | val_1_rmse: 0.5346  |  0:00:53s
epoch 27 | loss: 0.273   | val_0_rmse: 0.52283 | val_1_rmse: 0.53162 |  0:00:55s
epoch 28 | loss: 0.27316 | val_0_rmse: 0.5424  | val_1_rmse: 0.55336 |  0:00:57s
epoch 29 | loss: 0.2803  | val_0_rmse: 0.55032 | val_1_rmse: 0.56274 |  0:00:59s
epoch 30 | loss: 0.27472 | val_0_rmse: 0.51437 | val_1_rmse: 0.52635 |  0:01:01s
epoch 31 | loss: 0.27317 | val_0_rmse: 0.52548 | val_1_rmse: 0.53462 |  0:01:03s
epoch 32 | loss: 0.27134 | val_0_rmse: 0.52977 | val_1_rmse: 0.54415 |  0:01:05s
epoch 33 | loss: 0.26713 | val_0_rmse: 0.50995 | val_1_rmse: 0.51878 |  0:01:07s
epoch 34 | loss: 0.27103 | val_0_rmse: 0.50892 | val_1_rmse: 0.522   |  0:01:09s
epoch 35 | loss: 0.26711 | val_0_rmse: 0.51839 | val_1_rmse: 0.53075 |  0:01:11s
epoch 36 | loss: 0.26042 | val_0_rmse: 0.569   | val_1_rmse: 0.58019 |  0:01:13s
epoch 37 | loss: 0.25943 | val_0_rmse: 0.55045 | val_1_rmse: 0.55945 |  0:01:15s
epoch 38 | loss: 0.26139 | val_0_rmse: 0.54512 | val_1_rmse: 0.55487 |  0:01:17s
epoch 39 | loss: 0.2637  | val_0_rmse: 0.50608 | val_1_rmse: 0.51763 |  0:01:19s
epoch 40 | loss: 0.25928 | val_0_rmse: 0.519   | val_1_rmse: 0.52968 |  0:01:21s
epoch 41 | loss: 0.27437 | val_0_rmse: 0.52051 | val_1_rmse: 0.52895 |  0:01:23s
epoch 42 | loss: 0.26343 | val_0_rmse: 0.57186 | val_1_rmse: 0.58281 |  0:01:25s
epoch 43 | loss: 0.26461 | val_0_rmse: 0.55939 | val_1_rmse: 0.56864 |  0:01:27s
epoch 44 | loss: 0.26275 | val_0_rmse: 0.50397 | val_1_rmse: 0.51265 |  0:01:29s
epoch 45 | loss: 0.25928 | val_0_rmse: 0.50184 | val_1_rmse: 0.51451 |  0:01:31s
epoch 46 | loss: 0.26045 | val_0_rmse: 0.56573 | val_1_rmse: 0.57521 |  0:01:33s
epoch 47 | loss: 0.26228 | val_0_rmse: 0.52518 | val_1_rmse: 0.53459 |  0:01:35s
epoch 48 | loss: 0.26241 | val_0_rmse: 0.50655 | val_1_rmse: 0.51434 |  0:01:37s
epoch 49 | loss: 0.2524  | val_0_rmse: 0.54178 | val_1_rmse: 0.55657 |  0:01:39s
epoch 50 | loss: 0.25111 | val_0_rmse: 0.54352 | val_1_rmse: 0.5524  |  0:01:41s
epoch 51 | loss: 0.25404 | val_0_rmse: 0.49313 | val_1_rmse: 0.50344 |  0:01:43s
epoch 52 | loss: 0.25071 | val_0_rmse: 0.60436 | val_1_rmse: 0.61513 |  0:01:45s
epoch 53 | loss: 0.25221 | val_0_rmse: 0.52245 | val_1_rmse: 0.5278  |  0:01:47s
epoch 54 | loss: 0.25407 | val_0_rmse: 0.62261 | val_1_rmse: 0.62856 |  0:01:49s
epoch 55 | loss: 0.24933 | val_0_rmse: 0.51387 | val_1_rmse: 0.52079 |  0:01:51s
epoch 56 | loss: 0.25066 | val_0_rmse: 0.52412 | val_1_rmse: 0.53481 |  0:01:53s
epoch 57 | loss: 0.24819 | val_0_rmse: 0.51723 | val_1_rmse: 0.52578 |  0:01:55s
epoch 58 | loss: 0.24596 | val_0_rmse: 0.5837  | val_1_rmse: 0.59181 |  0:01:57s
epoch 59 | loss: 0.24848 | val_0_rmse: 0.49945 | val_1_rmse: 0.50898 |  0:01:59s
epoch 60 | loss: 0.2488  | val_0_rmse: 0.52765 | val_1_rmse: 0.53771 |  0:02:01s
epoch 61 | loss: 0.24834 | val_0_rmse: 0.53407 | val_1_rmse: 0.5415  |  0:02:03s
epoch 62 | loss: 0.24653 | val_0_rmse: 0.54861 | val_1_rmse: 0.56018 |  0:02:05s
epoch 63 | loss: 0.24564 | val_0_rmse: 0.60456 | val_1_rmse: 0.61309 |  0:02:07s
epoch 64 | loss: 0.24484 | val_0_rmse: 0.49409 | val_1_rmse: 0.50371 |  0:02:09s
epoch 65 | loss: 0.24359 | val_0_rmse: 0.55938 | val_1_rmse: 0.57035 |  0:02:11s
epoch 66 | loss: 0.25861 | val_0_rmse: 0.64186 | val_1_rmse: 0.64808 |  0:02:13s
epoch 67 | loss: 0.25406 | val_0_rmse: 0.51362 | val_1_rmse: 0.5211  |  0:02:15s
epoch 68 | loss: 0.24899 | val_0_rmse: 0.52472 | val_1_rmse: 0.53344 |  0:02:17s
epoch 69 | loss: 0.24823 | val_0_rmse: 0.53534 | val_1_rmse: 0.54509 |  0:02:19s
epoch 70 | loss: 0.24766 | val_0_rmse: 0.49037 | val_1_rmse: 0.49652 |  0:02:21s
epoch 71 | loss: 0.24451 | val_0_rmse: 0.52871 | val_1_rmse: 0.5328  |  0:02:23s
epoch 72 | loss: 0.24428 | val_0_rmse: 0.61685 | val_1_rmse: 0.6242  |  0:02:25s
epoch 73 | loss: 0.24081 | val_0_rmse: 0.48477 | val_1_rmse: 0.49244 |  0:02:27s
epoch 74 | loss: 0.2412  | val_0_rmse: 0.49806 | val_1_rmse: 0.50283 |  0:02:29s
epoch 75 | loss: 0.24018 | val_0_rmse: 0.49409 | val_1_rmse: 0.50473 |  0:02:31s
epoch 76 | loss: 0.24068 | val_0_rmse: 0.62968 | val_1_rmse: 0.63682 |  0:02:33s
epoch 77 | loss: 0.24471 | val_0_rmse: 0.64347 | val_1_rmse: 0.64722 |  0:02:35s
epoch 78 | loss: 0.24254 | val_0_rmse: 0.52003 | val_1_rmse: 0.52631 |  0:02:37s
epoch 79 | loss: 0.23917 | val_0_rmse: 0.52352 | val_1_rmse: 0.52951 |  0:02:39s
epoch 80 | loss: 0.24194 | val_0_rmse: 0.52521 | val_1_rmse: 0.53005 |  0:02:41s
epoch 81 | loss: 0.23827 | val_0_rmse: 0.51411 | val_1_rmse: 0.51791 |  0:02:43s
epoch 82 | loss: 0.2395  | val_0_rmse: 0.51655 | val_1_rmse: 0.52431 |  0:02:45s
epoch 83 | loss: 0.25722 | val_0_rmse: 0.58151 | val_1_rmse: 0.58865 |  0:02:47s
epoch 84 | loss: 0.27028 | val_0_rmse: 0.56138 | val_1_rmse: 0.56626 |  0:02:49s
epoch 85 | loss: 0.26121 | val_0_rmse: 0.52419 | val_1_rmse: 0.53048 |  0:02:51s
epoch 86 | loss: 0.25235 | val_0_rmse: 0.52206 | val_1_rmse: 0.53337 |  0:02:53s
epoch 87 | loss: 0.24948 | val_0_rmse: 0.52945 | val_1_rmse: 0.53562 |  0:02:55s
epoch 88 | loss: 0.24463 | val_0_rmse: 0.51084 | val_1_rmse: 0.52049 |  0:02:57s
epoch 89 | loss: 0.24312 | val_0_rmse: 0.51379 | val_1_rmse: 0.52424 |  0:02:59s
epoch 90 | loss: 0.2416  | val_0_rmse: 0.50729 | val_1_rmse: 0.51512 |  0:03:01s
epoch 91 | loss: 0.24149 | val_0_rmse: 0.55834 | val_1_rmse: 0.57543 |  0:03:03s
epoch 92 | loss: 0.24555 | val_0_rmse: 0.55252 | val_1_rmse: 0.56078 |  0:03:05s
epoch 93 | loss: 0.24318 | val_0_rmse: 0.51884 | val_1_rmse: 0.52936 |  0:03:07s
epoch 94 | loss: 0.24372 | val_0_rmse: 0.49076 | val_1_rmse: 0.50327 |  0:03:09s
epoch 95 | loss: 0.24288 | val_0_rmse: 0.4941  | val_1_rmse: 0.50612 |  0:03:11s
epoch 96 | loss: 0.24702 | val_0_rmse: 0.50633 | val_1_rmse: 0.52154 |  0:03:13s
epoch 97 | loss: 0.24312 | val_0_rmse: 0.57195 | val_1_rmse: 0.577   |  0:03:15s
epoch 98 | loss: 0.24218 | val_0_rmse: 0.52237 | val_1_rmse: 0.53374 |  0:03:17s
epoch 99 | loss: 0.24197 | val_0_rmse: 0.56769 | val_1_rmse: 0.57685 |  0:03:19s
epoch 100| loss: 0.24245 | val_0_rmse: 0.50856 | val_1_rmse: 0.51779 |  0:03:21s
epoch 101| loss: 0.23831 | val_0_rmse: 0.53069 | val_1_rmse: 0.54112 |  0:03:23s
epoch 102| loss: 0.23793 | val_0_rmse: 0.56498 | val_1_rmse: 0.5712  |  0:03:25s
epoch 103| loss: 0.23791 | val_0_rmse: 0.49803 | val_1_rmse: 0.50695 |  0:03:27s

Early stopping occured at epoch 103 with best_epoch = 73 and best_val_1_rmse = 0.49244
Best weights from best epoch are automatically used!
ended training at: 00:53:22
Feature importance:
[('Area', 0.2978068936088399), ('Baths', 0.2492213212444354), ('Beds', 0.09990079958212274), ('Latitude', 0.10233173530524092), ('Longitude', 0.25073925025936106), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 938711705.8590094
Mean absolute error:20796.667680489987
MAPE:0.25531502255404764
R2 score:0.7669916566279477
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:53:22
epoch 0  | loss: 0.54911 | val_0_rmse: 0.62584 | val_1_rmse: 0.61946 |  0:00:02s
epoch 1  | loss: 0.33343 | val_0_rmse: 0.55573 | val_1_rmse: 0.54894 |  0:00:04s
epoch 2  | loss: 0.32078 | val_0_rmse: 0.55686 | val_1_rmse: 0.54854 |  0:00:06s
epoch 3  | loss: 0.31016 | val_0_rmse: 0.55681 | val_1_rmse: 0.55028 |  0:00:08s
epoch 4  | loss: 0.31244 | val_0_rmse: 0.54049 | val_1_rmse: 0.54089 |  0:00:10s
epoch 5  | loss: 0.30761 | val_0_rmse: 0.55607 | val_1_rmse: 0.55154 |  0:00:12s
epoch 6  | loss: 0.30823 | val_0_rmse: 0.54575 | val_1_rmse: 0.54543 |  0:00:14s
epoch 7  | loss: 0.29824 | val_0_rmse: 0.53925 | val_1_rmse: 0.5392  |  0:00:15s
epoch 8  | loss: 0.29792 | val_0_rmse: 0.54092 | val_1_rmse: 0.54042 |  0:00:17s
epoch 9  | loss: 0.30601 | val_0_rmse: 0.55927 | val_1_rmse: 0.55483 |  0:00:19s
epoch 10 | loss: 0.31682 | val_0_rmse: 0.58279 | val_1_rmse: 0.58041 |  0:00:21s
epoch 11 | loss: 0.33332 | val_0_rmse: 0.55582 | val_1_rmse: 0.55607 |  0:00:23s
epoch 12 | loss: 0.30426 | val_0_rmse: 0.53387 | val_1_rmse: 0.53523 |  0:00:25s
epoch 13 | loss: 0.29881 | val_0_rmse: 0.53357 | val_1_rmse: 0.53609 |  0:00:27s
epoch 14 | loss: 0.29091 | val_0_rmse: 0.54464 | val_1_rmse: 0.54663 |  0:00:29s
epoch 15 | loss: 0.29339 | val_0_rmse: 0.52527 | val_1_rmse: 0.52612 |  0:00:31s
epoch 16 | loss: 0.28488 | val_0_rmse: 0.5284  | val_1_rmse: 0.53186 |  0:00:33s
epoch 17 | loss: 0.28846 | val_0_rmse: 0.52981 | val_1_rmse: 0.53245 |  0:00:35s
epoch 18 | loss: 0.28254 | val_0_rmse: 0.52471 | val_1_rmse: 0.52365 |  0:00:37s
epoch 19 | loss: 0.28432 | val_0_rmse: 0.5234  | val_1_rmse: 0.5235  |  0:00:39s
epoch 20 | loss: 0.28485 | val_0_rmse: 0.52723 | val_1_rmse: 0.53113 |  0:00:41s
epoch 21 | loss: 0.28309 | val_0_rmse: 0.5341  | val_1_rmse: 0.53344 |  0:00:43s
epoch 22 | loss: 0.27906 | val_0_rmse: 0.53222 | val_1_rmse: 0.53267 |  0:00:45s
epoch 23 | loss: 0.28458 | val_0_rmse: 0.51913 | val_1_rmse: 0.52076 |  0:00:47s
epoch 24 | loss: 0.28484 | val_0_rmse: 0.51915 | val_1_rmse: 0.52229 |  0:00:49s
epoch 25 | loss: 0.27723 | val_0_rmse: 0.55944 | val_1_rmse: 0.55492 |  0:00:51s
epoch 26 | loss: 0.27958 | val_0_rmse: 0.53022 | val_1_rmse: 0.5338  |  0:00:53s
epoch 27 | loss: 0.28469 | val_0_rmse: 0.53216 | val_1_rmse: 0.53052 |  0:00:55s
epoch 28 | loss: 0.2756  | val_0_rmse: 0.52402 | val_1_rmse: 0.52694 |  0:00:57s
epoch 29 | loss: 0.27609 | val_0_rmse: 0.51577 | val_1_rmse: 0.52048 |  0:00:59s
epoch 30 | loss: 0.27523 | val_0_rmse: 0.50576 | val_1_rmse: 0.50779 |  0:01:01s
epoch 31 | loss: 0.27091 | val_0_rmse: 0.50781 | val_1_rmse: 0.50855 |  0:01:03s
epoch 32 | loss: 0.27069 | val_0_rmse: 0.52804 | val_1_rmse: 0.52538 |  0:01:05s
epoch 33 | loss: 0.27297 | val_0_rmse: 0.52207 | val_1_rmse: 0.51904 |  0:01:07s
epoch 34 | loss: 0.26601 | val_0_rmse: 0.60894 | val_1_rmse: 0.6066  |  0:01:09s
epoch 35 | loss: 0.26662 | val_0_rmse: 0.53532 | val_1_rmse: 0.53517 |  0:01:11s
epoch 36 | loss: 0.26082 | val_0_rmse: 0.49527 | val_1_rmse: 0.50122 |  0:01:13s
epoch 37 | loss: 0.25997 | val_0_rmse: 0.52447 | val_1_rmse: 0.52657 |  0:01:15s
epoch 38 | loss: 0.2642  | val_0_rmse: 0.51453 | val_1_rmse: 0.51896 |  0:01:17s
epoch 39 | loss: 0.26316 | val_0_rmse: 0.52804 | val_1_rmse: 0.52759 |  0:01:19s
epoch 40 | loss: 0.2712  | val_0_rmse: 0.52219 | val_1_rmse: 0.52277 |  0:01:21s
epoch 41 | loss: 0.26537 | val_0_rmse: 0.54395 | val_1_rmse: 0.54476 |  0:01:23s
epoch 42 | loss: 0.26669 | val_0_rmse: 0.52225 | val_1_rmse: 0.52075 |  0:01:25s
epoch 43 | loss: 0.2637  | val_0_rmse: 0.5046  | val_1_rmse: 0.5098  |  0:01:27s
epoch 44 | loss: 0.26633 | val_0_rmse: 0.49936 | val_1_rmse: 0.5069  |  0:01:29s
epoch 45 | loss: 0.26179 | val_0_rmse: 0.53017 | val_1_rmse: 0.53098 |  0:01:31s
epoch 46 | loss: 0.26054 | val_0_rmse: 0.51171 | val_1_rmse: 0.51717 |  0:01:33s
epoch 47 | loss: 0.25914 | val_0_rmse: 0.51837 | val_1_rmse: 0.52362 |  0:01:35s
epoch 48 | loss: 0.26135 | val_0_rmse: 0.49929 | val_1_rmse: 0.50621 |  0:01:37s
epoch 49 | loss: 0.25935 | val_0_rmse: 0.51425 | val_1_rmse: 0.51912 |  0:01:39s
epoch 50 | loss: 0.25661 | val_0_rmse: 0.4952  | val_1_rmse: 0.50119 |  0:01:41s
epoch 51 | loss: 0.25681 | val_0_rmse: 0.56798 | val_1_rmse: 0.57106 |  0:01:43s
epoch 52 | loss: 0.2597  | val_0_rmse: 0.48688 | val_1_rmse: 0.49442 |  0:01:45s
epoch 53 | loss: 0.25247 | val_0_rmse: 0.48883 | val_1_rmse: 0.49718 |  0:01:47s
epoch 54 | loss: 0.25282 | val_0_rmse: 0.4962  | val_1_rmse: 0.50134 |  0:01:49s
epoch 55 | loss: 0.24852 | val_0_rmse: 0.50867 | val_1_rmse: 0.51697 |  0:01:51s
epoch 56 | loss: 0.25572 | val_0_rmse: 0.65805 | val_1_rmse: 0.65089 |  0:01:53s
epoch 57 | loss: 0.25858 | val_0_rmse: 0.53497 | val_1_rmse: 0.53939 |  0:01:55s
epoch 58 | loss: 0.25329 | val_0_rmse: 0.51852 | val_1_rmse: 0.52141 |  0:01:57s
epoch 59 | loss: 0.25262 | val_0_rmse: 0.49525 | val_1_rmse: 0.50006 |  0:01:59s
epoch 60 | loss: 0.25215 | val_0_rmse: 0.51686 | val_1_rmse: 0.52002 |  0:02:01s
epoch 61 | loss: 0.25517 | val_0_rmse: 0.5136  | val_1_rmse: 0.51537 |  0:02:03s
epoch 62 | loss: 0.25547 | val_0_rmse: 0.50424 | val_1_rmse: 0.51234 |  0:02:05s
epoch 63 | loss: 0.25723 | val_0_rmse: 0.50657 | val_1_rmse: 0.50887 |  0:02:07s
epoch 64 | loss: 0.25063 | val_0_rmse: 0.58011 | val_1_rmse: 0.58167 |  0:02:09s
epoch 65 | loss: 0.25691 | val_0_rmse: 0.50655 | val_1_rmse: 0.50987 |  0:02:11s
epoch 66 | loss: 0.25194 | val_0_rmse: 0.52364 | val_1_rmse: 0.52858 |  0:02:13s
epoch 67 | loss: 0.24945 | val_0_rmse: 0.49277 | val_1_rmse: 0.50203 |  0:02:15s
epoch 68 | loss: 0.24891 | val_0_rmse: 0.49034 | val_1_rmse: 0.50112 |  0:02:17s
epoch 69 | loss: 0.24495 | val_0_rmse: 0.49439 | val_1_rmse: 0.50238 |  0:02:19s
epoch 70 | loss: 0.24915 | val_0_rmse: 0.49486 | val_1_rmse: 0.50271 |  0:02:21s
epoch 71 | loss: 0.24864 | val_0_rmse: 0.53457 | val_1_rmse: 0.53762 |  0:02:23s
epoch 72 | loss: 0.24531 | val_0_rmse: 0.55393 | val_1_rmse: 0.55879 |  0:02:25s
epoch 73 | loss: 0.24547 | val_0_rmse: 0.49473 | val_1_rmse: 0.50099 |  0:02:27s
epoch 74 | loss: 0.24707 | val_0_rmse: 0.50827 | val_1_rmse: 0.51883 |  0:02:29s
epoch 75 | loss: 0.24454 | val_0_rmse: 0.52343 | val_1_rmse: 0.53121 |  0:02:31s
epoch 76 | loss: 0.25142 | val_0_rmse: 0.50603 | val_1_rmse: 0.51647 |  0:02:33s
epoch 77 | loss: 0.25058 | val_0_rmse: 0.54037 | val_1_rmse: 0.54594 |  0:02:34s
epoch 78 | loss: 0.24759 | val_0_rmse: 0.52539 | val_1_rmse: 0.53057 |  0:02:36s
epoch 79 | loss: 0.24434 | val_0_rmse: 0.48461 | val_1_rmse: 0.49788 |  0:02:38s
epoch 80 | loss: 0.24362 | val_0_rmse: 0.50407 | val_1_rmse: 0.51494 |  0:02:40s
epoch 81 | loss: 0.24525 | val_0_rmse: 0.58924 | val_1_rmse: 0.59237 |  0:02:42s
epoch 82 | loss: 0.24579 | val_0_rmse: 0.49194 | val_1_rmse: 0.49848 |  0:02:44s

Early stopping occured at epoch 82 with best_epoch = 52 and best_val_1_rmse = 0.49442
Best weights from best epoch are automatically used!
ended training at: 00:56:08
Feature importance:
[('Area', 0.3414230250104366), ('Baths', 0.10047300001671909), ('Beds', 0.06691755928908379), ('Latitude', 0.12377951722021145), ('Longitude', 0.3673812786133665), ('Month', 2.561985018259635e-05), ('Year', 0.0)]
Mean squared error is of 935057168.7015324
Mean absolute error:20836.05517409501
MAPE:0.2584948149325889
R2 score:0.7625800088912332
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:56:08
epoch 0  | loss: 0.52277 | val_0_rmse: 0.60139 | val_1_rmse: 0.59398 |  0:00:02s
epoch 1  | loss: 0.34955 | val_0_rmse: 0.57448 | val_1_rmse: 0.56245 |  0:00:04s
epoch 2  | loss: 0.33849 | val_0_rmse: 0.56849 | val_1_rmse: 0.55701 |  0:00:06s
epoch 3  | loss: 0.3299  | val_0_rmse: 0.56871 | val_1_rmse: 0.55653 |  0:00:07s
epoch 4  | loss: 0.30689 | val_0_rmse: 0.54689 | val_1_rmse: 0.53877 |  0:00:09s
epoch 5  | loss: 0.29998 | val_0_rmse: 0.6865  | val_1_rmse: 0.67925 |  0:00:11s
epoch 6  | loss: 0.28837 | val_0_rmse: 0.56139 | val_1_rmse: 0.54708 |  0:00:13s
epoch 7  | loss: 0.28358 | val_0_rmse: 0.5425  | val_1_rmse: 0.53184 |  0:00:15s
epoch 8  | loss: 0.28068 | val_0_rmse: 0.50998 | val_1_rmse: 0.49926 |  0:00:17s
epoch 9  | loss: 0.27652 | val_0_rmse: 0.58952 | val_1_rmse: 0.57947 |  0:00:19s
epoch 10 | loss: 0.27544 | val_0_rmse: 0.56105 | val_1_rmse: 0.551   |  0:00:21s
epoch 11 | loss: 0.2792  | val_0_rmse: 0.71339 | val_1_rmse: 0.69753 |  0:00:23s
epoch 12 | loss: 0.27849 | val_0_rmse: 0.52098 | val_1_rmse: 0.50754 |  0:00:25s
epoch 13 | loss: 0.27557 | val_0_rmse: 0.54085 | val_1_rmse: 0.53207 |  0:00:27s
epoch 14 | loss: 0.26917 | val_0_rmse: 0.59476 | val_1_rmse: 0.58549 |  0:00:29s
epoch 15 | loss: 0.27431 | val_0_rmse: 0.51785 | val_1_rmse: 0.50796 |  0:00:31s
epoch 16 | loss: 0.27265 | val_0_rmse: 0.53609 | val_1_rmse: 0.52499 |  0:00:33s
epoch 17 | loss: 0.28419 | val_0_rmse: 0.5719  | val_1_rmse: 0.56062 |  0:00:35s
epoch 18 | loss: 0.2744  | val_0_rmse: 0.61662 | val_1_rmse: 0.60913 |  0:00:37s
epoch 19 | loss: 0.27639 | val_0_rmse: 0.53069 | val_1_rmse: 0.51718 |  0:00:39s
epoch 20 | loss: 0.27458 | val_0_rmse: 0.62935 | val_1_rmse: 0.61657 |  0:00:41s
epoch 21 | loss: 0.26433 | val_0_rmse: 0.55191 | val_1_rmse: 0.54262 |  0:00:43s
epoch 22 | loss: 0.26651 | val_0_rmse: 0.56955 | val_1_rmse: 0.55638 |  0:00:45s
epoch 23 | loss: 0.26423 | val_0_rmse: 0.55797 | val_1_rmse: 0.5485  |  0:00:47s
epoch 24 | loss: 0.2658  | val_0_rmse: 0.51299 | val_1_rmse: 0.50075 |  0:00:49s
epoch 25 | loss: 0.25941 | val_0_rmse: 0.52899 | val_1_rmse: 0.52034 |  0:00:51s
epoch 26 | loss: 0.25929 | val_0_rmse: 0.60725 | val_1_rmse: 0.59935 |  0:00:53s
epoch 27 | loss: 0.25992 | val_0_rmse: 0.50339 | val_1_rmse: 0.48811 |  0:00:55s
epoch 28 | loss: 0.25555 | val_0_rmse: 0.48939 | val_1_rmse: 0.47581 |  0:00:57s
epoch 29 | loss: 0.25753 | val_0_rmse: 0.55562 | val_1_rmse: 0.54552 |  0:00:59s
epoch 30 | loss: 0.26123 | val_0_rmse: 0.59502 | val_1_rmse: 0.58618 |  0:01:01s
epoch 31 | loss: 0.25643 | val_0_rmse: 0.62212 | val_1_rmse: 0.60907 |  0:01:03s
epoch 32 | loss: 0.26353 | val_0_rmse: 0.53167 | val_1_rmse: 0.52161 |  0:01:05s
epoch 33 | loss: 0.25724 | val_0_rmse: 0.5195  | val_1_rmse: 0.51084 |  0:01:07s
epoch 34 | loss: 0.25444 | val_0_rmse: 0.52668 | val_1_rmse: 0.51534 |  0:01:09s
epoch 35 | loss: 0.25383 | val_0_rmse: 0.55363 | val_1_rmse: 0.54921 |  0:01:11s
epoch 36 | loss: 0.25778 | val_0_rmse: 0.62057 | val_1_rmse: 0.61105 |  0:01:13s
epoch 37 | loss: 0.25142 | val_0_rmse: 0.50411 | val_1_rmse: 0.49624 |  0:01:15s
epoch 38 | loss: 0.25632 | val_0_rmse: 0.53536 | val_1_rmse: 0.52791 |  0:01:17s
epoch 39 | loss: 0.25119 | val_0_rmse: 0.50053 | val_1_rmse: 0.49036 |  0:01:19s
epoch 40 | loss: 0.24609 | val_0_rmse: 0.53209 | val_1_rmse: 0.52437 |  0:01:21s
epoch 41 | loss: 0.25324 | val_0_rmse: 0.49442 | val_1_rmse: 0.48037 |  0:01:23s
epoch 42 | loss: 0.24816 | val_0_rmse: 0.52876 | val_1_rmse: 0.52174 |  0:01:25s
epoch 43 | loss: 0.25161 | val_0_rmse: 0.53494 | val_1_rmse: 0.52723 |  0:01:27s
epoch 44 | loss: 0.24615 | val_0_rmse: 0.51423 | val_1_rmse: 0.50544 |  0:01:29s
epoch 45 | loss: 0.25282 | val_0_rmse: 0.51838 | val_1_rmse: 0.50821 |  0:01:31s
epoch 46 | loss: 0.25619 | val_0_rmse: 0.56758 | val_1_rmse: 0.56249 |  0:01:33s
epoch 47 | loss: 0.24871 | val_0_rmse: 0.53706 | val_1_rmse: 0.52707 |  0:01:35s
epoch 48 | loss: 0.24816 | val_0_rmse: 0.48777 | val_1_rmse: 0.4769  |  0:01:37s
epoch 49 | loss: 0.24576 | val_0_rmse: 0.49725 | val_1_rmse: 0.48772 |  0:01:39s
epoch 50 | loss: 0.24367 | val_0_rmse: 0.53991 | val_1_rmse: 0.53079 |  0:01:41s
epoch 51 | loss: 0.24271 | val_0_rmse: 0.53363 | val_1_rmse: 0.52516 |  0:01:43s
epoch 52 | loss: 0.24442 | val_0_rmse: 0.4898  | val_1_rmse: 0.47674 |  0:01:45s
epoch 53 | loss: 0.24272 | val_0_rmse: 0.5006  | val_1_rmse: 0.48868 |  0:01:47s
epoch 54 | loss: 0.24324 | val_0_rmse: 0.53569 | val_1_rmse: 0.52897 |  0:01:49s
epoch 55 | loss: 0.24524 | val_0_rmse: 0.5098  | val_1_rmse: 0.50325 |  0:01:51s
epoch 56 | loss: 0.24252 | val_0_rmse: 0.57528 | val_1_rmse: 0.5703  |  0:01:53s
epoch 57 | loss: 0.24216 | val_0_rmse: 0.57926 | val_1_rmse: 0.56899 |  0:01:55s
epoch 58 | loss: 0.24361 | val_0_rmse: 0.51688 | val_1_rmse: 0.51002 |  0:01:57s

Early stopping occured at epoch 58 with best_epoch = 28 and best_val_1_rmse = 0.47581
Best weights from best epoch are automatically used!
ended training at: 00:58:06
Feature importance:
[('Area', 0.3202537709874663), ('Baths', 2.4807506596097653e-07), ('Beds', 0.15779615287570234), ('Latitude', 0.10618167832946135), ('Longitude', 0.2471111658286721), ('Month', 0.0728132856410695), ('Year', 0.09584369826256245)]
Mean squared error is of 1000393335.398128
Mean absolute error:21605.667209808093
MAPE:0.27320223889746703
R2 score:0.7485528535553035
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:58:07
epoch 0  | loss: 0.45334 | val_0_rmse: 0.57241 | val_1_rmse: 0.58341 |  0:00:03s
epoch 1  | loss: 0.30566 | val_0_rmse: 0.53072 | val_1_rmse: 0.53332 |  0:00:06s
epoch 2  | loss: 0.27073 | val_0_rmse: 0.49517 | val_1_rmse: 0.49862 |  0:00:09s
epoch 3  | loss: 0.2523  | val_0_rmse: 0.47411 | val_1_rmse: 0.47905 |  0:00:12s
epoch 4  | loss: 0.24531 | val_0_rmse: 0.48626 | val_1_rmse: 0.49021 |  0:00:15s
epoch 5  | loss: 0.23537 | val_0_rmse: 0.48968 | val_1_rmse: 0.4928  |  0:00:19s
epoch 6  | loss: 0.23333 | val_0_rmse: 0.45815 | val_1_rmse: 0.46375 |  0:00:22s
epoch 7  | loss: 0.232   | val_0_rmse: 0.45605 | val_1_rmse: 0.46505 |  0:00:25s
epoch 8  | loss: 0.22607 | val_0_rmse: 0.49326 | val_1_rmse: 0.49858 |  0:00:28s
epoch 9  | loss: 0.22897 | val_0_rmse: 0.45451 | val_1_rmse: 0.46131 |  0:00:31s
epoch 10 | loss: 0.22573 | val_0_rmse: 0.44497 | val_1_rmse: 0.44924 |  0:00:35s
epoch 11 | loss: 0.2237  | val_0_rmse: 0.45235 | val_1_rmse: 0.45844 |  0:00:38s
epoch 12 | loss: 0.21277 | val_0_rmse: 0.44521 | val_1_rmse: 0.4486  |  0:00:41s
epoch 13 | loss: 0.21403 | val_0_rmse: 0.43684 | val_1_rmse: 0.44181 |  0:00:44s
epoch 14 | loss: 0.21228 | val_0_rmse: 0.43976 | val_1_rmse: 0.44578 |  0:00:47s
epoch 15 | loss: 0.20728 | val_0_rmse: 0.43653 | val_1_rmse: 0.44146 |  0:00:51s
epoch 16 | loss: 0.20961 | val_0_rmse: 0.45981 | val_1_rmse: 0.46282 |  0:00:54s
epoch 17 | loss: 0.20424 | val_0_rmse: 0.45103 | val_1_rmse: 0.45557 |  0:00:57s
epoch 18 | loss: 0.20838 | val_0_rmse: 0.44206 | val_1_rmse: 0.44669 |  0:01:00s
epoch 19 | loss: 0.20543 | val_0_rmse: 0.43293 | val_1_rmse: 0.43482 |  0:01:03s
epoch 20 | loss: 0.20522 | val_0_rmse: 0.4439  | val_1_rmse: 0.44805 |  0:01:07s
epoch 21 | loss: 0.19978 | val_0_rmse: 0.43103 | val_1_rmse: 0.43449 |  0:01:10s
epoch 22 | loss: 0.20429 | val_0_rmse: 0.43789 | val_1_rmse: 0.4425  |  0:01:13s
epoch 23 | loss: 0.20741 | val_0_rmse: 0.42882 | val_1_rmse: 0.43478 |  0:01:16s
epoch 24 | loss: 0.2031  | val_0_rmse: 0.45366 | val_1_rmse: 0.45586 |  0:01:19s
epoch 25 | loss: 0.20244 | val_0_rmse: 0.47108 | val_1_rmse: 0.47646 |  0:01:23s
epoch 26 | loss: 0.20563 | val_0_rmse: 0.4294  | val_1_rmse: 0.43641 |  0:01:26s
epoch 27 | loss: 0.20112 | val_0_rmse: 0.42577 | val_1_rmse: 0.43218 |  0:01:29s
epoch 28 | loss: 0.20095 | val_0_rmse: 0.44922 | val_1_rmse: 0.45089 |  0:01:32s
epoch 29 | loss: 0.19408 | val_0_rmse: 0.43432 | val_1_rmse: 0.43905 |  0:01:35s
epoch 30 | loss: 0.19393 | val_0_rmse: 0.42307 | val_1_rmse: 0.42899 |  0:01:39s
epoch 31 | loss: 0.19271 | val_0_rmse: 0.44316 | val_1_rmse: 0.44679 |  0:01:42s
epoch 32 | loss: 0.19406 | val_0_rmse: 0.42721 | val_1_rmse: 0.43344 |  0:01:45s
epoch 33 | loss: 0.19226 | val_0_rmse: 0.4325  | val_1_rmse: 0.43717 |  0:01:48s
epoch 34 | loss: 0.19613 | val_0_rmse: 0.42623 | val_1_rmse: 0.43188 |  0:01:51s
epoch 35 | loss: 0.1959  | val_0_rmse: 0.4388  | val_1_rmse: 0.44496 |  0:01:54s
epoch 36 | loss: 0.1981  | val_0_rmse: 0.41986 | val_1_rmse: 0.42581 |  0:01:58s
epoch 37 | loss: 0.19522 | val_0_rmse: 0.41726 | val_1_rmse: 0.42345 |  0:02:01s
epoch 38 | loss: 0.19332 | val_0_rmse: 0.42021 | val_1_rmse: 0.42556 |  0:02:04s
epoch 39 | loss: 0.19554 | val_0_rmse: 0.42713 | val_1_rmse: 0.43458 |  0:02:07s
epoch 40 | loss: 0.19171 | val_0_rmse: 0.42    | val_1_rmse: 0.42805 |  0:02:10s
epoch 41 | loss: 0.19773 | val_0_rmse: 0.42599 | val_1_rmse: 0.43195 |  0:02:14s
epoch 42 | loss: 0.1895  | val_0_rmse: 0.41581 | val_1_rmse: 0.42194 |  0:02:17s
epoch 43 | loss: 0.19537 | val_0_rmse: 0.4243  | val_1_rmse: 0.4322  |  0:02:20s
epoch 44 | loss: 0.19144 | val_0_rmse: 0.41407 | val_1_rmse: 0.42156 |  0:02:23s
epoch 45 | loss: 0.19349 | val_0_rmse: 0.41541 | val_1_rmse: 0.42331 |  0:02:26s
epoch 46 | loss: 0.19169 | val_0_rmse: 0.41838 | val_1_rmse: 0.42544 |  0:02:30s
epoch 47 | loss: 0.18938 | val_0_rmse: 0.422   | val_1_rmse: 0.42969 |  0:02:33s
epoch 48 | loss: 0.19681 | val_0_rmse: 0.42578 | val_1_rmse: 0.4324  |  0:02:36s
epoch 49 | loss: 0.19263 | val_0_rmse: 0.45189 | val_1_rmse: 0.4607  |  0:02:39s
epoch 50 | loss: 0.19281 | val_0_rmse: 0.42382 | val_1_rmse: 0.4296  |  0:02:42s
epoch 51 | loss: 0.19415 | val_0_rmse: 0.43574 | val_1_rmse: 0.44256 |  0:02:46s
epoch 52 | loss: 0.19164 | val_0_rmse: 0.41307 | val_1_rmse: 0.42091 |  0:02:49s
epoch 53 | loss: 0.19024 | val_0_rmse: 0.43503 | val_1_rmse: 0.44171 |  0:02:52s
epoch 54 | loss: 0.1873  | val_0_rmse: 0.41842 | val_1_rmse: 0.42735 |  0:02:55s
epoch 55 | loss: 0.1869  | val_0_rmse: 0.43209 | val_1_rmse: 0.44063 |  0:02:58s
epoch 56 | loss: 0.19219 | val_0_rmse: 0.42088 | val_1_rmse: 0.43015 |  0:03:01s
epoch 57 | loss: 0.18999 | val_0_rmse: 0.42096 | val_1_rmse: 0.42867 |  0:03:05s
epoch 58 | loss: 0.18846 | val_0_rmse: 0.41638 | val_1_rmse: 0.4253  |  0:03:08s
epoch 59 | loss: 0.1863  | val_0_rmse: 0.41352 | val_1_rmse: 0.42293 |  0:03:11s
epoch 60 | loss: 0.18557 | val_0_rmse: 0.40797 | val_1_rmse: 0.41878 |  0:03:14s
epoch 61 | loss: 0.18985 | val_0_rmse: 0.42846 | val_1_rmse: 0.43775 |  0:03:17s
epoch 62 | loss: 0.19125 | val_0_rmse: 0.41642 | val_1_rmse: 0.42442 |  0:03:21s
epoch 63 | loss: 0.19188 | val_0_rmse: 0.41149 | val_1_rmse: 0.42173 |  0:03:24s
epoch 64 | loss: 0.18846 | val_0_rmse: 0.40882 | val_1_rmse: 0.41849 |  0:03:27s
epoch 65 | loss: 0.18739 | val_0_rmse: 0.4124  | val_1_rmse: 0.4206  |  0:03:30s
epoch 66 | loss: 0.18559 | val_0_rmse: 0.40796 | val_1_rmse: 0.41846 |  0:03:33s
epoch 67 | loss: 0.19012 | val_0_rmse: 0.41778 | val_1_rmse: 0.42641 |  0:03:37s
epoch 68 | loss: 0.19061 | val_0_rmse: 0.40684 | val_1_rmse: 0.41675 |  0:03:40s
epoch 69 | loss: 0.18614 | val_0_rmse: 0.41812 | val_1_rmse: 0.42596 |  0:03:43s
epoch 70 | loss: 0.18564 | val_0_rmse: 0.41359 | val_1_rmse: 0.4229  |  0:03:46s
epoch 71 | loss: 0.1857  | val_0_rmse: 0.41056 | val_1_rmse: 0.42154 |  0:03:49s
epoch 72 | loss: 0.1875  | val_0_rmse: 0.42602 | val_1_rmse: 0.43601 |  0:03:53s
epoch 73 | loss: 0.18874 | val_0_rmse: 0.42089 | val_1_rmse: 0.43257 |  0:03:56s
epoch 74 | loss: 0.18668 | val_0_rmse: 0.41605 | val_1_rmse: 0.42769 |  0:03:59s
epoch 75 | loss: 0.18565 | val_0_rmse: 0.41204 | val_1_rmse: 0.42178 |  0:04:02s
epoch 76 | loss: 0.18326 | val_0_rmse: 0.4141  | val_1_rmse: 0.42536 |  0:04:05s
epoch 77 | loss: 0.18469 | val_0_rmse: 0.40954 | val_1_rmse: 0.42095 |  0:04:08s
epoch 78 | loss: 0.18775 | val_0_rmse: 0.42633 | val_1_rmse: 0.43882 |  0:04:12s
epoch 79 | loss: 0.18519 | val_0_rmse: 0.41604 | val_1_rmse: 0.42659 |  0:04:15s
epoch 80 | loss: 0.18365 | val_0_rmse: 0.41894 | val_1_rmse: 0.43164 |  0:04:18s
epoch 81 | loss: 0.18381 | val_0_rmse: 0.41673 | val_1_rmse: 0.4279  |  0:04:21s
epoch 82 | loss: 0.18306 | val_0_rmse: 0.41299 | val_1_rmse: 0.4247  |  0:04:24s
epoch 83 | loss: 0.18394 | val_0_rmse: 0.4209  | val_1_rmse: 0.43486 |  0:04:28s
epoch 84 | loss: 0.18328 | val_0_rmse: 0.40905 | val_1_rmse: 0.42095 |  0:04:31s
epoch 85 | loss: 0.18418 | val_0_rmse: 0.40379 | val_1_rmse: 0.41706 |  0:04:34s
epoch 86 | loss: 0.1847  | val_0_rmse: 0.41405 | val_1_rmse: 0.4251  |  0:04:37s
epoch 87 | loss: 0.18563 | val_0_rmse: 0.40409 | val_1_rmse: 0.41739 |  0:04:40s
epoch 88 | loss: 0.18497 | val_0_rmse: 0.41152 | val_1_rmse: 0.42382 |  0:04:44s
epoch 89 | loss: 0.18303 | val_0_rmse: 0.4108  | val_1_rmse: 0.42446 |  0:04:47s
epoch 90 | loss: 0.17944 | val_0_rmse: 0.4002  | val_1_rmse: 0.41519 |  0:04:50s
epoch 91 | loss: 0.17909 | val_0_rmse: 0.42765 | val_1_rmse: 0.44113 |  0:04:53s
epoch 92 | loss: 0.18199 | val_0_rmse: 0.40349 | val_1_rmse: 0.41728 |  0:04:56s
epoch 93 | loss: 0.17823 | val_0_rmse: 0.40647 | val_1_rmse: 0.41999 |  0:04:59s
epoch 94 | loss: 0.18343 | val_0_rmse: 0.41374 | val_1_rmse: 0.4274  |  0:05:03s
epoch 95 | loss: 0.18026 | val_0_rmse: 0.40451 | val_1_rmse: 0.41813 |  0:05:06s
epoch 96 | loss: 0.17953 | val_0_rmse: 0.40857 | val_1_rmse: 0.42255 |  0:05:09s
epoch 97 | loss: 0.18199 | val_0_rmse: 0.42759 | val_1_rmse: 0.44186 |  0:05:12s
epoch 98 | loss: 0.17934 | val_0_rmse: 0.41892 | val_1_rmse: 0.43263 |  0:05:15s
epoch 99 | loss: 0.18085 | val_0_rmse: 0.43004 | val_1_rmse: 0.44587 |  0:05:19s
epoch 100| loss: 0.17797 | val_0_rmse: 0.43107 | val_1_rmse: 0.44304 |  0:05:22s
epoch 101| loss: 0.18117 | val_0_rmse: 0.40414 | val_1_rmse: 0.41925 |  0:05:25s
epoch 102| loss: 0.1795  | val_0_rmse: 0.40541 | val_1_rmse: 0.41812 |  0:05:28s
epoch 103| loss: 0.18019 | val_0_rmse: 0.404   | val_1_rmse: 0.41918 |  0:05:31s
epoch 104| loss: 0.18137 | val_0_rmse: 0.40804 | val_1_rmse: 0.4204  |  0:05:35s
epoch 105| loss: 0.17983 | val_0_rmse: 0.41611 | val_1_rmse: 0.42853 |  0:05:38s
epoch 106| loss: 0.17931 | val_0_rmse: 0.4014  | val_1_rmse: 0.41657 |  0:05:41s
epoch 107| loss: 0.17874 | val_0_rmse: 0.40653 | val_1_rmse: 0.41895 |  0:05:44s
epoch 108| loss: 0.17931 | val_0_rmse: 0.40899 | val_1_rmse: 0.42168 |  0:05:47s
epoch 109| loss: 0.18153 | val_0_rmse: 0.41435 | val_1_rmse: 0.42984 |  0:05:51s
epoch 110| loss: 0.17726 | val_0_rmse: 0.41692 | val_1_rmse: 0.43184 |  0:05:54s
epoch 111| loss: 0.18189 | val_0_rmse: 0.41624 | val_1_rmse: 0.42898 |  0:05:57s
epoch 112| loss: 0.17903 | val_0_rmse: 0.39937 | val_1_rmse: 0.41306 |  0:06:00s
epoch 113| loss: 0.17537 | val_0_rmse: 0.40525 | val_1_rmse: 0.42052 |  0:06:04s
epoch 114| loss: 0.18036 | val_0_rmse: 0.40974 | val_1_rmse: 0.4233  |  0:06:07s
epoch 115| loss: 0.17728 | val_0_rmse: 0.41578 | val_1_rmse: 0.43011 |  0:06:10s
epoch 116| loss: 0.18168 | val_0_rmse: 0.40467 | val_1_rmse: 0.42264 |  0:06:13s
epoch 117| loss: 0.17869 | val_0_rmse: 0.39878 | val_1_rmse: 0.41498 |  0:06:16s
epoch 118| loss: 0.17423 | val_0_rmse: 0.40098 | val_1_rmse: 0.41701 |  0:06:20s
epoch 119| loss: 0.17975 | val_0_rmse: 0.4052  | val_1_rmse: 0.42017 |  0:06:23s
epoch 120| loss: 0.17657 | val_0_rmse: 0.41209 | val_1_rmse: 0.42691 |  0:06:26s
epoch 121| loss: 0.17521 | val_0_rmse: 0.4019  | val_1_rmse: 0.41823 |  0:06:29s
epoch 122| loss: 0.17769 | val_0_rmse: 0.40196 | val_1_rmse: 0.41877 |  0:06:32s
epoch 123| loss: 0.17543 | val_0_rmse: 0.40214 | val_1_rmse: 0.41764 |  0:06:36s
epoch 124| loss: 0.17598 | val_0_rmse: 0.40594 | val_1_rmse: 0.42168 |  0:06:39s
epoch 125| loss: 0.17441 | val_0_rmse: 0.41019 | val_1_rmse: 0.42684 |  0:06:42s
epoch 126| loss: 0.17475 | val_0_rmse: 0.40283 | val_1_rmse: 0.42025 |  0:06:45s
epoch 127| loss: 0.17551 | val_0_rmse: 0.39485 | val_1_rmse: 0.41242 |  0:06:48s
epoch 128| loss: 0.17356 | val_0_rmse: 0.41026 | val_1_rmse: 0.42772 |  0:06:52s
epoch 129| loss: 0.17352 | val_0_rmse: 0.4043  | val_1_rmse: 0.42197 |  0:06:55s
epoch 130| loss: 0.17792 | val_0_rmse: 0.40899 | val_1_rmse: 0.42494 |  0:06:58s
epoch 131| loss: 0.17569 | val_0_rmse: 0.39981 | val_1_rmse: 0.41746 |  0:07:01s
epoch 132| loss: 0.17674 | val_0_rmse: 0.407   | val_1_rmse: 0.42307 |  0:07:04s
epoch 133| loss: 0.17838 | val_0_rmse: 0.3979  | val_1_rmse: 0.41462 |  0:07:08s
epoch 134| loss: 0.17485 | val_0_rmse: 0.4005  | val_1_rmse: 0.41682 |  0:07:11s
epoch 135| loss: 0.17299 | val_0_rmse: 0.39853 | val_1_rmse: 0.41549 |  0:07:14s
epoch 136| loss: 0.18136 | val_0_rmse: 0.40706 | val_1_rmse: 0.42437 |  0:07:17s
epoch 137| loss: 0.17821 | val_0_rmse: 0.41325 | val_1_rmse: 0.42816 |  0:07:20s
epoch 138| loss: 0.17672 | val_0_rmse: 0.39861 | val_1_rmse: 0.41735 |  0:07:23s
epoch 139| loss: 0.17631 | val_0_rmse: 0.39763 | val_1_rmse: 0.41599 |  0:07:27s
epoch 140| loss: 0.17243 | val_0_rmse: 0.3965  | val_1_rmse: 0.41636 |  0:07:30s
epoch 141| loss: 0.17458 | val_0_rmse: 0.39496 | val_1_rmse: 0.41354 |  0:07:33s
epoch 142| loss: 0.17579 | val_0_rmse: 0.39582 | val_1_rmse: 0.41644 |  0:07:36s
epoch 143| loss: 0.17344 | val_0_rmse: 0.40113 | val_1_rmse: 0.41663 |  0:07:39s
epoch 144| loss: 0.17493 | val_0_rmse: 0.3945  | val_1_rmse: 0.41465 |  0:07:43s
epoch 145| loss: 0.17338 | val_0_rmse: 0.40091 | val_1_rmse: 0.41845 |  0:07:46s
epoch 146| loss: 0.17269 | val_0_rmse: 0.39464 | val_1_rmse: 0.41606 |  0:07:49s
epoch 147| loss: 0.17132 | val_0_rmse: 0.39815 | val_1_rmse: 0.41713 |  0:07:52s
epoch 148| loss: 0.17392 | val_0_rmse: 0.40051 | val_1_rmse: 0.42079 |  0:07:55s
epoch 149| loss: 0.17131 | val_0_rmse: 0.39597 | val_1_rmse: 0.41471 |  0:07:59s
Stop training because you reached max_epochs = 150 with best_epoch = 127 and best_val_1_rmse = 0.41242
Best weights from best epoch are automatically used!
ended training at: 01:06:07
Feature importance:
[('Area', 0.15711063779550136), ('Baths', 0.15067161582766908), ('Beds', 0.06753771491714113), ('Latitude', 0.11641829693285338), ('Longitude', 0.2647238311912665), ('Month', 0.0), ('Year', 0.24353790333556852)]
Mean squared error is of 9639780041.890804
Mean absolute error:68034.04065123961
MAPE:0.27495984173866184
R2 score:0.8324277636274983
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:06:08
epoch 0  | loss: 0.47746 | val_0_rmse: 0.64366 | val_1_rmse: 0.64065 |  0:00:03s
epoch 1  | loss: 0.31162 | val_0_rmse: 0.51832 | val_1_rmse: 0.52201 |  0:00:06s
epoch 2  | loss: 0.2679  | val_0_rmse: 0.49347 | val_1_rmse: 0.49572 |  0:00:09s
epoch 3  | loss: 0.25315 | val_0_rmse: 0.48331 | val_1_rmse: 0.48395 |  0:00:12s
epoch 4  | loss: 0.24931 | val_0_rmse: 0.46841 | val_1_rmse: 0.47159 |  0:00:16s
epoch 5  | loss: 0.23834 | val_0_rmse: 0.48498 | val_1_rmse: 0.48351 |  0:00:19s
epoch 6  | loss: 0.24259 | val_0_rmse: 0.48406 | val_1_rmse: 0.48247 |  0:00:22s
epoch 7  | loss: 0.23362 | val_0_rmse: 0.47794 | val_1_rmse: 0.47377 |  0:00:25s
epoch 8  | loss: 0.22722 | val_0_rmse: 0.47647 | val_1_rmse: 0.47623 |  0:00:28s
epoch 9  | loss: 0.22944 | val_0_rmse: 0.47779 | val_1_rmse: 0.48181 |  0:00:32s
epoch 10 | loss: 0.2261  | val_0_rmse: 0.45676 | val_1_rmse: 0.4574  |  0:00:35s
epoch 11 | loss: 0.22715 | val_0_rmse: 0.45373 | val_1_rmse: 0.45476 |  0:00:38s
epoch 12 | loss: 0.21854 | val_0_rmse: 0.44926 | val_1_rmse: 0.45154 |  0:00:41s
epoch 13 | loss: 0.21964 | val_0_rmse: 0.44553 | val_1_rmse: 0.44341 |  0:00:44s
epoch 14 | loss: 0.21187 | val_0_rmse: 0.44759 | val_1_rmse: 0.45151 |  0:00:48s
epoch 15 | loss: 0.21409 | val_0_rmse: 0.44256 | val_1_rmse: 0.44277 |  0:00:51s
epoch 16 | loss: 0.21468 | val_0_rmse: 0.44481 | val_1_rmse: 0.44417 |  0:00:54s
epoch 17 | loss: 0.21358 | val_0_rmse: 0.45922 | val_1_rmse: 0.45776 |  0:00:57s
epoch 18 | loss: 0.21102 | val_0_rmse: 0.43829 | val_1_rmse: 0.43884 |  0:01:00s
epoch 19 | loss: 0.21226 | val_0_rmse: 0.44291 | val_1_rmse: 0.44163 |  0:01:04s
epoch 20 | loss: 0.20785 | val_0_rmse: 0.44811 | val_1_rmse: 0.4498  |  0:01:07s
epoch 21 | loss: 0.2086  | val_0_rmse: 0.44515 | val_1_rmse: 0.44389 |  0:01:10s
epoch 22 | loss: 0.20973 | val_0_rmse: 0.44057 | val_1_rmse: 0.4392  |  0:01:13s
epoch 23 | loss: 0.20933 | val_0_rmse: 0.43759 | val_1_rmse: 0.44113 |  0:01:17s
epoch 24 | loss: 0.20319 | val_0_rmse: 0.43131 | val_1_rmse: 0.43066 |  0:01:20s
epoch 25 | loss: 0.20288 | val_0_rmse: 0.4449  | val_1_rmse: 0.44693 |  0:01:23s
epoch 26 | loss: 0.20749 | val_0_rmse: 0.43759 | val_1_rmse: 0.43891 |  0:01:26s
epoch 27 | loss: 0.20708 | val_0_rmse: 0.43434 | val_1_rmse: 0.43482 |  0:01:29s
epoch 28 | loss: 0.20132 | val_0_rmse: 0.45599 | val_1_rmse: 0.46033 |  0:01:33s
epoch 29 | loss: 0.19858 | val_0_rmse: 0.42635 | val_1_rmse: 0.42791 |  0:01:36s
epoch 30 | loss: 0.19844 | val_0_rmse: 0.42649 | val_1_rmse: 0.42643 |  0:01:39s
epoch 31 | loss: 0.2005  | val_0_rmse: 0.43058 | val_1_rmse: 0.42893 |  0:01:42s
epoch 32 | loss: 0.19996 | val_0_rmse: 0.43239 | val_1_rmse: 0.43655 |  0:01:45s
epoch 33 | loss: 0.20087 | val_0_rmse: 0.43548 | val_1_rmse: 0.43695 |  0:01:49s
epoch 34 | loss: 0.19817 | val_0_rmse: 0.43036 | val_1_rmse: 0.42951 |  0:01:52s
epoch 35 | loss: 0.1983  | val_0_rmse: 0.46392 | val_1_rmse: 0.46324 |  0:01:55s
epoch 36 | loss: 0.20081 | val_0_rmse: 0.43605 | val_1_rmse: 0.43996 |  0:01:58s
epoch 37 | loss: 0.19816 | val_0_rmse: 0.43112 | val_1_rmse: 0.43302 |  0:02:01s
epoch 38 | loss: 0.19843 | val_0_rmse: 0.43699 | val_1_rmse: 0.44121 |  0:02:04s
epoch 39 | loss: 0.19602 | val_0_rmse: 0.43892 | val_1_rmse: 0.43904 |  0:02:08s
epoch 40 | loss: 0.19467 | val_0_rmse: 0.43024 | val_1_rmse: 0.42957 |  0:02:11s
epoch 41 | loss: 0.19511 | val_0_rmse: 0.43479 | val_1_rmse: 0.43717 |  0:02:14s
epoch 42 | loss: 0.19378 | val_0_rmse: 0.42616 | val_1_rmse: 0.42609 |  0:02:17s
epoch 43 | loss: 0.19353 | val_0_rmse: 0.42096 | val_1_rmse: 0.4235  |  0:02:21s
epoch 44 | loss: 0.19261 | val_0_rmse: 0.41717 | val_1_rmse: 0.41973 |  0:02:24s
epoch 45 | loss: 0.19198 | val_0_rmse: 0.42706 | val_1_rmse: 0.42896 |  0:02:27s
epoch 46 | loss: 0.19195 | val_0_rmse: 0.42506 | val_1_rmse: 0.42978 |  0:02:30s
epoch 47 | loss: 0.19578 | val_0_rmse: 0.42327 | val_1_rmse: 0.42584 |  0:02:33s
epoch 48 | loss: 0.18976 | val_0_rmse: 0.42855 | val_1_rmse: 0.43152 |  0:02:37s
epoch 49 | loss: 0.19382 | val_0_rmse: 0.4135  | val_1_rmse: 0.41736 |  0:02:40s
epoch 50 | loss: 0.19025 | val_0_rmse: 0.42034 | val_1_rmse: 0.42246 |  0:02:43s
epoch 51 | loss: 0.18915 | val_0_rmse: 0.42857 | val_1_rmse: 0.4333  |  0:02:46s
epoch 52 | loss: 0.18995 | val_0_rmse: 0.42526 | val_1_rmse: 0.427   |  0:02:49s
epoch 53 | loss: 0.19121 | val_0_rmse: 0.41835 | val_1_rmse: 0.42175 |  0:02:53s
epoch 54 | loss: 0.18881 | val_0_rmse: 0.41472 | val_1_rmse: 0.41689 |  0:02:56s
epoch 55 | loss: 0.18876 | val_0_rmse: 0.41946 | val_1_rmse: 0.42442 |  0:02:59s
epoch 56 | loss: 0.18673 | val_0_rmse: 0.43728 | val_1_rmse: 0.43736 |  0:03:02s
epoch 57 | loss: 0.19083 | val_0_rmse: 0.43093 | val_1_rmse: 0.43247 |  0:03:05s
epoch 58 | loss: 0.1929  | val_0_rmse: 0.43208 | val_1_rmse: 0.44046 |  0:03:09s
epoch 59 | loss: 0.19234 | val_0_rmse: 0.42436 | val_1_rmse: 0.42804 |  0:03:12s
epoch 60 | loss: 0.18719 | val_0_rmse: 0.42099 | val_1_rmse: 0.42709 |  0:03:15s
epoch 61 | loss: 0.18582 | val_0_rmse: 0.41838 | val_1_rmse: 0.42512 |  0:03:18s
epoch 62 | loss: 0.18594 | val_0_rmse: 0.41521 | val_1_rmse: 0.42144 |  0:03:21s
epoch 63 | loss: 0.18715 | val_0_rmse: 0.42784 | val_1_rmse: 0.43055 |  0:03:25s
epoch 64 | loss: 0.1879  | val_0_rmse: 0.42386 | val_1_rmse: 0.42549 |  0:03:28s
epoch 65 | loss: 0.18917 | val_0_rmse: 0.42424 | val_1_rmse: 0.43281 |  0:03:31s
epoch 66 | loss: 0.18743 | val_0_rmse: 0.43271 | val_1_rmse: 0.43948 |  0:03:34s
epoch 67 | loss: 0.19469 | val_0_rmse: 0.44478 | val_1_rmse: 0.44676 |  0:03:37s
epoch 68 | loss: 0.18475 | val_0_rmse: 0.41775 | val_1_rmse: 0.42497 |  0:03:41s
epoch 69 | loss: 0.1862  | val_0_rmse: 0.42099 | val_1_rmse: 0.4231  |  0:03:44s
epoch 70 | loss: 0.18775 | val_0_rmse: 0.41878 | val_1_rmse: 0.42311 |  0:03:47s
epoch 71 | loss: 0.18356 | val_0_rmse: 0.40666 | val_1_rmse: 0.41423 |  0:03:50s
epoch 72 | loss: 0.18489 | val_0_rmse: 0.41274 | val_1_rmse: 0.42202 |  0:03:53s
epoch 73 | loss: 0.18423 | val_0_rmse: 0.45013 | val_1_rmse: 0.45239 |  0:03:57s
epoch 74 | loss: 0.18838 | val_0_rmse: 0.41844 | val_1_rmse: 0.42573 |  0:04:00s
epoch 75 | loss: 0.19121 | val_0_rmse: 0.40983 | val_1_rmse: 0.41473 |  0:04:03s
epoch 76 | loss: 0.18668 | val_0_rmse: 0.41973 | val_1_rmse: 0.42282 |  0:04:06s
epoch 77 | loss: 0.18597 | val_0_rmse: 0.41321 | val_1_rmse: 0.41991 |  0:04:09s
epoch 78 | loss: 0.18506 | val_0_rmse: 0.41697 | val_1_rmse: 0.41989 |  0:04:13s
epoch 79 | loss: 0.1821  | val_0_rmse: 0.40899 | val_1_rmse: 0.41476 |  0:04:16s
epoch 80 | loss: 0.18321 | val_0_rmse: 0.41441 | val_1_rmse: 0.41679 |  0:04:19s
epoch 81 | loss: 0.18353 | val_0_rmse: 0.41571 | val_1_rmse: 0.41963 |  0:04:22s
epoch 82 | loss: 0.18592 | val_0_rmse: 0.40611 | val_1_rmse: 0.41178 |  0:04:25s
epoch 83 | loss: 0.18266 | val_0_rmse: 0.41523 | val_1_rmse: 0.41947 |  0:04:29s
epoch 84 | loss: 0.18363 | val_0_rmse: 0.40868 | val_1_rmse: 0.41368 |  0:04:32s
epoch 85 | loss: 0.18343 | val_0_rmse: 0.40899 | val_1_rmse: 0.41405 |  0:04:35s
epoch 86 | loss: 0.18165 | val_0_rmse: 0.41559 | val_1_rmse: 0.41993 |  0:04:38s
epoch 87 | loss: 0.18752 | val_0_rmse: 0.42808 | val_1_rmse: 0.4322  |  0:04:41s
epoch 88 | loss: 0.18748 | val_0_rmse: 0.41176 | val_1_rmse: 0.41714 |  0:04:45s
epoch 89 | loss: 0.18233 | val_0_rmse: 0.40775 | val_1_rmse: 0.41244 |  0:04:48s
epoch 90 | loss: 0.18311 | val_0_rmse: 0.41608 | val_1_rmse: 0.42001 |  0:04:51s
epoch 91 | loss: 0.18056 | val_0_rmse: 0.4079  | val_1_rmse: 0.41697 |  0:04:54s
epoch 92 | loss: 0.18308 | val_0_rmse: 0.41991 | val_1_rmse: 0.42661 |  0:04:57s
epoch 93 | loss: 0.18302 | val_0_rmse: 0.42141 | val_1_rmse: 0.42712 |  0:05:01s
epoch 94 | loss: 0.18466 | val_0_rmse: 0.4198  | val_1_rmse: 0.4259  |  0:05:04s
epoch 95 | loss: 0.18287 | val_0_rmse: 0.4202  | val_1_rmse: 0.42676 |  0:05:07s
epoch 96 | loss: 0.18085 | val_0_rmse: 0.41451 | val_1_rmse: 0.41954 |  0:05:10s
epoch 97 | loss: 0.17844 | val_0_rmse: 0.41167 | val_1_rmse: 0.42072 |  0:05:13s
epoch 98 | loss: 0.18177 | val_0_rmse: 0.42675 | val_1_rmse: 0.43679 |  0:05:17s
epoch 99 | loss: 0.17923 | val_0_rmse: 0.42732 | val_1_rmse: 0.43023 |  0:05:20s
epoch 100| loss: 0.1821  | val_0_rmse: 0.4068  | val_1_rmse: 0.41582 |  0:05:23s
epoch 101| loss: 0.1828  | val_0_rmse: 0.40406 | val_1_rmse: 0.4097  |  0:05:26s
epoch 102| loss: 0.18011 | val_0_rmse: 0.41196 | val_1_rmse: 0.42198 |  0:05:29s
epoch 103| loss: 0.18062 | val_0_rmse: 0.41138 | val_1_rmse: 0.41936 |  0:05:33s
epoch 104| loss: 0.17781 | val_0_rmse: 0.41381 | val_1_rmse: 0.42266 |  0:05:36s
epoch 105| loss: 0.18208 | val_0_rmse: 0.42083 | val_1_rmse: 0.43124 |  0:05:39s
epoch 106| loss: 0.17956 | val_0_rmse: 0.40188 | val_1_rmse: 0.41046 |  0:05:42s
epoch 107| loss: 0.17915 | val_0_rmse: 0.42175 | val_1_rmse: 0.43244 |  0:05:45s
epoch 108| loss: 0.1813  | val_0_rmse: 0.41709 | val_1_rmse: 0.42078 |  0:05:49s
epoch 109| loss: 0.18042 | val_0_rmse: 0.40659 | val_1_rmse: 0.41396 |  0:05:52s
epoch 110| loss: 0.18057 | val_0_rmse: 0.41874 | val_1_rmse: 0.42759 |  0:05:55s
epoch 111| loss: 0.18593 | val_0_rmse: 0.42294 | val_1_rmse: 0.42667 |  0:05:58s
epoch 112| loss: 0.18198 | val_0_rmse: 0.40876 | val_1_rmse: 0.41605 |  0:06:01s
epoch 113| loss: 0.17926 | val_0_rmse: 0.40479 | val_1_rmse: 0.41156 |  0:06:05s
epoch 114| loss: 0.17935 | val_0_rmse: 0.43686 | val_1_rmse: 0.44576 |  0:06:08s
epoch 115| loss: 0.18278 | val_0_rmse: 0.43016 | val_1_rmse: 0.43417 |  0:06:11s
epoch 116| loss: 0.18699 | val_0_rmse: 0.40871 | val_1_rmse: 0.4123  |  0:06:14s
epoch 117| loss: 0.17856 | val_0_rmse: 0.42688 | val_1_rmse: 0.43446 |  0:06:18s
epoch 118| loss: 0.18046 | val_0_rmse: 0.40514 | val_1_rmse: 0.41433 |  0:06:21s
epoch 119| loss: 0.174   | val_0_rmse: 0.40218 | val_1_rmse: 0.40808 |  0:06:24s
epoch 120| loss: 0.17853 | val_0_rmse: 0.41554 | val_1_rmse: 0.4257  |  0:06:27s
epoch 121| loss: 0.17849 | val_0_rmse: 0.40533 | val_1_rmse: 0.41363 |  0:06:30s
epoch 122| loss: 0.18022 | val_0_rmse: 0.44545 | val_1_rmse: 0.45256 |  0:06:34s
epoch 123| loss: 0.18458 | val_0_rmse: 0.4347  | val_1_rmse: 0.43772 |  0:06:37s
epoch 124| loss: 0.18183 | val_0_rmse: 0.40314 | val_1_rmse: 0.41114 |  0:06:40s
epoch 125| loss: 0.17577 | val_0_rmse: 0.40895 | val_1_rmse: 0.41458 |  0:06:43s
epoch 126| loss: 0.17653 | val_0_rmse: 0.40348 | val_1_rmse: 0.4139  |  0:06:46s
epoch 127| loss: 0.178   | val_0_rmse: 0.4121  | val_1_rmse: 0.41831 |  0:06:50s
epoch 128| loss: 0.17886 | val_0_rmse: 0.40904 | val_1_rmse: 0.41487 |  0:06:53s
epoch 129| loss: 0.18257 | val_0_rmse: 0.42908 | val_1_rmse: 0.43419 |  0:06:56s
epoch 130| loss: 0.17904 | val_0_rmse: 0.40793 | val_1_rmse: 0.41655 |  0:06:59s
epoch 131| loss: 0.17725 | val_0_rmse: 0.40927 | val_1_rmse: 0.41683 |  0:07:02s
epoch 132| loss: 0.17541 | val_0_rmse: 0.41464 | val_1_rmse: 0.42553 |  0:07:06s
epoch 133| loss: 0.177   | val_0_rmse: 0.40144 | val_1_rmse: 0.41408 |  0:07:09s
epoch 134| loss: 0.17553 | val_0_rmse: 0.42135 | val_1_rmse: 0.42683 |  0:07:12s
epoch 135| loss: 0.17706 | val_0_rmse: 0.40364 | val_1_rmse: 0.41583 |  0:07:15s
epoch 136| loss: 0.17656 | val_0_rmse: 0.39636 | val_1_rmse: 0.40849 |  0:07:18s
epoch 137| loss: 0.17632 | val_0_rmse: 0.40923 | val_1_rmse: 0.417   |  0:07:22s
epoch 138| loss: 0.18055 | val_0_rmse: 0.41546 | val_1_rmse: 0.42266 |  0:07:25s
epoch 139| loss: 0.18222 | val_0_rmse: 0.4065  | val_1_rmse: 0.41645 |  0:07:28s
epoch 140| loss: 0.17404 | val_0_rmse: 0.40662 | val_1_rmse: 0.41502 |  0:07:31s
epoch 141| loss: 0.17888 | val_0_rmse: 0.40233 | val_1_rmse: 0.41211 |  0:07:34s
epoch 142| loss: 0.17463 | val_0_rmse: 0.40228 | val_1_rmse: 0.40886 |  0:07:38s
epoch 143| loss: 0.17755 | val_0_rmse: 0.42489 | val_1_rmse: 0.4332  |  0:07:41s
epoch 144| loss: 0.18427 | val_0_rmse: 0.41603 | val_1_rmse: 0.42405 |  0:07:44s
epoch 145| loss: 0.17843 | val_0_rmse: 0.40181 | val_1_rmse: 0.40744 |  0:07:47s
epoch 146| loss: 0.17641 | val_0_rmse: 0.41048 | val_1_rmse: 0.42204 |  0:07:50s
epoch 147| loss: 0.17459 | val_0_rmse: 0.4014  | val_1_rmse: 0.4107  |  0:07:54s
epoch 148| loss: 0.17533 | val_0_rmse: 0.40247 | val_1_rmse: 0.41108 |  0:07:57s
epoch 149| loss: 0.17709 | val_0_rmse: 0.40327 | val_1_rmse: 0.41216 |  0:08:00s
Stop training because you reached max_epochs = 150 with best_epoch = 145 and best_val_1_rmse = 0.40744
Best weights from best epoch are automatically used!
ended training at: 01:14:09
Feature importance:
[('Area', 0.040918896864216604), ('Baths', 0.2519643543412535), ('Beds', 0.11236357566646193), ('Latitude', 0.16476522820731276), ('Longitude', 0.1704578210408689), ('Month', 0.04908901387550474), ('Year', 0.21044111000438154)]
Mean squared error is of 10146160783.694763
Mean absolute error:69017.16183494533
MAPE:0.2831330340413047
R2 score:0.8250320498271113
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:14:09
epoch 0  | loss: 0.48249 | val_0_rmse: 0.60575 | val_1_rmse: 0.60433 |  0:00:03s
epoch 1  | loss: 0.32427 | val_0_rmse: 0.52798 | val_1_rmse: 0.52503 |  0:00:06s
epoch 2  | loss: 0.28914 | val_0_rmse: 0.51661 | val_1_rmse: 0.51284 |  0:00:09s
epoch 3  | loss: 0.27828 | val_0_rmse: 0.50913 | val_1_rmse: 0.50686 |  0:00:12s
epoch 4  | loss: 0.26942 | val_0_rmse: 0.48689 | val_1_rmse: 0.48486 |  0:00:16s
epoch 5  | loss: 0.2528  | val_0_rmse: 0.5044  | val_1_rmse: 0.49867 |  0:00:19s
epoch 6  | loss: 0.24899 | val_0_rmse: 0.49076 | val_1_rmse: 0.48923 |  0:00:22s
epoch 7  | loss: 0.24859 | val_0_rmse: 0.49965 | val_1_rmse: 0.49761 |  0:00:25s
epoch 8  | loss: 0.24458 | val_0_rmse: 0.47372 | val_1_rmse: 0.46579 |  0:00:28s
epoch 9  | loss: 0.24025 | val_0_rmse: 0.46204 | val_1_rmse: 0.46041 |  0:00:32s
epoch 10 | loss: 0.23139 | val_0_rmse: 0.46869 | val_1_rmse: 0.46335 |  0:00:35s
epoch 11 | loss: 0.23279 | val_0_rmse: 0.45773 | val_1_rmse: 0.45827 |  0:00:38s
epoch 12 | loss: 0.22559 | val_0_rmse: 0.45801 | val_1_rmse: 0.45421 |  0:00:41s
epoch 13 | loss: 0.22799 | val_0_rmse: 0.47408 | val_1_rmse: 0.47648 |  0:00:44s
epoch 14 | loss: 0.2191  | val_0_rmse: 0.45013 | val_1_rmse: 0.44808 |  0:00:47s
epoch 15 | loss: 0.21305 | val_0_rmse: 0.45272 | val_1_rmse: 0.44818 |  0:00:51s
epoch 16 | loss: 0.21599 | val_0_rmse: 0.44781 | val_1_rmse: 0.44559 |  0:00:54s
epoch 17 | loss: 0.21393 | val_0_rmse: 0.45746 | val_1_rmse: 0.45444 |  0:00:57s
epoch 18 | loss: 0.20826 | val_0_rmse: 0.44065 | val_1_rmse: 0.44188 |  0:01:00s
epoch 19 | loss: 0.21302 | val_0_rmse: 0.43765 | val_1_rmse: 0.43599 |  0:01:03s
epoch 20 | loss: 0.20549 | val_0_rmse: 0.47297 | val_1_rmse: 0.47243 |  0:01:07s
epoch 21 | loss: 0.20958 | val_0_rmse: 0.43881 | val_1_rmse: 0.43557 |  0:01:10s
epoch 22 | loss: 0.21064 | val_0_rmse: 0.43536 | val_1_rmse: 0.43797 |  0:01:13s
epoch 23 | loss: 0.20524 | val_0_rmse: 0.43351 | val_1_rmse: 0.43458 |  0:01:16s
epoch 24 | loss: 0.20391 | val_0_rmse: 0.43391 | val_1_rmse: 0.43553 |  0:01:20s
epoch 25 | loss: 0.20503 | val_0_rmse: 0.44931 | val_1_rmse: 0.45379 |  0:01:23s
epoch 26 | loss: 0.19952 | val_0_rmse: 0.43479 | val_1_rmse: 0.43454 |  0:01:26s
epoch 27 | loss: 0.19976 | val_0_rmse: 0.43499 | val_1_rmse: 0.43496 |  0:01:29s
epoch 28 | loss: 0.19778 | val_0_rmse: 0.42579 | val_1_rmse: 0.42736 |  0:01:32s
epoch 29 | loss: 0.20118 | val_0_rmse: 0.42903 | val_1_rmse: 0.42835 |  0:01:35s
epoch 30 | loss: 0.20123 | val_0_rmse: 0.43953 | val_1_rmse: 0.44211 |  0:01:39s
epoch 31 | loss: 0.20107 | val_0_rmse: 0.43958 | val_1_rmse: 0.44209 |  0:01:42s
epoch 32 | loss: 0.20064 | val_0_rmse: 0.44107 | val_1_rmse: 0.44039 |  0:01:45s
epoch 33 | loss: 0.20257 | val_0_rmse: 0.4343  | val_1_rmse: 0.43515 |  0:01:48s
epoch 34 | loss: 0.20272 | val_0_rmse: 0.42833 | val_1_rmse: 0.42984 |  0:01:51s
epoch 35 | loss: 0.19874 | val_0_rmse: 0.44625 | val_1_rmse: 0.44368 |  0:01:55s
epoch 36 | loss: 0.19865 | val_0_rmse: 0.43246 | val_1_rmse: 0.43444 |  0:01:58s
epoch 37 | loss: 0.19824 | val_0_rmse: 0.44127 | val_1_rmse: 0.44145 |  0:02:01s
epoch 38 | loss: 0.20415 | val_0_rmse: 0.44895 | val_1_rmse: 0.45129 |  0:02:04s
epoch 39 | loss: 0.19595 | val_0_rmse: 0.44907 | val_1_rmse: 0.45296 |  0:02:07s
epoch 40 | loss: 0.20113 | val_0_rmse: 0.43517 | val_1_rmse: 0.43522 |  0:02:11s
epoch 41 | loss: 0.20175 | val_0_rmse: 0.45109 | val_1_rmse: 0.45108 |  0:02:14s
epoch 42 | loss: 0.19673 | val_0_rmse: 0.43392 | val_1_rmse: 0.43567 |  0:02:17s
epoch 43 | loss: 0.19811 | val_0_rmse: 0.42696 | val_1_rmse: 0.42787 |  0:02:20s
epoch 44 | loss: 0.19863 | val_0_rmse: 0.45903 | val_1_rmse: 0.46423 |  0:02:23s
epoch 45 | loss: 0.19814 | val_0_rmse: 0.44671 | val_1_rmse: 0.44705 |  0:02:27s
epoch 46 | loss: 0.20233 | val_0_rmse: 0.42095 | val_1_rmse: 0.42294 |  0:02:30s
epoch 47 | loss: 0.19792 | val_0_rmse: 0.4378  | val_1_rmse: 0.44015 |  0:02:33s
epoch 48 | loss: 0.19563 | val_0_rmse: 0.43143 | val_1_rmse: 0.43167 |  0:02:36s
epoch 49 | loss: 0.19483 | val_0_rmse: 0.42027 | val_1_rmse: 0.4232  |  0:02:39s
epoch 50 | loss: 0.19358 | val_0_rmse: 0.43257 | val_1_rmse: 0.4353  |  0:02:42s
epoch 51 | loss: 0.19384 | val_0_rmse: 0.418   | val_1_rmse: 0.42064 |  0:02:46s
epoch 52 | loss: 0.1962  | val_0_rmse: 0.42252 | val_1_rmse: 0.42481 |  0:02:49s
epoch 53 | loss: 0.19715 | val_0_rmse: 0.44013 | val_1_rmse: 0.4433  |  0:02:52s
epoch 54 | loss: 0.19774 | val_0_rmse: 0.42818 | val_1_rmse: 0.43059 |  0:02:55s
epoch 55 | loss: 0.20033 | val_0_rmse: 0.42952 | val_1_rmse: 0.43302 |  0:02:58s
epoch 56 | loss: 0.19448 | val_0_rmse: 0.42556 | val_1_rmse: 0.42746 |  0:03:02s
epoch 57 | loss: 0.20079 | val_0_rmse: 0.43541 | val_1_rmse: 0.43809 |  0:03:05s
epoch 58 | loss: 0.19649 | val_0_rmse: 0.42988 | val_1_rmse: 0.43681 |  0:03:08s
epoch 59 | loss: 0.19405 | val_0_rmse: 0.41963 | val_1_rmse: 0.4243  |  0:03:11s
epoch 60 | loss: 0.19454 | val_0_rmse: 0.43752 | val_1_rmse: 0.44007 |  0:03:14s
epoch 61 | loss: 0.19343 | val_0_rmse: 0.41991 | val_1_rmse: 0.42238 |  0:03:18s
epoch 62 | loss: 0.19439 | val_0_rmse: 0.42285 | val_1_rmse: 0.42447 |  0:03:21s
epoch 63 | loss: 0.20048 | val_0_rmse: 0.42472 | val_1_rmse: 0.43024 |  0:03:24s
epoch 64 | loss: 0.19455 | val_0_rmse: 0.45429 | val_1_rmse: 0.45457 |  0:03:27s
epoch 65 | loss: 0.19283 | val_0_rmse: 0.43544 | val_1_rmse: 0.44085 |  0:03:30s
epoch 66 | loss: 0.18931 | val_0_rmse: 0.42741 | val_1_rmse: 0.43174 |  0:03:34s
epoch 67 | loss: 0.18871 | val_0_rmse: 0.43797 | val_1_rmse: 0.43844 |  0:03:37s
epoch 68 | loss: 0.19046 | val_0_rmse: 0.44399 | val_1_rmse: 0.44941 |  0:03:40s
epoch 69 | loss: 0.1954  | val_0_rmse: 0.43879 | val_1_rmse: 0.44419 |  0:03:43s
epoch 70 | loss: 0.19046 | val_0_rmse: 0.42657 | val_1_rmse: 0.4303  |  0:03:46s
epoch 71 | loss: 0.18977 | val_0_rmse: 0.43252 | val_1_rmse: 0.43744 |  0:03:50s
epoch 72 | loss: 0.19515 | val_0_rmse: 0.43592 | val_1_rmse: 0.4398  |  0:03:53s
epoch 73 | loss: 0.19334 | val_0_rmse: 0.4445  | val_1_rmse: 0.44994 |  0:03:56s
epoch 74 | loss: 0.19284 | val_0_rmse: 0.41589 | val_1_rmse: 0.42048 |  0:03:59s
epoch 75 | loss: 0.19103 | val_0_rmse: 0.43368 | val_1_rmse: 0.43454 |  0:04:02s
epoch 76 | loss: 0.18882 | val_0_rmse: 0.41578 | val_1_rmse: 0.41892 |  0:04:06s
epoch 77 | loss: 0.19141 | val_0_rmse: 0.43485 | val_1_rmse: 0.43937 |  0:04:09s
epoch 78 | loss: 0.18999 | val_0_rmse: 0.4576  | val_1_rmse: 0.45892 |  0:04:12s
epoch 79 | loss: 0.18866 | val_0_rmse: 0.41644 | val_1_rmse: 0.42233 |  0:04:15s
epoch 80 | loss: 0.19326 | val_0_rmse: 0.43067 | val_1_rmse: 0.43552 |  0:04:18s
epoch 81 | loss: 0.19069 | val_0_rmse: 0.44891 | val_1_rmse: 0.45436 |  0:04:21s
epoch 82 | loss: 0.19195 | val_0_rmse: 0.42978 | val_1_rmse: 0.43328 |  0:04:25s
epoch 83 | loss: 0.18998 | val_0_rmse: 0.42956 | val_1_rmse: 0.4351  |  0:04:28s
epoch 84 | loss: 0.19108 | val_0_rmse: 0.43094 | val_1_rmse: 0.43637 |  0:04:31s
epoch 85 | loss: 0.19531 | val_0_rmse: 0.42809 | val_1_rmse: 0.43079 |  0:04:34s
epoch 86 | loss: 0.21304 | val_0_rmse: 0.47734 | val_1_rmse: 0.47808 |  0:04:37s
epoch 87 | loss: 0.21317 | val_0_rmse: 0.52299 | val_1_rmse: 0.52448 |  0:04:41s
epoch 88 | loss: 0.21139 | val_0_rmse: 0.44486 | val_1_rmse: 0.44457 |  0:04:44s
epoch 89 | loss: 0.2123  | val_0_rmse: 0.44539 | val_1_rmse: 0.44465 |  0:04:47s
epoch 90 | loss: 0.2101  | val_0_rmse: 0.45975 | val_1_rmse: 0.4606  |  0:04:50s
epoch 91 | loss: 0.2037  | val_0_rmse: 0.46231 | val_1_rmse: 0.46319 |  0:04:53s
epoch 92 | loss: 0.20243 | val_0_rmse: 0.44085 | val_1_rmse: 0.44125 |  0:04:57s
epoch 93 | loss: 0.19908 | val_0_rmse: 0.4237  | val_1_rmse: 0.42321 |  0:05:00s
epoch 94 | loss: 0.19896 | val_0_rmse: 0.44744 | val_1_rmse: 0.44818 |  0:05:03s
epoch 95 | loss: 0.19826 | val_0_rmse: 0.4383  | val_1_rmse: 0.44168 |  0:05:06s
epoch 96 | loss: 0.2013  | val_0_rmse: 0.45856 | val_1_rmse: 0.45706 |  0:05:09s
epoch 97 | loss: 0.20077 | val_0_rmse: 0.44873 | val_1_rmse: 0.44642 |  0:05:13s
epoch 98 | loss: 0.19964 | val_0_rmse: 0.45382 | val_1_rmse: 0.45114 |  0:05:16s
epoch 99 | loss: 0.19788 | val_0_rmse: 0.45396 | val_1_rmse: 0.45374 |  0:05:19s
epoch 100| loss: 0.19699 | val_0_rmse: 0.43599 | val_1_rmse: 0.43675 |  0:05:22s
epoch 101| loss: 0.19582 | val_0_rmse: 0.47308 | val_1_rmse: 0.46991 |  0:05:25s
epoch 102| loss: 0.19691 | val_0_rmse: 0.41925 | val_1_rmse: 0.42359 |  0:05:29s
epoch 103| loss: 0.19314 | val_0_rmse: 0.42824 | val_1_rmse: 0.43117 |  0:05:32s
epoch 104| loss: 0.19294 | val_0_rmse: 0.43747 | val_1_rmse: 0.43575 |  0:05:35s
epoch 105| loss: 0.19543 | val_0_rmse: 0.4248  | val_1_rmse: 0.42529 |  0:05:38s
epoch 106| loss: 0.19548 | val_0_rmse: 0.46518 | val_1_rmse: 0.46903 |  0:05:41s

Early stopping occured at epoch 106 with best_epoch = 76 and best_val_1_rmse = 0.41892
Best weights from best epoch are automatically used!
ended training at: 01:19:52
Feature importance:
[('Area', 0.08950556325507887), ('Baths', 0.11129168659078086), ('Beds', 0.09731782111492732), ('Latitude', 0.2134322526023312), ('Longitude', 0.2747764889213866), ('Month', 0.04458519899732008), ('Year', 0.16909098851817506)]
Mean squared error is of 10125922996.512165
Mean absolute error:70224.07740912333
MAPE:0.29511034106306205
R2 score:0.8261601708064231
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:19:53
epoch 0  | loss: 0.46504 | val_0_rmse: 0.55429 | val_1_rmse: 0.55252 |  0:00:03s
epoch 1  | loss: 0.32659 | val_0_rmse: 0.54038 | val_1_rmse: 0.53336 |  0:00:06s
epoch 2  | loss: 0.28938 | val_0_rmse: 0.51015 | val_1_rmse: 0.49787 |  0:00:09s
epoch 3  | loss: 0.26732 | val_0_rmse: 0.49188 | val_1_rmse: 0.48638 |  0:00:12s
epoch 4  | loss: 0.25785 | val_0_rmse: 0.48242 | val_1_rmse: 0.47179 |  0:00:16s
epoch 5  | loss: 0.23999 | val_0_rmse: 0.46066 | val_1_rmse: 0.4511  |  0:00:19s
epoch 6  | loss: 0.23379 | val_0_rmse: 0.46687 | val_1_rmse: 0.45489 |  0:00:22s
epoch 7  | loss: 0.23804 | val_0_rmse: 0.49348 | val_1_rmse: 0.48401 |  0:00:25s
epoch 8  | loss: 0.236   | val_0_rmse: 0.48006 | val_1_rmse: 0.4723  |  0:00:28s
epoch 9  | loss: 0.2301  | val_0_rmse: 0.45146 | val_1_rmse: 0.44198 |  0:00:32s
epoch 10 | loss: 0.22149 | val_0_rmse: 0.44427 | val_1_rmse: 0.43595 |  0:00:35s
epoch 11 | loss: 0.21989 | val_0_rmse: 0.44474 | val_1_rmse: 0.43201 |  0:00:38s
epoch 12 | loss: 0.21418 | val_0_rmse: 0.4721  | val_1_rmse: 0.46276 |  0:00:41s
epoch 13 | loss: 0.21936 | val_0_rmse: 0.49082 | val_1_rmse: 0.48078 |  0:00:44s
epoch 14 | loss: 0.21674 | val_0_rmse: 0.43845 | val_1_rmse: 0.42778 |  0:00:48s
epoch 15 | loss: 0.22258 | val_0_rmse: 0.54688 | val_1_rmse: 0.5437  |  0:00:51s
epoch 16 | loss: 0.22916 | val_0_rmse: 0.4882  | val_1_rmse: 0.47779 |  0:00:54s
epoch 17 | loss: 0.22307 | val_0_rmse: 0.47571 | val_1_rmse: 0.47074 |  0:00:57s
epoch 18 | loss: 0.22188 | val_0_rmse: 0.45562 | val_1_rmse: 0.44495 |  0:01:01s
epoch 19 | loss: 0.22102 | val_0_rmse: 0.44902 | val_1_rmse: 0.43836 |  0:01:04s
epoch 20 | loss: 0.21715 | val_0_rmse: 0.46553 | val_1_rmse: 0.45408 |  0:01:07s
epoch 21 | loss: 0.21701 | val_0_rmse: 0.45811 | val_1_rmse: 0.44664 |  0:01:10s
epoch 22 | loss: 0.21277 | val_0_rmse: 0.45618 | val_1_rmse: 0.44368 |  0:01:13s
epoch 23 | loss: 0.21261 | val_0_rmse: 0.46432 | val_1_rmse: 0.4546  |  0:01:17s
epoch 24 | loss: 0.21192 | val_0_rmse: 0.43458 | val_1_rmse: 0.42302 |  0:01:20s
epoch 25 | loss: 0.20902 | val_0_rmse: 0.45914 | val_1_rmse: 0.44867 |  0:01:23s
epoch 26 | loss: 0.21021 | val_0_rmse: 0.43596 | val_1_rmse: 0.42293 |  0:01:26s
epoch 27 | loss: 0.20518 | val_0_rmse: 0.44565 | val_1_rmse: 0.43888 |  0:01:29s
epoch 28 | loss: 0.20685 | val_0_rmse: 0.441   | val_1_rmse: 0.43149 |  0:01:33s
epoch 29 | loss: 0.20779 | val_0_rmse: 0.44813 | val_1_rmse: 0.44176 |  0:01:36s
epoch 30 | loss: 0.20703 | val_0_rmse: 0.4408  | val_1_rmse: 0.43128 |  0:01:39s
epoch 31 | loss: 0.20144 | val_0_rmse: 0.43279 | val_1_rmse: 0.42472 |  0:01:42s
epoch 32 | loss: 0.20393 | val_0_rmse: 0.4279  | val_1_rmse: 0.42208 |  0:01:45s
epoch 33 | loss: 0.19916 | val_0_rmse: 0.46052 | val_1_rmse: 0.45863 |  0:01:49s
epoch 34 | loss: 0.19921 | val_0_rmse: 0.50604 | val_1_rmse: 0.50201 |  0:01:52s
epoch 35 | loss: 0.20895 | val_0_rmse: 0.42695 | val_1_rmse: 0.4179  |  0:01:55s
epoch 36 | loss: 0.19896 | val_0_rmse: 0.43128 | val_1_rmse: 0.4248  |  0:01:58s
epoch 37 | loss: 0.19766 | val_0_rmse: 0.50059 | val_1_rmse: 0.4934  |  0:02:01s
epoch 38 | loss: 0.1981  | val_0_rmse: 0.42988 | val_1_rmse: 0.42466 |  0:02:05s
epoch 39 | loss: 0.20209 | val_0_rmse: 0.44137 | val_1_rmse: 0.43743 |  0:02:08s
epoch 40 | loss: 0.19747 | val_0_rmse: 0.43804 | val_1_rmse: 0.43145 |  0:02:11s
epoch 41 | loss: 0.19684 | val_0_rmse: 0.44144 | val_1_rmse: 0.43349 |  0:02:14s
epoch 42 | loss: 0.19731 | val_0_rmse: 0.42531 | val_1_rmse: 0.4183  |  0:02:17s
epoch 43 | loss: 0.19647 | val_0_rmse: 0.43924 | val_1_rmse: 0.4303  |  0:02:21s
epoch 44 | loss: 0.20643 | val_0_rmse: 0.43746 | val_1_rmse: 0.42731 |  0:02:24s
epoch 45 | loss: 0.20614 | val_0_rmse: 0.45399 | val_1_rmse: 0.44781 |  0:02:27s
epoch 46 | loss: 0.20232 | val_0_rmse: 0.45046 | val_1_rmse: 0.44145 |  0:02:30s
epoch 47 | loss: 0.20816 | val_0_rmse: 0.4532  | val_1_rmse: 0.44944 |  0:02:33s
epoch 48 | loss: 0.1979  | val_0_rmse: 0.431   | val_1_rmse: 0.42705 |  0:02:37s
epoch 49 | loss: 0.19337 | val_0_rmse: 0.4373  | val_1_rmse: 0.42973 |  0:02:40s
epoch 50 | loss: 0.20322 | val_0_rmse: 0.43959 | val_1_rmse: 0.43075 |  0:02:43s
epoch 51 | loss: 0.20564 | val_0_rmse: 0.44687 | val_1_rmse: 0.43637 |  0:02:46s
epoch 52 | loss: 0.20186 | val_0_rmse: 0.44217 | val_1_rmse: 0.4325  |  0:02:49s
epoch 53 | loss: 0.20721 | val_0_rmse: 0.44185 | val_1_rmse: 0.43235 |  0:02:53s
epoch 54 | loss: 0.20993 | val_0_rmse: 0.44562 | val_1_rmse: 0.44053 |  0:02:56s
epoch 55 | loss: 0.20554 | val_0_rmse: 0.45742 | val_1_rmse: 0.4561  |  0:02:59s
epoch 56 | loss: 0.20039 | val_0_rmse: 0.42797 | val_1_rmse: 0.42431 |  0:03:02s
epoch 57 | loss: 0.20213 | val_0_rmse: 0.44258 | val_1_rmse: 0.43479 |  0:03:05s
epoch 58 | loss: 0.19998 | val_0_rmse: 0.43289 | val_1_rmse: 0.42549 |  0:03:08s
epoch 59 | loss: 0.20429 | val_0_rmse: 0.43786 | val_1_rmse: 0.42957 |  0:03:12s
epoch 60 | loss: 0.19819 | val_0_rmse: 0.43856 | val_1_rmse: 0.43471 |  0:03:15s
epoch 61 | loss: 0.19949 | val_0_rmse: 0.43581 | val_1_rmse: 0.43166 |  0:03:18s
epoch 62 | loss: 0.19676 | val_0_rmse: 0.43111 | val_1_rmse: 0.42405 |  0:03:21s
epoch 63 | loss: 0.19883 | val_0_rmse: 0.44381 | val_1_rmse: 0.43517 |  0:03:25s
epoch 64 | loss: 0.20329 | val_0_rmse: 0.44283 | val_1_rmse: 0.43974 |  0:03:28s
epoch 65 | loss: 0.1959  | val_0_rmse: 0.42809 | val_1_rmse: 0.42163 |  0:03:31s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.4179
Best weights from best epoch are automatically used!
ended training at: 01:23:25
Feature importance:
[('Area', 0.0), ('Baths', 0.16912587986765648), ('Beds', 0.10602259823739701), ('Latitude', 0.13477394858202443), ('Longitude', 0.3174477640556261), ('Month', 0.016791079954548763), ('Year', 0.25583872930274726)]
Mean squared error is of 10796651361.911453
Mean absolute error:72075.21092915125
MAPE:0.2905697091883209
R2 score:0.8163031257783944
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:23:25
epoch 0  | loss: 0.43636 | val_0_rmse: 0.55658 | val_1_rmse: 0.55198 |  0:00:03s
epoch 1  | loss: 0.27875 | val_0_rmse: 0.49102 | val_1_rmse: 0.48654 |  0:00:06s
epoch 2  | loss: 0.25268 | val_0_rmse: 0.47454 | val_1_rmse: 0.46847 |  0:00:09s
epoch 3  | loss: 0.24375 | val_0_rmse: 0.47509 | val_1_rmse: 0.46904 |  0:00:12s
epoch 4  | loss: 0.22902 | val_0_rmse: 0.48598 | val_1_rmse: 0.47742 |  0:00:16s
epoch 5  | loss: 0.22972 | val_0_rmse: 0.45276 | val_1_rmse: 0.44449 |  0:00:19s
epoch 6  | loss: 0.22689 | val_0_rmse: 0.46691 | val_1_rmse: 0.45766 |  0:00:22s
epoch 7  | loss: 0.22184 | val_0_rmse: 0.45356 | val_1_rmse: 0.44544 |  0:00:25s
epoch 8  | loss: 0.21636 | val_0_rmse: 0.45129 | val_1_rmse: 0.44373 |  0:00:28s
epoch 9  | loss: 0.21596 | val_0_rmse: 0.44884 | val_1_rmse: 0.44259 |  0:00:32s
epoch 10 | loss: 0.21536 | val_0_rmse: 0.4597  | val_1_rmse: 0.45097 |  0:00:35s
epoch 11 | loss: 0.21498 | val_0_rmse: 0.44779 | val_1_rmse: 0.43981 |  0:00:38s
epoch 12 | loss: 0.21264 | val_0_rmse: 0.44481 | val_1_rmse: 0.439   |  0:00:41s
epoch 13 | loss: 0.21045 | val_0_rmse: 0.44015 | val_1_rmse: 0.43278 |  0:00:44s
epoch 14 | loss: 0.20491 | val_0_rmse: 0.44234 | val_1_rmse: 0.4355  |  0:00:48s
epoch 15 | loss: 0.21054 | val_0_rmse: 0.43247 | val_1_rmse: 0.42834 |  0:00:51s
epoch 16 | loss: 0.20464 | val_0_rmse: 0.4389  | val_1_rmse: 0.43113 |  0:00:54s
epoch 17 | loss: 0.20844 | val_0_rmse: 0.44182 | val_1_rmse: 0.43438 |  0:00:57s
epoch 18 | loss: 0.20513 | val_0_rmse: 0.43643 | val_1_rmse: 0.43103 |  0:01:00s
epoch 19 | loss: 0.20704 | val_0_rmse: 0.42735 | val_1_rmse: 0.42251 |  0:01:04s
epoch 20 | loss: 0.20336 | val_0_rmse: 0.42995 | val_1_rmse: 0.42268 |  0:01:07s
epoch 21 | loss: 0.20686 | val_0_rmse: 0.43858 | val_1_rmse: 0.43298 |  0:01:10s
epoch 22 | loss: 0.20405 | val_0_rmse: 0.43192 | val_1_rmse: 0.42575 |  0:01:13s
epoch 23 | loss: 0.20328 | val_0_rmse: 0.4578  | val_1_rmse: 0.45171 |  0:01:16s
epoch 24 | loss: 0.20252 | val_0_rmse: 0.4344  | val_1_rmse: 0.43052 |  0:01:20s
epoch 25 | loss: 0.20446 | val_0_rmse: 0.42738 | val_1_rmse: 0.42256 |  0:01:23s
epoch 26 | loss: 0.199   | val_0_rmse: 0.42789 | val_1_rmse: 0.42297 |  0:01:26s
epoch 27 | loss: 0.20273 | val_0_rmse: 0.44659 | val_1_rmse: 0.43893 |  0:01:29s
epoch 28 | loss: 0.19875 | val_0_rmse: 0.4344  | val_1_rmse: 0.42813 |  0:01:32s
epoch 29 | loss: 0.20118 | val_0_rmse: 0.43567 | val_1_rmse: 0.43232 |  0:01:35s
epoch 30 | loss: 0.19954 | val_0_rmse: 0.43969 | val_1_rmse: 0.43615 |  0:01:39s
epoch 31 | loss: 0.20006 | val_0_rmse: 0.44711 | val_1_rmse: 0.44097 |  0:01:42s
epoch 32 | loss: 0.20224 | val_0_rmse: 0.42692 | val_1_rmse: 0.41878 |  0:01:45s
epoch 33 | loss: 0.19962 | val_0_rmse: 0.42398 | val_1_rmse: 0.41802 |  0:01:48s
epoch 34 | loss: 0.19917 | val_0_rmse: 0.42554 | val_1_rmse: 0.41905 |  0:01:52s
epoch 35 | loss: 0.19586 | val_0_rmse: 0.42022 | val_1_rmse: 0.41886 |  0:01:55s
epoch 36 | loss: 0.19736 | val_0_rmse: 0.42035 | val_1_rmse: 0.41422 |  0:01:58s
epoch 37 | loss: 0.19871 | val_0_rmse: 0.42381 | val_1_rmse: 0.42073 |  0:02:01s
epoch 38 | loss: 0.1949  | val_0_rmse: 0.43182 | val_1_rmse: 0.42458 |  0:02:04s
epoch 39 | loss: 0.19574 | val_0_rmse: 0.42487 | val_1_rmse: 0.41915 |  0:02:07s
epoch 40 | loss: 0.19699 | val_0_rmse: 0.41825 | val_1_rmse: 0.41417 |  0:02:11s
epoch 41 | loss: 0.20137 | val_0_rmse: 0.44508 | val_1_rmse: 0.43932 |  0:02:14s
epoch 42 | loss: 0.1996  | val_0_rmse: 0.42626 | val_1_rmse: 0.41888 |  0:02:17s
epoch 43 | loss: 0.19601 | val_0_rmse: 0.41989 | val_1_rmse: 0.41419 |  0:02:20s
epoch 44 | loss: 0.19596 | val_0_rmse: 0.41868 | val_1_rmse: 0.41514 |  0:02:23s
epoch 45 | loss: 0.19294 | val_0_rmse: 0.41934 | val_1_rmse: 0.41714 |  0:02:27s
epoch 46 | loss: 0.19274 | val_0_rmse: 0.41736 | val_1_rmse: 0.41438 |  0:02:30s
epoch 47 | loss: 0.195   | val_0_rmse: 0.42941 | val_1_rmse: 0.42456 |  0:02:33s
epoch 48 | loss: 0.19087 | val_0_rmse: 0.41926 | val_1_rmse: 0.41941 |  0:02:36s
epoch 49 | loss: 0.19067 | val_0_rmse: 0.41859 | val_1_rmse: 0.41639 |  0:02:39s
epoch 50 | loss: 0.21867 | val_0_rmse: 0.43573 | val_1_rmse: 0.43046 |  0:02:43s
epoch 51 | loss: 0.20488 | val_0_rmse: 0.42889 | val_1_rmse: 0.42368 |  0:02:46s
epoch 52 | loss: 0.20184 | val_0_rmse: 0.44003 | val_1_rmse: 0.43729 |  0:02:49s
epoch 53 | loss: 0.19702 | val_0_rmse: 0.42306 | val_1_rmse: 0.41864 |  0:02:52s
epoch 54 | loss: 0.20037 | val_0_rmse: 0.42084 | val_1_rmse: 0.41672 |  0:02:55s
epoch 55 | loss: 0.19719 | val_0_rmse: 0.42633 | val_1_rmse: 0.42281 |  0:02:58s
epoch 56 | loss: 0.20177 | val_0_rmse: 0.47586 | val_1_rmse: 0.47016 |  0:03:02s
epoch 57 | loss: 0.20797 | val_0_rmse: 0.44363 | val_1_rmse: 0.44075 |  0:03:05s
epoch 58 | loss: 0.19752 | val_0_rmse: 0.41851 | val_1_rmse: 0.41637 |  0:03:08s
epoch 59 | loss: 0.19579 | val_0_rmse: 0.42464 | val_1_rmse: 0.42039 |  0:03:11s
epoch 60 | loss: 0.19204 | val_0_rmse: 0.41731 | val_1_rmse: 0.41573 |  0:03:14s
epoch 61 | loss: 0.19312 | val_0_rmse: 0.44644 | val_1_rmse: 0.44578 |  0:03:18s
epoch 62 | loss: 0.19588 | val_0_rmse: 0.41677 | val_1_rmse: 0.4145  |  0:03:21s
epoch 63 | loss: 0.19221 | val_0_rmse: 0.44033 | val_1_rmse: 0.43821 |  0:03:24s
epoch 64 | loss: 0.19167 | val_0_rmse: 0.4122  | val_1_rmse: 0.40976 |  0:03:27s
epoch 65 | loss: 0.18942 | val_0_rmse: 0.41497 | val_1_rmse: 0.41394 |  0:03:30s
epoch 66 | loss: 0.18799 | val_0_rmse: 0.41288 | val_1_rmse: 0.41026 |  0:03:33s
epoch 67 | loss: 0.18657 | val_0_rmse: 0.41427 | val_1_rmse: 0.41429 |  0:03:37s
epoch 68 | loss: 0.19015 | val_0_rmse: 0.42652 | val_1_rmse: 0.42524 |  0:03:40s
epoch 69 | loss: 0.18643 | val_0_rmse: 0.41662 | val_1_rmse: 0.41774 |  0:03:43s
epoch 70 | loss: 0.1871  | val_0_rmse: 0.41273 | val_1_rmse: 0.41548 |  0:03:46s
epoch 71 | loss: 0.18706 | val_0_rmse: 0.42167 | val_1_rmse: 0.41935 |  0:03:49s
epoch 72 | loss: 0.18405 | val_0_rmse: 0.41115 | val_1_rmse: 0.41071 |  0:03:53s
epoch 73 | loss: 0.18732 | val_0_rmse: 0.411   | val_1_rmse: 0.41119 |  0:03:56s
epoch 74 | loss: 0.18485 | val_0_rmse: 0.42503 | val_1_rmse: 0.42494 |  0:03:59s
epoch 75 | loss: 0.18756 | val_0_rmse: 0.4075  | val_1_rmse: 0.40773 |  0:04:02s
epoch 76 | loss: 0.18303 | val_0_rmse: 0.42473 | val_1_rmse: 0.42411 |  0:04:06s
epoch 77 | loss: 0.18957 | val_0_rmse: 0.41667 | val_1_rmse: 0.41786 |  0:04:09s
epoch 78 | loss: 0.18352 | val_0_rmse: 0.42179 | val_1_rmse: 0.4192  |  0:04:12s
epoch 79 | loss: 0.18341 | val_0_rmse: 0.41522 | val_1_rmse: 0.41421 |  0:04:15s
epoch 80 | loss: 0.18559 | val_0_rmse: 0.418   | val_1_rmse: 0.41677 |  0:04:18s
epoch 81 | loss: 0.18457 | val_0_rmse: 0.42175 | val_1_rmse: 0.42073 |  0:04:22s
epoch 82 | loss: 0.18395 | val_0_rmse: 0.40765 | val_1_rmse: 0.40936 |  0:04:25s
epoch 83 | loss: 0.18311 | val_0_rmse: 0.40983 | val_1_rmse: 0.40778 |  0:04:28s
epoch 84 | loss: 0.18904 | val_0_rmse: 0.41838 | val_1_rmse: 0.41958 |  0:04:31s
epoch 85 | loss: 0.18445 | val_0_rmse: 0.40208 | val_1_rmse: 0.40245 |  0:04:34s
epoch 86 | loss: 0.1867  | val_0_rmse: 0.42123 | val_1_rmse: 0.42249 |  0:04:37s
epoch 87 | loss: 0.18482 | val_0_rmse: 0.40953 | val_1_rmse: 0.41136 |  0:04:41s
epoch 88 | loss: 0.18214 | val_0_rmse: 0.40489 | val_1_rmse: 0.40464 |  0:04:44s
epoch 89 | loss: 0.18143 | val_0_rmse: 0.40482 | val_1_rmse: 0.40604 |  0:04:47s
epoch 90 | loss: 0.18337 | val_0_rmse: 0.41402 | val_1_rmse: 0.41511 |  0:04:50s
epoch 91 | loss: 0.18153 | val_0_rmse: 0.40637 | val_1_rmse: 0.40707 |  0:04:53s
epoch 92 | loss: 0.18059 | val_0_rmse: 0.41172 | val_1_rmse: 0.41511 |  0:04:57s
epoch 93 | loss: 0.18291 | val_0_rmse: 0.40005 | val_1_rmse: 0.40244 |  0:05:00s
epoch 94 | loss: 0.17941 | val_0_rmse: 0.40569 | val_1_rmse: 0.40939 |  0:05:03s
epoch 95 | loss: 0.18269 | val_0_rmse: 0.40517 | val_1_rmse: 0.40745 |  0:05:06s
epoch 96 | loss: 0.18151 | val_0_rmse: 0.42055 | val_1_rmse: 0.4239  |  0:05:09s
epoch 97 | loss: 0.182   | val_0_rmse: 0.41244 | val_1_rmse: 0.41854 |  0:05:13s
epoch 98 | loss: 0.18081 | val_0_rmse: 0.41035 | val_1_rmse: 0.41007 |  0:05:16s
epoch 99 | loss: 0.17948 | val_0_rmse: 0.40312 | val_1_rmse: 0.40589 |  0:05:19s
epoch 100| loss: 0.18102 | val_0_rmse: 0.40407 | val_1_rmse: 0.40667 |  0:05:22s
epoch 101| loss: 0.17922 | val_0_rmse: 0.41717 | val_1_rmse: 0.42182 |  0:05:25s
epoch 102| loss: 0.17994 | val_0_rmse: 0.4053  | val_1_rmse: 0.40812 |  0:05:29s
epoch 103| loss: 0.17818 | val_0_rmse: 0.4089  | val_1_rmse: 0.41264 |  0:05:32s
epoch 104| loss: 0.17962 | val_0_rmse: 0.39847 | val_1_rmse: 0.40239 |  0:05:35s
epoch 105| loss: 0.18215 | val_0_rmse: 0.40454 | val_1_rmse: 0.40873 |  0:05:38s
epoch 106| loss: 0.18046 | val_0_rmse: 0.46771 | val_1_rmse: 0.46531 |  0:05:41s
epoch 107| loss: 0.18243 | val_0_rmse: 0.41199 | val_1_rmse: 0.41383 |  0:05:44s
epoch 108| loss: 0.18241 | val_0_rmse: 0.40188 | val_1_rmse: 0.40502 |  0:05:48s
epoch 109| loss: 0.17805 | val_0_rmse: 0.40119 | val_1_rmse: 0.4065  |  0:05:51s
epoch 110| loss: 0.17763 | val_0_rmse: 0.40021 | val_1_rmse: 0.4052  |  0:05:54s
epoch 111| loss: 0.17616 | val_0_rmse: 0.42635 | val_1_rmse: 0.43081 |  0:05:57s
epoch 112| loss: 0.1806  | val_0_rmse: 0.40291 | val_1_rmse: 0.40914 |  0:06:00s
epoch 113| loss: 0.17844 | val_0_rmse: 0.41228 | val_1_rmse: 0.41771 |  0:06:04s
epoch 114| loss: 0.17766 | val_0_rmse: 0.40347 | val_1_rmse: 0.40804 |  0:06:07s
epoch 115| loss: 0.1805  | val_0_rmse: 0.4072  | val_1_rmse: 0.40987 |  0:06:10s
epoch 116| loss: 0.1838  | val_0_rmse: 0.41219 | val_1_rmse: 0.41458 |  0:06:13s
epoch 117| loss: 0.18284 | val_0_rmse: 0.42867 | val_1_rmse: 0.43147 |  0:06:16s
epoch 118| loss: 0.17965 | val_0_rmse: 0.42363 | val_1_rmse: 0.42578 |  0:06:20s
epoch 119| loss: 0.17748 | val_0_rmse: 0.41192 | val_1_rmse: 0.4182  |  0:06:23s
epoch 120| loss: 0.1776  | val_0_rmse: 0.41089 | val_1_rmse: 0.41389 |  0:06:26s
epoch 121| loss: 0.18149 | val_0_rmse: 0.40005 | val_1_rmse: 0.40534 |  0:06:29s
epoch 122| loss: 0.17867 | val_0_rmse: 0.40211 | val_1_rmse: 0.40771 |  0:06:32s
epoch 123| loss: 0.17815 | val_0_rmse: 0.40615 | val_1_rmse: 0.41314 |  0:06:36s
epoch 124| loss: 0.1798  | val_0_rmse: 0.41188 | val_1_rmse: 0.41786 |  0:06:39s
epoch 125| loss: 0.17759 | val_0_rmse: 0.3998  | val_1_rmse: 0.40453 |  0:06:42s
epoch 126| loss: 0.17433 | val_0_rmse: 0.40285 | val_1_rmse: 0.40952 |  0:06:45s
epoch 127| loss: 0.17866 | val_0_rmse: 0.41656 | val_1_rmse: 0.42117 |  0:06:48s
epoch 128| loss: 0.17803 | val_0_rmse: 0.42541 | val_1_rmse: 0.42746 |  0:06:52s
epoch 129| loss: 0.1789  | val_0_rmse: 0.40352 | val_1_rmse: 0.4101  |  0:06:55s
epoch 130| loss: 0.17717 | val_0_rmse: 0.40362 | val_1_rmse: 0.40715 |  0:06:58s
epoch 131| loss: 0.17804 | val_0_rmse: 0.40727 | val_1_rmse: 0.41047 |  0:07:01s
epoch 132| loss: 0.17612 | val_0_rmse: 0.4042  | val_1_rmse: 0.40753 |  0:07:04s
epoch 133| loss: 0.17625 | val_0_rmse: 0.4039  | val_1_rmse: 0.40912 |  0:07:08s
epoch 134| loss: 0.17496 | val_0_rmse: 0.42146 | val_1_rmse: 0.42806 |  0:07:11s

Early stopping occured at epoch 134 with best_epoch = 104 and best_val_1_rmse = 0.40239
Best weights from best epoch are automatically used!
ended training at: 01:30:38
Feature importance:
[('Area', 0.011561278374068583), ('Baths', 0.16939911557037815), ('Beds', 0.05126336835353608), ('Latitude', 0.09662892703011701), ('Longitude', 0.2541695025269944), ('Month', 0.15897781770300065), ('Year', 0.25799999044190514)]
Mean squared error is of 9771368368.652245
Mean absolute error:67426.8410940412
MAPE:0.27448623036247444
R2 score:0.8291795612159971
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:30:38
epoch 0  | loss: 0.71374 | val_0_rmse: 0.83051 | val_1_rmse: 0.84219 |  0:00:00s
epoch 1  | loss: 0.40183 | val_0_rmse: 0.60759 | val_1_rmse: 0.61975 |  0:00:01s
epoch 2  | loss: 0.34078 | val_0_rmse: 0.57984 | val_1_rmse: 0.60604 |  0:00:02s
epoch 3  | loss: 0.31539 | val_0_rmse: 0.55366 | val_1_rmse: 0.57153 |  0:00:03s
epoch 4  | loss: 0.30566 | val_0_rmse: 0.54912 | val_1_rmse: 0.56918 |  0:00:04s
epoch 5  | loss: 0.30286 | val_0_rmse: 0.52751 | val_1_rmse: 0.54547 |  0:00:05s
epoch 6  | loss: 0.29566 | val_0_rmse: 0.5323  | val_1_rmse: 0.54912 |  0:00:06s
epoch 7  | loss: 0.29256 | val_0_rmse: 0.52747 | val_1_rmse: 0.54519 |  0:00:07s
epoch 8  | loss: 0.29513 | val_0_rmse: 0.53091 | val_1_rmse: 0.54314 |  0:00:08s
epoch 9  | loss: 0.29532 | val_0_rmse: 0.54396 | val_1_rmse: 0.55889 |  0:00:09s
epoch 10 | loss: 0.28951 | val_0_rmse: 0.52262 | val_1_rmse: 0.54113 |  0:00:10s
epoch 11 | loss: 0.28348 | val_0_rmse: 0.52845 | val_1_rmse: 0.54208 |  0:00:11s
epoch 12 | loss: 0.27871 | val_0_rmse: 0.51541 | val_1_rmse: 0.53379 |  0:00:12s
epoch 13 | loss: 0.26848 | val_0_rmse: 0.51013 | val_1_rmse: 0.52883 |  0:00:13s
epoch 14 | loss: 0.27849 | val_0_rmse: 0.50967 | val_1_rmse: 0.53035 |  0:00:14s
epoch 15 | loss: 0.27589 | val_0_rmse: 0.50862 | val_1_rmse: 0.52533 |  0:00:15s
epoch 16 | loss: 0.26985 | val_0_rmse: 0.50746 | val_1_rmse: 0.53084 |  0:00:16s
epoch 17 | loss: 0.26365 | val_0_rmse: 0.50074 | val_1_rmse: 0.52357 |  0:00:17s
epoch 18 | loss: 0.2614  | val_0_rmse: 0.50467 | val_1_rmse: 0.52443 |  0:00:18s
epoch 19 | loss: 0.26233 | val_0_rmse: 0.5071  | val_1_rmse: 0.52685 |  0:00:19s
epoch 20 | loss: 0.2637  | val_0_rmse: 0.50235 | val_1_rmse: 0.52474 |  0:00:20s
epoch 21 | loss: 0.25716 | val_0_rmse: 0.50167 | val_1_rmse: 0.52279 |  0:00:21s
epoch 22 | loss: 0.25651 | val_0_rmse: 0.49481 | val_1_rmse: 0.51565 |  0:00:22s
epoch 23 | loss: 0.25855 | val_0_rmse: 0.49611 | val_1_rmse: 0.51997 |  0:00:22s
epoch 24 | loss: 0.2664  | val_0_rmse: 0.49167 | val_1_rmse: 0.52    |  0:00:23s
epoch 25 | loss: 0.25734 | val_0_rmse: 0.49925 | val_1_rmse: 0.51822 |  0:00:24s
epoch 26 | loss: 0.2522  | val_0_rmse: 0.52237 | val_1_rmse: 0.54333 |  0:00:25s
epoch 27 | loss: 0.24878 | val_0_rmse: 0.48935 | val_1_rmse: 0.51148 |  0:00:26s
epoch 28 | loss: 0.24332 | val_0_rmse: 0.48024 | val_1_rmse: 0.50618 |  0:00:27s
epoch 29 | loss: 0.24242 | val_0_rmse: 0.4702  | val_1_rmse: 0.49443 |  0:00:28s
epoch 30 | loss: 0.23868 | val_0_rmse: 0.47656 | val_1_rmse: 0.49831 |  0:00:29s
epoch 31 | loss: 0.244   | val_0_rmse: 0.48167 | val_1_rmse: 0.50925 |  0:00:30s
epoch 32 | loss: 0.24538 | val_0_rmse: 0.48074 | val_1_rmse: 0.50561 |  0:00:31s
epoch 33 | loss: 0.23496 | val_0_rmse: 0.47496 | val_1_rmse: 0.50037 |  0:00:32s
epoch 34 | loss: 0.23482 | val_0_rmse: 0.46463 | val_1_rmse: 0.48806 |  0:00:33s
epoch 35 | loss: 0.23388 | val_0_rmse: 0.47209 | val_1_rmse: 0.496   |  0:00:34s
epoch 36 | loss: 0.23662 | val_0_rmse: 0.47555 | val_1_rmse: 0.49552 |  0:00:35s
epoch 37 | loss: 0.23479 | val_0_rmse: 0.46549 | val_1_rmse: 0.48999 |  0:00:36s
epoch 38 | loss: 0.2347  | val_0_rmse: 0.48414 | val_1_rmse: 0.50659 |  0:00:37s
epoch 39 | loss: 0.24661 | val_0_rmse: 0.4727  | val_1_rmse: 0.49895 |  0:00:38s
epoch 40 | loss: 0.23537 | val_0_rmse: 0.47075 | val_1_rmse: 0.498   |  0:00:39s
epoch 41 | loss: 0.2254  | val_0_rmse: 0.47279 | val_1_rmse: 0.49957 |  0:00:40s
epoch 42 | loss: 0.2374  | val_0_rmse: 0.48011 | val_1_rmse: 0.51007 |  0:00:40s
epoch 43 | loss: 0.23444 | val_0_rmse: 0.46757 | val_1_rmse: 0.4943  |  0:00:41s
epoch 44 | loss: 0.2305  | val_0_rmse: 0.47403 | val_1_rmse: 0.49771 |  0:00:42s
epoch 45 | loss: 0.22019 | val_0_rmse: 0.45644 | val_1_rmse: 0.48488 |  0:00:43s
epoch 46 | loss: 0.22726 | val_0_rmse: 0.47961 | val_1_rmse: 0.51084 |  0:00:44s
epoch 47 | loss: 0.23481 | val_0_rmse: 0.46225 | val_1_rmse: 0.49439 |  0:00:45s
epoch 48 | loss: 0.22562 | val_0_rmse: 0.46807 | val_1_rmse: 0.49567 |  0:00:46s
epoch 49 | loss: 0.23301 | val_0_rmse: 0.48033 | val_1_rmse: 0.50419 |  0:00:47s
epoch 50 | loss: 0.22645 | val_0_rmse: 0.46579 | val_1_rmse: 0.49145 |  0:00:48s
epoch 51 | loss: 0.2237  | val_0_rmse: 0.45074 | val_1_rmse: 0.47691 |  0:00:49s
epoch 52 | loss: 0.221   | val_0_rmse: 0.45794 | val_1_rmse: 0.48694 |  0:00:50s
epoch 53 | loss: 0.23145 | val_0_rmse: 0.50305 | val_1_rmse: 0.53454 |  0:00:51s
epoch 54 | loss: 0.23806 | val_0_rmse: 0.48192 | val_1_rmse: 0.50684 |  0:00:52s
epoch 55 | loss: 0.24616 | val_0_rmse: 0.51148 | val_1_rmse: 0.53508 |  0:00:53s
epoch 56 | loss: 0.25667 | val_0_rmse: 0.49888 | val_1_rmse: 0.52304 |  0:00:54s
epoch 57 | loss: 0.24318 | val_0_rmse: 0.46833 | val_1_rmse: 0.49876 |  0:00:55s
epoch 58 | loss: 0.23906 | val_0_rmse: 0.47247 | val_1_rmse: 0.50547 |  0:00:56s
epoch 59 | loss: 0.22828 | val_0_rmse: 0.4617  | val_1_rmse: 0.49035 |  0:00:57s
epoch 60 | loss: 0.22586 | val_0_rmse: 0.46429 | val_1_rmse: 0.49192 |  0:00:58s
epoch 61 | loss: 0.23286 | val_0_rmse: 0.46058 | val_1_rmse: 0.486   |  0:00:58s
epoch 62 | loss: 0.22724 | val_0_rmse: 0.4582  | val_1_rmse: 0.48441 |  0:00:59s
epoch 63 | loss: 0.22331 | val_0_rmse: 0.46449 | val_1_rmse: 0.49066 |  0:01:00s
epoch 64 | loss: 0.22784 | val_0_rmse: 0.45459 | val_1_rmse: 0.48199 |  0:01:01s
epoch 65 | loss: 0.22032 | val_0_rmse: 0.4642  | val_1_rmse: 0.49455 |  0:01:02s
epoch 66 | loss: 0.22869 | val_0_rmse: 0.45166 | val_1_rmse: 0.47828 |  0:01:03s
epoch 67 | loss: 0.2251  | val_0_rmse: 0.44899 | val_1_rmse: 0.47933 |  0:01:04s
epoch 68 | loss: 0.21791 | val_0_rmse: 0.45624 | val_1_rmse: 0.48756 |  0:01:05s
epoch 69 | loss: 0.21676 | val_0_rmse: 0.45385 | val_1_rmse: 0.48657 |  0:01:06s
epoch 70 | loss: 0.21655 | val_0_rmse: 0.44737 | val_1_rmse: 0.47964 |  0:01:07s
epoch 71 | loss: 0.21547 | val_0_rmse: 0.44887 | val_1_rmse: 0.48342 |  0:01:08s
epoch 72 | loss: 0.21109 | val_0_rmse: 0.44309 | val_1_rmse: 0.47695 |  0:01:09s
epoch 73 | loss: 0.21507 | val_0_rmse: 0.45054 | val_1_rmse: 0.48083 |  0:01:10s
epoch 74 | loss: 0.21098 | val_0_rmse: 0.45505 | val_1_rmse: 0.48997 |  0:01:11s
epoch 75 | loss: 0.21577 | val_0_rmse: 0.44712 | val_1_rmse: 0.48256 |  0:01:12s
epoch 76 | loss: 0.21306 | val_0_rmse: 0.44429 | val_1_rmse: 0.47889 |  0:01:13s
epoch 77 | loss: 0.214   | val_0_rmse: 0.44645 | val_1_rmse: 0.48002 |  0:01:14s
epoch 78 | loss: 0.21018 | val_0_rmse: 0.4458  | val_1_rmse: 0.47873 |  0:01:15s
epoch 79 | loss: 0.21109 | val_0_rmse: 0.43993 | val_1_rmse: 0.47704 |  0:01:15s
epoch 80 | loss: 0.21172 | val_0_rmse: 0.4392  | val_1_rmse: 0.47435 |  0:01:16s
epoch 81 | loss: 0.21122 | val_0_rmse: 0.44215 | val_1_rmse: 0.47746 |  0:01:17s
epoch 82 | loss: 0.20895 | val_0_rmse: 0.43637 | val_1_rmse: 0.47399 |  0:01:18s
epoch 83 | loss: 0.21587 | val_0_rmse: 0.47117 | val_1_rmse: 0.50404 |  0:01:19s
epoch 84 | loss: 0.22474 | val_0_rmse: 0.44615 | val_1_rmse: 0.4822  |  0:01:20s
epoch 85 | loss: 0.21462 | val_0_rmse: 0.45498 | val_1_rmse: 0.49533 |  0:01:21s
epoch 86 | loss: 0.20935 | val_0_rmse: 0.44131 | val_1_rmse: 0.47666 |  0:01:22s
epoch 87 | loss: 0.20971 | val_0_rmse: 0.44103 | val_1_rmse: 0.47662 |  0:01:23s
epoch 88 | loss: 0.20911 | val_0_rmse: 0.47503 | val_1_rmse: 0.51169 |  0:01:24s
epoch 89 | loss: 0.21939 | val_0_rmse: 0.45202 | val_1_rmse: 0.48907 |  0:01:25s
epoch 90 | loss: 0.21852 | val_0_rmse: 0.44339 | val_1_rmse: 0.48161 |  0:01:26s
epoch 91 | loss: 0.21267 | val_0_rmse: 0.44848 | val_1_rmse: 0.4811  |  0:01:27s
epoch 92 | loss: 0.20998 | val_0_rmse: 0.43592 | val_1_rmse: 0.47582 |  0:01:28s
epoch 93 | loss: 0.21046 | val_0_rmse: 0.44102 | val_1_rmse: 0.4828  |  0:01:29s
epoch 94 | loss: 0.20978 | val_0_rmse: 0.45644 | val_1_rmse: 0.49444 |  0:01:30s
epoch 95 | loss: 0.21998 | val_0_rmse: 0.47613 | val_1_rmse: 0.51013 |  0:01:31s
epoch 96 | loss: 0.20947 | val_0_rmse: 0.43392 | val_1_rmse: 0.47274 |  0:01:31s
epoch 97 | loss: 0.20348 | val_0_rmse: 0.436   | val_1_rmse: 0.47555 |  0:01:32s
epoch 98 | loss: 0.20387 | val_0_rmse: 0.42788 | val_1_rmse: 0.46903 |  0:01:33s
epoch 99 | loss: 0.20041 | val_0_rmse: 0.43519 | val_1_rmse: 0.47256 |  0:01:34s
epoch 100| loss: 0.20077 | val_0_rmse: 0.43259 | val_1_rmse: 0.47492 |  0:01:35s
epoch 101| loss: 0.20571 | val_0_rmse: 0.43479 | val_1_rmse: 0.47487 |  0:01:36s
epoch 102| loss: 0.20312 | val_0_rmse: 0.45041 | val_1_rmse: 0.4847  |  0:01:37s
epoch 103| loss: 0.21231 | val_0_rmse: 0.43138 | val_1_rmse: 0.47202 |  0:01:38s
epoch 104| loss: 0.20957 | val_0_rmse: 0.44904 | val_1_rmse: 0.48468 |  0:01:39s
epoch 105| loss: 0.20961 | val_0_rmse: 0.44212 | val_1_rmse: 0.47797 |  0:01:40s
epoch 106| loss: 0.21362 | val_0_rmse: 0.44963 | val_1_rmse: 0.48258 |  0:01:41s
epoch 107| loss: 0.21456 | val_0_rmse: 0.4385  | val_1_rmse: 0.47851 |  0:01:42s
epoch 108| loss: 0.20695 | val_0_rmse: 0.43828 | val_1_rmse: 0.4737  |  0:01:43s
epoch 109| loss: 0.21173 | val_0_rmse: 0.44281 | val_1_rmse: 0.48099 |  0:01:44s
epoch 110| loss: 0.21192 | val_0_rmse: 0.44057 | val_1_rmse: 0.47642 |  0:01:45s
epoch 111| loss: 0.21125 | val_0_rmse: 0.43484 | val_1_rmse: 0.47819 |  0:01:46s
epoch 112| loss: 0.20234 | val_0_rmse: 0.43175 | val_1_rmse: 0.47296 |  0:01:47s
epoch 113| loss: 0.20792 | val_0_rmse: 0.44252 | val_1_rmse: 0.47863 |  0:01:48s
epoch 114| loss: 0.20781 | val_0_rmse: 0.43057 | val_1_rmse: 0.46576 |  0:01:49s
epoch 115| loss: 0.20046 | val_0_rmse: 0.4311  | val_1_rmse: 0.46836 |  0:01:50s
epoch 116| loss: 0.20374 | val_0_rmse: 0.42953 | val_1_rmse: 0.46805 |  0:01:50s
epoch 117| loss: 0.20376 | val_0_rmse: 0.43948 | val_1_rmse: 0.47989 |  0:01:51s
epoch 118| loss: 0.20891 | val_0_rmse: 0.45345 | val_1_rmse: 0.49445 |  0:01:52s
epoch 119| loss: 0.20951 | val_0_rmse: 0.43967 | val_1_rmse: 0.48079 |  0:01:53s
epoch 120| loss: 0.20887 | val_0_rmse: 0.44227 | val_1_rmse: 0.48532 |  0:01:54s
epoch 121| loss: 0.20535 | val_0_rmse: 0.43094 | val_1_rmse: 0.46969 |  0:01:55s
epoch 122| loss: 0.20507 | val_0_rmse: 0.44426 | val_1_rmse: 0.47777 |  0:01:56s
epoch 123| loss: 0.20832 | val_0_rmse: 0.4382  | val_1_rmse: 0.47824 |  0:01:57s
epoch 124| loss: 0.2079  | val_0_rmse: 0.43991 | val_1_rmse: 0.48038 |  0:01:58s
epoch 125| loss: 0.2067  | val_0_rmse: 0.43487 | val_1_rmse: 0.47569 |  0:01:59s
epoch 126| loss: 0.20298 | val_0_rmse: 0.43743 | val_1_rmse: 0.47445 |  0:02:00s
epoch 127| loss: 0.19928 | val_0_rmse: 0.42638 | val_1_rmse: 0.46742 |  0:02:01s
epoch 128| loss: 0.1987  | val_0_rmse: 0.42551 | val_1_rmse: 0.46728 |  0:02:02s
epoch 129| loss: 0.19709 | val_0_rmse: 0.4265  | val_1_rmse: 0.46918 |  0:02:03s
epoch 130| loss: 0.19654 | val_0_rmse: 0.41994 | val_1_rmse: 0.46408 |  0:02:04s
epoch 131| loss: 0.19312 | val_0_rmse: 0.4296  | val_1_rmse: 0.46677 |  0:02:05s
epoch 132| loss: 0.20011 | val_0_rmse: 0.4251  | val_1_rmse: 0.47244 |  0:02:06s
epoch 133| loss: 0.20258 | val_0_rmse: 0.43736 | val_1_rmse: 0.47998 |  0:02:06s
epoch 134| loss: 0.20618 | val_0_rmse: 0.43319 | val_1_rmse: 0.47559 |  0:02:07s
epoch 135| loss: 0.20536 | val_0_rmse: 0.42564 | val_1_rmse: 0.46787 |  0:02:08s
epoch 136| loss: 0.20137 | val_0_rmse: 0.42823 | val_1_rmse: 0.46807 |  0:02:09s
epoch 137| loss: 0.19717 | val_0_rmse: 0.43376 | val_1_rmse: 0.47328 |  0:02:10s
epoch 138| loss: 0.19741 | val_0_rmse: 0.4229  | val_1_rmse: 0.46484 |  0:02:11s
epoch 139| loss: 0.20172 | val_0_rmse: 0.43156 | val_1_rmse: 0.47107 |  0:02:12s
epoch 140| loss: 0.20388 | val_0_rmse: 0.43043 | val_1_rmse: 0.4713  |  0:02:13s
epoch 141| loss: 0.20032 | val_0_rmse: 0.42524 | val_1_rmse: 0.46784 |  0:02:14s
epoch 142| loss: 0.20578 | val_0_rmse: 0.42361 | val_1_rmse: 0.46577 |  0:02:15s
epoch 143| loss: 0.195   | val_0_rmse: 0.43881 | val_1_rmse: 0.48071 |  0:02:16s
epoch 144| loss: 0.20072 | val_0_rmse: 0.43041 | val_1_rmse: 0.47133 |  0:02:17s
epoch 145| loss: 0.19604 | val_0_rmse: 0.42726 | val_1_rmse: 0.46842 |  0:02:18s
epoch 146| loss: 0.20054 | val_0_rmse: 0.42575 | val_1_rmse: 0.46224 |  0:02:19s
epoch 147| loss: 0.19988 | val_0_rmse: 0.43205 | val_1_rmse: 0.47    |  0:02:20s
epoch 148| loss: 0.20178 | val_0_rmse: 0.43931 | val_1_rmse: 0.47951 |  0:02:21s
epoch 149| loss: 0.20146 | val_0_rmse: 0.42588 | val_1_rmse: 0.46361 |  0:02:22s
Stop training because you reached max_epochs = 150 with best_epoch = 146 and best_val_1_rmse = 0.46224
Best weights from best epoch are automatically used!
ended training at: 01:33:01
Feature importance:
[('Area', 0.328889781814521), ('Baths', 0.017748657570784684), ('Beds', 0.0), ('Latitude', 0.3347275863503372), ('Longitude', 0.20485184025547623), ('Month', 0.07808158065386189), ('Year', 0.03570055335501894)]
Mean squared error is of 6052736644.642707
Mean absolute error:55372.43817680548
MAPE:0.14871213018786408
R2 score:0.8010887584559107
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:33:01
epoch 0  | loss: 0.66859 | val_0_rmse: 0.78594 | val_1_rmse: 0.78201 |  0:00:00s
epoch 1  | loss: 0.38393 | val_0_rmse: 0.60245 | val_1_rmse: 0.59567 |  0:00:01s
epoch 2  | loss: 0.34243 | val_0_rmse: 0.57354 | val_1_rmse: 0.56929 |  0:00:02s
epoch 3  | loss: 0.33144 | val_0_rmse: 0.56728 | val_1_rmse: 0.56596 |  0:00:03s
epoch 4  | loss: 0.31197 | val_0_rmse: 0.55595 | val_1_rmse: 0.55123 |  0:00:04s
epoch 5  | loss: 0.30454 | val_0_rmse: 0.53515 | val_1_rmse: 0.52862 |  0:00:05s
epoch 6  | loss: 0.29677 | val_0_rmse: 0.53084 | val_1_rmse: 0.52378 |  0:00:06s
epoch 7  | loss: 0.28354 | val_0_rmse: 0.51783 | val_1_rmse: 0.51146 |  0:00:07s
epoch 8  | loss: 0.28707 | val_0_rmse: 0.5312  | val_1_rmse: 0.52118 |  0:00:08s
epoch 9  | loss: 0.28581 | val_0_rmse: 0.5156  | val_1_rmse: 0.50482 |  0:00:09s
epoch 10 | loss: 0.27889 | val_0_rmse: 0.51308 | val_1_rmse: 0.50128 |  0:00:10s
epoch 11 | loss: 0.27501 | val_0_rmse: 0.51151 | val_1_rmse: 0.50331 |  0:00:11s
epoch 12 | loss: 0.27061 | val_0_rmse: 0.51359 | val_1_rmse: 0.50658 |  0:00:12s
epoch 13 | loss: 0.27779 | val_0_rmse: 0.55827 | val_1_rmse: 0.55015 |  0:00:13s
epoch 14 | loss: 0.2824  | val_0_rmse: 0.50795 | val_1_rmse: 0.49945 |  0:00:14s
epoch 15 | loss: 0.27298 | val_0_rmse: 0.51317 | val_1_rmse: 0.50131 |  0:00:15s
epoch 16 | loss: 0.27068 | val_0_rmse: 0.51635 | val_1_rmse: 0.50569 |  0:00:16s
epoch 17 | loss: 0.27594 | val_0_rmse: 0.51433 | val_1_rmse: 0.50839 |  0:00:17s
epoch 18 | loss: 0.26933 | val_0_rmse: 0.50627 | val_1_rmse: 0.50109 |  0:00:18s
epoch 19 | loss: 0.26602 | val_0_rmse: 0.50103 | val_1_rmse: 0.49156 |  0:00:19s
epoch 20 | loss: 0.26524 | val_0_rmse: 0.50432 | val_1_rmse: 0.4983  |  0:00:20s
epoch 21 | loss: 0.26752 | val_0_rmse: 0.49396 | val_1_rmse: 0.48754 |  0:00:20s
epoch 22 | loss: 0.26378 | val_0_rmse: 0.48754 | val_1_rmse: 0.48442 |  0:00:21s
epoch 23 | loss: 0.25867 | val_0_rmse: 0.49952 | val_1_rmse: 0.49309 |  0:00:22s
epoch 24 | loss: 0.27052 | val_0_rmse: 0.52524 | val_1_rmse: 0.51441 |  0:00:23s
epoch 25 | loss: 0.26459 | val_0_rmse: 0.49805 | val_1_rmse: 0.49073 |  0:00:24s
epoch 26 | loss: 0.25809 | val_0_rmse: 0.49714 | val_1_rmse: 0.48869 |  0:00:25s
epoch 27 | loss: 0.25548 | val_0_rmse: 0.50211 | val_1_rmse: 0.49564 |  0:00:26s
epoch 28 | loss: 0.27009 | val_0_rmse: 0.48821 | val_1_rmse: 0.48543 |  0:00:27s
epoch 29 | loss: 0.26151 | val_0_rmse: 0.49116 | val_1_rmse: 0.48797 |  0:00:28s
epoch 30 | loss: 0.24915 | val_0_rmse: 0.48711 | val_1_rmse: 0.48042 |  0:00:29s
epoch 31 | loss: 0.25485 | val_0_rmse: 0.49155 | val_1_rmse: 0.48042 |  0:00:30s
epoch 32 | loss: 0.2486  | val_0_rmse: 0.48151 | val_1_rmse: 0.47251 |  0:00:31s
epoch 33 | loss: 0.24772 | val_0_rmse: 0.47739 | val_1_rmse: 0.46772 |  0:00:32s
epoch 34 | loss: 0.24463 | val_0_rmse: 0.47221 | val_1_rmse: 0.46264 |  0:00:33s
epoch 35 | loss: 0.24091 | val_0_rmse: 0.48384 | val_1_rmse: 0.47829 |  0:00:34s
epoch 36 | loss: 0.24205 | val_0_rmse: 0.47942 | val_1_rmse: 0.46893 |  0:00:35s
epoch 37 | loss: 0.25643 | val_0_rmse: 0.48964 | val_1_rmse: 0.48212 |  0:00:36s
epoch 38 | loss: 0.25339 | val_0_rmse: 0.48269 | val_1_rmse: 0.47521 |  0:00:37s
epoch 39 | loss: 0.24255 | val_0_rmse: 0.48099 | val_1_rmse: 0.47778 |  0:00:38s
epoch 40 | loss: 0.24068 | val_0_rmse: 0.47385 | val_1_rmse: 0.46631 |  0:00:38s
epoch 41 | loss: 0.24783 | val_0_rmse: 0.48567 | val_1_rmse: 0.47894 |  0:00:39s
epoch 42 | loss: 0.246   | val_0_rmse: 0.49748 | val_1_rmse: 0.49363 |  0:00:40s
epoch 43 | loss: 0.24601 | val_0_rmse: 0.4754  | val_1_rmse: 0.46606 |  0:00:41s
epoch 44 | loss: 0.25053 | val_0_rmse: 0.48265 | val_1_rmse: 0.47246 |  0:00:42s
epoch 45 | loss: 0.25647 | val_0_rmse: 0.48587 | val_1_rmse: 0.47731 |  0:00:43s
epoch 46 | loss: 0.25182 | val_0_rmse: 0.48336 | val_1_rmse: 0.4772  |  0:00:44s
epoch 47 | loss: 0.24557 | val_0_rmse: 0.47524 | val_1_rmse: 0.47116 |  0:00:45s
epoch 48 | loss: 0.23558 | val_0_rmse: 0.47584 | val_1_rmse: 0.46587 |  0:00:46s
epoch 49 | loss: 0.23685 | val_0_rmse: 0.47858 | val_1_rmse: 0.47765 |  0:00:47s
epoch 50 | loss: 0.24342 | val_0_rmse: 0.47557 | val_1_rmse: 0.46682 |  0:00:48s
epoch 51 | loss: 0.25207 | val_0_rmse: 0.49734 | val_1_rmse: 0.49142 |  0:00:49s
epoch 52 | loss: 0.25897 | val_0_rmse: 0.48809 | val_1_rmse: 0.48196 |  0:00:50s
epoch 53 | loss: 0.25289 | val_0_rmse: 0.50097 | val_1_rmse: 0.48881 |  0:00:51s
epoch 54 | loss: 0.25111 | val_0_rmse: 0.4805  | val_1_rmse: 0.46709 |  0:00:52s
epoch 55 | loss: 0.25064 | val_0_rmse: 0.48407 | val_1_rmse: 0.47461 |  0:00:53s
epoch 56 | loss: 0.23588 | val_0_rmse: 0.46329 | val_1_rmse: 0.45658 |  0:00:54s
epoch 57 | loss: 0.23474 | val_0_rmse: 0.46836 | val_1_rmse: 0.46407 |  0:00:55s
epoch 58 | loss: 0.24097 | val_0_rmse: 0.47025 | val_1_rmse: 0.46238 |  0:00:56s
epoch 59 | loss: 0.23559 | val_0_rmse: 0.48153 | val_1_rmse: 0.47114 |  0:00:56s
epoch 60 | loss: 0.24264 | val_0_rmse: 0.47498 | val_1_rmse: 0.46615 |  0:00:57s
epoch 61 | loss: 0.2583  | val_0_rmse: 0.51911 | val_1_rmse: 0.50998 |  0:00:58s
epoch 62 | loss: 0.25796 | val_0_rmse: 0.50088 | val_1_rmse: 0.49181 |  0:00:59s
epoch 63 | loss: 0.2477  | val_0_rmse: 0.48377 | val_1_rmse: 0.47752 |  0:01:00s
epoch 64 | loss: 0.24883 | val_0_rmse: 0.46684 | val_1_rmse: 0.46092 |  0:01:01s
epoch 65 | loss: 0.2405  | val_0_rmse: 0.46677 | val_1_rmse: 0.45976 |  0:01:02s
epoch 66 | loss: 0.23934 | val_0_rmse: 0.47    | val_1_rmse: 0.46987 |  0:01:03s
epoch 67 | loss: 0.23971 | val_0_rmse: 0.46968 | val_1_rmse: 0.46831 |  0:01:04s
epoch 68 | loss: 0.24115 | val_0_rmse: 0.46817 | val_1_rmse: 0.45903 |  0:01:05s
epoch 69 | loss: 0.24026 | val_0_rmse: 0.46889 | val_1_rmse: 0.46217 |  0:01:06s
epoch 70 | loss: 0.23637 | val_0_rmse: 0.46474 | val_1_rmse: 0.45846 |  0:01:07s
epoch 71 | loss: 0.23882 | val_0_rmse: 0.46388 | val_1_rmse: 0.4588  |  0:01:08s
epoch 72 | loss: 0.23141 | val_0_rmse: 0.47395 | val_1_rmse: 0.47168 |  0:01:09s
epoch 73 | loss: 0.23485 | val_0_rmse: 0.47157 | val_1_rmse: 0.46449 |  0:01:10s
epoch 74 | loss: 0.22915 | val_0_rmse: 0.45732 | val_1_rmse: 0.44837 |  0:01:11s
epoch 75 | loss: 0.22368 | val_0_rmse: 0.45342 | val_1_rmse: 0.44855 |  0:01:12s
epoch 76 | loss: 0.23013 | val_0_rmse: 0.47019 | val_1_rmse: 0.46747 |  0:01:13s
epoch 77 | loss: 0.22561 | val_0_rmse: 0.45104 | val_1_rmse: 0.44775 |  0:01:14s
epoch 78 | loss: 0.22944 | val_0_rmse: 0.45421 | val_1_rmse: 0.44642 |  0:01:15s
epoch 79 | loss: 0.23413 | val_0_rmse: 0.46216 | val_1_rmse: 0.46167 |  0:01:15s
epoch 80 | loss: 0.23087 | val_0_rmse: 0.45529 | val_1_rmse: 0.44852 |  0:01:16s
epoch 81 | loss: 0.22634 | val_0_rmse: 0.46701 | val_1_rmse: 0.46251 |  0:01:17s
epoch 82 | loss: 0.22805 | val_0_rmse: 0.46622 | val_1_rmse: 0.46478 |  0:01:18s
epoch 83 | loss: 0.2262  | val_0_rmse: 0.45844 | val_1_rmse: 0.45712 |  0:01:19s
epoch 84 | loss: 0.22644 | val_0_rmse: 0.45604 | val_1_rmse: 0.45529 |  0:01:20s
epoch 85 | loss: 0.23016 | val_0_rmse: 0.46361 | val_1_rmse: 0.46596 |  0:01:21s
epoch 86 | loss: 0.23138 | val_0_rmse: 0.46614 | val_1_rmse: 0.46051 |  0:01:22s
epoch 87 | loss: 0.23865 | val_0_rmse: 0.47406 | val_1_rmse: 0.47334 |  0:01:23s
epoch 88 | loss: 0.23404 | val_0_rmse: 0.46478 | val_1_rmse: 0.46101 |  0:01:24s
epoch 89 | loss: 0.22308 | val_0_rmse: 0.46344 | val_1_rmse: 0.45594 |  0:01:25s
epoch 90 | loss: 0.21999 | val_0_rmse: 0.45357 | val_1_rmse: 0.45331 |  0:01:26s
epoch 91 | loss: 0.22206 | val_0_rmse: 0.48238 | val_1_rmse: 0.47285 |  0:01:27s
epoch 92 | loss: 0.2252  | val_0_rmse: 0.46469 | val_1_rmse: 0.45795 |  0:01:28s
epoch 93 | loss: 0.21999 | val_0_rmse: 0.44835 | val_1_rmse: 0.44744 |  0:01:29s
epoch 94 | loss: 0.22382 | val_0_rmse: 0.45388 | val_1_rmse: 0.45488 |  0:01:30s
epoch 95 | loss: 0.22354 | val_0_rmse: 0.44531 | val_1_rmse: 0.44165 |  0:01:31s
epoch 96 | loss: 0.22033 | val_0_rmse: 0.44647 | val_1_rmse: 0.44402 |  0:01:32s
epoch 97 | loss: 0.21759 | val_0_rmse: 0.45998 | val_1_rmse: 0.46127 |  0:01:32s
epoch 98 | loss: 0.22177 | val_0_rmse: 0.47428 | val_1_rmse: 0.4761  |  0:01:33s
epoch 99 | loss: 0.22439 | val_0_rmse: 0.45482 | val_1_rmse: 0.45486 |  0:01:34s
epoch 100| loss: 0.22101 | val_0_rmse: 0.46003 | val_1_rmse: 0.45781 |  0:01:35s
epoch 101| loss: 0.23217 | val_0_rmse: 0.48004 | val_1_rmse: 0.48171 |  0:01:36s
epoch 102| loss: 0.23851 | val_0_rmse: 0.4771  | val_1_rmse: 0.47498 |  0:01:37s
epoch 103| loss: 0.23186 | val_0_rmse: 0.45297 | val_1_rmse: 0.44757 |  0:01:38s
epoch 104| loss: 0.22172 | val_0_rmse: 0.46119 | val_1_rmse: 0.4546  |  0:01:39s
epoch 105| loss: 0.22289 | val_0_rmse: 0.45527 | val_1_rmse: 0.4523  |  0:01:40s
epoch 106| loss: 0.21629 | val_0_rmse: 0.44778 | val_1_rmse: 0.44512 |  0:01:41s
epoch 107| loss: 0.21561 | val_0_rmse: 0.44955 | val_1_rmse: 0.45585 |  0:01:42s
epoch 108| loss: 0.21628 | val_0_rmse: 0.44262 | val_1_rmse: 0.4428  |  0:01:43s
epoch 109| loss: 0.21933 | val_0_rmse: 0.45009 | val_1_rmse: 0.44582 |  0:01:44s
epoch 110| loss: 0.21499 | val_0_rmse: 0.45258 | val_1_rmse: 0.44766 |  0:01:45s
epoch 111| loss: 0.21368 | val_0_rmse: 0.44585 | val_1_rmse: 0.44903 |  0:01:46s
epoch 112| loss: 0.21975 | val_0_rmse: 0.45184 | val_1_rmse: 0.45029 |  0:01:47s
epoch 113| loss: 0.22082 | val_0_rmse: 0.45402 | val_1_rmse: 0.45183 |  0:01:48s
epoch 114| loss: 0.22172 | val_0_rmse: 0.47248 | val_1_rmse: 0.47084 |  0:01:48s
epoch 115| loss: 0.22452 | val_0_rmse: 0.45088 | val_1_rmse: 0.45287 |  0:01:49s
epoch 116| loss: 0.2151  | val_0_rmse: 0.45133 | val_1_rmse: 0.45046 |  0:01:50s
epoch 117| loss: 0.22067 | val_0_rmse: 0.45661 | val_1_rmse: 0.45503 |  0:01:51s
epoch 118| loss: 0.21575 | val_0_rmse: 0.44649 | val_1_rmse: 0.44911 |  0:01:52s
epoch 119| loss: 0.21742 | val_0_rmse: 0.46075 | val_1_rmse: 0.46015 |  0:01:53s
epoch 120| loss: 0.2211  | val_0_rmse: 0.45636 | val_1_rmse: 0.458   |  0:01:54s
epoch 121| loss: 0.21977 | val_0_rmse: 0.45842 | val_1_rmse: 0.4537  |  0:01:55s
epoch 122| loss: 0.21589 | val_0_rmse: 0.44292 | val_1_rmse: 0.44524 |  0:01:56s
epoch 123| loss: 0.21361 | val_0_rmse: 0.44838 | val_1_rmse: 0.45125 |  0:01:57s
epoch 124| loss: 0.21709 | val_0_rmse: 0.44518 | val_1_rmse: 0.44348 |  0:01:58s
epoch 125| loss: 0.22389 | val_0_rmse: 0.45093 | val_1_rmse: 0.45118 |  0:01:59s

Early stopping occured at epoch 125 with best_epoch = 95 and best_val_1_rmse = 0.44165
Best weights from best epoch are automatically used!
ended training at: 01:35:01
Feature importance:
[('Area', 0.4470212032434608), ('Baths', 0.06358470317573284), ('Beds', 0.025697781191138275), ('Latitude', 0.3159847469047573), ('Longitude', 0.056420822810875584), ('Month', 0.04159852688348661), ('Year', 0.04969221579054861)]
Mean squared error is of 5781092145.991912
Mean absolute error:55505.83938088199
MAPE:0.1531285522433521
R2 score:0.8092019548552264
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:35:01
epoch 0  | loss: 0.71503 | val_0_rmse: 0.69653 | val_1_rmse: 0.67937 |  0:00:00s
epoch 1  | loss: 0.38645 | val_0_rmse: 0.60757 | val_1_rmse: 0.59282 |  0:00:01s
epoch 2  | loss: 0.32374 | val_0_rmse: 0.58176 | val_1_rmse: 0.5725  |  0:00:02s
epoch 3  | loss: 0.30779 | val_0_rmse: 0.53716 | val_1_rmse: 0.51758 |  0:00:03s
epoch 4  | loss: 0.30208 | val_0_rmse: 0.53965 | val_1_rmse: 0.51921 |  0:00:04s
epoch 5  | loss: 0.30309 | val_0_rmse: 0.54487 | val_1_rmse: 0.53161 |  0:00:05s
epoch 6  | loss: 0.30227 | val_0_rmse: 0.54916 | val_1_rmse: 0.53699 |  0:00:06s
epoch 7  | loss: 0.31214 | val_0_rmse: 0.55939 | val_1_rmse: 0.54414 |  0:00:07s
epoch 8  | loss: 0.30689 | val_0_rmse: 0.54125 | val_1_rmse: 0.5239  |  0:00:08s
epoch 9  | loss: 0.29985 | val_0_rmse: 0.5367  | val_1_rmse: 0.51352 |  0:00:09s
epoch 10 | loss: 0.29488 | val_0_rmse: 0.53021 | val_1_rmse: 0.50801 |  0:00:10s
epoch 11 | loss: 0.28976 | val_0_rmse: 0.52267 | val_1_rmse: 0.50286 |  0:00:11s
epoch 12 | loss: 0.28697 | val_0_rmse: 0.52259 | val_1_rmse: 0.50681 |  0:00:12s
epoch 13 | loss: 0.2829  | val_0_rmse: 0.51525 | val_1_rmse: 0.4995  |  0:00:13s
epoch 14 | loss: 0.27922 | val_0_rmse: 0.51532 | val_1_rmse: 0.49573 |  0:00:14s
epoch 15 | loss: 0.28506 | val_0_rmse: 0.54747 | val_1_rmse: 0.5238  |  0:00:15s
epoch 16 | loss: 0.28348 | val_0_rmse: 0.51438 | val_1_rmse: 0.49306 |  0:00:16s
epoch 17 | loss: 0.28125 | val_0_rmse: 0.53534 | val_1_rmse: 0.52668 |  0:00:17s
epoch 18 | loss: 0.27838 | val_0_rmse: 0.51074 | val_1_rmse: 0.49671 |  0:00:18s
epoch 19 | loss: 0.26932 | val_0_rmse: 0.50518 | val_1_rmse: 0.4909  |  0:00:19s
epoch 20 | loss: 0.26686 | val_0_rmse: 0.50447 | val_1_rmse: 0.4886  |  0:00:19s
epoch 21 | loss: 0.27169 | val_0_rmse: 0.50611 | val_1_rmse: 0.4879  |  0:00:20s
epoch 22 | loss: 0.2684  | val_0_rmse: 0.5081  | val_1_rmse: 0.49186 |  0:00:21s
epoch 23 | loss: 0.26831 | val_0_rmse: 0.50293 | val_1_rmse: 0.48499 |  0:00:22s
epoch 24 | loss: 0.26441 | val_0_rmse: 0.5041  | val_1_rmse: 0.49053 |  0:00:23s
epoch 25 | loss: 0.26527 | val_0_rmse: 0.49673 | val_1_rmse: 0.48149 |  0:00:24s
epoch 26 | loss: 0.25806 | val_0_rmse: 0.49894 | val_1_rmse: 0.47818 |  0:00:25s
epoch 27 | loss: 0.26063 | val_0_rmse: 0.49077 | val_1_rmse: 0.47271 |  0:00:26s
epoch 28 | loss: 0.25902 | val_0_rmse: 0.49014 | val_1_rmse: 0.47246 |  0:00:27s
epoch 29 | loss: 0.26048 | val_0_rmse: 0.49717 | val_1_rmse: 0.48422 |  0:00:28s
epoch 30 | loss: 0.26534 | val_0_rmse: 0.51665 | val_1_rmse: 0.50994 |  0:00:29s
epoch 31 | loss: 0.2624  | val_0_rmse: 0.49766 | val_1_rmse: 0.48266 |  0:00:30s
epoch 32 | loss: 0.25227 | val_0_rmse: 0.49098 | val_1_rmse: 0.47689 |  0:00:31s
epoch 33 | loss: 0.25434 | val_0_rmse: 0.49282 | val_1_rmse: 0.47529 |  0:00:32s
epoch 34 | loss: 0.24989 | val_0_rmse: 0.48103 | val_1_rmse: 0.47069 |  0:00:33s
epoch 35 | loss: 0.24347 | val_0_rmse: 0.48032 | val_1_rmse: 0.46985 |  0:00:34s
epoch 36 | loss: 0.24949 | val_0_rmse: 0.48418 | val_1_rmse: 0.47054 |  0:00:35s
epoch 37 | loss: 0.24903 | val_0_rmse: 0.47884 | val_1_rmse: 0.46589 |  0:00:36s
epoch 38 | loss: 0.24141 | val_0_rmse: 0.47751 | val_1_rmse: 0.46562 |  0:00:36s
epoch 39 | loss: 0.23925 | val_0_rmse: 0.4745  | val_1_rmse: 0.46001 |  0:00:37s
epoch 40 | loss: 0.24298 | val_0_rmse: 0.47773 | val_1_rmse: 0.4659  |  0:00:38s
epoch 41 | loss: 0.24298 | val_0_rmse: 0.49061 | val_1_rmse: 0.47979 |  0:00:39s
epoch 42 | loss: 0.24384 | val_0_rmse: 0.4686  | val_1_rmse: 0.46076 |  0:00:40s
epoch 43 | loss: 0.24219 | val_0_rmse: 0.47789 | val_1_rmse: 0.4626  |  0:00:41s
epoch 44 | loss: 0.23659 | val_0_rmse: 0.46554 | val_1_rmse: 0.45518 |  0:00:42s
epoch 45 | loss: 0.23806 | val_0_rmse: 0.46829 | val_1_rmse: 0.4536  |  0:00:43s
epoch 46 | loss: 0.23687 | val_0_rmse: 0.48115 | val_1_rmse: 0.47044 |  0:00:44s
epoch 47 | loss: 0.24334 | val_0_rmse: 0.47034 | val_1_rmse: 0.46163 |  0:00:45s
epoch 48 | loss: 0.24454 | val_0_rmse: 0.48815 | val_1_rmse: 0.4753  |  0:00:46s
epoch 49 | loss: 0.24016 | val_0_rmse: 0.46825 | val_1_rmse: 0.45511 |  0:00:47s
epoch 50 | loss: 0.23941 | val_0_rmse: 0.47589 | val_1_rmse: 0.45933 |  0:00:48s
epoch 51 | loss: 0.23382 | val_0_rmse: 0.46406 | val_1_rmse: 0.45686 |  0:00:49s
epoch 52 | loss: 0.23325 | val_0_rmse: 0.45986 | val_1_rmse: 0.44967 |  0:00:50s
epoch 53 | loss: 0.23665 | val_0_rmse: 0.46385 | val_1_rmse: 0.44907 |  0:00:51s
epoch 54 | loss: 0.23405 | val_0_rmse: 0.45908 | val_1_rmse: 0.44904 |  0:00:52s
epoch 55 | loss: 0.23336 | val_0_rmse: 0.47463 | val_1_rmse: 0.46074 |  0:00:53s
epoch 56 | loss: 0.2342  | val_0_rmse: 0.46464 | val_1_rmse: 0.45304 |  0:00:54s
epoch 57 | loss: 0.22751 | val_0_rmse: 0.47241 | val_1_rmse: 0.46714 |  0:00:55s
epoch 58 | loss: 0.237   | val_0_rmse: 0.46044 | val_1_rmse: 0.45656 |  0:00:55s
epoch 59 | loss: 0.22999 | val_0_rmse: 0.47355 | val_1_rmse: 0.46655 |  0:00:56s
epoch 60 | loss: 0.23142 | val_0_rmse: 0.46526 | val_1_rmse: 0.45431 |  0:00:57s
epoch 61 | loss: 0.23568 | val_0_rmse: 0.4635  | val_1_rmse: 0.45108 |  0:00:58s
epoch 62 | loss: 0.22605 | val_0_rmse: 0.46691 | val_1_rmse: 0.46147 |  0:00:59s
epoch 63 | loss: 0.23189 | val_0_rmse: 0.45598 | val_1_rmse: 0.44628 |  0:01:00s
epoch 64 | loss: 0.22599 | val_0_rmse: 0.46614 | val_1_rmse: 0.46049 |  0:01:01s
epoch 65 | loss: 0.22549 | val_0_rmse: 0.46426 | val_1_rmse: 0.45629 |  0:01:02s
epoch 66 | loss: 0.22601 | val_0_rmse: 0.45482 | val_1_rmse: 0.44647 |  0:01:03s
epoch 67 | loss: 0.22723 | val_0_rmse: 0.46115 | val_1_rmse: 0.45138 |  0:01:04s
epoch 68 | loss: 0.23238 | val_0_rmse: 0.46554 | val_1_rmse: 0.45695 |  0:01:05s
epoch 69 | loss: 0.23617 | val_0_rmse: 0.47277 | val_1_rmse: 0.46361 |  0:01:06s
epoch 70 | loss: 0.23161 | val_0_rmse: 0.46253 | val_1_rmse: 0.45635 |  0:01:07s
epoch 71 | loss: 0.23006 | val_0_rmse: 0.45938 | val_1_rmse: 0.44966 |  0:01:08s
epoch 72 | loss: 0.23083 | val_0_rmse: 0.45837 | val_1_rmse: 0.45193 |  0:01:09s
epoch 73 | loss: 0.23044 | val_0_rmse: 0.45941 | val_1_rmse: 0.44718 |  0:01:10s
epoch 74 | loss: 0.22551 | val_0_rmse: 0.46272 | val_1_rmse: 0.45351 |  0:01:11s
epoch 75 | loss: 0.22894 | val_0_rmse: 0.4675  | val_1_rmse: 0.46414 |  0:01:12s
epoch 76 | loss: 0.22334 | val_0_rmse: 0.47551 | val_1_rmse: 0.47147 |  0:01:12s
epoch 77 | loss: 0.23026 | val_0_rmse: 0.44888 | val_1_rmse: 0.44202 |  0:01:13s
epoch 78 | loss: 0.22654 | val_0_rmse: 0.44877 | val_1_rmse: 0.44001 |  0:01:14s
epoch 79 | loss: 0.22014 | val_0_rmse: 0.45446 | val_1_rmse: 0.45004 |  0:01:15s
epoch 80 | loss: 0.22681 | val_0_rmse: 0.45275 | val_1_rmse: 0.44902 |  0:01:16s
epoch 81 | loss: 0.23085 | val_0_rmse: 0.45776 | val_1_rmse: 0.44811 |  0:01:17s
epoch 82 | loss: 0.22842 | val_0_rmse: 0.45778 | val_1_rmse: 0.44599 |  0:01:18s
epoch 83 | loss: 0.23176 | val_0_rmse: 0.47909 | val_1_rmse: 0.47482 |  0:01:19s
epoch 84 | loss: 0.22157 | val_0_rmse: 0.45001 | val_1_rmse: 0.44372 |  0:01:20s
epoch 85 | loss: 0.22359 | val_0_rmse: 0.45895 | val_1_rmse: 0.44887 |  0:01:21s
epoch 86 | loss: 0.22383 | val_0_rmse: 0.47154 | val_1_rmse: 0.46189 |  0:01:22s
epoch 87 | loss: 0.22912 | val_0_rmse: 0.45985 | val_1_rmse: 0.45556 |  0:01:23s
epoch 88 | loss: 0.21962 | val_0_rmse: 0.45025 | val_1_rmse: 0.44211 |  0:01:24s
epoch 89 | loss: 0.22365 | val_0_rmse: 0.46285 | val_1_rmse: 0.45818 |  0:01:25s
epoch 90 | loss: 0.22336 | val_0_rmse: 0.47699 | val_1_rmse: 0.47184 |  0:01:26s
epoch 91 | loss: 0.2187  | val_0_rmse: 0.45311 | val_1_rmse: 0.44697 |  0:01:27s
epoch 92 | loss: 0.21487 | val_0_rmse: 0.45305 | val_1_rmse: 0.45156 |  0:01:28s
epoch 93 | loss: 0.21625 | val_0_rmse: 0.45115 | val_1_rmse: 0.45198 |  0:01:29s
epoch 94 | loss: 0.21418 | val_0_rmse: 0.45849 | val_1_rmse: 0.44892 |  0:01:30s
epoch 95 | loss: 0.21999 | val_0_rmse: 0.44143 | val_1_rmse: 0.437   |  0:01:31s
epoch 96 | loss: 0.21921 | val_0_rmse: 0.45184 | val_1_rmse: 0.44995 |  0:01:31s
epoch 97 | loss: 0.22535 | val_0_rmse: 0.45888 | val_1_rmse: 0.44829 |  0:01:32s
epoch 98 | loss: 0.22032 | val_0_rmse: 0.44764 | val_1_rmse: 0.44251 |  0:01:33s
epoch 99 | loss: 0.21778 | val_0_rmse: 0.44601 | val_1_rmse: 0.44291 |  0:01:34s
epoch 100| loss: 0.22058 | val_0_rmse: 0.45588 | val_1_rmse: 0.45205 |  0:01:35s
epoch 101| loss: 0.21834 | val_0_rmse: 0.44564 | val_1_rmse: 0.44458 |  0:01:36s
epoch 102| loss: 0.21659 | val_0_rmse: 0.45825 | val_1_rmse: 0.45187 |  0:01:37s
epoch 103| loss: 0.2184  | val_0_rmse: 0.4513  | val_1_rmse: 0.45205 |  0:01:38s
epoch 104| loss: 0.21779 | val_0_rmse: 0.45074 | val_1_rmse: 0.45177 |  0:01:39s
epoch 105| loss: 0.22304 | val_0_rmse: 0.44846 | val_1_rmse: 0.44831 |  0:01:40s
epoch 106| loss: 0.21615 | val_0_rmse: 0.44975 | val_1_rmse: 0.44484 |  0:01:41s
epoch 107| loss: 0.21397 | val_0_rmse: 0.45185 | val_1_rmse: 0.44873 |  0:01:42s
epoch 108| loss: 0.21477 | val_0_rmse: 0.44823 | val_1_rmse: 0.44697 |  0:01:43s
epoch 109| loss: 0.21321 | val_0_rmse: 0.4432  | val_1_rmse: 0.44031 |  0:01:44s
epoch 110| loss: 0.21642 | val_0_rmse: 0.48868 | val_1_rmse: 0.48348 |  0:01:45s
epoch 111| loss: 0.22742 | val_0_rmse: 0.4449  | val_1_rmse: 0.44445 |  0:01:46s
epoch 112| loss: 0.2207  | val_0_rmse: 0.45675 | val_1_rmse: 0.45426 |  0:01:47s
epoch 113| loss: 0.21929 | val_0_rmse: 0.44834 | val_1_rmse: 0.45084 |  0:01:48s
epoch 114| loss: 0.21529 | val_0_rmse: 0.45739 | val_1_rmse: 0.46149 |  0:01:48s
epoch 115| loss: 0.22325 | val_0_rmse: 0.46239 | val_1_rmse: 0.46152 |  0:01:49s
epoch 116| loss: 0.22078 | val_0_rmse: 0.44459 | val_1_rmse: 0.44448 |  0:01:50s
epoch 117| loss: 0.21722 | val_0_rmse: 0.44021 | val_1_rmse: 0.44047 |  0:01:51s
epoch 118| loss: 0.21187 | val_0_rmse: 0.44609 | val_1_rmse: 0.45092 |  0:01:52s
epoch 119| loss: 0.21399 | val_0_rmse: 0.44278 | val_1_rmse: 0.43751 |  0:01:53s
epoch 120| loss: 0.20953 | val_0_rmse: 0.43587 | val_1_rmse: 0.43778 |  0:01:54s
epoch 121| loss: 0.21028 | val_0_rmse: 0.44171 | val_1_rmse: 0.44685 |  0:01:55s
epoch 122| loss: 0.21189 | val_0_rmse: 0.43919 | val_1_rmse: 0.44112 |  0:01:56s
epoch 123| loss: 0.22075 | val_0_rmse: 0.45507 | val_1_rmse: 0.45409 |  0:01:57s
epoch 124| loss: 0.21959 | val_0_rmse: 0.44553 | val_1_rmse: 0.44927 |  0:01:58s
epoch 125| loss: 0.21619 | val_0_rmse: 0.44283 | val_1_rmse: 0.44214 |  0:01:59s

Early stopping occured at epoch 125 with best_epoch = 95 and best_val_1_rmse = 0.437
Best weights from best epoch are automatically used!
ended training at: 01:37:01
Feature importance:
[('Area', 0.49283990714205156), ('Baths', 0.0017919792651098225), ('Beds', 0.06351856591350726), ('Latitude', 0.328049097958384), ('Longitude', 0.036815244525978834), ('Month', 0.02037654097937414), ('Year', 0.05660866421559439)]
Mean squared error is of 5896471531.720782
Mean absolute error:55081.703387613234
MAPE:0.14586539285079822
R2 score:0.8055387786284962
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:37:01
epoch 0  | loss: 0.65222 | val_0_rmse: 0.73534 | val_1_rmse: 0.7404  |  0:00:00s
epoch 1  | loss: 0.35367 | val_0_rmse: 0.57748 | val_1_rmse: 0.5832  |  0:00:01s
epoch 2  | loss: 0.3211  | val_0_rmse: 0.54748 | val_1_rmse: 0.54973 |  0:00:02s
epoch 3  | loss: 0.31078 | val_0_rmse: 0.5368  | val_1_rmse: 0.5403  |  0:00:03s
epoch 4  | loss: 0.30392 | val_0_rmse: 0.53065 | val_1_rmse: 0.52926 |  0:00:04s
epoch 5  | loss: 0.29905 | val_0_rmse: 0.5316  | val_1_rmse: 0.53421 |  0:00:05s
epoch 6  | loss: 0.29313 | val_0_rmse: 0.53441 | val_1_rmse: 0.53702 |  0:00:06s
epoch 7  | loss: 0.28504 | val_0_rmse: 0.51935 | val_1_rmse: 0.52578 |  0:00:07s
epoch 8  | loss: 0.28768 | val_0_rmse: 0.53229 | val_1_rmse: 0.53047 |  0:00:08s
epoch 9  | loss: 0.29134 | val_0_rmse: 0.51426 | val_1_rmse: 0.51957 |  0:00:09s
epoch 10 | loss: 0.28755 | val_0_rmse: 0.52954 | val_1_rmse: 0.53132 |  0:00:10s
epoch 11 | loss: 0.28206 | val_0_rmse: 0.52412 | val_1_rmse: 0.52108 |  0:00:11s
epoch 12 | loss: 0.27709 | val_0_rmse: 0.50572 | val_1_rmse: 0.50714 |  0:00:12s
epoch 13 | loss: 0.27104 | val_0_rmse: 0.50582 | val_1_rmse: 0.51241 |  0:00:13s
epoch 14 | loss: 0.27579 | val_0_rmse: 0.52291 | val_1_rmse: 0.51922 |  0:00:14s
epoch 15 | loss: 0.27198 | val_0_rmse: 0.51692 | val_1_rmse: 0.51452 |  0:00:15s
epoch 16 | loss: 0.27217 | val_0_rmse: 0.53328 | val_1_rmse: 0.53672 |  0:00:16s
epoch 17 | loss: 0.26469 | val_0_rmse: 0.49985 | val_1_rmse: 0.50099 |  0:00:17s
epoch 18 | loss: 0.26711 | val_0_rmse: 0.53581 | val_1_rmse: 0.54249 |  0:00:18s
epoch 19 | loss: 0.26681 | val_0_rmse: 0.49007 | val_1_rmse: 0.49107 |  0:00:18s
epoch 20 | loss: 0.2584  | val_0_rmse: 0.49731 | val_1_rmse: 0.50247 |  0:00:19s
epoch 21 | loss: 0.26037 | val_0_rmse: 0.50293 | val_1_rmse: 0.5084  |  0:00:20s
epoch 22 | loss: 0.25851 | val_0_rmse: 0.51364 | val_1_rmse: 0.52269 |  0:00:21s
epoch 23 | loss: 0.24959 | val_0_rmse: 0.47856 | val_1_rmse: 0.48206 |  0:00:22s
epoch 24 | loss: 0.25039 | val_0_rmse: 0.4939  | val_1_rmse: 0.49771 |  0:00:23s
epoch 25 | loss: 0.25204 | val_0_rmse: 0.49383 | val_1_rmse: 0.49213 |  0:00:24s
epoch 26 | loss: 0.2565  | val_0_rmse: 0.48702 | val_1_rmse: 0.48902 |  0:00:25s
epoch 27 | loss: 0.24993 | val_0_rmse: 0.49079 | val_1_rmse: 0.49559 |  0:00:26s
epoch 28 | loss: 0.24586 | val_0_rmse: 0.47703 | val_1_rmse: 0.48096 |  0:00:27s
epoch 29 | loss: 0.25028 | val_0_rmse: 0.47948 | val_1_rmse: 0.48781 |  0:00:28s
epoch 30 | loss: 0.25523 | val_0_rmse: 0.49495 | val_1_rmse: 0.50573 |  0:00:29s
epoch 31 | loss: 0.257   | val_0_rmse: 0.4963  | val_1_rmse: 0.5049  |  0:00:30s
epoch 32 | loss: 0.25897 | val_0_rmse: 0.50819 | val_1_rmse: 0.51352 |  0:00:31s
epoch 33 | loss: 0.25345 | val_0_rmse: 0.50262 | val_1_rmse: 0.51076 |  0:00:32s
epoch 34 | loss: 0.25408 | val_0_rmse: 0.48331 | val_1_rmse: 0.49125 |  0:00:33s
epoch 35 | loss: 0.25064 | val_0_rmse: 0.48785 | val_1_rmse: 0.4947  |  0:00:34s
epoch 36 | loss: 0.2538  | val_0_rmse: 0.49273 | val_1_rmse: 0.49436 |  0:00:35s
epoch 37 | loss: 0.25322 | val_0_rmse: 0.48612 | val_1_rmse: 0.49301 |  0:00:35s
epoch 38 | loss: 0.24818 | val_0_rmse: 0.48528 | val_1_rmse: 0.49243 |  0:00:36s
epoch 39 | loss: 0.24937 | val_0_rmse: 0.48455 | val_1_rmse: 0.48993 |  0:00:37s
epoch 40 | loss: 0.24866 | val_0_rmse: 0.48169 | val_1_rmse: 0.49425 |  0:00:38s
epoch 41 | loss: 0.24018 | val_0_rmse: 0.47606 | val_1_rmse: 0.48323 |  0:00:39s
epoch 42 | loss: 0.24213 | val_0_rmse: 0.48816 | val_1_rmse: 0.49111 |  0:00:40s
epoch 43 | loss: 0.24898 | val_0_rmse: 0.48452 | val_1_rmse: 0.49094 |  0:00:41s
epoch 44 | loss: 0.2465  | val_0_rmse: 0.49757 | val_1_rmse: 0.5074  |  0:00:42s
epoch 45 | loss: 0.24437 | val_0_rmse: 0.47953 | val_1_rmse: 0.48884 |  0:00:43s
epoch 46 | loss: 0.23897 | val_0_rmse: 0.47368 | val_1_rmse: 0.48128 |  0:00:44s
epoch 47 | loss: 0.23919 | val_0_rmse: 0.47852 | val_1_rmse: 0.48337 |  0:00:45s
epoch 48 | loss: 0.23814 | val_0_rmse: 0.47116 | val_1_rmse: 0.47571 |  0:00:46s
epoch 49 | loss: 0.23322 | val_0_rmse: 0.47204 | val_1_rmse: 0.47788 |  0:00:47s
epoch 50 | loss: 0.23282 | val_0_rmse: 0.46777 | val_1_rmse: 0.46981 |  0:00:48s
epoch 51 | loss: 0.23514 | val_0_rmse: 0.46608 | val_1_rmse: 0.4715  |  0:00:49s
epoch 52 | loss: 0.24188 | val_0_rmse: 0.47035 | val_1_rmse: 0.47374 |  0:00:50s
epoch 53 | loss: 0.23406 | val_0_rmse: 0.48116 | val_1_rmse: 0.48737 |  0:00:51s
epoch 54 | loss: 0.23685 | val_0_rmse: 0.4738  | val_1_rmse: 0.47729 |  0:00:52s
epoch 55 | loss: 0.23614 | val_0_rmse: 0.46446 | val_1_rmse: 0.4721  |  0:00:53s
epoch 56 | loss: 0.22706 | val_0_rmse: 0.48192 | val_1_rmse: 0.48823 |  0:00:53s
epoch 57 | loss: 0.23823 | val_0_rmse: 0.47774 | val_1_rmse: 0.48734 |  0:00:54s
epoch 58 | loss: 0.23956 | val_0_rmse: 0.47803 | val_1_rmse: 0.48198 |  0:00:55s
epoch 59 | loss: 0.25221 | val_0_rmse: 0.47945 | val_1_rmse: 0.48693 |  0:00:56s
epoch 60 | loss: 0.24266 | val_0_rmse: 0.48295 | val_1_rmse: 0.48712 |  0:00:57s
epoch 61 | loss: 0.23739 | val_0_rmse: 0.46783 | val_1_rmse: 0.47152 |  0:00:58s
epoch 62 | loss: 0.24199 | val_0_rmse: 0.46937 | val_1_rmse: 0.47588 |  0:00:59s
epoch 63 | loss: 0.23359 | val_0_rmse: 0.48196 | val_1_rmse: 0.49016 |  0:01:00s
epoch 64 | loss: 0.23521 | val_0_rmse: 0.46255 | val_1_rmse: 0.46673 |  0:01:01s
epoch 65 | loss: 0.23256 | val_0_rmse: 0.46278 | val_1_rmse: 0.46738 |  0:01:02s
epoch 66 | loss: 0.23208 | val_0_rmse: 0.47456 | val_1_rmse: 0.4802  |  0:01:03s
epoch 67 | loss: 0.22788 | val_0_rmse: 0.46885 | val_1_rmse: 0.46696 |  0:01:04s
epoch 68 | loss: 0.23596 | val_0_rmse: 0.46229 | val_1_rmse: 0.46988 |  0:01:05s
epoch 69 | loss: 0.22793 | val_0_rmse: 0.47448 | val_1_rmse: 0.47636 |  0:01:06s
epoch 70 | loss: 0.23187 | val_0_rmse: 0.46698 | val_1_rmse: 0.4726  |  0:01:07s
epoch 71 | loss: 0.23097 | val_0_rmse: 0.48426 | val_1_rmse: 0.4855  |  0:01:08s
epoch 72 | loss: 0.23649 | val_0_rmse: 0.48292 | val_1_rmse: 0.495   |  0:01:09s
epoch 73 | loss: 0.23254 | val_0_rmse: 0.45616 | val_1_rmse: 0.4624  |  0:01:10s
epoch 74 | loss: 0.22665 | val_0_rmse: 0.45663 | val_1_rmse: 0.46003 |  0:01:11s
epoch 75 | loss: 0.2243  | val_0_rmse: 0.45712 | val_1_rmse: 0.46754 |  0:01:11s
epoch 76 | loss: 0.2239  | val_0_rmse: 0.47619 | val_1_rmse: 0.47706 |  0:01:12s
epoch 77 | loss: 0.22844 | val_0_rmse: 0.45399 | val_1_rmse: 0.46021 |  0:01:13s
epoch 78 | loss: 0.22272 | val_0_rmse: 0.4533  | val_1_rmse: 0.45974 |  0:01:14s
epoch 79 | loss: 0.22503 | val_0_rmse: 0.49416 | val_1_rmse: 0.50395 |  0:01:15s
epoch 80 | loss: 0.23127 | val_0_rmse: 0.46348 | val_1_rmse: 0.46596 |  0:01:16s
epoch 81 | loss: 0.23254 | val_0_rmse: 0.45824 | val_1_rmse: 0.46748 |  0:01:17s
epoch 82 | loss: 0.22369 | val_0_rmse: 0.46104 | val_1_rmse: 0.46748 |  0:01:18s
epoch 83 | loss: 0.22318 | val_0_rmse: 0.46292 | val_1_rmse: 0.46922 |  0:01:19s
epoch 84 | loss: 0.22915 | val_0_rmse: 0.45371 | val_1_rmse: 0.46012 |  0:01:20s
epoch 85 | loss: 0.22182 | val_0_rmse: 0.45347 | val_1_rmse: 0.46061 |  0:01:21s
epoch 86 | loss: 0.22366 | val_0_rmse: 0.45236 | val_1_rmse: 0.46313 |  0:01:22s
epoch 87 | loss: 0.22524 | val_0_rmse: 0.46562 | val_1_rmse: 0.4706  |  0:01:23s
epoch 88 | loss: 0.22749 | val_0_rmse: 0.45952 | val_1_rmse: 0.46495 |  0:01:24s
epoch 89 | loss: 0.22569 | val_0_rmse: 0.47659 | val_1_rmse: 0.48828 |  0:01:25s
epoch 90 | loss: 0.23167 | val_0_rmse: 0.48452 | val_1_rmse: 0.49198 |  0:01:26s
epoch 91 | loss: 0.22941 | val_0_rmse: 0.47581 | val_1_rmse: 0.48353 |  0:01:27s
epoch 92 | loss: 0.23429 | val_0_rmse: 0.52239 | val_1_rmse: 0.52513 |  0:01:28s
epoch 93 | loss: 0.23698 | val_0_rmse: 0.46289 | val_1_rmse: 0.46749 |  0:01:28s
epoch 94 | loss: 0.22588 | val_0_rmse: 0.45889 | val_1_rmse: 0.46778 |  0:01:29s
epoch 95 | loss: 0.22715 | val_0_rmse: 0.46434 | val_1_rmse: 0.47783 |  0:01:30s
epoch 96 | loss: 0.22444 | val_0_rmse: 0.46955 | val_1_rmse: 0.48083 |  0:01:31s
epoch 97 | loss: 0.22774 | val_0_rmse: 0.47047 | val_1_rmse: 0.4777  |  0:01:32s
epoch 98 | loss: 0.22749 | val_0_rmse: 0.46234 | val_1_rmse: 0.46737 |  0:01:33s
epoch 99 | loss: 0.22961 | val_0_rmse: 0.48615 | val_1_rmse: 0.49539 |  0:01:34s
epoch 100| loss: 0.23299 | val_0_rmse: 0.49585 | val_1_rmse: 0.49998 |  0:01:35s
epoch 101| loss: 0.23635 | val_0_rmse: 0.4759  | val_1_rmse: 0.48724 |  0:01:36s
epoch 102| loss: 0.22764 | val_0_rmse: 0.46471 | val_1_rmse: 0.46581 |  0:01:37s
epoch 103| loss: 0.22015 | val_0_rmse: 0.45372 | val_1_rmse: 0.45812 |  0:01:38s
epoch 104| loss: 0.22025 | val_0_rmse: 0.44968 | val_1_rmse: 0.46196 |  0:01:39s
epoch 105| loss: 0.21601 | val_0_rmse: 0.44799 | val_1_rmse: 0.4574  |  0:01:40s
epoch 106| loss: 0.22215 | val_0_rmse: 0.45757 | val_1_rmse: 0.46695 |  0:01:41s
epoch 107| loss: 0.21989 | val_0_rmse: 0.45097 | val_1_rmse: 0.45517 |  0:01:42s
epoch 108| loss: 0.21825 | val_0_rmse: 0.45621 | val_1_rmse: 0.46699 |  0:01:43s
epoch 109| loss: 0.21776 | val_0_rmse: 0.44917 | val_1_rmse: 0.45657 |  0:01:44s
epoch 110| loss: 0.21599 | val_0_rmse: 0.4477  | val_1_rmse: 0.45669 |  0:01:45s
epoch 111| loss: 0.21897 | val_0_rmse: 0.46901 | val_1_rmse: 0.47462 |  0:01:46s
epoch 112| loss: 0.22606 | val_0_rmse: 0.463   | val_1_rmse: 0.47279 |  0:01:46s
epoch 113| loss: 0.23063 | val_0_rmse: 0.46308 | val_1_rmse: 0.4627  |  0:01:47s
epoch 114| loss: 0.22347 | val_0_rmse: 0.46816 | val_1_rmse: 0.47877 |  0:01:48s
epoch 115| loss: 0.22549 | val_0_rmse: 0.45346 | val_1_rmse: 0.45958 |  0:01:49s
epoch 116| loss: 0.22195 | val_0_rmse: 0.46414 | val_1_rmse: 0.47302 |  0:01:50s
epoch 117| loss: 0.22462 | val_0_rmse: 0.46361 | val_1_rmse: 0.47035 |  0:01:51s
epoch 118| loss: 0.2272  | val_0_rmse: 0.45772 | val_1_rmse: 0.46306 |  0:01:52s
epoch 119| loss: 0.22203 | val_0_rmse: 0.45076 | val_1_rmse: 0.45887 |  0:01:53s
epoch 120| loss: 0.22293 | val_0_rmse: 0.45435 | val_1_rmse: 0.45807 |  0:01:54s
epoch 121| loss: 0.21815 | val_0_rmse: 0.4492  | val_1_rmse: 0.46005 |  0:01:55s
epoch 122| loss: 0.21812 | val_0_rmse: 0.45806 | val_1_rmse: 0.47057 |  0:01:56s
epoch 123| loss: 0.21878 | val_0_rmse: 0.45307 | val_1_rmse: 0.46064 |  0:01:57s
epoch 124| loss: 0.21617 | val_0_rmse: 0.44983 | val_1_rmse: 0.46387 |  0:01:58s
epoch 125| loss: 0.21805 | val_0_rmse: 0.45729 | val_1_rmse: 0.46553 |  0:01:59s
epoch 126| loss: 0.22242 | val_0_rmse: 0.45387 | val_1_rmse: 0.46405 |  0:02:00s
epoch 127| loss: 0.21519 | val_0_rmse: 0.45004 | val_1_rmse: 0.46365 |  0:02:01s
epoch 128| loss: 0.21422 | val_0_rmse: 0.44638 | val_1_rmse: 0.45914 |  0:02:02s
epoch 129| loss: 0.2169  | val_0_rmse: 0.45868 | val_1_rmse: 0.47108 |  0:02:02s
epoch 130| loss: 0.22091 | val_0_rmse: 0.45842 | val_1_rmse: 0.46743 |  0:02:03s
epoch 131| loss: 0.21585 | val_0_rmse: 0.44884 | val_1_rmse: 0.45715 |  0:02:04s
epoch 132| loss: 0.2247  | val_0_rmse: 0.48116 | val_1_rmse: 0.49246 |  0:02:05s
epoch 133| loss: 0.22509 | val_0_rmse: 0.47705 | val_1_rmse: 0.47946 |  0:02:06s
epoch 134| loss: 0.22357 | val_0_rmse: 0.46236 | val_1_rmse: 0.464   |  0:02:07s
epoch 135| loss: 0.21787 | val_0_rmse: 0.45748 | val_1_rmse: 0.46035 |  0:02:08s
epoch 136| loss: 0.21975 | val_0_rmse: 0.48056 | val_1_rmse: 0.48343 |  0:02:09s
epoch 137| loss: 0.22493 | val_0_rmse: 0.4715  | val_1_rmse: 0.47745 |  0:02:10s

Early stopping occured at epoch 137 with best_epoch = 107 and best_val_1_rmse = 0.45517
Best weights from best epoch are automatically used!
ended training at: 01:39:12
Feature importance:
[('Area', 0.45184063845711603), ('Baths', 0.020752597442725177), ('Beds', 0.024018736210090142), ('Latitude', 0.4138697999763736), ('Longitude', 0.0802769983647375), ('Month', 0.009241229548957557), ('Year', 0.0)]
Mean squared error is of 6155871776.791418
Mean absolute error:55841.27475852416
MAPE:0.15272364399715133
R2 score:0.7922226879709757
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:39:12
epoch 0  | loss: 0.66568 | val_0_rmse: 0.66819 | val_1_rmse: 0.67984 |  0:00:00s
epoch 1  | loss: 0.37995 | val_0_rmse: 0.61153 | val_1_rmse: 0.62012 |  0:00:01s
epoch 2  | loss: 0.3293  | val_0_rmse: 0.55823 | val_1_rmse: 0.5722  |  0:00:02s
epoch 3  | loss: 0.30421 | val_0_rmse: 0.54941 | val_1_rmse: 0.56338 |  0:00:03s
epoch 4  | loss: 0.30727 | val_0_rmse: 0.53895 | val_1_rmse: 0.55585 |  0:00:04s
epoch 5  | loss: 0.29567 | val_0_rmse: 0.52086 | val_1_rmse: 0.53364 |  0:00:05s
epoch 6  | loss: 0.28348 | val_0_rmse: 0.52699 | val_1_rmse: 0.54081 |  0:00:06s
epoch 7  | loss: 0.28049 | val_0_rmse: 0.51167 | val_1_rmse: 0.53044 |  0:00:07s
epoch 8  | loss: 0.28071 | val_0_rmse: 0.5324  | val_1_rmse: 0.54847 |  0:00:08s
epoch 9  | loss: 0.2762  | val_0_rmse: 0.50598 | val_1_rmse: 0.52555 |  0:00:09s
epoch 10 | loss: 0.26708 | val_0_rmse: 0.49988 | val_1_rmse: 0.51697 |  0:00:10s
epoch 11 | loss: 0.26458 | val_0_rmse: 0.49794 | val_1_rmse: 0.5147  |  0:00:11s
epoch 12 | loss: 0.26047 | val_0_rmse: 0.50133 | val_1_rmse: 0.52346 |  0:00:12s
epoch 13 | loss: 0.27402 | val_0_rmse: 0.51263 | val_1_rmse: 0.53158 |  0:00:13s
epoch 14 | loss: 0.26844 | val_0_rmse: 0.4951  | val_1_rmse: 0.51466 |  0:00:14s
epoch 15 | loss: 0.27119 | val_0_rmse: 0.50798 | val_1_rmse: 0.52493 |  0:00:15s
epoch 16 | loss: 0.2571  | val_0_rmse: 0.49731 | val_1_rmse: 0.51571 |  0:00:16s
epoch 17 | loss: 0.25344 | val_0_rmse: 0.48268 | val_1_rmse: 0.50343 |  0:00:17s
epoch 18 | loss: 0.25008 | val_0_rmse: 0.48148 | val_1_rmse: 0.50364 |  0:00:18s
epoch 19 | loss: 0.25212 | val_0_rmse: 0.48489 | val_1_rmse: 0.50535 |  0:00:19s
epoch 20 | loss: 0.24562 | val_0_rmse: 0.47197 | val_1_rmse: 0.49398 |  0:00:19s
epoch 21 | loss: 0.24857 | val_0_rmse: 0.47998 | val_1_rmse: 0.50244 |  0:00:20s
epoch 22 | loss: 0.25236 | val_0_rmse: 0.4745  | val_1_rmse: 0.49391 |  0:00:21s
epoch 23 | loss: 0.2388  | val_0_rmse: 0.48566 | val_1_rmse: 0.50781 |  0:00:22s
epoch 24 | loss: 0.25143 | val_0_rmse: 0.48566 | val_1_rmse: 0.50714 |  0:00:23s
epoch 25 | loss: 0.23618 | val_0_rmse: 0.4635  | val_1_rmse: 0.48811 |  0:00:24s
epoch 26 | loss: 0.23662 | val_0_rmse: 0.47768 | val_1_rmse: 0.49967 |  0:00:25s
epoch 27 | loss: 0.2432  | val_0_rmse: 0.46487 | val_1_rmse: 0.48874 |  0:00:26s
epoch 28 | loss: 0.23236 | val_0_rmse: 0.45639 | val_1_rmse: 0.48288 |  0:00:27s
epoch 29 | loss: 0.23079 | val_0_rmse: 0.4536  | val_1_rmse: 0.47774 |  0:00:28s
epoch 30 | loss: 0.23799 | val_0_rmse: 0.48133 | val_1_rmse: 0.50186 |  0:00:29s
epoch 31 | loss: 0.23281 | val_0_rmse: 0.45607 | val_1_rmse: 0.48607 |  0:00:30s
epoch 32 | loss: 0.22885 | val_0_rmse: 0.45669 | val_1_rmse: 0.48054 |  0:00:31s
epoch 33 | loss: 0.22151 | val_0_rmse: 0.46094 | val_1_rmse: 0.48353 |  0:00:32s
epoch 34 | loss: 0.23332 | val_0_rmse: 0.4761  | val_1_rmse: 0.4954  |  0:00:33s
epoch 35 | loss: 0.23487 | val_0_rmse: 0.4627  | val_1_rmse: 0.48204 |  0:00:34s
epoch 36 | loss: 0.22928 | val_0_rmse: 0.4603  | val_1_rmse: 0.4801  |  0:00:35s
epoch 37 | loss: 0.23185 | val_0_rmse: 0.46395 | val_1_rmse: 0.48393 |  0:00:36s
epoch 38 | loss: 0.24053 | val_0_rmse: 0.46838 | val_1_rmse: 0.48301 |  0:00:36s
epoch 39 | loss: 0.22735 | val_0_rmse: 0.46026 | val_1_rmse: 0.4861  |  0:00:37s
epoch 40 | loss: 0.22734 | val_0_rmse: 0.46537 | val_1_rmse: 0.48564 |  0:00:38s
epoch 41 | loss: 0.22121 | val_0_rmse: 0.4566  | val_1_rmse: 0.47723 |  0:00:39s
epoch 42 | loss: 0.22088 | val_0_rmse: 0.46728 | val_1_rmse: 0.48675 |  0:00:40s
epoch 43 | loss: 0.22157 | val_0_rmse: 0.44201 | val_1_rmse: 0.46451 |  0:00:41s
epoch 44 | loss: 0.21892 | val_0_rmse: 0.44364 | val_1_rmse: 0.46536 |  0:00:42s
epoch 45 | loss: 0.2179  | val_0_rmse: 0.44181 | val_1_rmse: 0.46914 |  0:00:43s
epoch 46 | loss: 0.21143 | val_0_rmse: 0.45193 | val_1_rmse: 0.4712  |  0:00:44s
epoch 47 | loss: 0.21655 | val_0_rmse: 0.44051 | val_1_rmse: 0.46099 |  0:00:45s
epoch 48 | loss: 0.21166 | val_0_rmse: 0.4507  | val_1_rmse: 0.46655 |  0:00:46s
epoch 49 | loss: 0.21284 | val_0_rmse: 0.44818 | val_1_rmse: 0.47639 |  0:00:47s
epoch 50 | loss: 0.21899 | val_0_rmse: 0.44898 | val_1_rmse: 0.46987 |  0:00:48s
epoch 51 | loss: 0.21649 | val_0_rmse: 0.43486 | val_1_rmse: 0.45607 |  0:00:49s
epoch 52 | loss: 0.2148  | val_0_rmse: 0.43858 | val_1_rmse: 0.45969 |  0:00:50s
epoch 53 | loss: 0.21271 | val_0_rmse: 0.44269 | val_1_rmse: 0.46188 |  0:00:51s
epoch 54 | loss: 0.21442 | val_0_rmse: 0.43953 | val_1_rmse: 0.45962 |  0:00:52s
epoch 55 | loss: 0.21528 | val_0_rmse: 0.43394 | val_1_rmse: 0.45704 |  0:00:53s
epoch 56 | loss: 0.2084  | val_0_rmse: 0.43676 | val_1_rmse: 0.46007 |  0:00:54s
epoch 57 | loss: 0.21104 | val_0_rmse: 0.43669 | val_1_rmse: 0.46376 |  0:00:55s
epoch 58 | loss: 0.21038 | val_0_rmse: 0.44356 | val_1_rmse: 0.47028 |  0:00:55s
epoch 59 | loss: 0.21376 | val_0_rmse: 0.43792 | val_1_rmse: 0.46179 |  0:00:56s
epoch 60 | loss: 0.20315 | val_0_rmse: 0.44157 | val_1_rmse: 0.4656  |  0:00:57s
epoch 61 | loss: 0.21122 | val_0_rmse: 0.44402 | val_1_rmse: 0.47135 |  0:00:58s
epoch 62 | loss: 0.21105 | val_0_rmse: 0.43388 | val_1_rmse: 0.46318 |  0:00:59s
epoch 63 | loss: 0.21133 | val_0_rmse: 0.46218 | val_1_rmse: 0.48612 |  0:01:00s
epoch 64 | loss: 0.21308 | val_0_rmse: 0.43123 | val_1_rmse: 0.45916 |  0:01:01s
epoch 65 | loss: 0.21174 | val_0_rmse: 0.43527 | val_1_rmse: 0.46247 |  0:01:02s
epoch 66 | loss: 0.2119  | val_0_rmse: 0.42731 | val_1_rmse: 0.45125 |  0:01:03s
epoch 67 | loss: 0.20754 | val_0_rmse: 0.43532 | val_1_rmse: 0.46606 |  0:01:04s
epoch 68 | loss: 0.20758 | val_0_rmse: 0.4543  | val_1_rmse: 0.47828 |  0:01:05s
epoch 69 | loss: 0.21214 | val_0_rmse: 0.43262 | val_1_rmse: 0.4656  |  0:01:06s
epoch 70 | loss: 0.20639 | val_0_rmse: 0.43779 | val_1_rmse: 0.45845 |  0:01:07s
epoch 71 | loss: 0.20295 | val_0_rmse: 0.43917 | val_1_rmse: 0.46134 |  0:01:08s
epoch 72 | loss: 0.20544 | val_0_rmse: 0.44347 | val_1_rmse: 0.46605 |  0:01:09s
epoch 73 | loss: 0.21023 | val_0_rmse: 0.444   | val_1_rmse: 0.47089 |  0:01:10s
epoch 74 | loss: 0.20751 | val_0_rmse: 0.42648 | val_1_rmse: 0.45249 |  0:01:11s
epoch 75 | loss: 0.20912 | val_0_rmse: 0.45685 | val_1_rmse: 0.4808  |  0:01:12s
epoch 76 | loss: 0.20935 | val_0_rmse: 0.44628 | val_1_rmse: 0.47533 |  0:01:13s
epoch 77 | loss: 0.20406 | val_0_rmse: 0.4317  | val_1_rmse: 0.45856 |  0:01:13s
epoch 78 | loss: 0.20292 | val_0_rmse: 0.43818 | val_1_rmse: 0.46406 |  0:01:14s
epoch 79 | loss: 0.20956 | val_0_rmse: 0.43896 | val_1_rmse: 0.46583 |  0:01:15s
epoch 80 | loss: 0.21532 | val_0_rmse: 0.45329 | val_1_rmse: 0.47695 |  0:01:16s
epoch 81 | loss: 0.20823 | val_0_rmse: 0.43499 | val_1_rmse: 0.4668  |  0:01:17s
epoch 82 | loss: 0.20842 | val_0_rmse: 0.43228 | val_1_rmse: 0.46128 |  0:01:18s
epoch 83 | loss: 0.20847 | val_0_rmse: 0.44044 | val_1_rmse: 0.46325 |  0:01:19s
epoch 84 | loss: 0.20752 | val_0_rmse: 0.44661 | val_1_rmse: 0.478   |  0:01:20s
epoch 85 | loss: 0.21113 | val_0_rmse: 0.45243 | val_1_rmse: 0.47623 |  0:01:21s
epoch 86 | loss: 0.21803 | val_0_rmse: 0.45125 | val_1_rmse: 0.47543 |  0:01:22s
epoch 87 | loss: 0.20683 | val_0_rmse: 0.45424 | val_1_rmse: 0.48018 |  0:01:23s
epoch 88 | loss: 0.20429 | val_0_rmse: 0.42624 | val_1_rmse: 0.45407 |  0:01:24s
epoch 89 | loss: 0.20884 | val_0_rmse: 0.43679 | val_1_rmse: 0.46791 |  0:01:25s
epoch 90 | loss: 0.20252 | val_0_rmse: 0.43946 | val_1_rmse: 0.4731  |  0:01:26s
epoch 91 | loss: 0.20247 | val_0_rmse: 0.43944 | val_1_rmse: 0.46734 |  0:01:27s
epoch 92 | loss: 0.2075  | val_0_rmse: 0.43124 | val_1_rmse: 0.46    |  0:01:28s
epoch 93 | loss: 0.20649 | val_0_rmse: 0.42826 | val_1_rmse: 0.45906 |  0:01:29s
epoch 94 | loss: 0.20155 | val_0_rmse: 0.42593 | val_1_rmse: 0.45542 |  0:01:29s
epoch 95 | loss: 0.20184 | val_0_rmse: 0.42774 | val_1_rmse: 0.45962 |  0:01:30s
epoch 96 | loss: 0.20643 | val_0_rmse: 0.46684 | val_1_rmse: 0.49274 |  0:01:31s

Early stopping occured at epoch 96 with best_epoch = 66 and best_val_1_rmse = 0.45125
Best weights from best epoch are automatically used!
ended training at: 01:40:44
Feature importance:
[('Area', 0.39834153417068086), ('Baths', 0.032712197497388504), ('Beds', 0.09862007926548205), ('Latitude', 0.19115610281679773), ('Longitude', 0.07938584039718352), ('Month', 0.061935668868953735), ('Year', 0.13784857698351358)]
Mean squared error is of 6099376579.07097
Mean absolute error:55909.77957681178
MAPE:0.14781278372566745
R2 score:0.8004266391205686
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:40:44
epoch 0  | loss: 1.06439 | val_0_rmse: 0.96956 | val_1_rmse: 1.03136 |  0:00:00s
epoch 1  | loss: 0.57085 | val_0_rmse: 0.92052 | val_1_rmse: 0.948   |  0:00:00s
epoch 2  | loss: 0.42149 | val_0_rmse: 0.77637 | val_1_rmse: 0.82209 |  0:00:01s
epoch 3  | loss: 0.38261 | val_0_rmse: 0.64515 | val_1_rmse: 0.66499 |  0:00:01s
epoch 4  | loss: 0.38075 | val_0_rmse: 0.61233 | val_1_rmse: 0.63724 |  0:00:02s
epoch 5  | loss: 0.35206 | val_0_rmse: 0.57529 | val_1_rmse: 0.59039 |  0:00:02s
epoch 6  | loss: 0.33859 | val_0_rmse: 0.57487 | val_1_rmse: 0.59799 |  0:00:03s
epoch 7  | loss: 0.32838 | val_0_rmse: 0.55871 | val_1_rmse: 0.58621 |  0:00:03s
epoch 8  | loss: 0.32395 | val_0_rmse: 0.54377 | val_1_rmse: 0.56637 |  0:00:04s
epoch 9  | loss: 0.31089 | val_0_rmse: 0.53708 | val_1_rmse: 0.55821 |  0:00:04s
epoch 10 | loss: 0.29948 | val_0_rmse: 0.52552 | val_1_rmse: 0.54668 |  0:00:05s
epoch 11 | loss: 0.29121 | val_0_rmse: 0.5205  | val_1_rmse: 0.53974 |  0:00:05s
epoch 12 | loss: 0.29411 | val_0_rmse: 0.53992 | val_1_rmse: 0.55841 |  0:00:05s
epoch 13 | loss: 0.30521 | val_0_rmse: 0.51785 | val_1_rmse: 0.533   |  0:00:06s
epoch 14 | loss: 0.29285 | val_0_rmse: 0.52941 | val_1_rmse: 0.54723 |  0:00:06s
epoch 15 | loss: 0.29415 | val_0_rmse: 0.52791 | val_1_rmse: 0.54189 |  0:00:07s
epoch 16 | loss: 0.29069 | val_0_rmse: 0.51071 | val_1_rmse: 0.53689 |  0:00:07s
epoch 17 | loss: 0.28843 | val_0_rmse: 0.52199 | val_1_rmse: 0.55583 |  0:00:08s
epoch 18 | loss: 0.28055 | val_0_rmse: 0.52049 | val_1_rmse: 0.5465  |  0:00:08s
epoch 19 | loss: 0.28132 | val_0_rmse: 0.50401 | val_1_rmse: 0.53267 |  0:00:09s
epoch 20 | loss: 0.28218 | val_0_rmse: 0.5174  | val_1_rmse: 0.54281 |  0:00:09s
epoch 21 | loss: 0.28844 | val_0_rmse: 0.5204  | val_1_rmse: 0.53515 |  0:00:10s
epoch 22 | loss: 0.27507 | val_0_rmse: 0.51189 | val_1_rmse: 0.53279 |  0:00:10s
epoch 23 | loss: 0.27845 | val_0_rmse: 0.5098  | val_1_rmse: 0.52545 |  0:00:10s
epoch 24 | loss: 0.27397 | val_0_rmse: 0.50226 | val_1_rmse: 0.52972 |  0:00:11s
epoch 25 | loss: 0.27437 | val_0_rmse: 0.51015 | val_1_rmse: 0.52803 |  0:00:11s
epoch 26 | loss: 0.27214 | val_0_rmse: 0.51591 | val_1_rmse: 0.54701 |  0:00:12s
epoch 27 | loss: 0.27872 | val_0_rmse: 0.53103 | val_1_rmse: 0.55014 |  0:00:12s
epoch 28 | loss: 0.27396 | val_0_rmse: 0.51471 | val_1_rmse: 0.5383  |  0:00:13s
epoch 29 | loss: 0.2707  | val_0_rmse: 0.50465 | val_1_rmse: 0.52369 |  0:00:13s
epoch 30 | loss: 0.27308 | val_0_rmse: 0.50145 | val_1_rmse: 0.53303 |  0:00:14s
epoch 31 | loss: 0.26854 | val_0_rmse: 0.5264  | val_1_rmse: 0.54649 |  0:00:14s
epoch 32 | loss: 0.26605 | val_0_rmse: 0.48894 | val_1_rmse: 0.51822 |  0:00:14s
epoch 33 | loss: 0.25644 | val_0_rmse: 0.48736 | val_1_rmse: 0.51202 |  0:00:15s
epoch 34 | loss: 0.25597 | val_0_rmse: 0.49209 | val_1_rmse: 0.51052 |  0:00:15s
epoch 35 | loss: 0.2596  | val_0_rmse: 0.49977 | val_1_rmse: 0.51863 |  0:00:16s
epoch 36 | loss: 0.26303 | val_0_rmse: 0.49337 | val_1_rmse: 0.51218 |  0:00:16s
epoch 37 | loss: 0.26857 | val_0_rmse: 0.49984 | val_1_rmse: 0.52392 |  0:00:17s
epoch 38 | loss: 0.26863 | val_0_rmse: 0.5487  | val_1_rmse: 0.57535 |  0:00:17s
epoch 39 | loss: 0.27089 | val_0_rmse: 0.49746 | val_1_rmse: 0.533   |  0:00:18s
epoch 40 | loss: 0.27436 | val_0_rmse: 0.48919 | val_1_rmse: 0.51096 |  0:00:18s
epoch 41 | loss: 0.26033 | val_0_rmse: 0.48399 | val_1_rmse: 0.51333 |  0:00:19s
epoch 42 | loss: 0.25453 | val_0_rmse: 0.51111 | val_1_rmse: 0.53136 |  0:00:19s
epoch 43 | loss: 0.26237 | val_0_rmse: 0.50219 | val_1_rmse: 0.51839 |  0:00:19s
epoch 44 | loss: 0.25841 | val_0_rmse: 0.50175 | val_1_rmse: 0.52926 |  0:00:20s
epoch 45 | loss: 0.26105 | val_0_rmse: 0.48609 | val_1_rmse: 0.5193  |  0:00:20s
epoch 46 | loss: 0.2585  | val_0_rmse: 0.50103 | val_1_rmse: 0.52608 |  0:00:21s
epoch 47 | loss: 0.26516 | val_0_rmse: 0.51132 | val_1_rmse: 0.53142 |  0:00:21s
epoch 48 | loss: 0.2722  | val_0_rmse: 0.49748 | val_1_rmse: 0.51452 |  0:00:22s
epoch 49 | loss: 0.26873 | val_0_rmse: 0.50125 | val_1_rmse: 0.52873 |  0:00:22s
epoch 50 | loss: 0.2701  | val_0_rmse: 0.49798 | val_1_rmse: 0.52989 |  0:00:23s
epoch 51 | loss: 0.26258 | val_0_rmse: 0.49824 | val_1_rmse: 0.52585 |  0:00:23s
epoch 52 | loss: 0.27056 | val_0_rmse: 0.49587 | val_1_rmse: 0.51614 |  0:00:24s
epoch 53 | loss: 0.25977 | val_0_rmse: 0.49118 | val_1_rmse: 0.51287 |  0:00:24s
epoch 54 | loss: 0.26104 | val_0_rmse: 0.48982 | val_1_rmse: 0.51272 |  0:00:24s
epoch 55 | loss: 0.25819 | val_0_rmse: 0.50215 | val_1_rmse: 0.5356  |  0:00:25s
epoch 56 | loss: 0.26501 | val_0_rmse: 0.50363 | val_1_rmse: 0.53629 |  0:00:25s
epoch 57 | loss: 0.25986 | val_0_rmse: 0.48407 | val_1_rmse: 0.51495 |  0:00:26s
epoch 58 | loss: 0.25147 | val_0_rmse: 0.50587 | val_1_rmse: 0.52966 |  0:00:26s
epoch 59 | loss: 0.26394 | val_0_rmse: 0.48959 | val_1_rmse: 0.51328 |  0:00:27s
epoch 60 | loss: 0.25348 | val_0_rmse: 0.4867  | val_1_rmse: 0.52122 |  0:00:27s
epoch 61 | loss: 0.24347 | val_0_rmse: 0.47991 | val_1_rmse: 0.50931 |  0:00:28s
epoch 62 | loss: 0.24785 | val_0_rmse: 0.47744 | val_1_rmse: 0.51205 |  0:00:28s
epoch 63 | loss: 0.24544 | val_0_rmse: 0.47983 | val_1_rmse: 0.51719 |  0:00:29s
epoch 64 | loss: 0.24449 | val_0_rmse: 0.47604 | val_1_rmse: 0.50905 |  0:00:29s
epoch 65 | loss: 0.24719 | val_0_rmse: 0.48158 | val_1_rmse: 0.51711 |  0:00:29s
epoch 66 | loss: 0.25211 | val_0_rmse: 0.4937  | val_1_rmse: 0.52395 |  0:00:30s
epoch 67 | loss: 0.24512 | val_0_rmse: 0.482   | val_1_rmse: 0.51536 |  0:00:30s
epoch 68 | loss: 0.25454 | val_0_rmse: 0.48807 | val_1_rmse: 0.51848 |  0:00:31s
epoch 69 | loss: 0.26474 | val_0_rmse: 0.5277  | val_1_rmse: 0.54869 |  0:00:31s
epoch 70 | loss: 0.27115 | val_0_rmse: 0.51499 | val_1_rmse: 0.54093 |  0:00:32s
epoch 71 | loss: 0.26632 | val_0_rmse: 0.49591 | val_1_rmse: 0.52317 |  0:00:32s
epoch 72 | loss: 0.261   | val_0_rmse: 0.49198 | val_1_rmse: 0.5229  |  0:00:33s
epoch 73 | loss: 0.25991 | val_0_rmse: 0.48365 | val_1_rmse: 0.50526 |  0:00:33s
epoch 74 | loss: 0.2574  | val_0_rmse: 0.50319 | val_1_rmse: 0.52656 |  0:00:33s
epoch 75 | loss: 0.26082 | val_0_rmse: 0.48941 | val_1_rmse: 0.51326 |  0:00:34s
epoch 76 | loss: 0.25497 | val_0_rmse: 0.49063 | val_1_rmse: 0.53074 |  0:00:34s
epoch 77 | loss: 0.24736 | val_0_rmse: 0.47939 | val_1_rmse: 0.51141 |  0:00:35s
epoch 78 | loss: 0.24658 | val_0_rmse: 0.47936 | val_1_rmse: 0.50964 |  0:00:35s
epoch 79 | loss: 0.24654 | val_0_rmse: 0.47982 | val_1_rmse: 0.51402 |  0:00:36s
epoch 80 | loss: 0.24499 | val_0_rmse: 0.47843 | val_1_rmse: 0.51148 |  0:00:36s
epoch 81 | loss: 0.2487  | val_0_rmse: 0.48414 | val_1_rmse: 0.50486 |  0:00:37s
epoch 82 | loss: 0.24737 | val_0_rmse: 0.48761 | val_1_rmse: 0.52018 |  0:00:37s
epoch 83 | loss: 0.24938 | val_0_rmse: 0.47572 | val_1_rmse: 0.50326 |  0:00:38s
epoch 84 | loss: 0.24374 | val_0_rmse: 0.47434 | val_1_rmse: 0.50814 |  0:00:38s
epoch 85 | loss: 0.2456  | val_0_rmse: 0.47579 | val_1_rmse: 0.50607 |  0:00:38s
epoch 86 | loss: 0.24091 | val_0_rmse: 0.47816 | val_1_rmse: 0.50606 |  0:00:39s
epoch 87 | loss: 0.24831 | val_0_rmse: 0.47379 | val_1_rmse: 0.50147 |  0:00:39s
epoch 88 | loss: 0.24448 | val_0_rmse: 0.47676 | val_1_rmse: 0.50388 |  0:00:40s
epoch 89 | loss: 0.24136 | val_0_rmse: 0.4718  | val_1_rmse: 0.50595 |  0:00:40s
epoch 90 | loss: 0.24984 | val_0_rmse: 0.49336 | val_1_rmse: 0.52616 |  0:00:41s
epoch 91 | loss: 0.2587  | val_0_rmse: 0.47068 | val_1_rmse: 0.5051  |  0:00:41s
epoch 92 | loss: 0.25547 | val_0_rmse: 0.50353 | val_1_rmse: 0.52314 |  0:00:42s
epoch 93 | loss: 0.24493 | val_0_rmse: 0.47697 | val_1_rmse: 0.5126  |  0:00:42s
epoch 94 | loss: 0.24682 | val_0_rmse: 0.48175 | val_1_rmse: 0.52258 |  0:00:43s
epoch 95 | loss: 0.24487 | val_0_rmse: 0.47272 | val_1_rmse: 0.50662 |  0:00:43s
epoch 96 | loss: 0.24977 | val_0_rmse: 0.47932 | val_1_rmse: 0.50637 |  0:00:43s
epoch 97 | loss: 0.25102 | val_0_rmse: 0.48398 | val_1_rmse: 0.5049  |  0:00:44s
epoch 98 | loss: 0.24718 | val_0_rmse: 0.4777  | val_1_rmse: 0.50994 |  0:00:44s
epoch 99 | loss: 0.24596 | val_0_rmse: 0.47981 | val_1_rmse: 0.51726 |  0:00:45s
epoch 100| loss: 0.24702 | val_0_rmse: 0.48172 | val_1_rmse: 0.5153  |  0:00:45s
epoch 101| loss: 0.2487  | val_0_rmse: 0.47308 | val_1_rmse: 0.50796 |  0:00:46s
epoch 102| loss: 0.24311 | val_0_rmse: 0.47687 | val_1_rmse: 0.50931 |  0:00:46s
epoch 103| loss: 0.24497 | val_0_rmse: 0.47791 | val_1_rmse: 0.517   |  0:00:47s
epoch 104| loss: 0.24219 | val_0_rmse: 0.49121 | val_1_rmse: 0.52766 |  0:00:47s
epoch 105| loss: 0.2395  | val_0_rmse: 0.46648 | val_1_rmse: 0.50608 |  0:00:48s
epoch 106| loss: 0.23963 | val_0_rmse: 0.47625 | val_1_rmse: 0.50083 |  0:00:48s
epoch 107| loss: 0.2438  | val_0_rmse: 0.47192 | val_1_rmse: 0.50601 |  0:00:48s
epoch 108| loss: 0.24872 | val_0_rmse: 0.47625 | val_1_rmse: 0.51608 |  0:00:49s
epoch 109| loss: 0.26122 | val_0_rmse: 0.47907 | val_1_rmse: 0.51475 |  0:00:49s
epoch 110| loss: 0.24925 | val_0_rmse: 0.48428 | val_1_rmse: 0.52308 |  0:00:50s
epoch 111| loss: 0.24679 | val_0_rmse: 0.47654 | val_1_rmse: 0.50982 |  0:00:50s
epoch 112| loss: 0.24342 | val_0_rmse: 0.47209 | val_1_rmse: 0.51    |  0:00:51s
epoch 113| loss: 0.24283 | val_0_rmse: 0.47347 | val_1_rmse: 0.50975 |  0:00:51s
epoch 114| loss: 0.24257 | val_0_rmse: 0.48487 | val_1_rmse: 0.51957 |  0:00:52s
epoch 115| loss: 0.24158 | val_0_rmse: 0.48434 | val_1_rmse: 0.52508 |  0:00:52s
epoch 116| loss: 0.24724 | val_0_rmse: 0.51096 | val_1_rmse: 0.54307 |  0:00:52s
epoch 117| loss: 0.25226 | val_0_rmse: 0.47071 | val_1_rmse: 0.51276 |  0:00:53s
epoch 118| loss: 0.24553 | val_0_rmse: 0.47187 | val_1_rmse: 0.50625 |  0:00:53s
epoch 119| loss: 0.23585 | val_0_rmse: 0.46389 | val_1_rmse: 0.50384 |  0:00:54s
epoch 120| loss: 0.23354 | val_0_rmse: 0.46223 | val_1_rmse: 0.50173 |  0:00:54s
epoch 121| loss: 0.23436 | val_0_rmse: 0.46013 | val_1_rmse: 0.5017  |  0:00:55s
epoch 122| loss: 0.23031 | val_0_rmse: 0.45859 | val_1_rmse: 0.49605 |  0:00:55s
epoch 123| loss: 0.23412 | val_0_rmse: 0.45961 | val_1_rmse: 0.49588 |  0:00:56s
epoch 124| loss: 0.23462 | val_0_rmse: 0.45521 | val_1_rmse: 0.50092 |  0:00:56s
epoch 125| loss: 0.228   | val_0_rmse: 0.46115 | val_1_rmse: 0.50559 |  0:00:57s
epoch 126| loss: 0.23074 | val_0_rmse: 0.4684  | val_1_rmse: 0.50772 |  0:00:57s
epoch 127| loss: 0.2298  | val_0_rmse: 0.45834 | val_1_rmse: 0.49492 |  0:00:57s
epoch 128| loss: 0.23229 | val_0_rmse: 0.46237 | val_1_rmse: 0.51115 |  0:00:58s
epoch 129| loss: 0.23051 | val_0_rmse: 0.46968 | val_1_rmse: 0.50856 |  0:00:58s
epoch 130| loss: 0.22871 | val_0_rmse: 0.45405 | val_1_rmse: 0.4995  |  0:00:59s
epoch 131| loss: 0.22773 | val_0_rmse: 0.47037 | val_1_rmse: 0.5142  |  0:00:59s
epoch 132| loss: 0.23483 | val_0_rmse: 0.46655 | val_1_rmse: 0.51428 |  0:01:00s
epoch 133| loss: 0.22806 | val_0_rmse: 0.45261 | val_1_rmse: 0.49267 |  0:01:00s
epoch 134| loss: 0.23047 | val_0_rmse: 0.45764 | val_1_rmse: 0.49738 |  0:01:01s
epoch 135| loss: 0.22815 | val_0_rmse: 0.47191 | val_1_rmse: 0.51547 |  0:01:01s
epoch 136| loss: 0.23254 | val_0_rmse: 0.45322 | val_1_rmse: 0.4979  |  0:01:02s
epoch 137| loss: 0.2254  | val_0_rmse: 0.46036 | val_1_rmse: 0.50657 |  0:01:02s
epoch 138| loss: 0.22734 | val_0_rmse: 0.46851 | val_1_rmse: 0.51531 |  0:01:02s
epoch 139| loss: 0.23332 | val_0_rmse: 0.45725 | val_1_rmse: 0.50513 |  0:01:03s
epoch 140| loss: 0.23028 | val_0_rmse: 0.46312 | val_1_rmse: 0.50726 |  0:01:03s
epoch 141| loss: 0.23571 | val_0_rmse: 0.46678 | val_1_rmse: 0.51547 |  0:01:04s
epoch 142| loss: 0.23076 | val_0_rmse: 0.47163 | val_1_rmse: 0.51114 |  0:01:04s
epoch 143| loss: 0.24155 | val_0_rmse: 0.46537 | val_1_rmse: 0.50801 |  0:01:05s
epoch 144| loss: 0.23749 | val_0_rmse: 0.4685  | val_1_rmse: 0.51391 |  0:01:05s
epoch 145| loss: 0.23723 | val_0_rmse: 0.47831 | val_1_rmse: 0.52445 |  0:01:06s
epoch 146| loss: 0.24332 | val_0_rmse: 0.46771 | val_1_rmse: 0.51399 |  0:01:06s
epoch 147| loss: 0.23904 | val_0_rmse: 0.45976 | val_1_rmse: 0.50143 |  0:01:07s
epoch 148| loss: 0.23275 | val_0_rmse: 0.46362 | val_1_rmse: 0.50832 |  0:01:07s
epoch 149| loss: 0.23519 | val_0_rmse: 0.47185 | val_1_rmse: 0.51355 |  0:01:07s
Stop training because you reached max_epochs = 150 with best_epoch = 133 and best_val_1_rmse = 0.49267
Best weights from best epoch are automatically used!
ended training at: 01:41:52
Feature importance:
[('Area', 0.20711900811184195), ('Baths', 0.04479103336479721), ('Beds', 0.13726785125765084), ('Latitude', 0.21442887260982452), ('Longitude', 0.32277939923965965), ('Month', 0.018814026529304174), ('Year', 0.05479980888692164)]
Mean squared error is of 20436772363.353397
Mean absolute error:102670.21295366719
MAPE:0.16831334009449725
R2 score:0.7495659466761717
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:41:53
epoch 0  | loss: 1.08061 | val_0_rmse: 0.90818 | val_1_rmse: 0.95525 |  0:00:00s
epoch 1  | loss: 0.55674 | val_0_rmse: 0.80501 | val_1_rmse: 0.82853 |  0:00:00s
epoch 2  | loss: 0.41607 | val_0_rmse: 0.7992  | val_1_rmse: 0.80385 |  0:00:01s
epoch 3  | loss: 0.37968 | val_0_rmse: 0.64129 | val_1_rmse: 0.62002 |  0:00:01s
epoch 4  | loss: 0.3538  | val_0_rmse: 0.58419 | val_1_rmse: 0.57028 |  0:00:02s
epoch 5  | loss: 0.34036 | val_0_rmse: 0.57679 | val_1_rmse: 0.56708 |  0:00:02s
epoch 6  | loss: 0.33281 | val_0_rmse: 0.56278 | val_1_rmse: 0.55216 |  0:00:03s
epoch 7  | loss: 0.33511 | val_0_rmse: 0.57505 | val_1_rmse: 0.57007 |  0:00:03s
epoch 8  | loss: 0.33963 | val_0_rmse: 0.57238 | val_1_rmse: 0.55899 |  0:00:04s
epoch 9  | loss: 0.33557 | val_0_rmse: 0.56366 | val_1_rmse: 0.5525  |  0:00:04s
epoch 10 | loss: 0.32422 | val_0_rmse: 0.54802 | val_1_rmse: 0.53535 |  0:00:05s
epoch 11 | loss: 0.31755 | val_0_rmse: 0.55347 | val_1_rmse: 0.53753 |  0:00:05s
epoch 12 | loss: 0.31751 | val_0_rmse: 0.54735 | val_1_rmse: 0.53426 |  0:00:05s
epoch 13 | loss: 0.31414 | val_0_rmse: 0.55559 | val_1_rmse: 0.54624 |  0:00:06s
epoch 14 | loss: 0.32247 | val_0_rmse: 0.54042 | val_1_rmse: 0.53441 |  0:00:06s
epoch 15 | loss: 0.31323 | val_0_rmse: 0.54246 | val_1_rmse: 0.54044 |  0:00:07s
epoch 16 | loss: 0.30348 | val_0_rmse: 0.5423  | val_1_rmse: 0.53994 |  0:00:07s
epoch 17 | loss: 0.30011 | val_0_rmse: 0.54821 | val_1_rmse: 0.55352 |  0:00:08s
epoch 18 | loss: 0.30636 | val_0_rmse: 0.54153 | val_1_rmse: 0.54752 |  0:00:08s
epoch 19 | loss: 0.30598 | val_0_rmse: 0.53627 | val_1_rmse: 0.52825 |  0:00:09s
epoch 20 | loss: 0.30162 | val_0_rmse: 0.54538 | val_1_rmse: 0.53077 |  0:00:09s
epoch 21 | loss: 0.31781 | val_0_rmse: 0.52743 | val_1_rmse: 0.52649 |  0:00:09s
epoch 22 | loss: 0.3025  | val_0_rmse: 0.51882 | val_1_rmse: 0.51777 |  0:00:10s
epoch 23 | loss: 0.29963 | val_0_rmse: 0.54591 | val_1_rmse: 0.54506 |  0:00:10s
epoch 24 | loss: 0.30754 | val_0_rmse: 0.52904 | val_1_rmse: 0.52903 |  0:00:11s
epoch 25 | loss: 0.30042 | val_0_rmse: 0.51398 | val_1_rmse: 0.51865 |  0:00:11s
epoch 26 | loss: 0.28599 | val_0_rmse: 0.51559 | val_1_rmse: 0.51761 |  0:00:12s
epoch 27 | loss: 0.28192 | val_0_rmse: 0.51383 | val_1_rmse: 0.51539 |  0:00:12s
epoch 28 | loss: 0.28678 | val_0_rmse: 0.50911 | val_1_rmse: 0.50975 |  0:00:13s
epoch 29 | loss: 0.27543 | val_0_rmse: 0.51794 | val_1_rmse: 0.51914 |  0:00:13s
epoch 30 | loss: 0.28066 | val_0_rmse: 0.5156  | val_1_rmse: 0.51514 |  0:00:14s
epoch 31 | loss: 0.27969 | val_0_rmse: 0.52134 | val_1_rmse: 0.52223 |  0:00:14s
epoch 32 | loss: 0.28812 | val_0_rmse: 0.51574 | val_1_rmse: 0.5197  |  0:00:14s
epoch 33 | loss: 0.28827 | val_0_rmse: 0.51052 | val_1_rmse: 0.51754 |  0:00:15s
epoch 34 | loss: 0.2787  | val_0_rmse: 0.50413 | val_1_rmse: 0.51141 |  0:00:15s
epoch 35 | loss: 0.28534 | val_0_rmse: 0.51245 | val_1_rmse: 0.51575 |  0:00:16s
epoch 36 | loss: 0.27801 | val_0_rmse: 0.50977 | val_1_rmse: 0.51627 |  0:00:16s
epoch 37 | loss: 0.27527 | val_0_rmse: 0.50291 | val_1_rmse: 0.51379 |  0:00:17s
epoch 38 | loss: 0.26876 | val_0_rmse: 0.50471 | val_1_rmse: 0.51079 |  0:00:17s
epoch 39 | loss: 0.26959 | val_0_rmse: 0.50065 | val_1_rmse: 0.50786 |  0:00:18s
epoch 40 | loss: 0.27506 | val_0_rmse: 0.50192 | val_1_rmse: 0.51066 |  0:00:18s
epoch 41 | loss: 0.27227 | val_0_rmse: 0.49137 | val_1_rmse: 0.50585 |  0:00:19s
epoch 42 | loss: 0.26974 | val_0_rmse: 0.49981 | val_1_rmse: 0.50223 |  0:00:19s
epoch 43 | loss: 0.26271 | val_0_rmse: 0.49651 | val_1_rmse: 0.50522 |  0:00:19s
epoch 44 | loss: 0.26901 | val_0_rmse: 0.51297 | val_1_rmse: 0.51951 |  0:00:20s
epoch 45 | loss: 0.26791 | val_0_rmse: 0.49959 | val_1_rmse: 0.50722 |  0:00:20s
epoch 46 | loss: 0.27281 | val_0_rmse: 0.4926  | val_1_rmse: 0.50899 |  0:00:21s
epoch 47 | loss: 0.27335 | val_0_rmse: 0.50844 | val_1_rmse: 0.52681 |  0:00:21s
epoch 48 | loss: 0.27084 | val_0_rmse: 0.49379 | val_1_rmse: 0.50653 |  0:00:22s
epoch 49 | loss: 0.26844 | val_0_rmse: 0.49892 | val_1_rmse: 0.51326 |  0:00:22s
epoch 50 | loss: 0.26574 | val_0_rmse: 0.49324 | val_1_rmse: 0.50127 |  0:00:23s
epoch 51 | loss: 0.26633 | val_0_rmse: 0.48682 | val_1_rmse: 0.50004 |  0:00:23s
epoch 52 | loss: 0.25652 | val_0_rmse: 0.49301 | val_1_rmse: 0.50362 |  0:00:23s
epoch 53 | loss: 0.25653 | val_0_rmse: 0.48847 | val_1_rmse: 0.50663 |  0:00:24s
epoch 54 | loss: 0.25684 | val_0_rmse: 0.49259 | val_1_rmse: 0.5027  |  0:00:24s
epoch 55 | loss: 0.25792 | val_0_rmse: 0.4891  | val_1_rmse: 0.50114 |  0:00:25s
epoch 56 | loss: 0.25848 | val_0_rmse: 0.48768 | val_1_rmse: 0.50455 |  0:00:25s
epoch 57 | loss: 0.25746 | val_0_rmse: 0.51519 | val_1_rmse: 0.52377 |  0:00:26s
epoch 58 | loss: 0.25731 | val_0_rmse: 0.50479 | val_1_rmse: 0.52383 |  0:00:26s
epoch 59 | loss: 0.26586 | val_0_rmse: 0.48988 | val_1_rmse: 0.5091  |  0:00:27s
epoch 60 | loss: 0.25866 | val_0_rmse: 0.4972  | val_1_rmse: 0.51698 |  0:00:27s
epoch 61 | loss: 0.26049 | val_0_rmse: 0.48567 | val_1_rmse: 0.50669 |  0:00:27s
epoch 62 | loss: 0.25611 | val_0_rmse: 0.49069 | val_1_rmse: 0.49986 |  0:00:28s
epoch 63 | loss: 0.26139 | val_0_rmse: 0.49276 | val_1_rmse: 0.51692 |  0:00:28s
epoch 64 | loss: 0.25811 | val_0_rmse: 0.48179 | val_1_rmse: 0.50315 |  0:00:29s
epoch 65 | loss: 0.2487  | val_0_rmse: 0.48764 | val_1_rmse: 0.5074  |  0:00:29s
epoch 66 | loss: 0.25887 | val_0_rmse: 0.48233 | val_1_rmse: 0.50402 |  0:00:30s
epoch 67 | loss: 0.25211 | val_0_rmse: 0.48136 | val_1_rmse: 0.49627 |  0:00:30s
epoch 68 | loss: 0.24781 | val_0_rmse: 0.48778 | val_1_rmse: 0.49915 |  0:00:31s
epoch 69 | loss: 0.24762 | val_0_rmse: 0.48823 | val_1_rmse: 0.50819 |  0:00:31s
epoch 70 | loss: 0.25584 | val_0_rmse: 0.49317 | val_1_rmse: 0.51091 |  0:00:32s
epoch 71 | loss: 0.255   | val_0_rmse: 0.48101 | val_1_rmse: 0.51456 |  0:00:32s
epoch 72 | loss: 0.2466  | val_0_rmse: 0.4742  | val_1_rmse: 0.49217 |  0:00:32s
epoch 73 | loss: 0.25268 | val_0_rmse: 0.48508 | val_1_rmse: 0.50038 |  0:00:33s
epoch 74 | loss: 0.25417 | val_0_rmse: 0.49779 | val_1_rmse: 0.51116 |  0:00:33s
epoch 75 | loss: 0.25862 | val_0_rmse: 0.49168 | val_1_rmse: 0.50524 |  0:00:34s
epoch 76 | loss: 0.25241 | val_0_rmse: 0.48277 | val_1_rmse: 0.49979 |  0:00:34s
epoch 77 | loss: 0.25027 | val_0_rmse: 0.49372 | val_1_rmse: 0.5086  |  0:00:35s
epoch 78 | loss: 0.25454 | val_0_rmse: 0.49186 | val_1_rmse: 0.50737 |  0:00:35s
epoch 79 | loss: 0.25115 | val_0_rmse: 0.47471 | val_1_rmse: 0.49426 |  0:00:36s
epoch 80 | loss: 0.25103 | val_0_rmse: 0.4822  | val_1_rmse: 0.50216 |  0:00:36s
epoch 81 | loss: 0.25413 | val_0_rmse: 0.47342 | val_1_rmse: 0.49455 |  0:00:37s
epoch 82 | loss: 0.24543 | val_0_rmse: 0.47254 | val_1_rmse: 0.49574 |  0:00:37s
epoch 83 | loss: 0.24653 | val_0_rmse: 0.48205 | val_1_rmse: 0.50539 |  0:00:38s
epoch 84 | loss: 0.24892 | val_0_rmse: 0.48008 | val_1_rmse: 0.50041 |  0:00:38s
epoch 85 | loss: 0.24553 | val_0_rmse: 0.47532 | val_1_rmse: 0.49228 |  0:00:38s
epoch 86 | loss: 0.24574 | val_0_rmse: 0.47772 | val_1_rmse: 0.50432 |  0:00:39s
epoch 87 | loss: 0.24484 | val_0_rmse: 0.47813 | val_1_rmse: 0.49734 |  0:00:39s
epoch 88 | loss: 0.24333 | val_0_rmse: 0.47979 | val_1_rmse: 0.49904 |  0:00:40s
epoch 89 | loss: 0.25083 | val_0_rmse: 0.49235 | val_1_rmse: 0.51027 |  0:00:40s
epoch 90 | loss: 0.25407 | val_0_rmse: 0.48855 | val_1_rmse: 0.50537 |  0:00:41s
epoch 91 | loss: 0.25487 | val_0_rmse: 0.47807 | val_1_rmse: 0.49459 |  0:00:41s
epoch 92 | loss: 0.24583 | val_0_rmse: 0.48299 | val_1_rmse: 0.50324 |  0:00:42s
epoch 93 | loss: 0.25294 | val_0_rmse: 0.49529 | val_1_rmse: 0.50795 |  0:00:42s
epoch 94 | loss: 0.25332 | val_0_rmse: 0.48364 | val_1_rmse: 0.5012  |  0:00:42s
epoch 95 | loss: 0.2435  | val_0_rmse: 0.46852 | val_1_rmse: 0.49794 |  0:00:43s
epoch 96 | loss: 0.23897 | val_0_rmse: 0.47062 | val_1_rmse: 0.49286 |  0:00:43s
epoch 97 | loss: 0.25034 | val_0_rmse: 0.48333 | val_1_rmse: 0.49996 |  0:00:44s
epoch 98 | loss: 0.24692 | val_0_rmse: 0.47815 | val_1_rmse: 0.50419 |  0:00:44s
epoch 99 | loss: 0.24889 | val_0_rmse: 0.49443 | val_1_rmse: 0.51435 |  0:00:45s
epoch 100| loss: 0.25753 | val_0_rmse: 0.47146 | val_1_rmse: 0.48928 |  0:00:45s
epoch 101| loss: 0.24316 | val_0_rmse: 0.47855 | val_1_rmse: 0.50496 |  0:00:46s
epoch 102| loss: 0.24293 | val_0_rmse: 0.4867  | val_1_rmse: 0.50637 |  0:00:46s
epoch 103| loss: 0.24776 | val_0_rmse: 0.47647 | val_1_rmse: 0.50176 |  0:00:47s
epoch 104| loss: 0.24504 | val_0_rmse: 0.47267 | val_1_rmse: 0.49748 |  0:00:47s
epoch 105| loss: 0.24216 | val_0_rmse: 0.47072 | val_1_rmse: 0.49711 |  0:00:47s
epoch 106| loss: 0.24333 | val_0_rmse: 0.47708 | val_1_rmse: 0.5036  |  0:00:48s
epoch 107| loss: 0.24012 | val_0_rmse: 0.46932 | val_1_rmse: 0.49483 |  0:00:48s
epoch 108| loss: 0.23541 | val_0_rmse: 0.46638 | val_1_rmse: 0.49029 |  0:00:49s
epoch 109| loss: 0.23723 | val_0_rmse: 0.47735 | val_1_rmse: 0.50125 |  0:00:49s
epoch 110| loss: 0.2453  | val_0_rmse: 0.47551 | val_1_rmse: 0.50225 |  0:00:50s
epoch 111| loss: 0.2407  | val_0_rmse: 0.47741 | val_1_rmse: 0.50415 |  0:00:50s
epoch 112| loss: 0.25255 | val_0_rmse: 0.47904 | val_1_rmse: 0.51031 |  0:00:51s
epoch 113| loss: 0.25336 | val_0_rmse: 0.47897 | val_1_rmse: 0.50984 |  0:00:51s
epoch 114| loss: 0.25192 | val_0_rmse: 0.48381 | val_1_rmse: 0.50961 |  0:00:51s
epoch 115| loss: 0.25162 | val_0_rmse: 0.49143 | val_1_rmse: 0.51535 |  0:00:52s
epoch 116| loss: 0.24723 | val_0_rmse: 0.47267 | val_1_rmse: 0.4927  |  0:00:52s
epoch 117| loss: 0.24302 | val_0_rmse: 0.48127 | val_1_rmse: 0.49893 |  0:00:53s
epoch 118| loss: 0.2478  | val_0_rmse: 0.48949 | val_1_rmse: 0.50634 |  0:00:53s
epoch 119| loss: 0.25491 | val_0_rmse: 0.48768 | val_1_rmse: 0.51076 |  0:00:54s
epoch 120| loss: 0.2474  | val_0_rmse: 0.47073 | val_1_rmse: 0.49302 |  0:00:54s
epoch 121| loss: 0.24372 | val_0_rmse: 0.46485 | val_1_rmse: 0.49231 |  0:00:55s
epoch 122| loss: 0.24274 | val_0_rmse: 0.47173 | val_1_rmse: 0.49989 |  0:00:55s
epoch 123| loss: 0.24824 | val_0_rmse: 0.46353 | val_1_rmse: 0.49033 |  0:00:55s
epoch 124| loss: 0.23855 | val_0_rmse: 0.46215 | val_1_rmse: 0.49222 |  0:00:56s
epoch 125| loss: 0.23897 | val_0_rmse: 0.47227 | val_1_rmse: 0.50504 |  0:00:56s
epoch 126| loss: 0.24523 | val_0_rmse: 0.46412 | val_1_rmse: 0.49625 |  0:00:57s
epoch 127| loss: 0.2396  | val_0_rmse: 0.47821 | val_1_rmse: 0.51053 |  0:00:57s
epoch 128| loss: 0.2421  | val_0_rmse: 0.482   | val_1_rmse: 0.50596 |  0:00:58s
epoch 129| loss: 0.24587 | val_0_rmse: 0.47491 | val_1_rmse: 0.50367 |  0:00:58s
epoch 130| loss: 0.23951 | val_0_rmse: 0.47035 | val_1_rmse: 0.49993 |  0:00:59s

Early stopping occured at epoch 130 with best_epoch = 100 and best_val_1_rmse = 0.48928
Best weights from best epoch are automatically used!
ended training at: 01:42:52
Feature importance:
[('Area', 0.23615271420118955), ('Baths', 0.0), ('Beds', 0.09215955062131621), ('Latitude', 0.2496673987372077), ('Longitude', 0.28151577485784507), ('Month', 0.09329917188355769), ('Year', 0.04720538969888375)]
Mean squared error is of 20082976042.303085
Mean absolute error:102190.73465927313
MAPE:0.1736830151971579
R2 score:0.7603171635192559
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:42:52
epoch 0  | loss: 1.00034 | val_0_rmse: 1.39675 | val_1_rmse: 1.39079 |  0:00:00s
epoch 1  | loss: 0.50676 | val_0_rmse: 0.88236 | val_1_rmse: 0.90446 |  0:00:00s
epoch 2  | loss: 0.41863 | val_0_rmse: 0.78958 | val_1_rmse: 0.83652 |  0:00:01s
epoch 3  | loss: 0.36625 | val_0_rmse: 0.66063 | val_1_rmse: 0.70881 |  0:00:01s
epoch 4  | loss: 0.35377 | val_0_rmse: 0.5968  | val_1_rmse: 0.64775 |  0:00:02s
epoch 5  | loss: 0.33495 | val_0_rmse: 0.55924 | val_1_rmse: 0.60863 |  0:00:02s
epoch 6  | loss: 0.31475 | val_0_rmse: 0.55011 | val_1_rmse: 0.60064 |  0:00:03s
epoch 7  | loss: 0.307   | val_0_rmse: 0.53832 | val_1_rmse: 0.58595 |  0:00:03s
epoch 8  | loss: 0.29441 | val_0_rmse: 0.52496 | val_1_rmse: 0.56097 |  0:00:04s
epoch 9  | loss: 0.29287 | val_0_rmse: 0.51586 | val_1_rmse: 0.55318 |  0:00:04s
epoch 10 | loss: 0.2859  | val_0_rmse: 0.52566 | val_1_rmse: 0.56868 |  0:00:05s
epoch 11 | loss: 0.28878 | val_0_rmse: 0.51124 | val_1_rmse: 0.54998 |  0:00:05s
epoch 12 | loss: 0.28165 | val_0_rmse: 0.53067 | val_1_rmse: 0.56647 |  0:00:05s
epoch 13 | loss: 0.28713 | val_0_rmse: 0.5079  | val_1_rmse: 0.54813 |  0:00:06s
epoch 14 | loss: 0.27657 | val_0_rmse: 0.51261 | val_1_rmse: 0.54624 |  0:00:06s
epoch 15 | loss: 0.27599 | val_0_rmse: 0.51285 | val_1_rmse: 0.54893 |  0:00:07s
epoch 16 | loss: 0.27117 | val_0_rmse: 0.50209 | val_1_rmse: 0.53783 |  0:00:07s
epoch 17 | loss: 0.27785 | val_0_rmse: 0.50698 | val_1_rmse: 0.54257 |  0:00:08s
epoch 18 | loss: 0.27021 | val_0_rmse: 0.50437 | val_1_rmse: 0.53499 |  0:00:08s
epoch 19 | loss: 0.27086 | val_0_rmse: 0.51057 | val_1_rmse: 0.55171 |  0:00:09s
epoch 20 | loss: 0.28002 | val_0_rmse: 0.5034  | val_1_rmse: 0.54145 |  0:00:09s
epoch 21 | loss: 0.27215 | val_0_rmse: 0.50438 | val_1_rmse: 0.54017 |  0:00:10s
epoch 22 | loss: 0.27346 | val_0_rmse: 0.52686 | val_1_rmse: 0.56816 |  0:00:10s
epoch 23 | loss: 0.27551 | val_0_rmse: 0.49995 | val_1_rmse: 0.53931 |  0:00:10s
epoch 24 | loss: 0.27642 | val_0_rmse: 0.52179 | val_1_rmse: 0.5486  |  0:00:11s
epoch 25 | loss: 0.27367 | val_0_rmse: 0.49811 | val_1_rmse: 0.536   |  0:00:11s
epoch 26 | loss: 0.26975 | val_0_rmse: 0.49731 | val_1_rmse: 0.53584 |  0:00:12s
epoch 27 | loss: 0.25898 | val_0_rmse: 0.50044 | val_1_rmse: 0.53602 |  0:00:12s
epoch 28 | loss: 0.27102 | val_0_rmse: 0.50163 | val_1_rmse: 0.54032 |  0:00:13s
epoch 29 | loss: 0.2648  | val_0_rmse: 0.49356 | val_1_rmse: 0.53566 |  0:00:13s
epoch 30 | loss: 0.26168 | val_0_rmse: 0.50116 | val_1_rmse: 0.54574 |  0:00:14s
epoch 31 | loss: 0.26358 | val_0_rmse: 0.49707 | val_1_rmse: 0.54139 |  0:00:14s
epoch 32 | loss: 0.25666 | val_0_rmse: 0.49094 | val_1_rmse: 0.53328 |  0:00:15s
epoch 33 | loss: 0.26763 | val_0_rmse: 0.50623 | val_1_rmse: 0.54504 |  0:00:15s
epoch 34 | loss: 0.27025 | val_0_rmse: 0.50269 | val_1_rmse: 0.5317  |  0:00:15s
epoch 35 | loss: 0.25918 | val_0_rmse: 0.49108 | val_1_rmse: 0.53091 |  0:00:16s
epoch 36 | loss: 0.25796 | val_0_rmse: 0.49027 | val_1_rmse: 0.53112 |  0:00:16s
epoch 37 | loss: 0.2584  | val_0_rmse: 0.49677 | val_1_rmse: 0.54258 |  0:00:17s
epoch 38 | loss: 0.26425 | val_0_rmse: 0.50606 | val_1_rmse: 0.5406  |  0:00:17s
epoch 39 | loss: 0.2657  | val_0_rmse: 0.50807 | val_1_rmse: 0.5471  |  0:00:18s
epoch 40 | loss: 0.26294 | val_0_rmse: 0.48818 | val_1_rmse: 0.52573 |  0:00:18s
epoch 41 | loss: 0.2551  | val_0_rmse: 0.49445 | val_1_rmse: 0.53864 |  0:00:19s
epoch 42 | loss: 0.2569  | val_0_rmse: 0.48724 | val_1_rmse: 0.52401 |  0:00:19s
epoch 43 | loss: 0.25286 | val_0_rmse: 0.48894 | val_1_rmse: 0.53089 |  0:00:19s
epoch 44 | loss: 0.25077 | val_0_rmse: 0.48479 | val_1_rmse: 0.52195 |  0:00:20s
epoch 45 | loss: 0.25598 | val_0_rmse: 0.50028 | val_1_rmse: 0.55349 |  0:00:20s
epoch 46 | loss: 0.25077 | val_0_rmse: 0.49142 | val_1_rmse: 0.53771 |  0:00:21s
epoch 47 | loss: 0.25228 | val_0_rmse: 0.49044 | val_1_rmse: 0.54105 |  0:00:21s
epoch 48 | loss: 0.25197 | val_0_rmse: 0.4929  | val_1_rmse: 0.5286  |  0:00:22s
epoch 49 | loss: 0.25863 | val_0_rmse: 0.49526 | val_1_rmse: 0.53954 |  0:00:22s
epoch 50 | loss: 0.26369 | val_0_rmse: 0.48899 | val_1_rmse: 0.53403 |  0:00:23s
epoch 51 | loss: 0.25914 | val_0_rmse: 0.4874  | val_1_rmse: 0.53224 |  0:00:23s
epoch 52 | loss: 0.26509 | val_0_rmse: 0.51051 | val_1_rmse: 0.56071 |  0:00:24s
epoch 53 | loss: 0.26828 | val_0_rmse: 0.51829 | val_1_rmse: 0.56044 |  0:00:24s
epoch 54 | loss: 0.26478 | val_0_rmse: 0.48874 | val_1_rmse: 0.53433 |  0:00:24s
epoch 55 | loss: 0.25716 | val_0_rmse: 0.48906 | val_1_rmse: 0.5319  |  0:00:25s
epoch 56 | loss: 0.25468 | val_0_rmse: 0.48913 | val_1_rmse: 0.53151 |  0:00:25s
epoch 57 | loss: 0.25686 | val_0_rmse: 0.49429 | val_1_rmse: 0.53805 |  0:00:26s
epoch 58 | loss: 0.25747 | val_0_rmse: 0.48381 | val_1_rmse: 0.52893 |  0:00:26s
epoch 59 | loss: 0.26077 | val_0_rmse: 0.4863  | val_1_rmse: 0.53385 |  0:00:27s
epoch 60 | loss: 0.25142 | val_0_rmse: 0.49617 | val_1_rmse: 0.54291 |  0:00:27s
epoch 61 | loss: 0.25449 | val_0_rmse: 0.49508 | val_1_rmse: 0.5383  |  0:00:28s
epoch 62 | loss: 0.25327 | val_0_rmse: 0.49082 | val_1_rmse: 0.53046 |  0:00:28s
epoch 63 | loss: 0.25829 | val_0_rmse: 0.49719 | val_1_rmse: 0.53436 |  0:00:29s
epoch 64 | loss: 0.26485 | val_0_rmse: 0.50324 | val_1_rmse: 0.55224 |  0:00:29s
epoch 65 | loss: 0.25778 | val_0_rmse: 0.48991 | val_1_rmse: 0.52547 |  0:00:29s
epoch 66 | loss: 0.25335 | val_0_rmse: 0.48284 | val_1_rmse: 0.53394 |  0:00:30s
epoch 67 | loss: 0.24972 | val_0_rmse: 0.4847  | val_1_rmse: 0.52486 |  0:00:30s
epoch 68 | loss: 0.24752 | val_0_rmse: 0.47728 | val_1_rmse: 0.52443 |  0:00:31s
epoch 69 | loss: 0.25438 | val_0_rmse: 0.48063 | val_1_rmse: 0.52778 |  0:00:31s
epoch 70 | loss: 0.24803 | val_0_rmse: 0.4802  | val_1_rmse: 0.52911 |  0:00:32s
epoch 71 | loss: 0.24884 | val_0_rmse: 0.47968 | val_1_rmse: 0.5317  |  0:00:32s
epoch 72 | loss: 0.25278 | val_0_rmse: 0.48241 | val_1_rmse: 0.53568 |  0:00:33s
epoch 73 | loss: 0.24887 | val_0_rmse: 0.49475 | val_1_rmse: 0.55694 |  0:00:33s
epoch 74 | loss: 0.2492  | val_0_rmse: 0.49814 | val_1_rmse: 0.55258 |  0:00:33s

Early stopping occured at epoch 74 with best_epoch = 44 and best_val_1_rmse = 0.52195
Best weights from best epoch are automatically used!
ended training at: 01:43:26
Feature importance:
[('Area', 0.24177125955162454), ('Baths', 0.03284657852653049), ('Beds', 0.04589985932717404), ('Latitude', 0.26823698370892246), ('Longitude', 0.35200040918354436), ('Month', 0.018652906278993374), ('Year', 0.04059200342321076)]
Mean squared error is of 21296396004.066086
Mean absolute error:104652.12766481057
MAPE:0.1777929522364845
R2 score:0.7451776997401072
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:43:26
epoch 0  | loss: 1.04478 | val_0_rmse: 0.87592 | val_1_rmse: 0.90057 |  0:00:00s
epoch 1  | loss: 0.61554 | val_0_rmse: 0.8061  | val_1_rmse: 0.82074 |  0:00:00s
epoch 2  | loss: 0.52939 | val_0_rmse: 0.71078 | val_1_rmse: 0.72715 |  0:00:01s
epoch 3  | loss: 0.45726 | val_0_rmse: 0.6746  | val_1_rmse: 0.699   |  0:00:01s
epoch 4  | loss: 0.40726 | val_0_rmse: 0.62553 | val_1_rmse: 0.63456 |  0:00:02s
epoch 5  | loss: 0.38349 | val_0_rmse: 0.61716 | val_1_rmse: 0.62945 |  0:00:02s
epoch 6  | loss: 0.37193 | val_0_rmse: 0.60526 | val_1_rmse: 0.59925 |  0:00:03s
epoch 7  | loss: 0.34365 | val_0_rmse: 0.56038 | val_1_rmse: 0.57203 |  0:00:03s
epoch 8  | loss: 0.33558 | val_0_rmse: 0.56193 | val_1_rmse: 0.57014 |  0:00:04s
epoch 9  | loss: 0.32825 | val_0_rmse: 0.56489 | val_1_rmse: 0.57721 |  0:00:04s
epoch 10 | loss: 0.32462 | val_0_rmse: 0.54771 | val_1_rmse: 0.5565  |  0:00:04s
epoch 11 | loss: 0.30945 | val_0_rmse: 0.54015 | val_1_rmse: 0.55229 |  0:00:05s
epoch 12 | loss: 0.30077 | val_0_rmse: 0.53673 | val_1_rmse: 0.54667 |  0:00:05s
epoch 13 | loss: 0.29968 | val_0_rmse: 0.54333 | val_1_rmse: 0.54335 |  0:00:06s
epoch 14 | loss: 0.30406 | val_0_rmse: 0.52806 | val_1_rmse: 0.5308  |  0:00:06s
epoch 15 | loss: 0.30118 | val_0_rmse: 0.53662 | val_1_rmse: 0.54334 |  0:00:07s
epoch 16 | loss: 0.29869 | val_0_rmse: 0.52462 | val_1_rmse: 0.53568 |  0:00:07s
epoch 17 | loss: 0.29963 | val_0_rmse: 0.52896 | val_1_rmse: 0.53991 |  0:00:08s
epoch 18 | loss: 0.29236 | val_0_rmse: 0.52322 | val_1_rmse: 0.52951 |  0:00:08s
epoch 19 | loss: 0.28853 | val_0_rmse: 0.52643 | val_1_rmse: 0.53419 |  0:00:09s
epoch 20 | loss: 0.28743 | val_0_rmse: 0.51271 | val_1_rmse: 0.52001 |  0:00:09s
epoch 21 | loss: 0.28209 | val_0_rmse: 0.52402 | val_1_rmse: 0.53324 |  0:00:09s
epoch 22 | loss: 0.29276 | val_0_rmse: 0.51985 | val_1_rmse: 0.5264  |  0:00:10s
epoch 23 | loss: 0.28171 | val_0_rmse: 0.51861 | val_1_rmse: 0.52466 |  0:00:10s
epoch 24 | loss: 0.27706 | val_0_rmse: 0.51399 | val_1_rmse: 0.52274 |  0:00:11s
epoch 25 | loss: 0.27822 | val_0_rmse: 0.52386 | val_1_rmse: 0.52614 |  0:00:11s
epoch 26 | loss: 0.29047 | val_0_rmse: 0.51271 | val_1_rmse: 0.52185 |  0:00:12s
epoch 27 | loss: 0.28884 | val_0_rmse: 0.54205 | val_1_rmse: 0.55562 |  0:00:12s
epoch 28 | loss: 0.28548 | val_0_rmse: 0.51002 | val_1_rmse: 0.52506 |  0:00:13s
epoch 29 | loss: 0.28901 | val_0_rmse: 0.53127 | val_1_rmse: 0.53688 |  0:00:13s
epoch 30 | loss: 0.27447 | val_0_rmse: 0.49816 | val_1_rmse: 0.51005 |  0:00:14s
epoch 31 | loss: 0.26977 | val_0_rmse: 0.49863 | val_1_rmse: 0.51897 |  0:00:14s
epoch 32 | loss: 0.27592 | val_0_rmse: 0.50047 | val_1_rmse: 0.51233 |  0:00:14s
epoch 33 | loss: 0.26802 | val_0_rmse: 0.49652 | val_1_rmse: 0.50948 |  0:00:15s
epoch 34 | loss: 0.2667  | val_0_rmse: 0.5034  | val_1_rmse: 0.52046 |  0:00:15s
epoch 35 | loss: 0.26658 | val_0_rmse: 0.49813 | val_1_rmse: 0.51221 |  0:00:16s
epoch 36 | loss: 0.26365 | val_0_rmse: 0.50068 | val_1_rmse: 0.51539 |  0:00:16s
epoch 37 | loss: 0.27511 | val_0_rmse: 0.50838 | val_1_rmse: 0.52289 |  0:00:17s
epoch 38 | loss: 0.28126 | val_0_rmse: 0.5097  | val_1_rmse: 0.52815 |  0:00:17s
epoch 39 | loss: 0.27211 | val_0_rmse: 0.4978  | val_1_rmse: 0.50822 |  0:00:18s
epoch 40 | loss: 0.27002 | val_0_rmse: 0.50539 | val_1_rmse: 0.51842 |  0:00:18s
epoch 41 | loss: 0.26832 | val_0_rmse: 0.50856 | val_1_rmse: 0.52591 |  0:00:18s
epoch 42 | loss: 0.27451 | val_0_rmse: 0.51474 | val_1_rmse: 0.52624 |  0:00:19s
epoch 43 | loss: 0.27221 | val_0_rmse: 0.53942 | val_1_rmse: 0.54847 |  0:00:19s
epoch 44 | loss: 0.26715 | val_0_rmse: 0.50631 | val_1_rmse: 0.52089 |  0:00:20s
epoch 45 | loss: 0.27104 | val_0_rmse: 0.49987 | val_1_rmse: 0.5173  |  0:00:20s
epoch 46 | loss: 0.28011 | val_0_rmse: 0.52432 | val_1_rmse: 0.54508 |  0:00:21s
epoch 47 | loss: 0.27695 | val_0_rmse: 0.49986 | val_1_rmse: 0.51579 |  0:00:21s
epoch 48 | loss: 0.26857 | val_0_rmse: 0.49469 | val_1_rmse: 0.51347 |  0:00:22s
epoch 49 | loss: 0.2641  | val_0_rmse: 0.49935 | val_1_rmse: 0.51209 |  0:00:22s
epoch 50 | loss: 0.2613  | val_0_rmse: 0.49538 | val_1_rmse: 0.51292 |  0:00:22s
epoch 51 | loss: 0.26361 | val_0_rmse: 0.51454 | val_1_rmse: 0.52865 |  0:00:23s
epoch 52 | loss: 0.26582 | val_0_rmse: 0.5164  | val_1_rmse: 0.53128 |  0:00:23s
epoch 53 | loss: 0.26584 | val_0_rmse: 0.4991  | val_1_rmse: 0.51515 |  0:00:24s
epoch 54 | loss: 0.26898 | val_0_rmse: 0.48803 | val_1_rmse: 0.50908 |  0:00:24s
epoch 55 | loss: 0.26823 | val_0_rmse: 0.52238 | val_1_rmse: 0.54115 |  0:00:25s
epoch 56 | loss: 0.26484 | val_0_rmse: 0.49228 | val_1_rmse: 0.50903 |  0:00:25s
epoch 57 | loss: 0.26029 | val_0_rmse: 0.4943  | val_1_rmse: 0.50884 |  0:00:26s
epoch 58 | loss: 0.2665  | val_0_rmse: 0.48837 | val_1_rmse: 0.50234 |  0:00:26s
epoch 59 | loss: 0.27314 | val_0_rmse: 0.50267 | val_1_rmse: 0.51206 |  0:00:27s
epoch 60 | loss: 0.26953 | val_0_rmse: 0.49787 | val_1_rmse: 0.51679 |  0:00:27s
epoch 61 | loss: 0.26418 | val_0_rmse: 0.49584 | val_1_rmse: 0.51067 |  0:00:27s
epoch 62 | loss: 0.25694 | val_0_rmse: 0.49454 | val_1_rmse: 0.51158 |  0:00:28s
epoch 63 | loss: 0.26192 | val_0_rmse: 0.49813 | val_1_rmse: 0.51214 |  0:00:28s
epoch 64 | loss: 0.25609 | val_0_rmse: 0.49014 | val_1_rmse: 0.50645 |  0:00:29s
epoch 65 | loss: 0.25884 | val_0_rmse: 0.49078 | val_1_rmse: 0.50758 |  0:00:29s
epoch 66 | loss: 0.2496  | val_0_rmse: 0.48598 | val_1_rmse: 0.50323 |  0:00:30s
epoch 67 | loss: 0.25344 | val_0_rmse: 0.48658 | val_1_rmse: 0.505   |  0:00:30s
epoch 68 | loss: 0.25718 | val_0_rmse: 0.49415 | val_1_rmse: 0.51007 |  0:00:31s
epoch 69 | loss: 0.25557 | val_0_rmse: 0.48355 | val_1_rmse: 0.50207 |  0:00:31s
epoch 70 | loss: 0.26057 | val_0_rmse: 0.48886 | val_1_rmse: 0.50527 |  0:00:31s
epoch 71 | loss: 0.2557  | val_0_rmse: 0.48818 | val_1_rmse: 0.50825 |  0:00:32s
epoch 72 | loss: 0.25594 | val_0_rmse: 0.4995  | val_1_rmse: 0.51749 |  0:00:32s
epoch 73 | loss: 0.26149 | val_0_rmse: 0.48078 | val_1_rmse: 0.50201 |  0:00:33s
epoch 74 | loss: 0.25172 | val_0_rmse: 0.49524 | val_1_rmse: 0.51802 |  0:00:33s
epoch 75 | loss: 0.25111 | val_0_rmse: 0.48725 | val_1_rmse: 0.50428 |  0:00:34s
epoch 76 | loss: 0.25395 | val_0_rmse: 0.49987 | val_1_rmse: 0.51301 |  0:00:34s
epoch 77 | loss: 0.25297 | val_0_rmse: 0.48715 | val_1_rmse: 0.50392 |  0:00:35s
epoch 78 | loss: 0.2577  | val_0_rmse: 0.49311 | val_1_rmse: 0.50743 |  0:00:35s
epoch 79 | loss: 0.24923 | val_0_rmse: 0.47952 | val_1_rmse: 0.50202 |  0:00:36s
epoch 80 | loss: 0.24903 | val_0_rmse: 0.4845  | val_1_rmse: 0.51236 |  0:00:36s
epoch 81 | loss: 0.25462 | val_0_rmse: 0.50192 | val_1_rmse: 0.51874 |  0:00:36s
epoch 82 | loss: 0.25562 | val_0_rmse: 0.48547 | val_1_rmse: 0.5103  |  0:00:37s
epoch 83 | loss: 0.24889 | val_0_rmse: 0.48649 | val_1_rmse: 0.50699 |  0:00:37s
epoch 84 | loss: 0.24572 | val_0_rmse: 0.47844 | val_1_rmse: 0.49498 |  0:00:38s
epoch 85 | loss: 0.24779 | val_0_rmse: 0.48501 | val_1_rmse: 0.50539 |  0:00:38s
epoch 86 | loss: 0.24728 | val_0_rmse: 0.48683 | val_1_rmse: 0.49812 |  0:00:39s
epoch 87 | loss: 0.25579 | val_0_rmse: 0.4895  | val_1_rmse: 0.50669 |  0:00:39s
epoch 88 | loss: 0.25088 | val_0_rmse: 0.49315 | val_1_rmse: 0.51011 |  0:00:40s
epoch 89 | loss: 0.25229 | val_0_rmse: 0.47986 | val_1_rmse: 0.49507 |  0:00:40s
epoch 90 | loss: 0.24851 | val_0_rmse: 0.48268 | val_1_rmse: 0.50073 |  0:00:40s
epoch 91 | loss: 0.25381 | val_0_rmse: 0.49415 | val_1_rmse: 0.50986 |  0:00:41s
epoch 92 | loss: 0.25179 | val_0_rmse: 0.47861 | val_1_rmse: 0.501   |  0:00:41s
epoch 93 | loss: 0.25235 | val_0_rmse: 0.47912 | val_1_rmse: 0.50397 |  0:00:42s
epoch 94 | loss: 0.24305 | val_0_rmse: 0.4895  | val_1_rmse: 0.50079 |  0:00:42s
epoch 95 | loss: 0.25383 | val_0_rmse: 0.5     | val_1_rmse: 0.51619 |  0:00:43s
epoch 96 | loss: 0.25894 | val_0_rmse: 0.48744 | val_1_rmse: 0.50321 |  0:00:43s
epoch 97 | loss: 0.2552  | val_0_rmse: 0.48984 | val_1_rmse: 0.50887 |  0:00:44s
epoch 98 | loss: 0.25575 | val_0_rmse: 0.48916 | val_1_rmse: 0.50891 |  0:00:44s
epoch 99 | loss: 0.2513  | val_0_rmse: 0.48428 | val_1_rmse: 0.50585 |  0:00:45s
epoch 100| loss: 0.25243 | val_0_rmse: 0.49395 | val_1_rmse: 0.51725 |  0:00:45s
epoch 101| loss: 0.25233 | val_0_rmse: 0.49058 | val_1_rmse: 0.51107 |  0:00:45s
epoch 102| loss: 0.25422 | val_0_rmse: 0.48267 | val_1_rmse: 0.50236 |  0:00:46s
epoch 103| loss: 0.2477  | val_0_rmse: 0.47122 | val_1_rmse: 0.49307 |  0:00:46s
epoch 104| loss: 0.24497 | val_0_rmse: 0.4826  | val_1_rmse: 0.50621 |  0:00:47s
epoch 105| loss: 0.2493  | val_0_rmse: 0.47546 | val_1_rmse: 0.49701 |  0:00:47s
epoch 106| loss: 0.24689 | val_0_rmse: 0.49839 | val_1_rmse: 0.5192  |  0:00:48s
epoch 107| loss: 0.24727 | val_0_rmse: 0.48924 | val_1_rmse: 0.51081 |  0:00:48s
epoch 108| loss: 0.24867 | val_0_rmse: 0.46975 | val_1_rmse: 0.49572 |  0:00:49s
epoch 109| loss: 0.24855 | val_0_rmse: 0.47468 | val_1_rmse: 0.49411 |  0:00:49s
epoch 110| loss: 0.24207 | val_0_rmse: 0.47506 | val_1_rmse: 0.50092 |  0:00:49s
epoch 111| loss: 0.23879 | val_0_rmse: 0.47626 | val_1_rmse: 0.503   |  0:00:50s
epoch 112| loss: 0.25121 | val_0_rmse: 0.47647 | val_1_rmse: 0.4993  |  0:00:50s
epoch 113| loss: 0.25911 | val_0_rmse: 0.47422 | val_1_rmse: 0.50219 |  0:00:51s
epoch 114| loss: 0.25404 | val_0_rmse: 0.47824 | val_1_rmse: 0.50222 |  0:00:51s
epoch 115| loss: 0.24717 | val_0_rmse: 0.48364 | val_1_rmse: 0.51058 |  0:00:52s
epoch 116| loss: 0.25188 | val_0_rmse: 0.48811 | val_1_rmse: 0.50572 |  0:00:52s
epoch 117| loss: 0.24907 | val_0_rmse: 0.4784  | val_1_rmse: 0.499   |  0:00:53s
epoch 118| loss: 0.2415  | val_0_rmse: 0.4747  | val_1_rmse: 0.50031 |  0:00:53s
epoch 119| loss: 0.24246 | val_0_rmse: 0.47154 | val_1_rmse: 0.49221 |  0:00:54s
epoch 120| loss: 0.2383  | val_0_rmse: 0.4914  | val_1_rmse: 0.51163 |  0:00:54s
epoch 121| loss: 0.24122 | val_0_rmse: 0.47234 | val_1_rmse: 0.49361 |  0:00:54s
epoch 122| loss: 0.24141 | val_0_rmse: 0.47338 | val_1_rmse: 0.49754 |  0:00:55s
epoch 123| loss: 0.24473 | val_0_rmse: 0.48852 | val_1_rmse: 0.51205 |  0:00:55s
epoch 124| loss: 0.24479 | val_0_rmse: 0.46856 | val_1_rmse: 0.4865  |  0:00:56s
epoch 125| loss: 0.2412  | val_0_rmse: 0.47271 | val_1_rmse: 0.48887 |  0:00:56s
epoch 126| loss: 0.23551 | val_0_rmse: 0.47819 | val_1_rmse: 0.50322 |  0:00:57s
epoch 127| loss: 0.23825 | val_0_rmse: 0.47061 | val_1_rmse: 0.49482 |  0:00:57s
epoch 128| loss: 0.23681 | val_0_rmse: 0.4711  | val_1_rmse: 0.49828 |  0:00:58s
epoch 129| loss: 0.24572 | val_0_rmse: 0.48107 | val_1_rmse: 0.50259 |  0:00:58s
epoch 130| loss: 0.24072 | val_0_rmse: 0.46671 | val_1_rmse: 0.48959 |  0:00:58s
epoch 131| loss: 0.23703 | val_0_rmse: 0.47823 | val_1_rmse: 0.50807 |  0:00:59s
epoch 132| loss: 0.24223 | val_0_rmse: 0.48835 | val_1_rmse: 0.51267 |  0:00:59s
epoch 133| loss: 0.24439 | val_0_rmse: 0.47976 | val_1_rmse: 0.50165 |  0:01:00s
epoch 134| loss: 0.25786 | val_0_rmse: 0.49429 | val_1_rmse: 0.52236 |  0:01:00s
epoch 135| loss: 0.24737 | val_0_rmse: 0.48113 | val_1_rmse: 0.5018  |  0:01:01s
epoch 136| loss: 0.24257 | val_0_rmse: 0.47858 | val_1_rmse: 0.50375 |  0:01:01s
epoch 137| loss: 0.24081 | val_0_rmse: 0.49616 | val_1_rmse: 0.51656 |  0:01:02s
epoch 138| loss: 0.26282 | val_0_rmse: 0.50555 | val_1_rmse: 0.52359 |  0:01:02s
epoch 139| loss: 0.26765 | val_0_rmse: 0.50119 | val_1_rmse: 0.51597 |  0:01:03s
epoch 140| loss: 0.26403 | val_0_rmse: 0.51182 | val_1_rmse: 0.52646 |  0:01:03s
epoch 141| loss: 0.25744 | val_0_rmse: 0.49368 | val_1_rmse: 0.51822 |  0:01:03s
epoch 142| loss: 0.25208 | val_0_rmse: 0.48444 | val_1_rmse: 0.50572 |  0:01:04s
epoch 143| loss: 0.24888 | val_0_rmse: 0.48372 | val_1_rmse: 0.50436 |  0:01:04s
epoch 144| loss: 0.26042 | val_0_rmse: 0.48552 | val_1_rmse: 0.5069  |  0:01:05s
epoch 145| loss: 0.24752 | val_0_rmse: 0.47769 | val_1_rmse: 0.50309 |  0:01:05s
epoch 146| loss: 0.24594 | val_0_rmse: 0.47203 | val_1_rmse: 0.4975  |  0:01:06s
epoch 147| loss: 0.23931 | val_0_rmse: 0.48176 | val_1_rmse: 0.50734 |  0:01:06s
epoch 148| loss: 0.23838 | val_0_rmse: 0.46985 | val_1_rmse: 0.4999  |  0:01:07s
epoch 149| loss: 0.24312 | val_0_rmse: 0.47069 | val_1_rmse: 0.49739 |  0:01:07s
Stop training because you reached max_epochs = 150 with best_epoch = 124 and best_val_1_rmse = 0.4865
Best weights from best epoch are automatically used!
ended training at: 01:44:34
Feature importance:
[('Area', 0.16203714867197272), ('Baths', 0.012469450105767511), ('Beds', 0.17817191728438592), ('Latitude', 0.27262645793999174), ('Longitude', 0.3068466881128892), ('Month', 0.05349050705284113), ('Year', 0.014357830832151785)]
Mean squared error is of 21189823120.34031
Mean absolute error:106935.93368986594
MAPE:0.1861431711908867
R2 score:0.7467272856506562
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:44:34
epoch 0  | loss: 1.01165 | val_0_rmse: 0.93042 | val_1_rmse: 0.91254 |  0:00:00s
epoch 1  | loss: 0.56006 | val_0_rmse: 0.80289 | val_1_rmse: 0.76214 |  0:00:00s
epoch 2  | loss: 0.46911 | val_0_rmse: 0.80958 | val_1_rmse: 0.77146 |  0:00:01s
epoch 3  | loss: 0.40789 | val_0_rmse: 0.68942 | val_1_rmse: 0.64413 |  0:00:01s
epoch 4  | loss: 0.38324 | val_0_rmse: 0.64269 | val_1_rmse: 0.59953 |  0:00:02s
epoch 5  | loss: 0.37472 | val_0_rmse: 0.68068 | val_1_rmse: 0.64069 |  0:00:02s
epoch 6  | loss: 0.38316 | val_0_rmse: 0.63579 | val_1_rmse: 0.60216 |  0:00:03s
epoch 7  | loss: 0.37843 | val_0_rmse: 0.60232 | val_1_rmse: 0.57049 |  0:00:03s
epoch 8  | loss: 0.35868 | val_0_rmse: 0.57723 | val_1_rmse: 0.55262 |  0:00:04s
epoch 9  | loss: 0.34238 | val_0_rmse: 0.56489 | val_1_rmse: 0.53897 |  0:00:04s
epoch 10 | loss: 0.34042 | val_0_rmse: 0.55975 | val_1_rmse: 0.52681 |  0:00:05s
epoch 11 | loss: 0.32302 | val_0_rmse: 0.56204 | val_1_rmse: 0.53458 |  0:00:05s
epoch 12 | loss: 0.32387 | val_0_rmse: 0.54253 | val_1_rmse: 0.51426 |  0:00:05s
epoch 13 | loss: 0.31855 | val_0_rmse: 0.54007 | val_1_rmse: 0.5113  |  0:00:06s
epoch 14 | loss: 0.32819 | val_0_rmse: 0.55346 | val_1_rmse: 0.51757 |  0:00:06s
epoch 15 | loss: 0.32424 | val_0_rmse: 0.5715  | val_1_rmse: 0.54395 |  0:00:07s
epoch 16 | loss: 0.32191 | val_0_rmse: 0.5584  | val_1_rmse: 0.52486 |  0:00:07s
epoch 17 | loss: 0.31956 | val_0_rmse: 0.54099 | val_1_rmse: 0.50732 |  0:00:08s
epoch 18 | loss: 0.30968 | val_0_rmse: 0.53082 | val_1_rmse: 0.5075  |  0:00:08s
epoch 19 | loss: 0.30662 | val_0_rmse: 0.54227 | val_1_rmse: 0.51501 |  0:00:09s
epoch 20 | loss: 0.30009 | val_0_rmse: 0.52835 | val_1_rmse: 0.50322 |  0:00:09s
epoch 21 | loss: 0.29831 | val_0_rmse: 0.53036 | val_1_rmse: 0.50273 |  0:00:09s
epoch 22 | loss: 0.29911 | val_0_rmse: 0.52781 | val_1_rmse: 0.4941  |  0:00:10s
epoch 23 | loss: 0.30485 | val_0_rmse: 0.53626 | val_1_rmse: 0.50095 |  0:00:10s
epoch 24 | loss: 0.30346 | val_0_rmse: 0.53382 | val_1_rmse: 0.51375 |  0:00:11s
epoch 25 | loss: 0.30639 | val_0_rmse: 0.52943 | val_1_rmse: 0.50725 |  0:00:11s
epoch 26 | loss: 0.30092 | val_0_rmse: 0.52644 | val_1_rmse: 0.49593 |  0:00:12s
epoch 27 | loss: 0.3023  | val_0_rmse: 0.54408 | val_1_rmse: 0.50192 |  0:00:12s
epoch 28 | loss: 0.2965  | val_0_rmse: 0.53494 | val_1_rmse: 0.49421 |  0:00:13s
epoch 29 | loss: 0.2961  | val_0_rmse: 0.51784 | val_1_rmse: 0.484   |  0:00:13s
epoch 30 | loss: 0.2897  | val_0_rmse: 0.52282 | val_1_rmse: 0.49434 |  0:00:14s
epoch 31 | loss: 0.29162 | val_0_rmse: 0.51277 | val_1_rmse: 0.48417 |  0:00:14s
epoch 32 | loss: 0.28885 | val_0_rmse: 0.51355 | val_1_rmse: 0.4888  |  0:00:14s
epoch 33 | loss: 0.28683 | val_0_rmse: 0.51602 | val_1_rmse: 0.48683 |  0:00:15s
epoch 34 | loss: 0.28042 | val_0_rmse: 0.51625 | val_1_rmse: 0.48996 |  0:00:15s
epoch 35 | loss: 0.28504 | val_0_rmse: 0.51884 | val_1_rmse: 0.49655 |  0:00:16s
epoch 36 | loss: 0.28178 | val_0_rmse: 0.51577 | val_1_rmse: 0.49135 |  0:00:16s
epoch 37 | loss: 0.27412 | val_0_rmse: 0.50719 | val_1_rmse: 0.48515 |  0:00:17s
epoch 38 | loss: 0.28137 | val_0_rmse: 0.5074  | val_1_rmse: 0.48302 |  0:00:17s
epoch 39 | loss: 0.28061 | val_0_rmse: 0.52171 | val_1_rmse: 0.49742 |  0:00:18s
epoch 40 | loss: 0.28746 | val_0_rmse: 0.51278 | val_1_rmse: 0.48187 |  0:00:18s
epoch 41 | loss: 0.28209 | val_0_rmse: 0.52076 | val_1_rmse: 0.48438 |  0:00:18s
epoch 42 | loss: 0.28052 | val_0_rmse: 0.52032 | val_1_rmse: 0.49336 |  0:00:19s
epoch 43 | loss: 0.28917 | val_0_rmse: 0.51006 | val_1_rmse: 0.48757 |  0:00:19s
epoch 44 | loss: 0.27944 | val_0_rmse: 0.5154  | val_1_rmse: 0.50006 |  0:00:20s
epoch 45 | loss: 0.28188 | val_0_rmse: 0.50723 | val_1_rmse: 0.47972 |  0:00:20s
epoch 46 | loss: 0.28087 | val_0_rmse: 0.50684 | val_1_rmse: 0.48042 |  0:00:21s
epoch 47 | loss: 0.27713 | val_0_rmse: 0.49997 | val_1_rmse: 0.47621 |  0:00:21s
epoch 48 | loss: 0.28292 | val_0_rmse: 0.49919 | val_1_rmse: 0.47354 |  0:00:22s
epoch 49 | loss: 0.28513 | val_0_rmse: 0.51913 | val_1_rmse: 0.50063 |  0:00:22s
epoch 50 | loss: 0.28777 | val_0_rmse: 0.51257 | val_1_rmse: 0.48894 |  0:00:23s
epoch 51 | loss: 0.2852  | val_0_rmse: 0.5039  | val_1_rmse: 0.48287 |  0:00:23s
epoch 52 | loss: 0.27394 | val_0_rmse: 0.50597 | val_1_rmse: 0.48822 |  0:00:23s
epoch 53 | loss: 0.27212 | val_0_rmse: 0.52058 | val_1_rmse: 0.50129 |  0:00:24s
epoch 54 | loss: 0.27519 | val_0_rmse: 0.49735 | val_1_rmse: 0.48023 |  0:00:24s
epoch 55 | loss: 0.26836 | val_0_rmse: 0.49435 | val_1_rmse: 0.4738  |  0:00:25s
epoch 56 | loss: 0.26416 | val_0_rmse: 0.4912  | val_1_rmse: 0.47414 |  0:00:25s
epoch 57 | loss: 0.2611  | val_0_rmse: 0.48956 | val_1_rmse: 0.47179 |  0:00:26s
epoch 58 | loss: 0.2675  | val_0_rmse: 0.50246 | val_1_rmse: 0.48589 |  0:00:26s
epoch 59 | loss: 0.26398 | val_0_rmse: 0.49049 | val_1_rmse: 0.47666 |  0:00:27s
epoch 60 | loss: 0.26781 | val_0_rmse: 0.51895 | val_1_rmse: 0.49379 |  0:00:27s
epoch 61 | loss: 0.27766 | val_0_rmse: 0.50059 | val_1_rmse: 0.48621 |  0:00:28s
epoch 62 | loss: 0.27338 | val_0_rmse: 0.49444 | val_1_rmse: 0.47308 |  0:00:28s
epoch 63 | loss: 0.27453 | val_0_rmse: 0.4937  | val_1_rmse: 0.4764  |  0:00:28s
epoch 64 | loss: 0.26938 | val_0_rmse: 0.5001  | val_1_rmse: 0.48742 |  0:00:29s
epoch 65 | loss: 0.27544 | val_0_rmse: 0.49912 | val_1_rmse: 0.48111 |  0:00:29s
epoch 66 | loss: 0.26534 | val_0_rmse: 0.4987  | val_1_rmse: 0.47856 |  0:00:30s
epoch 67 | loss: 0.25997 | val_0_rmse: 0.49055 | val_1_rmse: 0.47641 |  0:00:30s
epoch 68 | loss: 0.25902 | val_0_rmse: 0.49947 | val_1_rmse: 0.48469 |  0:00:31s
epoch 69 | loss: 0.26234 | val_0_rmse: 0.48856 | val_1_rmse: 0.47769 |  0:00:31s
epoch 70 | loss: 0.25824 | val_0_rmse: 0.48745 | val_1_rmse: 0.4756  |  0:00:32s
epoch 71 | loss: 0.26181 | val_0_rmse: 0.48557 | val_1_rmse: 0.46802 |  0:00:32s
epoch 72 | loss: 0.25665 | val_0_rmse: 0.49843 | val_1_rmse: 0.49148 |  0:00:32s
epoch 73 | loss: 0.25865 | val_0_rmse: 0.50651 | val_1_rmse: 0.48673 |  0:00:33s
epoch 74 | loss: 0.26616 | val_0_rmse: 0.49069 | val_1_rmse: 0.48087 |  0:00:33s
epoch 75 | loss: 0.26281 | val_0_rmse: 0.48885 | val_1_rmse: 0.48537 |  0:00:34s
epoch 76 | loss: 0.25945 | val_0_rmse: 0.47976 | val_1_rmse: 0.46737 |  0:00:34s
epoch 77 | loss: 0.24829 | val_0_rmse: 0.48534 | val_1_rmse: 0.48056 |  0:00:35s
epoch 78 | loss: 0.24824 | val_0_rmse: 0.47966 | val_1_rmse: 0.47377 |  0:00:35s
epoch 79 | loss: 0.25725 | val_0_rmse: 0.48257 | val_1_rmse: 0.47749 |  0:00:36s
epoch 80 | loss: 0.26206 | val_0_rmse: 0.48668 | val_1_rmse: 0.47738 |  0:00:36s
epoch 81 | loss: 0.25236 | val_0_rmse: 0.49227 | val_1_rmse: 0.48445 |  0:00:36s
epoch 82 | loss: 0.26013 | val_0_rmse: 0.4781  | val_1_rmse: 0.47016 |  0:00:37s
epoch 83 | loss: 0.25158 | val_0_rmse: 0.48916 | val_1_rmse: 0.48283 |  0:00:37s
epoch 84 | loss: 0.25091 | val_0_rmse: 0.4779  | val_1_rmse: 0.47003 |  0:00:38s
epoch 85 | loss: 0.26304 | val_0_rmse: 0.48197 | val_1_rmse: 0.47242 |  0:00:38s
epoch 86 | loss: 0.25173 | val_0_rmse: 0.48895 | val_1_rmse: 0.48935 |  0:00:39s
epoch 87 | loss: 0.26178 | val_0_rmse: 0.50224 | val_1_rmse: 0.49793 |  0:00:39s
epoch 88 | loss: 0.25674 | val_0_rmse: 0.48068 | val_1_rmse: 0.47336 |  0:00:40s
epoch 89 | loss: 0.25696 | val_0_rmse: 0.48595 | val_1_rmse: 0.47677 |  0:00:40s
epoch 90 | loss: 0.2558  | val_0_rmse: 0.48358 | val_1_rmse: 0.47156 |  0:00:41s
epoch 91 | loss: 0.2581  | val_0_rmse: 0.48367 | val_1_rmse: 0.47668 |  0:00:41s
epoch 92 | loss: 0.25325 | val_0_rmse: 0.47594 | val_1_rmse: 0.47091 |  0:00:41s
epoch 93 | loss: 0.24733 | val_0_rmse: 0.47578 | val_1_rmse: 0.47211 |  0:00:42s
epoch 94 | loss: 0.25098 | val_0_rmse: 0.47431 | val_1_rmse: 0.47605 |  0:00:42s
epoch 95 | loss: 0.2469  | val_0_rmse: 0.47561 | val_1_rmse: 0.47442 |  0:00:43s
epoch 96 | loss: 0.25211 | val_0_rmse: 0.48029 | val_1_rmse: 0.47713 |  0:00:43s
epoch 97 | loss: 0.25109 | val_0_rmse: 0.47412 | val_1_rmse: 0.4771  |  0:00:44s
epoch 98 | loss: 0.24434 | val_0_rmse: 0.47508 | val_1_rmse: 0.47406 |  0:00:44s
epoch 99 | loss: 0.2567  | val_0_rmse: 0.48855 | val_1_rmse: 0.48245 |  0:00:45s
epoch 100| loss: 0.25143 | val_0_rmse: 0.48854 | val_1_rmse: 0.48562 |  0:00:45s
epoch 101| loss: 0.24999 | val_0_rmse: 0.48459 | val_1_rmse: 0.48002 |  0:00:45s
epoch 102| loss: 0.2565  | val_0_rmse: 0.47023 | val_1_rmse: 0.46813 |  0:00:46s
epoch 103| loss: 0.25044 | val_0_rmse: 0.48093 | val_1_rmse: 0.48315 |  0:00:46s
epoch 104| loss: 0.25165 | val_0_rmse: 0.48577 | val_1_rmse: 0.48671 |  0:00:47s
epoch 105| loss: 0.24781 | val_0_rmse: 0.47515 | val_1_rmse: 0.48126 |  0:00:47s
epoch 106| loss: 0.25503 | val_0_rmse: 0.48013 | val_1_rmse: 0.4823  |  0:00:48s

Early stopping occured at epoch 106 with best_epoch = 76 and best_val_1_rmse = 0.46737
Best weights from best epoch are automatically used!
ended training at: 01:45:22
Feature importance:
[('Area', 0.2662215061407111), ('Baths', 0.03171316233431463), ('Beds', 0.06534818403012871), ('Latitude', 0.2734039941029911), ('Longitude', 0.34682811630335114), ('Month', 0.0), ('Year', 0.016485037088503247)]
Mean squared error is of 20444345489.3768
Mean absolute error:103763.77429449264
MAPE:0.17720110964983332
R2 score:0.7479850983994107
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:45:22
epoch 0  | loss: 1.52449 | val_0_rmse: 1.15526 | val_1_rmse: 1.19085 |  0:00:00s
epoch 1  | loss: 0.75482 | val_0_rmse: 1.05024 | val_1_rmse: 1.11714 |  0:00:00s
epoch 2  | loss: 0.68955 | val_0_rmse: 0.86988 | val_1_rmse: 0.86341 |  0:00:00s
epoch 3  | loss: 0.58091 | val_0_rmse: 0.91042 | val_1_rmse: 0.90732 |  0:00:00s
epoch 4  | loss: 0.50468 | val_0_rmse: 0.77856 | val_1_rmse: 0.80366 |  0:00:00s
epoch 5  | loss: 0.49204 | val_0_rmse: 0.78095 | val_1_rmse: 0.84614 |  0:00:00s
epoch 6  | loss: 0.46922 | val_0_rmse: 0.74479 | val_1_rmse: 0.76837 |  0:00:00s
epoch 7  | loss: 0.46832 | val_0_rmse: 0.73867 | val_1_rmse: 0.73833 |  0:00:01s
epoch 8  | loss: 0.47148 | val_0_rmse: 0.74876 | val_1_rmse: 0.72313 |  0:00:01s
epoch 9  | loss: 0.46776 | val_0_rmse: 0.75518 | val_1_rmse: 0.75867 |  0:00:01s
epoch 10 | loss: 0.46265 | val_0_rmse: 0.77667 | val_1_rmse: 0.79858 |  0:00:01s
epoch 11 | loss: 0.44947 | val_0_rmse: 0.75931 | val_1_rmse: 0.84098 |  0:00:01s
epoch 12 | loss: 0.4477  | val_0_rmse: 0.72597 | val_1_rmse: 0.81601 |  0:00:01s
epoch 13 | loss: 0.45986 | val_0_rmse: 0.70231 | val_1_rmse: 0.76678 |  0:00:01s
epoch 14 | loss: 0.44867 | val_0_rmse: 0.69752 | val_1_rmse: 0.75596 |  0:00:02s
epoch 15 | loss: 0.46114 | val_0_rmse: 0.71334 | val_1_rmse: 0.75142 |  0:00:02s
epoch 16 | loss: 0.46456 | val_0_rmse: 0.74889 | val_1_rmse: 0.78059 |  0:00:02s
epoch 17 | loss: 0.47863 | val_0_rmse: 0.69367 | val_1_rmse: 0.74956 |  0:00:02s
epoch 18 | loss: 0.46298 | val_0_rmse: 0.69907 | val_1_rmse: 0.73275 |  0:00:02s
epoch 19 | loss: 0.46401 | val_0_rmse: 0.68671 | val_1_rmse: 0.69841 |  0:00:02s
epoch 20 | loss: 0.46262 | val_0_rmse: 0.67663 | val_1_rmse: 0.72374 |  0:00:02s
epoch 21 | loss: 0.44834 | val_0_rmse: 0.66751 | val_1_rmse: 0.70063 |  0:00:02s
epoch 22 | loss: 0.43864 | val_0_rmse: 0.66196 | val_1_rmse: 0.6958  |  0:00:03s
epoch 23 | loss: 0.43515 | val_0_rmse: 0.66093 | val_1_rmse: 0.69958 |  0:00:03s
epoch 24 | loss: 0.42011 | val_0_rmse: 0.65376 | val_1_rmse: 0.70909 |  0:00:03s
epoch 25 | loss: 0.43546 | val_0_rmse: 0.64151 | val_1_rmse: 0.69435 |  0:00:03s
epoch 26 | loss: 0.42101 | val_0_rmse: 0.64222 | val_1_rmse: 0.68416 |  0:00:03s
epoch 27 | loss: 0.43049 | val_0_rmse: 0.64345 | val_1_rmse: 0.68235 |  0:00:03s
epoch 28 | loss: 0.42084 | val_0_rmse: 0.63565 | val_1_rmse: 0.67837 |  0:00:03s
epoch 29 | loss: 0.41639 | val_0_rmse: 0.64151 | val_1_rmse: 0.68345 |  0:00:04s
epoch 30 | loss: 0.41515 | val_0_rmse: 0.64477 | val_1_rmse: 0.68772 |  0:00:04s
epoch 31 | loss: 0.41514 | val_0_rmse: 0.63907 | val_1_rmse: 0.67829 |  0:00:04s
epoch 32 | loss: 0.40708 | val_0_rmse: 0.63683 | val_1_rmse: 0.67034 |  0:00:04s
epoch 33 | loss: 0.42192 | val_0_rmse: 0.63328 | val_1_rmse: 0.68146 |  0:00:04s
epoch 34 | loss: 0.40502 | val_0_rmse: 0.63886 | val_1_rmse: 0.69551 |  0:00:04s
epoch 35 | loss: 0.4075  | val_0_rmse: 0.65408 | val_1_rmse: 0.69895 |  0:00:04s
epoch 36 | loss: 0.43    | val_0_rmse: 0.64554 | val_1_rmse: 0.69018 |  0:00:04s
epoch 37 | loss: 0.42239 | val_0_rmse: 0.64745 | val_1_rmse: 0.67683 |  0:00:05s
epoch 38 | loss: 0.425   | val_0_rmse: 0.64807 | val_1_rmse: 0.68285 |  0:00:05s
epoch 39 | loss: 0.42512 | val_0_rmse: 0.65164 | val_1_rmse: 0.70197 |  0:00:05s
epoch 40 | loss: 0.41667 | val_0_rmse: 0.6388  | val_1_rmse: 0.69439 |  0:00:05s
epoch 41 | loss: 0.41333 | val_0_rmse: 0.64003 | val_1_rmse: 0.69902 |  0:00:05s
epoch 42 | loss: 0.41933 | val_0_rmse: 0.64056 | val_1_rmse: 0.68926 |  0:00:05s
epoch 43 | loss: 0.41515 | val_0_rmse: 0.6346  | val_1_rmse: 0.68981 |  0:00:05s
epoch 44 | loss: 0.41603 | val_0_rmse: 0.62901 | val_1_rmse: 0.69216 |  0:00:05s
epoch 45 | loss: 0.40226 | val_0_rmse: 0.62258 | val_1_rmse: 0.69004 |  0:00:06s
epoch 46 | loss: 0.40812 | val_0_rmse: 0.63166 | val_1_rmse: 0.71646 |  0:00:06s
epoch 47 | loss: 0.41272 | val_0_rmse: 0.63939 | val_1_rmse: 0.73585 |  0:00:06s
epoch 48 | loss: 0.40566 | val_0_rmse: 0.62995 | val_1_rmse: 0.7245  |  0:00:06s
epoch 49 | loss: 0.42231 | val_0_rmse: 0.62308 | val_1_rmse: 0.6985  |  0:00:06s
epoch 50 | loss: 0.39867 | val_0_rmse: 0.63636 | val_1_rmse: 0.70226 |  0:00:06s
epoch 51 | loss: 0.41444 | val_0_rmse: 0.62165 | val_1_rmse: 0.67931 |  0:00:06s
epoch 52 | loss: 0.40167 | val_0_rmse: 0.61962 | val_1_rmse: 0.68489 |  0:00:07s
epoch 53 | loss: 0.39141 | val_0_rmse: 0.62614 | val_1_rmse: 0.69514 |  0:00:07s
epoch 54 | loss: 0.39314 | val_0_rmse: 0.62081 | val_1_rmse: 0.6677  |  0:00:07s
epoch 55 | loss: 0.40432 | val_0_rmse: 0.61764 | val_1_rmse: 0.67    |  0:00:07s
epoch 56 | loss: 0.39533 | val_0_rmse: 0.62427 | val_1_rmse: 0.69575 |  0:00:07s
epoch 57 | loss: 0.38848 | val_0_rmse: 0.61823 | val_1_rmse: 0.67318 |  0:00:07s
epoch 58 | loss: 0.39912 | val_0_rmse: 0.6137  | val_1_rmse: 0.67795 |  0:00:07s
epoch 59 | loss: 0.39187 | val_0_rmse: 0.61828 | val_1_rmse: 0.69585 |  0:00:07s
epoch 60 | loss: 0.38873 | val_0_rmse: 0.61172 | val_1_rmse: 0.67571 |  0:00:08s
epoch 61 | loss: 0.38774 | val_0_rmse: 0.61518 | val_1_rmse: 0.68182 |  0:00:08s
epoch 62 | loss: 0.38215 | val_0_rmse: 0.62081 | val_1_rmse: 0.69021 |  0:00:08s
epoch 63 | loss: 0.39001 | val_0_rmse: 0.61782 | val_1_rmse: 0.68843 |  0:00:08s
epoch 64 | loss: 0.37342 | val_0_rmse: 0.61694 | val_1_rmse: 0.68179 |  0:00:08s
epoch 65 | loss: 0.38061 | val_0_rmse: 0.60999 | val_1_rmse: 0.65895 |  0:00:08s
epoch 66 | loss: 0.39436 | val_0_rmse: 0.61284 | val_1_rmse: 0.65707 |  0:00:08s
epoch 67 | loss: 0.37867 | val_0_rmse: 0.61904 | val_1_rmse: 0.66007 |  0:00:09s
epoch 68 | loss: 0.37944 | val_0_rmse: 0.61069 | val_1_rmse: 0.66252 |  0:00:09s
epoch 69 | loss: 0.37825 | val_0_rmse: 0.61884 | val_1_rmse: 0.67695 |  0:00:09s
epoch 70 | loss: 0.39001 | val_0_rmse: 0.60811 | val_1_rmse: 0.65734 |  0:00:09s
epoch 71 | loss: 0.38502 | val_0_rmse: 0.60437 | val_1_rmse: 0.64612 |  0:00:09s
epoch 72 | loss: 0.38063 | val_0_rmse: 0.6014  | val_1_rmse: 0.65987 |  0:00:09s
epoch 73 | loss: 0.36953 | val_0_rmse: 0.60565 | val_1_rmse: 0.66674 |  0:00:09s
epoch 74 | loss: 0.37495 | val_0_rmse: 0.60512 | val_1_rmse: 0.66829 |  0:00:09s
epoch 75 | loss: 0.37483 | val_0_rmse: 0.59251 | val_1_rmse: 0.66127 |  0:00:10s
epoch 76 | loss: 0.3737  | val_0_rmse: 0.5993  | val_1_rmse: 0.66868 |  0:00:10s
epoch 77 | loss: 0.37166 | val_0_rmse: 0.60425 | val_1_rmse: 0.66405 |  0:00:10s
epoch 78 | loss: 0.37661 | val_0_rmse: 0.58993 | val_1_rmse: 0.66183 |  0:00:10s
epoch 79 | loss: 0.36426 | val_0_rmse: 0.61332 | val_1_rmse: 0.67948 |  0:00:10s
epoch 80 | loss: 0.36548 | val_0_rmse: 0.60259 | val_1_rmse: 0.65284 |  0:00:10s
epoch 81 | loss: 0.37969 | val_0_rmse: 0.58939 | val_1_rmse: 0.6474  |  0:00:10s
epoch 82 | loss: 0.35984 | val_0_rmse: 0.61229 | val_1_rmse: 0.6787  |  0:00:10s
epoch 83 | loss: 0.36661 | val_0_rmse: 0.59314 | val_1_rmse: 0.65756 |  0:00:11s
epoch 84 | loss: 0.3796  | val_0_rmse: 0.60205 | val_1_rmse: 0.65768 |  0:00:11s
epoch 85 | loss: 0.3647  | val_0_rmse: 0.64421 | val_1_rmse: 0.70343 |  0:00:11s
epoch 86 | loss: 0.39303 | val_0_rmse: 0.66243 | val_1_rmse: 0.72332 |  0:00:11s
epoch 87 | loss: 0.36723 | val_0_rmse: 0.58797 | val_1_rmse: 0.64848 |  0:00:11s
epoch 88 | loss: 0.38548 | val_0_rmse: 0.61155 | val_1_rmse: 0.68585 |  0:00:11s
epoch 89 | loss: 0.3809  | val_0_rmse: 0.60394 | val_1_rmse: 0.67412 |  0:00:11s
epoch 90 | loss: 0.37025 | val_0_rmse: 0.60543 | val_1_rmse: 0.65562 |  0:00:11s
epoch 91 | loss: 0.36834 | val_0_rmse: 0.61615 | val_1_rmse: 0.66189 |  0:00:12s
epoch 92 | loss: 0.36389 | val_0_rmse: 0.61798 | val_1_rmse: 0.66736 |  0:00:12s
epoch 93 | loss: 0.3692  | val_0_rmse: 0.60842 | val_1_rmse: 0.66436 |  0:00:12s
epoch 94 | loss: 0.37194 | val_0_rmse: 0.60266 | val_1_rmse: 0.64676 |  0:00:12s
epoch 95 | loss: 0.37332 | val_0_rmse: 0.60655 | val_1_rmse: 0.6329  |  0:00:12s
epoch 96 | loss: 0.37696 | val_0_rmse: 0.62718 | val_1_rmse: 0.65412 |  0:00:12s
epoch 97 | loss: 0.38032 | val_0_rmse: 0.63315 | val_1_rmse: 0.67702 |  0:00:12s
epoch 98 | loss: 0.36912 | val_0_rmse: 0.60189 | val_1_rmse: 0.63644 |  0:00:13s
epoch 99 | loss: 0.36084 | val_0_rmse: 0.5925  | val_1_rmse: 0.62737 |  0:00:13s
epoch 100| loss: 0.3785  | val_0_rmse: 0.61545 | val_1_rmse: 0.67762 |  0:00:13s
epoch 101| loss: 0.37433 | val_0_rmse: 0.64675 | val_1_rmse: 0.68081 |  0:00:13s
epoch 102| loss: 0.38985 | val_0_rmse: 0.6444  | val_1_rmse: 0.65268 |  0:00:13s
epoch 103| loss: 0.38599 | val_0_rmse: 0.63578 | val_1_rmse: 0.66023 |  0:00:13s
epoch 104| loss: 0.40519 | val_0_rmse: 0.63436 | val_1_rmse: 0.66637 |  0:00:13s
epoch 105| loss: 0.38592 | val_0_rmse: 0.63844 | val_1_rmse: 0.67292 |  0:00:13s
epoch 106| loss: 0.39299 | val_0_rmse: 0.61335 | val_1_rmse: 0.66505 |  0:00:14s
epoch 107| loss: 0.37367 | val_0_rmse: 0.62507 | val_1_rmse: 0.68079 |  0:00:14s
epoch 108| loss: 0.37385 | val_0_rmse: 0.60662 | val_1_rmse: 0.65391 |  0:00:14s
epoch 109| loss: 0.36627 | val_0_rmse: 0.61315 | val_1_rmse: 0.66015 |  0:00:14s
epoch 110| loss: 0.36267 | val_0_rmse: 0.60979 | val_1_rmse: 0.65474 |  0:00:14s
epoch 111| loss: 0.37559 | val_0_rmse: 0.59846 | val_1_rmse: 0.63793 |  0:00:14s
epoch 112| loss: 0.38632 | val_0_rmse: 0.61178 | val_1_rmse: 0.66262 |  0:00:14s
epoch 113| loss: 0.3895  | val_0_rmse: 0.66234 | val_1_rmse: 0.71167 |  0:00:14s
epoch 114| loss: 0.38247 | val_0_rmse: 0.67904 | val_1_rmse: 0.71053 |  0:00:15s
epoch 115| loss: 0.37513 | val_0_rmse: 0.66555 | val_1_rmse: 0.70655 |  0:00:15s
epoch 116| loss: 0.37202 | val_0_rmse: 0.64392 | val_1_rmse: 0.68114 |  0:00:15s
epoch 117| loss: 0.35134 | val_0_rmse: 0.62954 | val_1_rmse: 0.67693 |  0:00:15s
epoch 118| loss: 0.35965 | val_0_rmse: 0.63248 | val_1_rmse: 0.67057 |  0:00:15s
epoch 119| loss: 0.35591 | val_0_rmse: 0.64104 | val_1_rmse: 0.67214 |  0:00:15s
epoch 120| loss: 0.34871 | val_0_rmse: 0.63876 | val_1_rmse: 0.67776 |  0:00:15s
epoch 121| loss: 0.35657 | val_0_rmse: 0.64273 | val_1_rmse: 0.67709 |  0:00:16s
epoch 122| loss: 0.38141 | val_0_rmse: 0.64676 | val_1_rmse: 0.68787 |  0:00:16s
epoch 123| loss: 0.34399 | val_0_rmse: 0.63972 | val_1_rmse: 0.67296 |  0:00:16s
epoch 124| loss: 0.34762 | val_0_rmse: 0.63255 | val_1_rmse: 0.66153 |  0:00:16s
epoch 125| loss: 0.35142 | val_0_rmse: 0.6219  | val_1_rmse: 0.66745 |  0:00:16s
epoch 126| loss: 0.33834 | val_0_rmse: 0.64659 | val_1_rmse: 0.68675 |  0:00:16s
epoch 127| loss: 0.33778 | val_0_rmse: 0.65698 | val_1_rmse: 0.67729 |  0:00:16s
epoch 128| loss: 0.35602 | val_0_rmse: 0.60326 | val_1_rmse: 0.64406 |  0:00:16s
epoch 129| loss: 0.33388 | val_0_rmse: 0.58399 | val_1_rmse: 0.65133 |  0:00:17s

Early stopping occured at epoch 129 with best_epoch = 99 and best_val_1_rmse = 0.62737
Best weights from best epoch are automatically used!
ended training at: 01:45:40
Feature importance:
[('Area', 0.2529871653874888), ('Baths', 0.15060456545114778), ('Beds', 0.04630766716693882), ('Latitude', 0.37116354846860045), ('Longitude', 0.04557082138417375), ('Month', 0.08951572960117363), ('Year', 0.043850502540476795)]
Mean squared error is of 2938593810.7650228
Mean absolute error:39043.20352870879
MAPE:0.36212849907711947
R2 score:0.6168105499448857
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:45:40
epoch 0  | loss: 1.75479 | val_0_rmse: 1.10771 | val_1_rmse: 1.07077 |  0:00:00s
epoch 1  | loss: 0.94004 | val_0_rmse: 0.87533 | val_1_rmse: 0.89262 |  0:00:00s
epoch 2  | loss: 0.57747 | val_0_rmse: 0.87181 | val_1_rmse: 0.87262 |  0:00:00s
epoch 3  | loss: 0.54663 | val_0_rmse: 0.81473 | val_1_rmse: 0.81229 |  0:00:00s
epoch 4  | loss: 0.49679 | val_0_rmse: 0.83968 | val_1_rmse: 0.84899 |  0:00:00s
epoch 5  | loss: 0.46427 | val_0_rmse: 0.94154 | val_1_rmse: 0.94064 |  0:00:00s
epoch 6  | loss: 0.46061 | val_0_rmse: 0.79861 | val_1_rmse: 0.79752 |  0:00:00s
epoch 7  | loss: 0.46735 | val_0_rmse: 0.78855 | val_1_rmse: 0.77222 |  0:00:01s
epoch 8  | loss: 0.44732 | val_0_rmse: 0.80961 | val_1_rmse: 0.80325 |  0:00:01s
epoch 9  | loss: 0.4511  | val_0_rmse: 0.74465 | val_1_rmse: 0.74799 |  0:00:01s
epoch 10 | loss: 0.44079 | val_0_rmse: 0.70773 | val_1_rmse: 0.71422 |  0:00:01s
epoch 11 | loss: 0.44727 | val_0_rmse: 0.70589 | val_1_rmse: 0.706   |  0:00:01s
epoch 12 | loss: 0.44235 | val_0_rmse: 0.71841 | val_1_rmse: 0.72475 |  0:00:01s
epoch 13 | loss: 0.44164 | val_0_rmse: 0.73758 | val_1_rmse: 0.73654 |  0:00:01s
epoch 14 | loss: 0.45196 | val_0_rmse: 0.70243 | val_1_rmse: 0.70841 |  0:00:02s
epoch 15 | loss: 0.44644 | val_0_rmse: 0.67208 | val_1_rmse: 0.67543 |  0:00:02s
epoch 16 | loss: 0.44489 | val_0_rmse: 0.66525 | val_1_rmse: 0.66738 |  0:00:02s
epoch 17 | loss: 0.43653 | val_0_rmse: 0.67248 | val_1_rmse: 0.68335 |  0:00:02s
epoch 18 | loss: 0.43103 | val_0_rmse: 0.68189 | val_1_rmse: 0.69278 |  0:00:02s
epoch 19 | loss: 0.44612 | val_0_rmse: 0.68999 | val_1_rmse: 0.69917 |  0:00:02s
epoch 20 | loss: 0.43885 | val_0_rmse: 0.69815 | val_1_rmse: 0.70982 |  0:00:02s
epoch 21 | loss: 0.42275 | val_0_rmse: 0.68749 | val_1_rmse: 0.69606 |  0:00:02s
epoch 22 | loss: 0.43861 | val_0_rmse: 0.66948 | val_1_rmse: 0.68351 |  0:00:03s
epoch 23 | loss: 0.41781 | val_0_rmse: 0.67664 | val_1_rmse: 0.70387 |  0:00:03s
epoch 24 | loss: 0.42532 | val_0_rmse: 0.67106 | val_1_rmse: 0.70126 |  0:00:03s
epoch 25 | loss: 0.43271 | val_0_rmse: 0.70954 | val_1_rmse: 0.7319  |  0:00:03s
epoch 26 | loss: 0.45327 | val_0_rmse: 0.69005 | val_1_rmse: 0.719   |  0:00:03s
epoch 27 | loss: 0.43377 | val_0_rmse: 0.65932 | val_1_rmse: 0.68456 |  0:00:03s
epoch 28 | loss: 0.4135  | val_0_rmse: 0.66439 | val_1_rmse: 0.68484 |  0:00:03s
epoch 29 | loss: 0.42443 | val_0_rmse: 0.65932 | val_1_rmse: 0.67993 |  0:00:03s
epoch 30 | loss: 0.41667 | val_0_rmse: 0.67817 | val_1_rmse: 0.70727 |  0:00:04s
epoch 31 | loss: 0.40847 | val_0_rmse: 0.68318 | val_1_rmse: 0.72851 |  0:00:04s
epoch 32 | loss: 0.42146 | val_0_rmse: 0.6785  | val_1_rmse: 0.74408 |  0:00:04s
epoch 33 | loss: 0.41308 | val_0_rmse: 0.73322 | val_1_rmse: 0.76841 |  0:00:04s
epoch 34 | loss: 0.41196 | val_0_rmse: 0.73704 | val_1_rmse: 0.75284 |  0:00:04s
epoch 35 | loss: 0.42169 | val_0_rmse: 0.7336  | val_1_rmse: 0.75315 |  0:00:04s
epoch 36 | loss: 0.39949 | val_0_rmse: 0.70748 | val_1_rmse: 0.78956 |  0:00:04s
epoch 37 | loss: 0.40721 | val_0_rmse: 0.66483 | val_1_rmse: 0.71094 |  0:00:05s
epoch 38 | loss: 0.39752 | val_0_rmse: 0.66649 | val_1_rmse: 0.7209  |  0:00:05s
epoch 39 | loss: 0.38739 | val_0_rmse: 0.64729 | val_1_rmse: 0.70672 |  0:00:05s
epoch 40 | loss: 0.3974  | val_0_rmse: 0.68319 | val_1_rmse: 0.70689 |  0:00:05s
epoch 41 | loss: 0.38064 | val_0_rmse: 0.74497 | val_1_rmse: 0.71873 |  0:00:05s
epoch 42 | loss: 0.37975 | val_0_rmse: 0.79152 | val_1_rmse: 0.75387 |  0:00:05s
epoch 43 | loss: 0.36569 | val_0_rmse: 0.65492 | val_1_rmse: 0.68521 |  0:00:05s
epoch 44 | loss: 0.37693 | val_0_rmse: 0.62795 | val_1_rmse: 0.64257 |  0:00:05s
epoch 45 | loss: 0.36346 | val_0_rmse: 0.76475 | val_1_rmse: 0.74382 |  0:00:06s
epoch 46 | loss: 0.36737 | val_0_rmse: 0.66371 | val_1_rmse: 0.67761 |  0:00:06s
epoch 47 | loss: 0.41157 | val_0_rmse: 0.71311 | val_1_rmse: 0.72788 |  0:00:06s
epoch 48 | loss: 0.37986 | val_0_rmse: 0.58742 | val_1_rmse: 0.6405  |  0:00:06s
epoch 49 | loss: 0.36592 | val_0_rmse: 0.6161  | val_1_rmse: 0.6519  |  0:00:06s
epoch 50 | loss: 0.36879 | val_0_rmse: 0.57892 | val_1_rmse: 0.63294 |  0:00:06s
epoch 51 | loss: 0.36073 | val_0_rmse: 0.58301 | val_1_rmse: 0.62259 |  0:00:06s
epoch 52 | loss: 0.3511  | val_0_rmse: 0.57172 | val_1_rmse: 0.61791 |  0:00:07s
epoch 53 | loss: 0.36332 | val_0_rmse: 0.57439 | val_1_rmse: 0.61865 |  0:00:07s
epoch 54 | loss: 0.35884 | val_0_rmse: 0.58415 | val_1_rmse: 0.61322 |  0:00:07s
epoch 55 | loss: 0.36326 | val_0_rmse: 0.60184 | val_1_rmse: 0.65361 |  0:00:07s
epoch 56 | loss: 0.36782 | val_0_rmse: 0.67887 | val_1_rmse: 0.72621 |  0:00:07s
epoch 57 | loss: 0.33305 | val_0_rmse: 0.7255  | val_1_rmse: 0.76396 |  0:00:07s
epoch 58 | loss: 0.34643 | val_0_rmse: 0.71633 | val_1_rmse: 0.73869 |  0:00:07s
epoch 59 | loss: 0.35836 | val_0_rmse: 0.65434 | val_1_rmse: 0.66377 |  0:00:07s
epoch 60 | loss: 0.3737  | val_0_rmse: 0.62538 | val_1_rmse: 0.63241 |  0:00:08s
epoch 61 | loss: 0.35253 | val_0_rmse: 0.60024 | val_1_rmse: 0.63109 |  0:00:08s
epoch 62 | loss: 0.34825 | val_0_rmse: 0.59091 | val_1_rmse: 0.60893 |  0:00:08s
epoch 63 | loss: 0.35794 | val_0_rmse: 0.58436 | val_1_rmse: 0.59487 |  0:00:08s
epoch 64 | loss: 0.37205 | val_0_rmse: 0.58052 | val_1_rmse: 0.60365 |  0:00:08s
epoch 65 | loss: 0.35935 | val_0_rmse: 0.67101 | val_1_rmse: 0.66377 |  0:00:08s
epoch 66 | loss: 0.34444 | val_0_rmse: 0.7238  | val_1_rmse: 0.70075 |  0:00:08s
epoch 67 | loss: 0.36728 | val_0_rmse: 0.67094 | val_1_rmse: 0.67314 |  0:00:09s
epoch 68 | loss: 0.34725 | val_0_rmse: 0.67853 | val_1_rmse: 0.68016 |  0:00:09s
epoch 69 | loss: 0.33547 | val_0_rmse: 0.61854 | val_1_rmse: 0.67251 |  0:00:09s
epoch 70 | loss: 0.34892 | val_0_rmse: 0.72689 | val_1_rmse: 0.76679 |  0:00:09s
epoch 71 | loss: 0.34686 | val_0_rmse: 0.7329  | val_1_rmse: 0.7555  |  0:00:09s
epoch 72 | loss: 0.36529 | val_0_rmse: 0.71947 | val_1_rmse: 0.75039 |  0:00:09s
epoch 73 | loss: 0.36111 | val_0_rmse: 0.62195 | val_1_rmse: 0.6653  |  0:00:09s
epoch 74 | loss: 0.35695 | val_0_rmse: 0.60802 | val_1_rmse: 0.64576 |  0:00:09s
epoch 75 | loss: 0.36384 | val_0_rmse: 0.598   | val_1_rmse: 0.62246 |  0:00:10s
epoch 76 | loss: 0.35652 | val_0_rmse: 0.61422 | val_1_rmse: 0.63391 |  0:00:10s
epoch 77 | loss: 0.35324 | val_0_rmse: 0.64962 | val_1_rmse: 0.66698 |  0:00:10s
epoch 78 | loss: 0.35276 | val_0_rmse: 0.64207 | val_1_rmse: 0.66735 |  0:00:10s
epoch 79 | loss: 0.34388 | val_0_rmse: 0.56794 | val_1_rmse: 0.59063 |  0:00:10s
epoch 80 | loss: 0.32669 | val_0_rmse: 0.66158 | val_1_rmse: 0.65031 |  0:00:10s
epoch 81 | loss: 0.33257 | val_0_rmse: 0.67297 | val_1_rmse: 0.65729 |  0:00:10s
epoch 82 | loss: 0.34409 | val_0_rmse: 0.63916 | val_1_rmse: 0.64655 |  0:00:10s
epoch 83 | loss: 0.33113 | val_0_rmse: 0.56809 | val_1_rmse: 0.59986 |  0:00:11s
epoch 84 | loss: 0.32543 | val_0_rmse: 0.57129 | val_1_rmse: 0.60082 |  0:00:11s
epoch 85 | loss: 0.32328 | val_0_rmse: 0.57374 | val_1_rmse: 0.59873 |  0:00:11s
epoch 86 | loss: 0.3314  | val_0_rmse: 0.56233 | val_1_rmse: 0.59636 |  0:00:11s
epoch 87 | loss: 0.33303 | val_0_rmse: 0.62721 | val_1_rmse: 0.65424 |  0:00:11s
epoch 88 | loss: 0.33165 | val_0_rmse: 0.62681 | val_1_rmse: 0.64262 |  0:00:11s
epoch 89 | loss: 0.33993 | val_0_rmse: 0.5868  | val_1_rmse: 0.61252 |  0:00:11s
epoch 90 | loss: 0.34762 | val_0_rmse: 0.62988 | val_1_rmse: 0.66788 |  0:00:12s
epoch 91 | loss: 0.34441 | val_0_rmse: 0.61225 | val_1_rmse: 0.67315 |  0:00:12s
epoch 92 | loss: 0.35351 | val_0_rmse: 0.68181 | val_1_rmse: 0.73099 |  0:00:12s
epoch 93 | loss: 0.36876 | val_0_rmse: 0.63353 | val_1_rmse: 0.67162 |  0:00:12s
epoch 94 | loss: 0.35224 | val_0_rmse: 0.73396 | val_1_rmse: 0.74882 |  0:00:12s
epoch 95 | loss: 0.37229 | val_0_rmse: 0.74295 | val_1_rmse: 0.75992 |  0:00:12s
epoch 96 | loss: 0.35389 | val_0_rmse: 0.68884 | val_1_rmse: 0.71098 |  0:00:12s
epoch 97 | loss: 0.35124 | val_0_rmse: 0.59407 | val_1_rmse: 0.65243 |  0:00:12s
epoch 98 | loss: 0.36363 | val_0_rmse: 0.5979  | val_1_rmse: 0.65349 |  0:00:13s
epoch 99 | loss: 0.36209 | val_0_rmse: 0.64678 | val_1_rmse: 0.68903 |  0:00:13s
epoch 100| loss: 0.34447 | val_0_rmse: 0.70516 | val_1_rmse: 0.71351 |  0:00:13s
epoch 101| loss: 0.35634 | val_0_rmse: 0.63731 | val_1_rmse: 0.66319 |  0:00:13s
epoch 102| loss: 0.33596 | val_0_rmse: 0.57266 | val_1_rmse: 0.63531 |  0:00:13s
epoch 103| loss: 0.34686 | val_0_rmse: 0.60081 | val_1_rmse: 0.65098 |  0:00:13s
epoch 104| loss: 0.34575 | val_0_rmse: 0.58049 | val_1_rmse: 0.6354  |  0:00:13s
epoch 105| loss: 0.33714 | val_0_rmse: 0.57437 | val_1_rmse: 0.62886 |  0:00:13s
epoch 106| loss: 0.34935 | val_0_rmse: 0.58898 | val_1_rmse: 0.65664 |  0:00:14s
epoch 107| loss: 0.34113 | val_0_rmse: 0.67251 | val_1_rmse: 0.71821 |  0:00:14s
epoch 108| loss: 0.34698 | val_0_rmse: 0.7     | val_1_rmse: 0.73195 |  0:00:14s
epoch 109| loss: 0.33797 | val_0_rmse: 0.6893  | val_1_rmse: 0.71625 |  0:00:14s

Early stopping occured at epoch 109 with best_epoch = 79 and best_val_1_rmse = 0.59063
Best weights from best epoch are automatically used!
ended training at: 01:45:54
Feature importance:
[('Area', 0.3089175593653592), ('Baths', 0.1046666564209826), ('Beds', 0.060454781812474684), ('Latitude', 0.3500750193809531), ('Longitude', 0.011905511496906532), ('Month', 0.16398047152332387), ('Year', 0.0)]
Mean squared error is of 2532393139.607566
Mean absolute error:35363.812792582416
MAPE:0.3076359668637447
R2 score:0.6260590145422601
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:45:55
epoch 0  | loss: 1.73572 | val_0_rmse: 1.05769 | val_1_rmse: 1.0583  |  0:00:00s
epoch 1  | loss: 0.8509  | val_0_rmse: 1.22783 | val_1_rmse: 1.34518 |  0:00:00s
epoch 2  | loss: 0.76356 | val_0_rmse: 0.91674 | val_1_rmse: 0.8983  |  0:00:00s
epoch 3  | loss: 0.57036 | val_0_rmse: 0.89942 | val_1_rmse: 1.18229 |  0:00:00s
epoch 4  | loss: 0.52845 | val_0_rmse: 0.90724 | val_1_rmse: 0.95179 |  0:00:00s
epoch 5  | loss: 0.50903 | val_0_rmse: 0.76756 | val_1_rmse: 0.84222 |  0:00:00s
epoch 6  | loss: 0.49439 | val_0_rmse: 0.75094 | val_1_rmse: 0.78544 |  0:00:00s
epoch 7  | loss: 0.48305 | val_0_rmse: 0.71102 | val_1_rmse: 0.77504 |  0:00:01s
epoch 8  | loss: 0.47841 | val_0_rmse: 0.73408 | val_1_rmse: 0.76999 |  0:00:01s
epoch 9  | loss: 0.47079 | val_0_rmse: 0.72879 | val_1_rmse: 0.74631 |  0:00:01s
epoch 10 | loss: 0.4641  | val_0_rmse: 0.73457 | val_1_rmse: 0.73906 |  0:00:01s
epoch 11 | loss: 0.45714 | val_0_rmse: 0.71223 | val_1_rmse: 0.73201 |  0:00:01s
epoch 12 | loss: 0.45239 | val_0_rmse: 0.69189 | val_1_rmse: 0.71369 |  0:00:01s
epoch 13 | loss: 0.45134 | val_0_rmse: 0.67982 | val_1_rmse: 0.71316 |  0:00:01s
epoch 14 | loss: 0.45199 | val_0_rmse: 0.68107 | val_1_rmse: 0.7133  |  0:00:02s
epoch 15 | loss: 0.45435 | val_0_rmse: 0.6688  | val_1_rmse: 0.70603 |  0:00:02s
epoch 16 | loss: 0.42993 | val_0_rmse: 0.66424 | val_1_rmse: 0.70592 |  0:00:02s
epoch 17 | loss: 0.43293 | val_0_rmse: 0.66854 | val_1_rmse: 0.70205 |  0:00:02s
epoch 18 | loss: 0.42474 | val_0_rmse: 0.67006 | val_1_rmse: 0.69557 |  0:00:02s
epoch 19 | loss: 0.42821 | val_0_rmse: 0.65347 | val_1_rmse: 0.68294 |  0:00:02s
epoch 20 | loss: 0.43192 | val_0_rmse: 0.64672 | val_1_rmse: 0.68087 |  0:00:02s
epoch 21 | loss: 0.41773 | val_0_rmse: 0.63744 | val_1_rmse: 0.69073 |  0:00:02s
epoch 22 | loss: 0.41378 | val_0_rmse: 0.63605 | val_1_rmse: 0.68694 |  0:00:03s
epoch 23 | loss: 0.40724 | val_0_rmse: 0.63019 | val_1_rmse: 0.67956 |  0:00:03s
epoch 24 | loss: 0.40323 | val_0_rmse: 0.64111 | val_1_rmse: 0.68342 |  0:00:03s
epoch 25 | loss: 0.41152 | val_0_rmse: 0.64893 | val_1_rmse: 0.6904  |  0:00:03s
epoch 26 | loss: 0.40439 | val_0_rmse: 0.62804 | val_1_rmse: 0.68814 |  0:00:03s
epoch 27 | loss: 0.41198 | val_0_rmse: 0.64439 | val_1_rmse: 0.68247 |  0:00:03s
epoch 28 | loss: 0.40956 | val_0_rmse: 0.65476 | val_1_rmse: 0.67385 |  0:00:03s
epoch 29 | loss: 0.42118 | val_0_rmse: 0.63844 | val_1_rmse: 0.6611  |  0:00:04s
epoch 30 | loss: 0.42226 | val_0_rmse: 0.63017 | val_1_rmse: 0.65617 |  0:00:04s
epoch 31 | loss: 0.41207 | val_0_rmse: 0.62915 | val_1_rmse: 0.66655 |  0:00:04s
epoch 32 | loss: 0.41175 | val_0_rmse: 0.62992 | val_1_rmse: 0.67626 |  0:00:04s
epoch 33 | loss: 0.4031  | val_0_rmse: 0.62624 | val_1_rmse: 0.6723  |  0:00:04s
epoch 34 | loss: 0.40218 | val_0_rmse: 0.62933 | val_1_rmse: 0.67451 |  0:00:04s
epoch 35 | loss: 0.40508 | val_0_rmse: 0.63047 | val_1_rmse: 0.67844 |  0:00:04s
epoch 36 | loss: 0.39472 | val_0_rmse: 0.64597 | val_1_rmse: 0.67684 |  0:00:04s
epoch 37 | loss: 0.40249 | val_0_rmse: 0.62701 | val_1_rmse: 0.66137 |  0:00:05s
epoch 38 | loss: 0.40035 | val_0_rmse: 0.62245 | val_1_rmse: 0.65476 |  0:00:05s
epoch 39 | loss: 0.40453 | val_0_rmse: 0.63102 | val_1_rmse: 0.6566  |  0:00:05s
epoch 40 | loss: 0.40058 | val_0_rmse: 0.63385 | val_1_rmse: 0.66451 |  0:00:05s
epoch 41 | loss: 0.40016 | val_0_rmse: 0.62172 | val_1_rmse: 0.66574 |  0:00:05s
epoch 42 | loss: 0.40553 | val_0_rmse: 0.61869 | val_1_rmse: 0.66881 |  0:00:05s
epoch 43 | loss: 0.3873  | val_0_rmse: 0.62227 | val_1_rmse: 0.65819 |  0:00:05s
epoch 44 | loss: 0.40219 | val_0_rmse: 0.62308 | val_1_rmse: 0.65658 |  0:00:06s
epoch 45 | loss: 0.38883 | val_0_rmse: 0.62388 | val_1_rmse: 0.66136 |  0:00:06s
epoch 46 | loss: 0.38818 | val_0_rmse: 0.61485 | val_1_rmse: 0.65359 |  0:00:06s
epoch 47 | loss: 0.39938 | val_0_rmse: 0.62536 | val_1_rmse: 0.65484 |  0:00:06s
epoch 48 | loss: 0.39615 | val_0_rmse: 0.61199 | val_1_rmse: 0.65478 |  0:00:06s
epoch 49 | loss: 0.3946  | val_0_rmse: 0.61232 | val_1_rmse: 0.66429 |  0:00:06s
epoch 50 | loss: 0.38892 | val_0_rmse: 0.60954 | val_1_rmse: 0.66358 |  0:00:06s
epoch 51 | loss: 0.3841  | val_0_rmse: 0.60351 | val_1_rmse: 0.66317 |  0:00:06s
epoch 52 | loss: 0.37321 | val_0_rmse: 0.60285 | val_1_rmse: 0.65646 |  0:00:07s
epoch 53 | loss: 0.38595 | val_0_rmse: 0.60543 | val_1_rmse: 0.64364 |  0:00:07s
epoch 54 | loss: 0.37815 | val_0_rmse: 0.62131 | val_1_rmse: 0.65054 |  0:00:07s
epoch 55 | loss: 0.38781 | val_0_rmse: 0.60551 | val_1_rmse: 0.65356 |  0:00:07s
epoch 56 | loss: 0.38147 | val_0_rmse: 0.60075 | val_1_rmse: 0.65756 |  0:00:07s
epoch 57 | loss: 0.37655 | val_0_rmse: 0.60163 | val_1_rmse: 0.64129 |  0:00:07s
epoch 58 | loss: 0.37216 | val_0_rmse: 0.60561 | val_1_rmse: 0.64858 |  0:00:07s
epoch 59 | loss: 0.37484 | val_0_rmse: 0.60006 | val_1_rmse: 0.65348 |  0:00:08s
epoch 60 | loss: 0.37322 | val_0_rmse: 0.59541 | val_1_rmse: 0.6659  |  0:00:08s
epoch 61 | loss: 0.37905 | val_0_rmse: 0.59519 | val_1_rmse: 0.68143 |  0:00:08s
epoch 62 | loss: 0.37773 | val_0_rmse: 0.59643 | val_1_rmse: 1.19321 |  0:00:08s
epoch 63 | loss: 0.36285 | val_0_rmse: 0.5987  | val_1_rmse: 1.29625 |  0:00:08s
epoch 64 | loss: 0.37427 | val_0_rmse: 0.59998 | val_1_rmse: 1.31449 |  0:00:08s
epoch 65 | loss: 0.36587 | val_0_rmse: 0.59834 | val_1_rmse: 1.17753 |  0:00:08s
epoch 66 | loss: 0.37142 | val_0_rmse: 0.59612 | val_1_rmse: 1.39777 |  0:00:08s
epoch 67 | loss: 0.37042 | val_0_rmse: 0.59597 | val_1_rmse: 0.67084 |  0:00:09s
epoch 68 | loss: 0.38873 | val_0_rmse: 0.60756 | val_1_rmse: 0.66116 |  0:00:09s
epoch 69 | loss: 0.36825 | val_0_rmse: 0.59802 | val_1_rmse: 0.65453 |  0:00:09s
epoch 70 | loss: 0.37238 | val_0_rmse: 0.59236 | val_1_rmse: 0.65774 |  0:00:09s
epoch 71 | loss: 0.35691 | val_0_rmse: 0.59292 | val_1_rmse: 0.65636 |  0:00:09s
epoch 72 | loss: 0.36521 | val_0_rmse: 0.59605 | val_1_rmse: 0.89288 |  0:00:09s
epoch 73 | loss: 0.37812 | val_0_rmse: 0.59908 | val_1_rmse: 0.99005 |  0:00:09s
epoch 74 | loss: 0.38486 | val_0_rmse: 0.59807 | val_1_rmse: 1.04772 |  0:00:09s
epoch 75 | loss: 0.37366 | val_0_rmse: 0.60187 | val_1_rmse: 1.12207 |  0:00:10s
epoch 76 | loss: 0.3715  | val_0_rmse: 0.59987 | val_1_rmse: 1.21998 |  0:00:10s
epoch 77 | loss: 0.37774 | val_0_rmse: 0.59557 | val_1_rmse: 1.17751 |  0:00:10s
epoch 78 | loss: 0.36916 | val_0_rmse: 0.5996  | val_1_rmse: 0.66878 |  0:00:10s
epoch 79 | loss: 0.36464 | val_0_rmse: 0.5942  | val_1_rmse: 0.66237 |  0:00:10s
epoch 80 | loss: 0.37083 | val_0_rmse: 0.59815 | val_1_rmse: 0.66711 |  0:00:10s
epoch 81 | loss: 0.36678 | val_0_rmse: 0.60339 | val_1_rmse: 0.67148 |  0:00:10s
epoch 82 | loss: 0.366   | val_0_rmse: 0.59893 | val_1_rmse: 0.66337 |  0:00:10s
epoch 83 | loss: 0.37218 | val_0_rmse: 0.59552 | val_1_rmse: 0.65727 |  0:00:11s
epoch 84 | loss: 0.36508 | val_0_rmse: 0.59623 | val_1_rmse: 0.6646  |  0:00:11s
epoch 85 | loss: 0.35941 | val_0_rmse: 0.59642 | val_1_rmse: 1.23088 |  0:00:11s
epoch 86 | loss: 0.35601 | val_0_rmse: 0.59495 | val_1_rmse: 1.14467 |  0:00:11s
epoch 87 | loss: 0.36208 | val_0_rmse: 0.59827 | val_1_rmse: 1.17452 |  0:00:11s

Early stopping occured at epoch 87 with best_epoch = 57 and best_val_1_rmse = 0.64129
Best weights from best epoch are automatically used!
ended training at: 01:46:06
Feature importance:
[('Area', 0.420534982633091), ('Baths', 0.2201370271798137), ('Beds', 0.048065622536807774), ('Latitude', 0.06725683421663053), ('Longitude', 0.1668817182852179), ('Month', 0.04705118839841516), ('Year', 0.030072626750023923)]
Mean squared error is of 3470828465.8146424
Mean absolute error:42870.27849285714
MAPE:0.40679558546054956
R2 score:0.5153350087813552
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:46:06
epoch 0  | loss: 1.54249 | val_0_rmse: 2.10428 | val_1_rmse: 1.05517 |  0:00:00s
epoch 1  | loss: 0.92737 | val_0_rmse: 1.13045 | val_1_rmse: 1.16035 |  0:00:00s
epoch 2  | loss: 0.69625 | val_0_rmse: 1.05423 | val_1_rmse: 1.12775 |  0:00:00s
epoch 3  | loss: 0.54805 | val_0_rmse: 0.97617 | val_1_rmse: 1.09698 |  0:00:00s
epoch 4  | loss: 0.52746 | val_0_rmse: 0.91037 | val_1_rmse: 0.99041 |  0:00:00s
epoch 5  | loss: 0.51662 | val_0_rmse: 0.80023 | val_1_rmse: 0.83054 |  0:00:00s
epoch 6  | loss: 0.46799 | val_0_rmse: 0.73318 | val_1_rmse: 0.79758 |  0:00:00s
epoch 7  | loss: 0.44193 | val_0_rmse: 0.77801 | val_1_rmse: 0.87238 |  0:00:01s
epoch 8  | loss: 0.44557 | val_0_rmse: 0.72551 | val_1_rmse: 0.83488 |  0:00:01s
epoch 9  | loss: 0.43155 | val_0_rmse: 0.65928 | val_1_rmse: 0.7529  |  0:00:01s
epoch 10 | loss: 0.42115 | val_0_rmse: 0.68259 | val_1_rmse: 0.74734 |  0:00:01s
epoch 11 | loss: 0.4171  | val_0_rmse: 0.68339 | val_1_rmse: 0.74366 |  0:00:01s
epoch 12 | loss: 0.42642 | val_0_rmse: 0.66323 | val_1_rmse: 0.71992 |  0:00:01s
epoch 13 | loss: 0.429   | val_0_rmse: 0.63894 | val_1_rmse: 0.68218 |  0:00:01s
epoch 14 | loss: 0.41885 | val_0_rmse: 0.63313 | val_1_rmse: 0.67789 |  0:00:02s
epoch 15 | loss: 0.41039 | val_0_rmse: 0.62755 | val_1_rmse: 0.69254 |  0:00:02s
epoch 16 | loss: 0.39859 | val_0_rmse: 0.63572 | val_1_rmse: 0.71002 |  0:00:02s
epoch 17 | loss: 0.40189 | val_0_rmse: 0.62936 | val_1_rmse: 0.67827 |  0:00:02s
epoch 18 | loss: 0.40044 | val_0_rmse: 0.63192 | val_1_rmse: 0.68672 |  0:00:02s
epoch 19 | loss: 0.40694 | val_0_rmse: 0.62113 | val_1_rmse: 0.67551 |  0:00:02s
epoch 20 | loss: 0.40503 | val_0_rmse: 0.63219 | val_1_rmse: 0.70343 |  0:00:02s
epoch 21 | loss: 0.39209 | val_0_rmse: 0.63312 | val_1_rmse: 0.7058  |  0:00:02s
epoch 22 | loss: 0.40688 | val_0_rmse: 0.62794 | val_1_rmse: 0.69424 |  0:00:03s
epoch 23 | loss: 0.38948 | val_0_rmse: 0.61456 | val_1_rmse: 0.68268 |  0:00:03s
epoch 24 | loss: 0.39118 | val_0_rmse: 0.62105 | val_1_rmse: 0.69322 |  0:00:03s
epoch 25 | loss: 0.37798 | val_0_rmse: 0.62245 | val_1_rmse: 0.7032  |  0:00:03s
epoch 26 | loss: 0.40127 | val_0_rmse: 0.62135 | val_1_rmse: 0.73808 |  0:00:03s
epoch 27 | loss: 0.39007 | val_0_rmse: 0.61428 | val_1_rmse: 0.68862 |  0:00:03s
epoch 28 | loss: 0.38186 | val_0_rmse: 0.62067 | val_1_rmse: 0.6803  |  0:00:03s
epoch 29 | loss: 0.40122 | val_0_rmse: 0.62606 | val_1_rmse: 0.71065 |  0:00:04s
epoch 30 | loss: 0.40558 | val_0_rmse: 0.62181 | val_1_rmse: 0.6939  |  0:00:04s
epoch 31 | loss: 0.40897 | val_0_rmse: 0.63214 | val_1_rmse: 0.70962 |  0:00:04s
epoch 32 | loss: 0.41945 | val_0_rmse: 0.62477 | val_1_rmse: 0.69667 |  0:00:04s
epoch 33 | loss: 0.39974 | val_0_rmse: 0.62757 | val_1_rmse: 0.69363 |  0:00:04s
epoch 34 | loss: 0.40412 | val_0_rmse: 0.62845 | val_1_rmse: 0.68626 |  0:00:04s
epoch 35 | loss: 0.41186 | val_0_rmse: 0.65164 | val_1_rmse: 0.73064 |  0:00:04s
epoch 36 | loss: 0.41675 | val_0_rmse: 0.63058 | val_1_rmse: 0.72318 |  0:00:04s
epoch 37 | loss: 0.39928 | val_0_rmse: 0.66021 | val_1_rmse: 0.76727 |  0:00:05s
epoch 38 | loss: 0.41112 | val_0_rmse: 0.62744 | val_1_rmse: 0.70837 |  0:00:05s
epoch 39 | loss: 0.40298 | val_0_rmse: 0.62281 | val_1_rmse: 0.69216 |  0:00:05s
epoch 40 | loss: 0.40677 | val_0_rmse: 0.62035 | val_1_rmse: 0.67553 |  0:00:05s
epoch 41 | loss: 0.39766 | val_0_rmse: 0.62576 | val_1_rmse: 0.6813  |  0:00:05s
epoch 42 | loss: 0.39209 | val_0_rmse: 0.62774 | val_1_rmse: 0.69551 |  0:00:05s
epoch 43 | loss: 0.40958 | val_0_rmse: 0.63199 | val_1_rmse: 0.69661 |  0:00:05s
epoch 44 | loss: 0.40319 | val_0_rmse: 0.62377 | val_1_rmse: 0.69725 |  0:00:05s
epoch 45 | loss: 0.40525 | val_0_rmse: 0.62195 | val_1_rmse: 0.70495 |  0:00:06s
epoch 46 | loss: 0.40453 | val_0_rmse: 0.62356 | val_1_rmse: 0.70599 |  0:00:06s
epoch 47 | loss: 0.39965 | val_0_rmse: 0.62779 | val_1_rmse: 0.71257 |  0:00:06s
epoch 48 | loss: 0.40298 | val_0_rmse: 0.62035 | val_1_rmse: 0.71153 |  0:00:06s
epoch 49 | loss: 0.38999 | val_0_rmse: 0.61868 | val_1_rmse: 0.71743 |  0:00:06s

Early stopping occured at epoch 49 with best_epoch = 19 and best_val_1_rmse = 0.67551
Best weights from best epoch are automatically used!
ended training at: 01:46:13
Feature importance:
[('Area', 0.38705373971814616), ('Baths', 0.19090141738698235), ('Beds', 0.07355903626579477), ('Latitude', 0.050186442086727394), ('Longitude', 0.1223623827232715), ('Month', 0.11822795074468487), ('Year', 0.057709031074392944)]
Mean squared error is of 3472314226.431758
Mean absolute error:42321.69438949176
MAPE:0.38664791341297367
R2 score:0.5227914895819437
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:46:13
epoch 0  | loss: 1.55339 | val_0_rmse: 1.38525 | val_1_rmse: 1.57006 |  0:00:00s
epoch 1  | loss: 0.93055 | val_0_rmse: 1.10018 | val_1_rmse: 1.09549 |  0:00:00s
epoch 2  | loss: 0.78397 | val_0_rmse: 0.91321 | val_1_rmse: 0.92439 |  0:00:00s
epoch 3  | loss: 0.61229 | val_0_rmse: 1.40141 | val_1_rmse: 0.96646 |  0:00:00s
epoch 4  | loss: 0.58457 | val_0_rmse: 1.10144 | val_1_rmse: 0.82539 |  0:00:00s
epoch 5  | loss: 0.5436  | val_0_rmse: 0.77401 | val_1_rmse: 0.82058 |  0:00:00s
epoch 6  | loss: 0.54054 | val_0_rmse: 0.71005 | val_1_rmse: 0.77435 |  0:00:00s
epoch 7  | loss: 0.53617 | val_0_rmse: 0.73347 | val_1_rmse: 0.81523 |  0:00:01s
epoch 8  | loss: 0.49768 | val_0_rmse: 0.79066 | val_1_rmse: 0.83799 |  0:00:01s
epoch 9  | loss: 0.49238 | val_0_rmse: 0.7411  | val_1_rmse: 0.7887  |  0:00:01s
epoch 10 | loss: 0.47624 | val_0_rmse: 0.72409 | val_1_rmse: 0.78294 |  0:00:01s
epoch 11 | loss: 0.47127 | val_0_rmse: 0.74385 | val_1_rmse: 0.80521 |  0:00:01s
epoch 12 | loss: 0.45802 | val_0_rmse: 0.7382  | val_1_rmse: 0.81104 |  0:00:01s
epoch 13 | loss: 0.44968 | val_0_rmse: 0.69846 | val_1_rmse: 0.76271 |  0:00:01s
epoch 14 | loss: 0.44006 | val_0_rmse: 0.68447 | val_1_rmse: 0.75861 |  0:00:02s
epoch 15 | loss: 0.43329 | val_0_rmse: 0.67236 | val_1_rmse: 0.76005 |  0:00:02s
epoch 16 | loss: 0.42788 | val_0_rmse: 0.64834 | val_1_rmse: 0.75052 |  0:00:02s
epoch 17 | loss: 0.42329 | val_0_rmse: 0.65401 | val_1_rmse: 0.7684  |  0:00:02s
epoch 18 | loss: 0.40775 | val_0_rmse: 0.64889 | val_1_rmse: 0.77598 |  0:00:02s
epoch 19 | loss: 0.40864 | val_0_rmse: 0.63593 | val_1_rmse: 0.75731 |  0:00:02s
epoch 20 | loss: 0.412   | val_0_rmse: 0.63543 | val_1_rmse: 0.74092 |  0:00:02s
epoch 21 | loss: 0.41189 | val_0_rmse: 0.63566 | val_1_rmse: 0.75008 |  0:00:02s
epoch 22 | loss: 0.40727 | val_0_rmse: 0.63385 | val_1_rmse: 0.74898 |  0:00:03s
epoch 23 | loss: 0.3973  | val_0_rmse: 0.62312 | val_1_rmse: 0.7302  |  0:00:03s
epoch 24 | loss: 0.39413 | val_0_rmse: 0.62449 | val_1_rmse: 0.72911 |  0:00:03s
epoch 25 | loss: 0.41055 | val_0_rmse: 0.62118 | val_1_rmse: 0.73888 |  0:00:03s
epoch 26 | loss: 0.39853 | val_0_rmse: 0.61633 | val_1_rmse: 0.73956 |  0:00:03s
epoch 27 | loss: 0.39838 | val_0_rmse: 0.61592 | val_1_rmse: 0.73494 |  0:00:03s
epoch 28 | loss: 0.39035 | val_0_rmse: 0.61858 | val_1_rmse: 0.73452 |  0:00:03s
epoch 29 | loss: 0.39214 | val_0_rmse: 0.61659 | val_1_rmse: 0.73512 |  0:00:04s
epoch 30 | loss: 0.39441 | val_0_rmse: 0.61882 | val_1_rmse: 0.74024 |  0:00:04s
epoch 31 | loss: 0.40071 | val_0_rmse: 0.62874 | val_1_rmse: 0.75849 |  0:00:04s
epoch 32 | loss: 0.40012 | val_0_rmse: 0.61715 | val_1_rmse: 0.74293 |  0:00:04s
epoch 33 | loss: 0.40027 | val_0_rmse: 0.61686 | val_1_rmse: 0.73343 |  0:00:04s
epoch 34 | loss: 0.38918 | val_0_rmse: 0.62716 | val_1_rmse: 0.75072 |  0:00:04s
epoch 35 | loss: 0.39608 | val_0_rmse: 0.61227 | val_1_rmse: 0.74004 |  0:00:04s
epoch 36 | loss: 0.38157 | val_0_rmse: 0.61605 | val_1_rmse: 0.74726 |  0:00:04s
epoch 37 | loss: 0.38565 | val_0_rmse: 0.62088 | val_1_rmse: 0.75906 |  0:00:05s
epoch 38 | loss: 0.38725 | val_0_rmse: 0.61486 | val_1_rmse: 0.73824 |  0:00:05s
epoch 39 | loss: 0.39084 | val_0_rmse: 0.61904 | val_1_rmse: 0.732   |  0:00:05s
epoch 40 | loss: 0.38683 | val_0_rmse: 0.61689 | val_1_rmse: 0.74345 |  0:00:05s
epoch 41 | loss: 0.37777 | val_0_rmse: 0.62171 | val_1_rmse: 0.75113 |  0:00:05s
epoch 42 | loss: 0.39563 | val_0_rmse: 0.61333 | val_1_rmse: 0.73737 |  0:00:05s
epoch 43 | loss: 0.3869  | val_0_rmse: 0.60677 | val_1_rmse: 0.72731 |  0:00:05s
epoch 44 | loss: 0.38726 | val_0_rmse: 0.61487 | val_1_rmse: 0.72346 |  0:00:05s
epoch 45 | loss: 0.37643 | val_0_rmse: 0.60784 | val_1_rmse: 0.71268 |  0:00:06s
epoch 46 | loss: 0.38719 | val_0_rmse: 0.60657 | val_1_rmse: 0.7132  |  0:00:06s
epoch 47 | loss: 0.37794 | val_0_rmse: 0.60928 | val_1_rmse: 0.72376 |  0:00:06s
epoch 48 | loss: 0.3941  | val_0_rmse: 0.61642 | val_1_rmse: 0.72768 |  0:00:06s
epoch 49 | loss: 0.38557 | val_0_rmse: 0.62278 | val_1_rmse: 0.71762 |  0:00:06s
epoch 50 | loss: 0.39037 | val_0_rmse: 0.62826 | val_1_rmse: 0.72776 |  0:00:06s
epoch 51 | loss: 0.39961 | val_0_rmse: 0.62816 | val_1_rmse: 0.7236  |  0:00:06s
epoch 52 | loss: 0.40089 | val_0_rmse: 0.62683 | val_1_rmse: 0.70961 |  0:00:07s
epoch 53 | loss: 0.39913 | val_0_rmse: 0.62173 | val_1_rmse: 0.71073 |  0:00:07s
epoch 54 | loss: 0.39922 | val_0_rmse: 0.62171 | val_1_rmse: 0.71661 |  0:00:07s
epoch 55 | loss: 0.39148 | val_0_rmse: 0.63378 | val_1_rmse: 0.72165 |  0:00:07s
epoch 56 | loss: 0.39944 | val_0_rmse: 0.6226  | val_1_rmse: 0.7123  |  0:00:07s
epoch 57 | loss: 0.38479 | val_0_rmse: 0.62392 | val_1_rmse: 0.71129 |  0:00:07s
epoch 58 | loss: 0.39465 | val_0_rmse: 0.63664 | val_1_rmse: 0.72499 |  0:00:07s
epoch 59 | loss: 0.39112 | val_0_rmse: 0.62773 | val_1_rmse: 0.73205 |  0:00:07s
epoch 60 | loss: 0.39934 | val_0_rmse: 0.62467 | val_1_rmse: 0.73699 |  0:00:08s
epoch 61 | loss: 0.3886  | val_0_rmse: 0.63549 | val_1_rmse: 0.7326  |  0:00:08s
epoch 62 | loss: 0.39181 | val_0_rmse: 0.62673 | val_1_rmse: 0.73899 |  0:00:08s
epoch 63 | loss: 0.37671 | val_0_rmse: 0.63101 | val_1_rmse: 0.72206 |  0:00:08s
epoch 64 | loss: 0.38874 | val_0_rmse: 0.64525 | val_1_rmse: 0.74568 |  0:00:08s
epoch 65 | loss: 0.38956 | val_0_rmse: 0.64068 | val_1_rmse: 0.74804 |  0:00:08s
epoch 66 | loss: 0.38435 | val_0_rmse: 0.64213 | val_1_rmse: 0.74541 |  0:00:08s
epoch 67 | loss: 0.38669 | val_0_rmse: 0.65314 | val_1_rmse: 0.74515 |  0:00:08s
epoch 68 | loss: 0.38114 | val_0_rmse: 0.6607  | val_1_rmse: 0.74606 |  0:00:09s
epoch 69 | loss: 0.38198 | val_0_rmse: 0.64099 | val_1_rmse: 0.72795 |  0:00:09s
epoch 70 | loss: 0.37681 | val_0_rmse: 0.63377 | val_1_rmse: 0.7238  |  0:00:09s
epoch 71 | loss: 0.37342 | val_0_rmse: 0.63489 | val_1_rmse: 0.72531 |  0:00:09s
epoch 72 | loss: 0.3713  | val_0_rmse: 0.62598 | val_1_rmse: 0.71109 |  0:00:09s
epoch 73 | loss: 0.38697 | val_0_rmse: 0.62932 | val_1_rmse: 0.71498 |  0:00:09s
epoch 74 | loss: 0.37508 | val_0_rmse: 0.62191 | val_1_rmse: 0.71042 |  0:00:09s
epoch 75 | loss: 0.38729 | val_0_rmse: 0.7767  | val_1_rmse: 0.85405 |  0:00:10s
epoch 76 | loss: 0.38967 | val_0_rmse: 0.67873 | val_1_rmse: 0.75445 |  0:00:10s
epoch 77 | loss: 0.38194 | val_0_rmse: 0.61593 | val_1_rmse: 0.69044 |  0:00:10s
epoch 78 | loss: 0.37695 | val_0_rmse: 0.60431 | val_1_rmse: 0.69069 |  0:00:10s
epoch 79 | loss: 0.37962 | val_0_rmse: 0.61766 | val_1_rmse: 0.69854 |  0:00:10s
epoch 80 | loss: 0.37662 | val_0_rmse: 0.655   | val_1_rmse: 0.74168 |  0:00:10s
epoch 81 | loss: 0.37776 | val_0_rmse: 0.64498 | val_1_rmse: 0.74679 |  0:00:10s
epoch 82 | loss: 0.3769  | val_0_rmse: 0.60103 | val_1_rmse: 0.69266 |  0:00:11s
epoch 83 | loss: 0.3799  | val_0_rmse: 0.59458 | val_1_rmse: 0.67692 |  0:00:11s
epoch 84 | loss: 0.36015 | val_0_rmse: 0.69695 | val_1_rmse: 0.77596 |  0:00:11s
epoch 85 | loss: 0.3947  | val_0_rmse: 0.60041 | val_1_rmse: 0.6689  |  0:00:11s
epoch 86 | loss: 0.35877 | val_0_rmse: 0.62245 | val_1_rmse: 0.67647 |  0:00:11s
epoch 87 | loss: 0.36462 | val_0_rmse: 0.63952 | val_1_rmse: 0.71229 |  0:00:11s
epoch 88 | loss: 0.35463 | val_0_rmse: 0.62188 | val_1_rmse: 0.70419 |  0:00:11s
epoch 89 | loss: 0.36024 | val_0_rmse: 0.59619 | val_1_rmse: 0.67046 |  0:00:11s
epoch 90 | loss: 0.35821 | val_0_rmse: 0.64851 | val_1_rmse: 0.71827 |  0:00:12s
epoch 91 | loss: 0.35877 | val_0_rmse: 0.6724  | val_1_rmse: 0.74501 |  0:00:12s
epoch 92 | loss: 0.34935 | val_0_rmse: 0.58062 | val_1_rmse: 0.66685 |  0:00:12s
epoch 93 | loss: 0.35029 | val_0_rmse: 0.58312 | val_1_rmse: 0.66871 |  0:00:12s
epoch 94 | loss: 0.34569 | val_0_rmse: 0.76403 | val_1_rmse: 0.86492 |  0:00:12s
epoch 95 | loss: 0.35362 | val_0_rmse: 0.78475 | val_1_rmse: 0.88273 |  0:00:12s
epoch 96 | loss: 0.35146 | val_0_rmse: 0.71867 | val_1_rmse: 0.80586 |  0:00:12s
epoch 97 | loss: 0.3447  | val_0_rmse: 0.7588  | val_1_rmse: 0.84465 |  0:00:12s
epoch 98 | loss: 0.33581 | val_0_rmse: 0.76004 | val_1_rmse: 0.86103 |  0:00:13s
epoch 99 | loss: 0.33786 | val_0_rmse: 0.68888 | val_1_rmse: 0.78343 |  0:00:13s
epoch 100| loss: 0.34069 | val_0_rmse: 0.65483 | val_1_rmse: 0.73172 |  0:00:13s
epoch 101| loss: 0.33515 | val_0_rmse: 0.57012 | val_1_rmse: 0.63404 |  0:00:13s
epoch 102| loss: 0.33051 | val_0_rmse: 0.58058 | val_1_rmse: 0.67527 |  0:00:13s
epoch 103| loss: 0.34095 | val_0_rmse: 0.5731  | val_1_rmse: 0.65077 |  0:00:13s
epoch 104| loss: 0.33764 | val_0_rmse: 0.58336 | val_1_rmse: 0.66408 |  0:00:13s
epoch 105| loss: 0.33154 | val_0_rmse: 0.59544 | val_1_rmse: 0.69948 |  0:00:14s
epoch 106| loss: 0.33621 | val_0_rmse: 0.62017 | val_1_rmse: 0.72842 |  0:00:14s
epoch 107| loss: 0.33576 | val_0_rmse: 0.60131 | val_1_rmse: 0.71125 |  0:00:14s
epoch 108| loss: 0.32688 | val_0_rmse: 0.57444 | val_1_rmse: 0.67951 |  0:00:14s
epoch 109| loss: 0.32431 | val_0_rmse: 0.59842 | val_1_rmse: 0.68938 |  0:00:14s
epoch 110| loss: 0.33926 | val_0_rmse: 0.59066 | val_1_rmse: 0.67963 |  0:00:14s
epoch 111| loss: 0.33863 | val_0_rmse: 0.57857 | val_1_rmse: 0.6686  |  0:00:14s
epoch 112| loss: 0.3298  | val_0_rmse: 0.58171 | val_1_rmse: 0.67705 |  0:00:14s
epoch 113| loss: 0.34268 | val_0_rmse: 0.59305 | val_1_rmse: 0.69183 |  0:00:15s
epoch 114| loss: 0.34337 | val_0_rmse: 0.61223 | val_1_rmse: 0.72327 |  0:00:15s
epoch 115| loss: 0.34649 | val_0_rmse: 0.57094 | val_1_rmse: 0.67666 |  0:00:15s
epoch 116| loss: 0.33807 | val_0_rmse: 0.56969 | val_1_rmse: 0.67051 |  0:00:15s
epoch 117| loss: 0.34434 | val_0_rmse: 0.61458 | val_1_rmse: 0.72858 |  0:00:15s
epoch 118| loss: 0.34638 | val_0_rmse: 0.67067 | val_1_rmse: 0.76483 |  0:00:15s
epoch 119| loss: 0.34303 | val_0_rmse: 0.66765 | val_1_rmse: 0.77162 |  0:00:15s
epoch 120| loss: 0.33716 | val_0_rmse: 0.61474 | val_1_rmse: 0.73443 |  0:00:15s
epoch 121| loss: 0.32818 | val_0_rmse: 0.56386 | val_1_rmse: 0.6409  |  0:00:16s
epoch 122| loss: 0.34    | val_0_rmse: 0.57947 | val_1_rmse: 0.66152 |  0:00:16s
epoch 123| loss: 0.33227 | val_0_rmse: 0.59257 | val_1_rmse: 0.67828 |  0:00:16s
epoch 124| loss: 0.33289 | val_0_rmse: 0.56008 | val_1_rmse: 0.63545 |  0:00:16s
epoch 125| loss: 0.32187 | val_0_rmse: 0.55461 | val_1_rmse: 0.65185 |  0:00:16s
epoch 126| loss: 0.31673 | val_0_rmse: 0.56394 | val_1_rmse: 0.68479 |  0:00:16s
epoch 127| loss: 0.34012 | val_0_rmse: 0.55054 | val_1_rmse: 0.65787 |  0:00:16s
epoch 128| loss: 0.30993 | val_0_rmse: 0.55651 | val_1_rmse: 0.65528 |  0:00:16s
epoch 129| loss: 0.3226  | val_0_rmse: 0.54611 | val_1_rmse: 0.64203 |  0:00:17s
epoch 130| loss: 0.30859 | val_0_rmse: 0.55131 | val_1_rmse: 0.65304 |  0:00:17s
epoch 131| loss: 0.32447 | val_0_rmse: 0.55872 | val_1_rmse: 0.6593  |  0:00:17s

Early stopping occured at epoch 131 with best_epoch = 101 and best_val_1_rmse = 0.63404
Best weights from best epoch are automatically used!
ended training at: 01:46:30
Feature importance:
[('Area', 0.2948578312618405), ('Baths', 0.23716313557789045), ('Beds', 0.015532625556829436), ('Latitude', 0.3491116698170967), ('Longitude', 0.033278939640593155), ('Month', 0.03742935464225385), ('Year', 0.0326264435034959)]
Mean squared error is of 2586891947.7351284
Mean absolute error:36422.55438502747
MAPE:0.3716240674260742
R2 score:0.6187390129806127
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:46:30
epoch 0  | loss: 1.30984 | val_0_rmse: 0.96608 | val_1_rmse: 0.89641 |  0:00:00s
epoch 1  | loss: 0.70507 | val_0_rmse: 0.91544 | val_1_rmse: 0.84096 |  0:00:00s
epoch 2  | loss: 0.59497 | val_0_rmse: 0.86669 | val_1_rmse: 0.81776 |  0:00:00s
epoch 3  | loss: 0.58278 | val_0_rmse: 0.75766 | val_1_rmse: 0.71813 |  0:00:01s
epoch 4  | loss: 0.52438 | val_0_rmse: 0.72696 | val_1_rmse: 0.68461 |  0:00:01s
epoch 5  | loss: 0.51967 | val_0_rmse: 0.71514 | val_1_rmse: 0.66228 |  0:00:01s
epoch 6  | loss: 0.50374 | val_0_rmse: 0.70687 | val_1_rmse: 0.64277 |  0:00:02s
epoch 7  | loss: 0.48572 | val_0_rmse: 0.68705 | val_1_rmse: 0.64802 |  0:00:02s
epoch 8  | loss: 0.4719  | val_0_rmse: 0.69604 | val_1_rmse: 0.66177 |  0:00:02s
epoch 9  | loss: 0.4545  | val_0_rmse: 0.69515 | val_1_rmse: 0.64647 |  0:00:02s
epoch 10 | loss: 0.44141 | val_0_rmse: 0.67703 | val_1_rmse: 0.63326 |  0:00:03s
epoch 11 | loss: 0.43543 | val_0_rmse: 0.65205 | val_1_rmse: 0.61648 |  0:00:03s
epoch 12 | loss: 0.41082 | val_0_rmse: 0.64487 | val_1_rmse: 0.60503 |  0:00:03s
epoch 13 | loss: 0.40873 | val_0_rmse: 0.63705 | val_1_rmse: 0.59615 |  0:00:04s
epoch 14 | loss: 0.40363 | val_0_rmse: 0.65154 | val_1_rmse: 0.6144  |  0:00:04s
epoch 15 | loss: 0.40552 | val_0_rmse: 0.62975 | val_1_rmse: 0.59757 |  0:00:04s
epoch 16 | loss: 0.39524 | val_0_rmse: 0.63077 | val_1_rmse: 0.60589 |  0:00:04s
epoch 17 | loss: 0.3909  | val_0_rmse: 0.61225 | val_1_rmse: 0.58469 |  0:00:05s
epoch 18 | loss: 0.38605 | val_0_rmse: 0.62829 | val_1_rmse: 0.60388 |  0:00:05s
epoch 19 | loss: 0.38654 | val_0_rmse: 0.61541 | val_1_rmse: 0.58127 |  0:00:05s
epoch 20 | loss: 0.37945 | val_0_rmse: 0.60992 | val_1_rmse: 0.57449 |  0:00:06s
epoch 21 | loss: 0.37817 | val_0_rmse: 0.60465 | val_1_rmse: 0.57608 |  0:00:06s
epoch 22 | loss: 0.36651 | val_0_rmse: 0.62252 | val_1_rmse: 0.58545 |  0:00:06s
epoch 23 | loss: 0.37144 | val_0_rmse: 0.62539 | val_1_rmse: 0.60283 |  0:00:07s
epoch 24 | loss: 0.37849 | val_0_rmse: 0.66216 | val_1_rmse: 0.62731 |  0:00:07s
epoch 25 | loss: 0.36692 | val_0_rmse: 0.69011 | val_1_rmse: 0.64041 |  0:00:07s
epoch 26 | loss: 0.35941 | val_0_rmse: 0.63441 | val_1_rmse: 0.59578 |  0:00:07s
epoch 27 | loss: 0.35918 | val_0_rmse: 0.64736 | val_1_rmse: 0.6064  |  0:00:08s
epoch 28 | loss: 0.36787 | val_0_rmse: 0.62845 | val_1_rmse: 0.58594 |  0:00:08s
epoch 29 | loss: 0.35774 | val_0_rmse: 0.64269 | val_1_rmse: 0.60465 |  0:00:08s
epoch 30 | loss: 0.3539  | val_0_rmse: 0.63266 | val_1_rmse: 0.61022 |  0:00:09s
epoch 31 | loss: 0.36244 | val_0_rmse: 0.63632 | val_1_rmse: 0.60292 |  0:00:09s
epoch 32 | loss: 0.38932 | val_0_rmse: 0.71684 | val_1_rmse: 0.67136 |  0:00:09s
epoch 33 | loss: 0.3866  | val_0_rmse: 0.63682 | val_1_rmse: 0.60795 |  0:00:09s
epoch 34 | loss: 0.37822 | val_0_rmse: 0.65953 | val_1_rmse: 0.62726 |  0:00:10s
epoch 35 | loss: 0.37963 | val_0_rmse: 0.67275 | val_1_rmse: 0.63352 |  0:00:10s
epoch 36 | loss: 0.35866 | val_0_rmse: 0.63702 | val_1_rmse: 0.60425 |  0:00:10s
epoch 37 | loss: 0.36491 | val_0_rmse: 0.61716 | val_1_rmse: 0.5908  |  0:00:11s
epoch 38 | loss: 0.36351 | val_0_rmse: 0.65989 | val_1_rmse: 0.61916 |  0:00:11s
epoch 39 | loss: 0.39337 | val_0_rmse: 0.67827 | val_1_rmse: 0.65376 |  0:00:11s
epoch 40 | loss: 0.37571 | val_0_rmse: 0.62946 | val_1_rmse: 0.59734 |  0:00:11s
epoch 41 | loss: 0.36286 | val_0_rmse: 0.65224 | val_1_rmse: 0.62337 |  0:00:12s
epoch 42 | loss: 0.36516 | val_0_rmse: 0.60991 | val_1_rmse: 0.58136 |  0:00:12s
epoch 43 | loss: 0.36894 | val_0_rmse: 0.61733 | val_1_rmse: 0.58626 |  0:00:12s
epoch 44 | loss: 0.34498 | val_0_rmse: 0.61309 | val_1_rmse: 0.57519 |  0:00:13s
epoch 45 | loss: 0.3445  | val_0_rmse: 0.62904 | val_1_rmse: 0.59227 |  0:00:13s
epoch 46 | loss: 0.37077 | val_0_rmse: 0.66997 | val_1_rmse: 0.63468 |  0:00:13s
epoch 47 | loss: 0.38082 | val_0_rmse: 0.64266 | val_1_rmse: 0.62151 |  0:00:13s
epoch 48 | loss: 0.38662 | val_0_rmse: 0.65031 | val_1_rmse: 0.62112 |  0:00:14s
epoch 49 | loss: 0.37944 | val_0_rmse: 0.66579 | val_1_rmse: 0.6305  |  0:00:14s
epoch 50 | loss: 0.37554 | val_0_rmse: 0.69688 | val_1_rmse: 0.64582 |  0:00:14s

Early stopping occured at epoch 50 with best_epoch = 20 and best_val_1_rmse = 0.57449
Best weights from best epoch are automatically used!
ended training at: 01:46:45
Feature importance:
[('Area', 0.26121750469963606), ('Baths', 0.12719634152915874), ('Beds', 0.05442666344922149), ('Latitude', 0.17315623871726185), ('Longitude', 0.34623700104073285), ('Month', 0.01816650307865772), ('Year', 0.01959974748533129)]
Mean squared error is of 3278097054.624247
Mean absolute error:39183.37280700539
MAPE:0.38930324363123087
R2 score:0.6179385358897977
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:46:46
epoch 0  | loss: 1.06174 | val_0_rmse: 0.98903 | val_1_rmse: 0.93412 |  0:00:00s
epoch 1  | loss: 0.58891 | val_0_rmse: 0.95958 | val_1_rmse: 0.92948 |  0:00:00s
epoch 2  | loss: 0.56569 | val_0_rmse: 0.85871 | val_1_rmse: 0.83615 |  0:00:00s
epoch 3  | loss: 0.54405 | val_0_rmse: 0.7798  | val_1_rmse: 0.79143 |  0:00:01s
epoch 4  | loss: 0.52594 | val_0_rmse: 0.74679 | val_1_rmse: 0.78576 |  0:00:01s
epoch 5  | loss: 0.52223 | val_0_rmse: 0.72812 | val_1_rmse: 0.76211 |  0:00:01s
epoch 6  | loss: 0.52437 | val_0_rmse: 0.72719 | val_1_rmse: 0.77033 |  0:00:02s
epoch 7  | loss: 0.51605 | val_0_rmse: 0.7117  | val_1_rmse: 0.73988 |  0:00:02s
epoch 8  | loss: 0.51465 | val_0_rmse: 0.71852 | val_1_rmse: 0.76413 |  0:00:02s
epoch 9  | loss: 0.49767 | val_0_rmse: 0.69734 | val_1_rmse: 0.73921 |  0:00:03s
epoch 10 | loss: 0.48073 | val_0_rmse: 0.70244 | val_1_rmse: 0.74081 |  0:00:03s
epoch 11 | loss: 0.47008 | val_0_rmse: 0.68429 | val_1_rmse: 0.74606 |  0:00:03s
epoch 12 | loss: 0.461   | val_0_rmse: 0.67865 | val_1_rmse: 0.72115 |  0:00:03s
epoch 13 | loss: 0.46723 | val_0_rmse: 0.68754 | val_1_rmse: 0.75382 |  0:00:04s
epoch 14 | loss: 0.47002 | val_0_rmse: 0.6821  | val_1_rmse: 0.7187  |  0:00:04s
epoch 15 | loss: 0.47277 | val_0_rmse: 0.69339 | val_1_rmse: 0.72463 |  0:00:04s
epoch 16 | loss: 0.4658  | val_0_rmse: 0.67615 | val_1_rmse: 0.71357 |  0:00:05s
epoch 17 | loss: 0.43727 | val_0_rmse: 0.6649  | val_1_rmse: 0.69159 |  0:00:05s
epoch 18 | loss: 0.43646 | val_0_rmse: 0.67971 | val_1_rmse: 0.71021 |  0:00:05s
epoch 19 | loss: 0.42358 | val_0_rmse: 0.67634 | val_1_rmse: 0.69345 |  0:00:05s
epoch 20 | loss: 0.42985 | val_0_rmse: 0.69775 | val_1_rmse: 0.7274  |  0:00:06s
epoch 21 | loss: 0.42549 | val_0_rmse: 0.65609 | val_1_rmse: 0.68027 |  0:00:06s
epoch 22 | loss: 0.41409 | val_0_rmse: 0.66412 | val_1_rmse: 0.69642 |  0:00:06s
epoch 23 | loss: 0.4263  | val_0_rmse: 0.65816 | val_1_rmse: 0.6855  |  0:00:07s
epoch 24 | loss: 0.41229 | val_0_rmse: 0.65716 | val_1_rmse: 0.67955 |  0:00:07s
epoch 25 | loss: 0.41081 | val_0_rmse: 0.65349 | val_1_rmse: 0.69247 |  0:00:07s
epoch 26 | loss: 0.39744 | val_0_rmse: 0.62975 | val_1_rmse: 0.67178 |  0:00:07s
epoch 27 | loss: 0.40773 | val_0_rmse: 0.65843 | val_1_rmse: 0.70305 |  0:00:08s
epoch 28 | loss: 0.39233 | val_0_rmse: 0.64574 | val_1_rmse: 0.68937 |  0:00:08s
epoch 29 | loss: 0.40416 | val_0_rmse: 0.65855 | val_1_rmse: 0.71015 |  0:00:08s
epoch 30 | loss: 0.41587 | val_0_rmse: 0.6196  | val_1_rmse: 0.65686 |  0:00:09s
epoch 31 | loss: 0.39637 | val_0_rmse: 0.64892 | val_1_rmse: 0.6893  |  0:00:09s
epoch 32 | loss: 0.39018 | val_0_rmse: 0.62892 | val_1_rmse: 0.66326 |  0:00:09s
epoch 33 | loss: 0.39673 | val_0_rmse: 0.63234 | val_1_rmse: 0.67303 |  0:00:09s
epoch 34 | loss: 0.3826  | val_0_rmse: 0.61671 | val_1_rmse: 0.65013 |  0:00:10s
epoch 35 | loss: 0.38472 | val_0_rmse: 0.61196 | val_1_rmse: 0.65111 |  0:00:10s
epoch 36 | loss: 0.37573 | val_0_rmse: 0.61245 | val_1_rmse: 0.64965 |  0:00:10s
epoch 37 | loss: 0.37308 | val_0_rmse: 0.61602 | val_1_rmse: 0.66309 |  0:00:11s
epoch 38 | loss: 0.39033 | val_0_rmse: 0.63063 | val_1_rmse: 0.67017 |  0:00:11s
epoch 39 | loss: 0.39304 | val_0_rmse: 0.61857 | val_1_rmse: 0.65502 |  0:00:11s
epoch 40 | loss: 0.38779 | val_0_rmse: 0.62906 | val_1_rmse: 0.68319 |  0:00:12s
epoch 41 | loss: 0.37698 | val_0_rmse: 0.63068 | val_1_rmse: 0.66771 |  0:00:12s
epoch 42 | loss: 0.37844 | val_0_rmse: 0.63015 | val_1_rmse: 0.6656  |  0:00:12s
epoch 43 | loss: 0.3621  | val_0_rmse: 0.6103  | val_1_rmse: 0.6414  |  0:00:12s
epoch 44 | loss: 0.36772 | val_0_rmse: 0.61858 | val_1_rmse: 0.65861 |  0:00:13s
epoch 45 | loss: 0.3707  | val_0_rmse: 0.61607 | val_1_rmse: 0.65612 |  0:00:13s
epoch 46 | loss: 0.3629  | val_0_rmse: 0.61172 | val_1_rmse: 0.64899 |  0:00:13s
epoch 47 | loss: 0.36332 | val_0_rmse: 0.63213 | val_1_rmse: 0.67353 |  0:00:14s
epoch 48 | loss: 0.3645  | val_0_rmse: 0.61045 | val_1_rmse: 0.65397 |  0:00:14s
epoch 49 | loss: 0.35342 | val_0_rmse: 0.62233 | val_1_rmse: 0.66395 |  0:00:14s
epoch 50 | loss: 0.35942 | val_0_rmse: 0.63147 | val_1_rmse: 0.67624 |  0:00:14s
epoch 51 | loss: 0.34953 | val_0_rmse: 0.66529 | val_1_rmse: 0.70672 |  0:00:15s
epoch 52 | loss: 0.35118 | val_0_rmse: 0.6281  | val_1_rmse: 0.67128 |  0:00:15s
epoch 53 | loss: 0.35719 | val_0_rmse: 0.65234 | val_1_rmse: 0.69991 |  0:00:15s
epoch 54 | loss: 0.35566 | val_0_rmse: 0.61575 | val_1_rmse: 0.65679 |  0:00:16s
epoch 55 | loss: 0.35252 | val_0_rmse: 0.63495 | val_1_rmse: 0.67885 |  0:00:16s
epoch 56 | loss: 0.34464 | val_0_rmse: 0.64722 | val_1_rmse: 0.69326 |  0:00:16s
epoch 57 | loss: 0.35336 | val_0_rmse: 0.64258 | val_1_rmse: 0.68022 |  0:00:17s
epoch 58 | loss: 0.36132 | val_0_rmse: 0.64504 | val_1_rmse: 0.6834  |  0:00:17s
epoch 59 | loss: 0.37283 | val_0_rmse: 0.62252 | val_1_rmse: 0.66327 |  0:00:17s
epoch 60 | loss: 0.37095 | val_0_rmse: 0.6098  | val_1_rmse: 0.65116 |  0:00:17s
epoch 61 | loss: 0.35167 | val_0_rmse: 0.6371  | val_1_rmse: 0.68323 |  0:00:18s
epoch 62 | loss: 0.35512 | val_0_rmse: 0.6172  | val_1_rmse: 0.65658 |  0:00:18s
epoch 63 | loss: 0.35732 | val_0_rmse: 0.64911 | val_1_rmse: 0.69554 |  0:00:18s
epoch 64 | loss: 0.35049 | val_0_rmse: 0.61526 | val_1_rmse: 0.6582  |  0:00:19s
epoch 65 | loss: 0.34561 | val_0_rmse: 0.64334 | val_1_rmse: 0.69345 |  0:00:19s
epoch 66 | loss: 0.34458 | val_0_rmse: 0.67025 | val_1_rmse: 0.72049 |  0:00:19s
epoch 67 | loss: 0.34979 | val_0_rmse: 0.60498 | val_1_rmse: 0.64825 |  0:00:19s
epoch 68 | loss: 0.36192 | val_0_rmse: 0.6597  | val_1_rmse: 0.70482 |  0:00:20s
epoch 69 | loss: 0.34915 | val_0_rmse: 0.60819 | val_1_rmse: 0.64763 |  0:00:20s
epoch 70 | loss: 0.34975 | val_0_rmse: 0.63618 | val_1_rmse: 0.67643 |  0:00:20s
epoch 71 | loss: 0.3654  | val_0_rmse: 0.63094 | val_1_rmse: 0.67262 |  0:00:21s
epoch 72 | loss: 0.36613 | val_0_rmse: 0.61257 | val_1_rmse: 0.64455 |  0:00:21s
epoch 73 | loss: 0.3787  | val_0_rmse: 0.64052 | val_1_rmse: 0.67965 |  0:00:21s

Early stopping occured at epoch 73 with best_epoch = 43 and best_val_1_rmse = 0.6414
Best weights from best epoch are automatically used!
ended training at: 01:47:08
Feature importance:
[('Area', 0.3878074156811224), ('Baths', 0.14039767554219448), ('Beds', 0.0852794835009105), ('Latitude', 0.21745234504029967), ('Longitude', 0.1690292366302394), ('Month', 3.38436052334973e-05), ('Year', 0.0)]
Mean squared error is of 3524264759.115421
Mean absolute error:40744.70191972623
MAPE:0.3461013433400169
R2 score:0.5587544802790962
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:47:08
epoch 0  | loss: 1.31515 | val_0_rmse: 1.01157 | val_1_rmse: 1.00267 |  0:00:00s
epoch 1  | loss: 0.68454 | val_0_rmse: 0.838   | val_1_rmse: 0.82801 |  0:00:00s
epoch 2  | loss: 0.56973 | val_0_rmse: 0.8048  | val_1_rmse: 0.81275 |  0:00:00s
epoch 3  | loss: 0.52941 | val_0_rmse: 0.78542 | val_1_rmse: 0.77941 |  0:00:01s
epoch 4  | loss: 0.51042 | val_0_rmse: 0.73396 | val_1_rmse: 0.73099 |  0:00:01s
epoch 5  | loss: 0.50338 | val_0_rmse: 0.7082  | val_1_rmse: 0.71205 |  0:00:01s
epoch 6  | loss: 0.48623 | val_0_rmse: 0.69958 | val_1_rmse: 0.69261 |  0:00:02s
epoch 7  | loss: 0.47542 | val_0_rmse: 0.6791  | val_1_rmse: 0.67179 |  0:00:02s
epoch 8  | loss: 0.46723 | val_0_rmse: 0.72146 | val_1_rmse: 0.71785 |  0:00:02s
epoch 9  | loss: 0.45997 | val_0_rmse: 0.66948 | val_1_rmse: 0.65993 |  0:00:02s
epoch 10 | loss: 0.46535 | val_0_rmse: 0.68808 | val_1_rmse: 0.68477 |  0:00:03s
epoch 11 | loss: 0.46294 | val_0_rmse: 0.67663 | val_1_rmse: 0.67269 |  0:00:03s
epoch 12 | loss: 0.45535 | val_0_rmse: 0.66879 | val_1_rmse: 0.67142 |  0:00:03s
epoch 13 | loss: 0.45062 | val_0_rmse: 0.65869 | val_1_rmse: 0.65019 |  0:00:04s
epoch 14 | loss: 0.44299 | val_0_rmse: 0.66046 | val_1_rmse: 0.65426 |  0:00:04s
epoch 15 | loss: 0.44481 | val_0_rmse: 0.6701  | val_1_rmse: 0.66596 |  0:00:04s
epoch 16 | loss: 0.43992 | val_0_rmse: 0.65889 | val_1_rmse: 0.65279 |  0:00:04s
epoch 17 | loss: 0.43258 | val_0_rmse: 0.65938 | val_1_rmse: 0.65454 |  0:00:05s
epoch 18 | loss: 0.43363 | val_0_rmse: 0.70225 | val_1_rmse: 0.69101 |  0:00:05s
epoch 19 | loss: 0.43162 | val_0_rmse: 0.66426 | val_1_rmse: 0.65724 |  0:00:05s
epoch 20 | loss: 0.42857 | val_0_rmse: 0.65672 | val_1_rmse: 0.64942 |  0:00:06s
epoch 21 | loss: 0.42909 | val_0_rmse: 0.68021 | val_1_rmse: 0.67612 |  0:00:06s
epoch 22 | loss: 0.43718 | val_0_rmse: 0.66607 | val_1_rmse: 0.65167 |  0:00:06s
epoch 23 | loss: 0.43317 | val_0_rmse: 0.65637 | val_1_rmse: 0.65866 |  0:00:06s
epoch 24 | loss: 0.42903 | val_0_rmse: 0.63926 | val_1_rmse: 0.63462 |  0:00:07s
epoch 25 | loss: 0.43359 | val_0_rmse: 0.65354 | val_1_rmse: 0.65237 |  0:00:07s
epoch 26 | loss: 0.40529 | val_0_rmse: 0.63492 | val_1_rmse: 0.63111 |  0:00:07s
epoch 27 | loss: 0.41057 | val_0_rmse: 0.68589 | val_1_rmse: 0.6652  |  0:00:08s
epoch 28 | loss: 0.44761 | val_0_rmse: 0.65742 | val_1_rmse: 0.64932 |  0:00:08s
epoch 29 | loss: 0.43718 | val_0_rmse: 0.65606 | val_1_rmse: 0.65325 |  0:00:08s
epoch 30 | loss: 0.43808 | val_0_rmse: 0.69836 | val_1_rmse: 0.73658 |  0:00:08s
epoch 31 | loss: 0.43196 | val_0_rmse: 0.6418  | val_1_rmse: 0.64689 |  0:00:09s
epoch 32 | loss: 0.4185  | val_0_rmse: 0.66385 | val_1_rmse: 0.6535  |  0:00:09s
epoch 33 | loss: 0.41603 | val_0_rmse: 0.63739 | val_1_rmse: 0.6366  |  0:00:09s
epoch 34 | loss: 0.40522 | val_0_rmse: 0.62569 | val_1_rmse: 0.63151 |  0:00:10s
epoch 35 | loss: 0.40505 | val_0_rmse: 0.62462 | val_1_rmse: 0.62449 |  0:00:10s
epoch 36 | loss: 0.39976 | val_0_rmse: 0.63539 | val_1_rmse: 0.64208 |  0:00:10s
epoch 37 | loss: 0.38969 | val_0_rmse: 0.62149 | val_1_rmse: 0.62465 |  0:00:11s
epoch 38 | loss: 0.39213 | val_0_rmse: 0.65255 | val_1_rmse: 0.65742 |  0:00:11s
epoch 39 | loss: 0.39536 | val_0_rmse: 0.61997 | val_1_rmse: 0.62062 |  0:00:11s
epoch 40 | loss: 0.38619 | val_0_rmse: 0.63959 | val_1_rmse: 0.63595 |  0:00:11s
epoch 41 | loss: 0.40131 | val_0_rmse: 0.63303 | val_1_rmse: 0.62169 |  0:00:12s
epoch 42 | loss: 0.38227 | val_0_rmse: 0.62596 | val_1_rmse: 0.62286 |  0:00:12s
epoch 43 | loss: 0.38443 | val_0_rmse: 0.62075 | val_1_rmse: 0.62243 |  0:00:12s
epoch 44 | loss: 0.38245 | val_0_rmse: 0.63197 | val_1_rmse: 0.63186 |  0:00:13s
epoch 45 | loss: 0.37521 | val_0_rmse: 0.6206  | val_1_rmse: 0.6209  |  0:00:13s
epoch 46 | loss: 0.38028 | val_0_rmse: 0.62066 | val_1_rmse: 0.62243 |  0:00:13s
epoch 47 | loss: 0.37531 | val_0_rmse: 0.61603 | val_1_rmse: 0.61784 |  0:00:13s
epoch 48 | loss: 0.38286 | val_0_rmse: 0.62205 | val_1_rmse: 0.62613 |  0:00:14s
epoch 49 | loss: 0.36965 | val_0_rmse: 0.66544 | val_1_rmse: 0.66843 |  0:00:14s
epoch 50 | loss: 0.38007 | val_0_rmse: 0.61152 | val_1_rmse: 0.60653 |  0:00:14s
epoch 51 | loss: 0.37749 | val_0_rmse: 0.62657 | val_1_rmse: 0.63386 |  0:00:15s
epoch 52 | loss: 0.39208 | val_0_rmse: 0.6217  | val_1_rmse: 0.61818 |  0:00:15s
epoch 53 | loss: 0.38284 | val_0_rmse: 0.64092 | val_1_rmse: 0.64495 |  0:00:15s
epoch 54 | loss: 0.37644 | val_0_rmse: 0.64127 | val_1_rmse: 0.6415  |  0:00:16s
epoch 55 | loss: 0.38407 | val_0_rmse: 0.64335 | val_1_rmse: 0.64059 |  0:00:16s
epoch 56 | loss: 0.37451 | val_0_rmse: 0.6264  | val_1_rmse: 0.61683 |  0:00:16s
epoch 57 | loss: 0.37247 | val_0_rmse: 0.63915 | val_1_rmse: 0.61758 |  0:00:16s
epoch 58 | loss: 0.38232 | val_0_rmse: 0.65848 | val_1_rmse: 0.65628 |  0:00:17s
epoch 59 | loss: 0.38654 | val_0_rmse: 0.65126 | val_1_rmse: 0.6441  |  0:00:17s
epoch 60 | loss: 0.38163 | val_0_rmse: 0.61784 | val_1_rmse: 0.61515 |  0:00:17s
epoch 61 | loss: 0.37932 | val_0_rmse: 0.62545 | val_1_rmse: 0.62727 |  0:00:18s
epoch 62 | loss: 0.37742 | val_0_rmse: 0.72049 | val_1_rmse: 0.7229  |  0:00:18s
epoch 63 | loss: 0.37638 | val_0_rmse: 0.64852 | val_1_rmse: 0.65303 |  0:00:18s
epoch 64 | loss: 0.37017 | val_0_rmse: 0.65878 | val_1_rmse: 0.66592 |  0:00:18s
epoch 65 | loss: 0.37369 | val_0_rmse: 0.63906 | val_1_rmse: 0.6394  |  0:00:19s
epoch 66 | loss: 0.37832 | val_0_rmse: 0.65353 | val_1_rmse: 0.65935 |  0:00:19s
epoch 67 | loss: 0.37695 | val_0_rmse: 0.64741 | val_1_rmse: 0.6447  |  0:00:19s
epoch 68 | loss: 0.37457 | val_0_rmse: 0.63984 | val_1_rmse: 0.63972 |  0:00:20s
epoch 69 | loss: 0.36737 | val_0_rmse: 0.62246 | val_1_rmse: 0.62384 |  0:00:20s
epoch 70 | loss: 0.37191 | val_0_rmse: 0.62129 | val_1_rmse: 0.61554 |  0:00:20s
epoch 71 | loss: 0.36641 | val_0_rmse: 0.61864 | val_1_rmse: 0.61647 |  0:00:20s
epoch 72 | loss: 0.36288 | val_0_rmse: 0.63384 | val_1_rmse: 0.63061 |  0:00:21s
epoch 73 | loss: 0.35442 | val_0_rmse: 0.65098 | val_1_rmse: 0.65062 |  0:00:21s
epoch 74 | loss: 0.37237 | val_0_rmse: 0.64868 | val_1_rmse: 0.64943 |  0:00:21s
epoch 75 | loss: 0.36918 | val_0_rmse: 0.64811 | val_1_rmse: 0.64689 |  0:00:22s
epoch 76 | loss: 0.36849 | val_0_rmse: 0.67354 | val_1_rmse: 0.67951 |  0:00:22s
epoch 77 | loss: 0.36236 | val_0_rmse: 0.64965 | val_1_rmse: 0.65442 |  0:00:22s
epoch 78 | loss: 0.36182 | val_0_rmse: 0.63581 | val_1_rmse: 0.64521 |  0:00:22s
epoch 79 | loss: 0.36513 | val_0_rmse: 0.62614 | val_1_rmse: 0.62411 |  0:00:23s
epoch 80 | loss: 0.35956 | val_0_rmse: 0.61448 | val_1_rmse: 0.6128  |  0:00:23s

Early stopping occured at epoch 80 with best_epoch = 50 and best_val_1_rmse = 0.60653
Best weights from best epoch are automatically used!
ended training at: 01:47:31
Feature importance:
[('Area', 0.34638038046936614), ('Baths', 0.20950944099508856), ('Beds', 0.006902233651422979), ('Latitude', 0.20847847691627797), ('Longitude', 0.14211732789867834), ('Month', 0.033551708160610406), ('Year', 0.05306043190855562)]
Mean squared error is of 3167337432.6865187
Mean absolute error:38179.80546148132
MAPE:0.38682238212920306
R2 score:0.6045570279411239
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:47:31
epoch 0  | loss: 1.41906 | val_0_rmse: 1.22107 | val_1_rmse: 1.13028 |  0:00:00s
epoch 1  | loss: 0.68583 | val_0_rmse: 1.1093  | val_1_rmse: 1.10411 |  0:00:00s
epoch 2  | loss: 0.59167 | val_0_rmse: 0.82707 | val_1_rmse: 0.83344 |  0:00:00s
epoch 3  | loss: 0.55273 | val_0_rmse: 0.76272 | val_1_rmse: 0.77463 |  0:00:01s
epoch 4  | loss: 0.51972 | val_0_rmse: 0.73155 | val_1_rmse: 0.75214 |  0:00:01s
epoch 5  | loss: 0.50849 | val_0_rmse: 0.71726 | val_1_rmse: 0.72118 |  0:00:01s
epoch 6  | loss: 0.49497 | val_0_rmse: 0.70459 | val_1_rmse: 0.72592 |  0:00:02s
epoch 7  | loss: 0.48238 | val_0_rmse: 0.71704 | val_1_rmse: 0.71412 |  0:00:02s
epoch 8  | loss: 0.47784 | val_0_rmse: 0.68722 | val_1_rmse: 0.70969 |  0:00:02s
epoch 9  | loss: 0.48522 | val_0_rmse: 0.69313 | val_1_rmse: 0.71504 |  0:00:02s
epoch 10 | loss: 0.47978 | val_0_rmse: 0.6819  | val_1_rmse: 0.69874 |  0:00:03s
epoch 11 | loss: 0.47699 | val_0_rmse: 0.68889 | val_1_rmse: 0.70792 |  0:00:03s
epoch 12 | loss: 0.47442 | val_0_rmse: 0.67881 | val_1_rmse: 0.68872 |  0:00:03s
epoch 13 | loss: 0.4734  | val_0_rmse: 0.67584 | val_1_rmse: 0.67264 |  0:00:04s
epoch 14 | loss: 0.46992 | val_0_rmse: 0.69473 | val_1_rmse: 0.68468 |  0:00:04s
epoch 15 | loss: 0.46575 | val_0_rmse: 0.68329 | val_1_rmse: 0.66533 |  0:00:04s
epoch 16 | loss: 0.46788 | val_0_rmse: 0.66902 | val_1_rmse: 0.66888 |  0:00:05s
epoch 17 | loss: 0.45603 | val_0_rmse: 0.66773 | val_1_rmse: 0.67018 |  0:00:05s
epoch 18 | loss: 0.44748 | val_0_rmse: 0.66609 | val_1_rmse: 0.67879 |  0:00:05s
epoch 19 | loss: 0.443   | val_0_rmse: 0.66157 | val_1_rmse: 0.67754 |  0:00:05s
epoch 20 | loss: 0.44553 | val_0_rmse: 0.65509 | val_1_rmse: 0.66803 |  0:00:06s
epoch 21 | loss: 0.4388  | val_0_rmse: 0.65451 | val_1_rmse: 0.66873 |  0:00:06s
epoch 22 | loss: 0.43452 | val_0_rmse: 0.64887 | val_1_rmse: 0.66707 |  0:00:06s
epoch 23 | loss: 0.44005 | val_0_rmse: 0.66095 | val_1_rmse: 0.68418 |  0:00:07s
epoch 24 | loss: 0.43417 | val_0_rmse: 0.65819 | val_1_rmse: 0.66658 |  0:00:07s
epoch 25 | loss: 0.4355  | val_0_rmse: 0.65387 | val_1_rmse: 0.66611 |  0:00:07s
epoch 26 | loss: 0.42954 | val_0_rmse: 0.65383 | val_1_rmse: 0.65964 |  0:00:07s
epoch 27 | loss: 0.42344 | val_0_rmse: 0.65577 | val_1_rmse: 0.66013 |  0:00:08s
epoch 28 | loss: 0.42241 | val_0_rmse: 0.65738 | val_1_rmse: 0.66234 |  0:00:08s
epoch 29 | loss: 0.42959 | val_0_rmse: 0.64342 | val_1_rmse: 0.64719 |  0:00:08s
epoch 30 | loss: 0.43068 | val_0_rmse: 0.66611 | val_1_rmse: 0.67501 |  0:00:09s
epoch 31 | loss: 0.41936 | val_0_rmse: 0.66366 | val_1_rmse: 0.65488 |  0:00:09s
epoch 32 | loss: 0.4279  | val_0_rmse: 0.68193 | val_1_rmse: 0.6692  |  0:00:09s
epoch 33 | loss: 0.43939 | val_0_rmse: 0.67875 | val_1_rmse: 0.67146 |  0:00:09s
epoch 34 | loss: 0.4447  | val_0_rmse: 0.6703  | val_1_rmse: 0.66834 |  0:00:10s
epoch 35 | loss: 0.43889 | val_0_rmse: 0.67581 | val_1_rmse: 0.67726 |  0:00:10s
epoch 36 | loss: 0.44232 | val_0_rmse: 0.65303 | val_1_rmse: 0.64818 |  0:00:10s
epoch 37 | loss: 0.42939 | val_0_rmse: 0.66505 | val_1_rmse: 0.67424 |  0:00:11s
epoch 38 | loss: 0.4329  | val_0_rmse: 0.65472 | val_1_rmse: 0.66266 |  0:00:11s
epoch 39 | loss: 0.42182 | val_0_rmse: 0.64767 | val_1_rmse: 0.66808 |  0:00:11s
epoch 40 | loss: 0.42138 | val_0_rmse: 0.65123 | val_1_rmse: 0.67439 |  0:00:11s
epoch 41 | loss: 0.4111  | val_0_rmse: 0.65098 | val_1_rmse: 0.66868 |  0:00:12s
epoch 42 | loss: 0.41898 | val_0_rmse: 0.64296 | val_1_rmse: 0.6398  |  0:00:12s
epoch 43 | loss: 0.41733 | val_0_rmse: 0.64466 | val_1_rmse: 0.64051 |  0:00:12s
epoch 44 | loss: 0.41273 | val_0_rmse: 0.64945 | val_1_rmse: 0.6634  |  0:00:13s
epoch 45 | loss: 0.41971 | val_0_rmse: 0.65398 | val_1_rmse: 0.67479 |  0:00:13s
epoch 46 | loss: 0.41687 | val_0_rmse: 0.65988 | val_1_rmse: 0.685   |  0:00:13s
epoch 47 | loss: 0.41117 | val_0_rmse: 0.64164 | val_1_rmse: 0.6589  |  0:00:13s
epoch 48 | loss: 0.41297 | val_0_rmse: 0.64412 | val_1_rmse: 0.64814 |  0:00:14s
epoch 49 | loss: 0.41243 | val_0_rmse: 0.6576  | val_1_rmse: 0.67777 |  0:00:14s
epoch 50 | loss: 0.4147  | val_0_rmse: 0.64513 | val_1_rmse: 0.65533 |  0:00:14s
epoch 51 | loss: 0.41194 | val_0_rmse: 0.64144 | val_1_rmse: 0.64326 |  0:00:15s
epoch 52 | loss: 0.40622 | val_0_rmse: 0.63755 | val_1_rmse: 0.64336 |  0:00:15s
epoch 53 | loss: 0.40971 | val_0_rmse: 0.63887 | val_1_rmse: 0.64568 |  0:00:15s
epoch 54 | loss: 0.40945 | val_0_rmse: 0.63582 | val_1_rmse: 0.64275 |  0:00:16s
epoch 55 | loss: 0.40935 | val_0_rmse: 0.64336 | val_1_rmse: 0.6467  |  0:00:16s
epoch 56 | loss: 0.40965 | val_0_rmse: 0.67159 | val_1_rmse: 0.68869 |  0:00:16s
epoch 57 | loss: 0.41742 | val_0_rmse: 0.6405  | val_1_rmse: 0.6481  |  0:00:16s
epoch 58 | loss: 0.40783 | val_0_rmse: 0.65534 | val_1_rmse: 0.66792 |  0:00:17s
epoch 59 | loss: 0.40708 | val_0_rmse: 0.63687 | val_1_rmse: 0.64098 |  0:00:17s
epoch 60 | loss: 0.40259 | val_0_rmse: 0.63176 | val_1_rmse: 0.64015 |  0:00:17s
epoch 61 | loss: 0.40276 | val_0_rmse: 0.63479 | val_1_rmse: 0.63077 |  0:00:18s
epoch 62 | loss: 0.3953  | val_0_rmse: 0.62836 | val_1_rmse: 0.62202 |  0:00:18s
epoch 63 | loss: 0.40334 | val_0_rmse: 0.62941 | val_1_rmse: 0.62778 |  0:00:18s
epoch 64 | loss: 0.38499 | val_0_rmse: 0.61995 | val_1_rmse: 0.61497 |  0:00:18s
epoch 65 | loss: 0.38163 | val_0_rmse: 0.62032 | val_1_rmse: 0.61604 |  0:00:19s
epoch 66 | loss: 0.37293 | val_0_rmse: 0.62889 | val_1_rmse: 0.6498  |  0:00:19s
epoch 67 | loss: 0.37643 | val_0_rmse: 0.62357 | val_1_rmse: 0.63533 |  0:00:19s
epoch 68 | loss: 0.37414 | val_0_rmse: 0.65987 | val_1_rmse: 0.66614 |  0:00:20s
epoch 69 | loss: 0.38624 | val_0_rmse: 0.64559 | val_1_rmse: 0.66259 |  0:00:20s
epoch 70 | loss: 0.37282 | val_0_rmse: 0.62297 | val_1_rmse: 0.64755 |  0:00:20s
epoch 71 | loss: 0.39825 | val_0_rmse: 0.68641 | val_1_rmse: 0.7028  |  0:00:20s
epoch 72 | loss: 0.39788 | val_0_rmse: 0.65578 | val_1_rmse: 0.67414 |  0:00:21s
epoch 73 | loss: 0.39223 | val_0_rmse: 0.65066 | val_1_rmse: 0.65397 |  0:00:21s
epoch 74 | loss: 0.38104 | val_0_rmse: 0.64119 | val_1_rmse: 0.63949 |  0:00:21s
epoch 75 | loss: 0.37154 | val_0_rmse: 0.63997 | val_1_rmse: 0.64495 |  0:00:22s
epoch 76 | loss: 0.36849 | val_0_rmse: 0.65294 | val_1_rmse: 0.66193 |  0:00:22s
epoch 77 | loss: 0.37507 | val_0_rmse: 0.64724 | val_1_rmse: 0.6694  |  0:00:22s
epoch 78 | loss: 0.37142 | val_0_rmse: 0.62405 | val_1_rmse: 0.65654 |  0:00:22s
epoch 79 | loss: 0.36548 | val_0_rmse: 0.59975 | val_1_rmse: 0.62577 |  0:00:23s
epoch 80 | loss: 0.36127 | val_0_rmse: 0.59388 | val_1_rmse: 0.61008 |  0:00:23s
epoch 81 | loss: 0.35195 | val_0_rmse: 0.58815 | val_1_rmse: 0.59939 |  0:00:23s
epoch 82 | loss: 0.3552  | val_0_rmse: 0.59214 | val_1_rmse: 0.60547 |  0:00:24s
epoch 83 | loss: 0.35558 | val_0_rmse: 0.60978 | val_1_rmse: 0.62077 |  0:00:24s
epoch 84 | loss: 0.35464 | val_0_rmse: 0.58657 | val_1_rmse: 0.60302 |  0:00:24s
epoch 85 | loss: 0.35268 | val_0_rmse: 0.59242 | val_1_rmse: 0.60328 |  0:00:25s
epoch 86 | loss: 0.35199 | val_0_rmse: 0.60755 | val_1_rmse: 0.60667 |  0:00:25s
epoch 87 | loss: 0.36323 | val_0_rmse: 0.6304  | val_1_rmse: 0.63304 |  0:00:25s
epoch 88 | loss: 0.35277 | val_0_rmse: 0.60243 | val_1_rmse: 0.60938 |  0:00:25s
epoch 89 | loss: 0.35774 | val_0_rmse: 0.61417 | val_1_rmse: 0.62002 |  0:00:26s
epoch 90 | loss: 0.35769 | val_0_rmse: 0.60605 | val_1_rmse: 0.61238 |  0:00:26s
epoch 91 | loss: 0.34343 | val_0_rmse: 0.59171 | val_1_rmse: 0.60076 |  0:00:26s
epoch 92 | loss: 0.35215 | val_0_rmse: 0.59872 | val_1_rmse: 0.60443 |  0:00:26s
epoch 93 | loss: 0.3442  | val_0_rmse: 0.59267 | val_1_rmse: 0.59593 |  0:00:27s
epoch 94 | loss: 0.34421 | val_0_rmse: 0.59288 | val_1_rmse: 0.59843 |  0:00:27s
epoch 95 | loss: 0.33831 | val_0_rmse: 0.58694 | val_1_rmse: 0.59683 |  0:00:27s
epoch 96 | loss: 0.33974 | val_0_rmse: 0.57598 | val_1_rmse: 0.58487 |  0:00:28s
epoch 97 | loss: 0.33553 | val_0_rmse: 0.57478 | val_1_rmse: 0.5868  |  0:00:28s
epoch 98 | loss: 0.33238 | val_0_rmse: 0.57464 | val_1_rmse: 0.57991 |  0:00:28s
epoch 99 | loss: 0.32971 | val_0_rmse: 0.58709 | val_1_rmse: 0.60211 |  0:00:29s
epoch 100| loss: 0.33984 | val_0_rmse: 0.59525 | val_1_rmse: 0.6005  |  0:00:29s
epoch 101| loss: 0.33642 | val_0_rmse: 0.57314 | val_1_rmse: 0.57634 |  0:00:29s
epoch 102| loss: 0.3366  | val_0_rmse: 0.58066 | val_1_rmse: 0.58027 |  0:00:29s
epoch 103| loss: 0.33206 | val_0_rmse: 0.59859 | val_1_rmse: 0.61631 |  0:00:30s
epoch 104| loss: 0.33482 | val_0_rmse: 0.6021  | val_1_rmse: 0.61128 |  0:00:30s
epoch 105| loss: 0.34471 | val_0_rmse: 0.60274 | val_1_rmse: 0.61874 |  0:00:30s
epoch 106| loss: 0.34394 | val_0_rmse: 0.56999 | val_1_rmse: 0.58141 |  0:00:31s
epoch 107| loss: 0.34321 | val_0_rmse: 0.60002 | val_1_rmse: 0.62224 |  0:00:31s
epoch 108| loss: 0.3343  | val_0_rmse: 0.59816 | val_1_rmse: 0.61452 |  0:00:31s
epoch 109| loss: 0.33167 | val_0_rmse: 0.60096 | val_1_rmse: 0.62857 |  0:00:31s
epoch 110| loss: 0.33747 | val_0_rmse: 0.5979  | val_1_rmse: 0.60483 |  0:00:32s
epoch 111| loss: 0.33683 | val_0_rmse: 0.5945  | val_1_rmse: 0.60963 |  0:00:32s
epoch 112| loss: 0.33961 | val_0_rmse: 0.57685 | val_1_rmse: 0.58554 |  0:00:32s
epoch 113| loss: 0.33065 | val_0_rmse: 0.58491 | val_1_rmse: 0.5979  |  0:00:33s
epoch 114| loss: 0.32377 | val_0_rmse: 0.57478 | val_1_rmse: 0.58766 |  0:00:33s
epoch 115| loss: 0.32504 | val_0_rmse: 0.61419 | val_1_rmse: 0.63167 |  0:00:33s
epoch 116| loss: 0.32795 | val_0_rmse: 0.58228 | val_1_rmse: 0.60237 |  0:00:34s
epoch 117| loss: 0.3275  | val_0_rmse: 0.58488 | val_1_rmse: 0.5998  |  0:00:34s
epoch 118| loss: 0.32123 | val_0_rmse: 0.59105 | val_1_rmse: 0.61209 |  0:00:34s
epoch 119| loss: 0.31753 | val_0_rmse: 0.57397 | val_1_rmse: 0.58412 |  0:00:34s
epoch 120| loss: 0.33191 | val_0_rmse: 0.58262 | val_1_rmse: 0.58607 |  0:00:35s
epoch 121| loss: 0.33478 | val_0_rmse: 0.57069 | val_1_rmse: 0.57761 |  0:00:35s
epoch 122| loss: 0.32993 | val_0_rmse: 0.58277 | val_1_rmse: 0.59091 |  0:00:35s
epoch 123| loss: 0.32374 | val_0_rmse: 0.57967 | val_1_rmse: 0.595   |  0:00:35s
epoch 124| loss: 0.3244  | val_0_rmse: 0.56691 | val_1_rmse: 0.58309 |  0:00:36s
epoch 125| loss: 0.3242  | val_0_rmse: 0.57307 | val_1_rmse: 0.59147 |  0:00:36s
epoch 126| loss: 0.31995 | val_0_rmse: 0.5525  | val_1_rmse: 0.57609 |  0:00:36s
epoch 127| loss: 0.31019 | val_0_rmse: 0.54609 | val_1_rmse: 0.55934 |  0:00:37s
epoch 128| loss: 0.32585 | val_0_rmse: 0.56308 | val_1_rmse: 0.57352 |  0:00:37s
epoch 129| loss: 0.34258 | val_0_rmse: 0.59683 | val_1_rmse: 0.60392 |  0:00:37s
epoch 130| loss: 0.32985 | val_0_rmse: 0.58327 | val_1_rmse: 0.60821 |  0:00:38s
epoch 131| loss: 0.33885 | val_0_rmse: 0.59001 | val_1_rmse: 0.61928 |  0:00:38s
epoch 132| loss: 0.32437 | val_0_rmse: 0.57586 | val_1_rmse: 0.60151 |  0:00:38s
epoch 133| loss: 0.3281  | val_0_rmse: 0.59637 | val_1_rmse: 0.61512 |  0:00:38s
epoch 134| loss: 0.31995 | val_0_rmse: 0.57229 | val_1_rmse: 0.59626 |  0:00:39s
epoch 135| loss: 0.31427 | val_0_rmse: 0.57208 | val_1_rmse: 0.58779 |  0:00:39s
epoch 136| loss: 0.31517 | val_0_rmse: 0.55662 | val_1_rmse: 0.58203 |  0:00:39s
epoch 137| loss: 0.32657 | val_0_rmse: 0.5644  | val_1_rmse: 0.59171 |  0:00:40s
epoch 138| loss: 0.30891 | val_0_rmse: 0.58121 | val_1_rmse: 0.61288 |  0:00:40s
epoch 139| loss: 0.31714 | val_0_rmse: 0.56116 | val_1_rmse: 0.59513 |  0:00:40s
epoch 140| loss: 0.3122  | val_0_rmse: 0.57702 | val_1_rmse: 0.60223 |  0:00:40s
epoch 141| loss: 0.32085 | val_0_rmse: 0.56867 | val_1_rmse: 0.59742 |  0:00:41s
epoch 142| loss: 0.31266 | val_0_rmse: 0.58688 | val_1_rmse: 0.61155 |  0:00:41s
epoch 143| loss: 0.31812 | val_0_rmse: 0.5692  | val_1_rmse: 0.59259 |  0:00:41s
epoch 144| loss: 0.30749 | val_0_rmse: 0.57865 | val_1_rmse: 0.59795 |  0:00:42s
epoch 145| loss: 0.31219 | val_0_rmse: 0.56956 | val_1_rmse: 0.59423 |  0:00:42s
epoch 146| loss: 0.31137 | val_0_rmse: 0.58121 | val_1_rmse: 0.60805 |  0:00:42s
epoch 147| loss: 0.31748 | val_0_rmse: 0.56734 | val_1_rmse: 0.58801 |  0:00:42s
epoch 148| loss: 0.31602 | val_0_rmse: 0.57522 | val_1_rmse: 0.58972 |  0:00:43s
epoch 149| loss: 0.32107 | val_0_rmse: 0.58758 | val_1_rmse: 0.62083 |  0:00:43s
Stop training because you reached max_epochs = 150 with best_epoch = 127 and best_val_1_rmse = 0.55934
Best weights from best epoch are automatically used!
ended training at: 01:48:15
Feature importance:
[('Area', 0.3304330027243734), ('Baths', 0.18201105786182015), ('Beds', 0.08751587150848478), ('Latitude', 0.28780100256620156), ('Longitude', 0.0), ('Month', 0.00719613330220041), ('Year', 0.10504293203691965)]
Mean squared error is of 2858663960.5754356
Mean absolute error:35243.72195650732
MAPE:0.2698264247383586
R2 score:0.6807958890740742
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:48:15
epoch 0  | loss: 1.19729 | val_0_rmse: 0.9512  | val_1_rmse: 0.96436 |  0:00:00s
epoch 1  | loss: 0.62995 | val_0_rmse: 0.82427 | val_1_rmse: 0.80117 |  0:00:00s
epoch 2  | loss: 0.59018 | val_0_rmse: 0.78995 | val_1_rmse: 0.71686 |  0:00:00s
epoch 3  | loss: 0.57186 | val_0_rmse: 0.76001 | val_1_rmse: 0.69788 |  0:00:01s
epoch 4  | loss: 0.55366 | val_0_rmse: 0.74837 | val_1_rmse: 0.70655 |  0:00:01s
epoch 5  | loss: 0.52039 | val_0_rmse: 0.76593 | val_1_rmse: 0.73356 |  0:00:01s
epoch 6  | loss: 0.50296 | val_0_rmse: 0.74454 | val_1_rmse: 0.71586 |  0:00:02s
epoch 7  | loss: 0.49214 | val_0_rmse: 0.7335  | val_1_rmse: 0.71446 |  0:00:02s
epoch 8  | loss: 0.48283 | val_0_rmse: 0.68616 | val_1_rmse: 0.68529 |  0:00:02s
epoch 9  | loss: 0.47478 | val_0_rmse: 0.69396 | val_1_rmse: 0.68267 |  0:00:02s
epoch 10 | loss: 0.46361 | val_0_rmse: 0.67848 | val_1_rmse: 0.65413 |  0:00:03s
epoch 11 | loss: 0.45433 | val_0_rmse: 0.67756 | val_1_rmse: 0.65399 |  0:00:03s
epoch 12 | loss: 0.44665 | val_0_rmse: 0.66657 | val_1_rmse: 0.64093 |  0:00:03s
epoch 13 | loss: 0.45122 | val_0_rmse: 0.66505 | val_1_rmse: 0.65838 |  0:00:04s
epoch 14 | loss: 0.44443 | val_0_rmse: 0.66376 | val_1_rmse: 0.64334 |  0:00:04s
epoch 15 | loss: 0.43752 | val_0_rmse: 0.68941 | val_1_rmse: 0.68004 |  0:00:04s
epoch 16 | loss: 0.43319 | val_0_rmse: 0.65715 | val_1_rmse: 0.64918 |  0:00:04s
epoch 17 | loss: 0.42765 | val_0_rmse: 0.70317 | val_1_rmse: 0.69783 |  0:00:05s
epoch 18 | loss: 0.42666 | val_0_rmse: 0.66126 | val_1_rmse: 0.64682 |  0:00:05s
epoch 19 | loss: 0.42374 | val_0_rmse: 0.68494 | val_1_rmse: 0.67983 |  0:00:05s
epoch 20 | loss: 0.42419 | val_0_rmse: 0.64623 | val_1_rmse: 0.62475 |  0:00:06s
epoch 21 | loss: 0.41558 | val_0_rmse: 0.63806 | val_1_rmse: 0.62917 |  0:00:06s
epoch 22 | loss: 0.39796 | val_0_rmse: 0.65415 | val_1_rmse: 0.63412 |  0:00:06s
epoch 23 | loss: 0.40454 | val_0_rmse: 0.65268 | val_1_rmse: 0.64647 |  0:00:07s
epoch 24 | loss: 0.38705 | val_0_rmse: 0.6252  | val_1_rmse: 0.61939 |  0:00:07s
epoch 25 | loss: 0.39524 | val_0_rmse: 0.61501 | val_1_rmse: 0.59995 |  0:00:07s
epoch 26 | loss: 0.39901 | val_0_rmse: 0.63105 | val_1_rmse: 0.61015 |  0:00:07s
epoch 27 | loss: 0.39069 | val_0_rmse: 0.61719 | val_1_rmse: 0.59522 |  0:00:08s
epoch 28 | loss: 0.37633 | val_0_rmse: 0.60779 | val_1_rmse: 0.59676 |  0:00:08s
epoch 29 | loss: 0.36618 | val_0_rmse: 0.59794 | val_1_rmse: 0.59265 |  0:00:08s
epoch 30 | loss: 0.37534 | val_0_rmse: 0.65492 | val_1_rmse: 0.65395 |  0:00:09s
epoch 31 | loss: 0.36658 | val_0_rmse: 0.60279 | val_1_rmse: 0.58636 |  0:00:09s
epoch 32 | loss: 0.36942 | val_0_rmse: 0.6275  | val_1_rmse: 0.61904 |  0:00:09s
epoch 33 | loss: 0.36444 | val_0_rmse: 0.62159 | val_1_rmse: 0.70547 |  0:00:09s
epoch 34 | loss: 0.38245 | val_0_rmse: 0.63566 | val_1_rmse: 0.63715 |  0:00:10s
epoch 35 | loss: 0.37289 | val_0_rmse: 0.6008  | val_1_rmse: 0.59528 |  0:00:10s
epoch 36 | loss: 0.37764 | val_0_rmse: 0.61217 | val_1_rmse: 0.61362 |  0:00:10s
epoch 37 | loss: 0.37191 | val_0_rmse: 0.62458 | val_1_rmse: 0.63243 |  0:00:11s
epoch 38 | loss: 0.3673  | val_0_rmse: 0.6212  | val_1_rmse: 0.62169 |  0:00:11s
epoch 39 | loss: 0.36505 | val_0_rmse: 0.63346 | val_1_rmse: 0.62304 |  0:00:11s
epoch 40 | loss: 0.36612 | val_0_rmse: 0.60767 | val_1_rmse: 0.613   |  0:00:12s
epoch 41 | loss: 0.35219 | val_0_rmse: 0.61612 | val_1_rmse: 0.62592 |  0:00:12s
epoch 42 | loss: 0.36208 | val_0_rmse: 0.58151 | val_1_rmse: 0.62065 |  0:00:12s
epoch 43 | loss: 0.34779 | val_0_rmse: 0.61208 | val_1_rmse: 0.6028  |  0:00:12s
epoch 44 | loss: 0.35507 | val_0_rmse: 0.57954 | val_1_rmse: 0.56461 |  0:00:13s
epoch 45 | loss: 0.34512 | val_0_rmse: 0.58912 | val_1_rmse: 0.58217 |  0:00:13s
epoch 46 | loss: 0.32754 | val_0_rmse: 0.57929 | val_1_rmse: 0.55844 |  0:00:13s
epoch 47 | loss: 0.34343 | val_0_rmse: 0.61065 | val_1_rmse: 0.60046 |  0:00:14s
epoch 48 | loss: 0.34113 | val_0_rmse: 0.57549 | val_1_rmse: 0.55386 |  0:00:14s
epoch 49 | loss: 0.32593 | val_0_rmse: 0.5784  | val_1_rmse: 0.56693 |  0:00:14s
epoch 50 | loss: 0.33342 | val_0_rmse: 0.5823  | val_1_rmse: 0.57207 |  0:00:14s
epoch 51 | loss: 0.3399  | val_0_rmse: 0.57614 | val_1_rmse: 0.57386 |  0:00:15s
epoch 52 | loss: 0.33516 | val_0_rmse: 0.56859 | val_1_rmse: 0.5559  |  0:00:15s
epoch 53 | loss: 0.33623 | val_0_rmse: 0.61882 | val_1_rmse: 0.6145  |  0:00:15s
epoch 54 | loss: 0.32277 | val_0_rmse: 0.60257 | val_1_rmse: 0.6006  |  0:00:16s
epoch 55 | loss: 0.32478 | val_0_rmse: 0.61027 | val_1_rmse: 0.6046  |  0:00:16s
epoch 56 | loss: 0.33417 | val_0_rmse: 0.61631 | val_1_rmse: 0.59387 |  0:00:16s
epoch 57 | loss: 0.33958 | val_0_rmse: 0.58517 | val_1_rmse: 0.56421 |  0:00:16s
epoch 58 | loss: 0.3289  | val_0_rmse: 0.58584 | val_1_rmse: 0.58386 |  0:00:17s
epoch 59 | loss: 0.31647 | val_0_rmse: 0.55731 | val_1_rmse: 0.55634 |  0:00:17s
epoch 60 | loss: 0.32638 | val_0_rmse: 0.56487 | val_1_rmse: 0.5662  |  0:00:17s
epoch 61 | loss: 0.32651 | val_0_rmse: 0.5917  | val_1_rmse: 0.60269 |  0:00:18s
epoch 62 | loss: 0.31515 | val_0_rmse: 0.58526 | val_1_rmse: 0.59223 |  0:00:18s
epoch 63 | loss: 0.32794 | val_0_rmse: 0.58638 | val_1_rmse: 0.5969  |  0:00:18s
epoch 64 | loss: 0.3098  | val_0_rmse: 0.56059 | val_1_rmse: 0.56067 |  0:00:18s
epoch 65 | loss: 0.31246 | val_0_rmse: 0.57871 | val_1_rmse: 0.58155 |  0:00:19s
epoch 66 | loss: 0.30649 | val_0_rmse: 0.56832 | val_1_rmse: 0.57219 |  0:00:19s
epoch 67 | loss: 0.31595 | val_0_rmse: 0.54178 | val_1_rmse: 0.54834 |  0:00:19s
epoch 68 | loss: 0.30792 | val_0_rmse: 0.55316 | val_1_rmse: 0.55283 |  0:00:20s
epoch 69 | loss: 0.31566 | val_0_rmse: 0.57191 | val_1_rmse: 0.56364 |  0:00:20s
epoch 70 | loss: 0.31904 | val_0_rmse: 0.57875 | val_1_rmse: 0.58575 |  0:00:20s
epoch 71 | loss: 0.32007 | val_0_rmse: 0.55392 | val_1_rmse: 0.55971 |  0:00:21s
epoch 72 | loss: 0.30598 | val_0_rmse: 0.60182 | val_1_rmse: 0.62265 |  0:00:21s
epoch 73 | loss: 0.30197 | val_0_rmse: 0.54153 | val_1_rmse: 0.55486 |  0:00:21s
epoch 74 | loss: 0.30988 | val_0_rmse: 0.62754 | val_1_rmse: 0.64593 |  0:00:21s
epoch 75 | loss: 0.31873 | val_0_rmse: 0.54325 | val_1_rmse: 0.55303 |  0:00:22s
epoch 76 | loss: 0.30521 | val_0_rmse: 0.55359 | val_1_rmse: 0.56994 |  0:00:22s
epoch 77 | loss: 0.30516 | val_0_rmse: 0.56628 | val_1_rmse: 0.57126 |  0:00:22s
epoch 78 | loss: 0.30775 | val_0_rmse: 0.62456 | val_1_rmse: 0.63891 |  0:00:23s
epoch 79 | loss: 0.31389 | val_0_rmse: 0.5679  | val_1_rmse: 0.57615 |  0:00:23s
epoch 80 | loss: 0.30545 | val_0_rmse: 0.57086 | val_1_rmse: 0.58177 |  0:00:23s
epoch 81 | loss: 0.30305 | val_0_rmse: 0.55189 | val_1_rmse: 0.55664 |  0:00:23s
epoch 82 | loss: 0.29773 | val_0_rmse: 0.54463 | val_1_rmse: 0.5438  |  0:00:24s
epoch 83 | loss: 0.29378 | val_0_rmse: 0.53351 | val_1_rmse: 0.54141 |  0:00:24s
epoch 84 | loss: 0.2928  | val_0_rmse: 0.54799 | val_1_rmse: 0.55247 |  0:00:24s
epoch 85 | loss: 0.30735 | val_0_rmse: 0.56873 | val_1_rmse: 0.57163 |  0:00:25s
epoch 86 | loss: 0.30866 | val_0_rmse: 0.58452 | val_1_rmse: 0.58253 |  0:00:25s
epoch 87 | loss: 0.29968 | val_0_rmse: 0.57141 | val_1_rmse: 0.57183 |  0:00:25s
epoch 88 | loss: 0.29619 | val_0_rmse: 0.57437 | val_1_rmse: 0.58376 |  0:00:25s
epoch 89 | loss: 0.30132 | val_0_rmse: 0.56695 | val_1_rmse: 0.58271 |  0:00:26s
epoch 90 | loss: 0.30105 | val_0_rmse: 0.54123 | val_1_rmse: 0.5615  |  0:00:26s
epoch 91 | loss: 0.29814 | val_0_rmse: 0.55522 | val_1_rmse: 0.56288 |  0:00:26s
epoch 92 | loss: 0.29486 | val_0_rmse: 0.55089 | val_1_rmse: 0.55531 |  0:00:27s
epoch 93 | loss: 0.28818 | val_0_rmse: 0.52355 | val_1_rmse: 0.53529 |  0:00:27s
epoch 94 | loss: 0.29827 | val_0_rmse: 0.52359 | val_1_rmse: 0.52986 |  0:00:27s
epoch 95 | loss: 0.29359 | val_0_rmse: 0.54544 | val_1_rmse: 0.54718 |  0:00:28s
epoch 96 | loss: 0.28891 | val_0_rmse: 0.53988 | val_1_rmse: 0.54884 |  0:00:28s
epoch 97 | loss: 0.29618 | val_0_rmse: 0.57737 | val_1_rmse: 0.59818 |  0:00:28s
epoch 98 | loss: 0.29466 | val_0_rmse: 0.54108 | val_1_rmse: 0.54943 |  0:00:28s
epoch 99 | loss: 0.28823 | val_0_rmse: 0.55385 | val_1_rmse: 0.56615 |  0:00:29s
epoch 100| loss: 0.28968 | val_0_rmse: 0.53884 | val_1_rmse: 0.54622 |  0:00:29s
epoch 101| loss: 0.29092 | val_0_rmse: 0.54311 | val_1_rmse: 0.55395 |  0:00:29s
epoch 102| loss: 0.29623 | val_0_rmse: 0.57952 | val_1_rmse: 0.60364 |  0:00:29s
epoch 103| loss: 0.30495 | val_0_rmse: 0.53873 | val_1_rmse: 0.55399 |  0:00:30s
epoch 104| loss: 0.30706 | val_0_rmse: 0.57296 | val_1_rmse: 0.58491 |  0:00:30s
epoch 105| loss: 0.29659 | val_0_rmse: 0.54084 | val_1_rmse: 0.55095 |  0:00:30s
epoch 106| loss: 0.29254 | val_0_rmse: 0.58826 | val_1_rmse: 0.60462 |  0:00:31s
epoch 107| loss: 0.28943 | val_0_rmse: 0.52269 | val_1_rmse: 0.55063 |  0:00:31s
epoch 108| loss: 0.28129 | val_0_rmse: 0.52866 | val_1_rmse: 0.54896 |  0:00:31s
epoch 109| loss: 0.28248 | val_0_rmse: 0.53279 | val_1_rmse: 0.54273 |  0:00:32s
epoch 110| loss: 0.28047 | val_0_rmse: 0.55681 | val_1_rmse: 0.56909 |  0:00:32s
epoch 111| loss: 0.28574 | val_0_rmse: 0.52705 | val_1_rmse: 0.53687 |  0:00:32s
epoch 112| loss: 0.29349 | val_0_rmse: 0.52308 | val_1_rmse: 0.53858 |  0:00:32s
epoch 113| loss: 0.28717 | val_0_rmse: 0.529   | val_1_rmse: 0.54791 |  0:00:33s
epoch 114| loss: 0.28364 | val_0_rmse: 0.53097 | val_1_rmse: 0.54989 |  0:00:33s
epoch 115| loss: 0.27696 | val_0_rmse: 0.58504 | val_1_rmse: 0.59609 |  0:00:33s
epoch 116| loss: 0.28209 | val_0_rmse: 0.53177 | val_1_rmse: 0.55129 |  0:00:34s
epoch 117| loss: 0.28567 | val_0_rmse: 0.55707 | val_1_rmse: 0.58495 |  0:00:34s
epoch 118| loss: 0.29517 | val_0_rmse: 0.5821  | val_1_rmse: 0.59847 |  0:00:34s
epoch 119| loss: 0.28523 | val_0_rmse: 0.61498 | val_1_rmse: 0.64192 |  0:00:34s
epoch 120| loss: 0.28148 | val_0_rmse: 0.55579 | val_1_rmse: 0.56719 |  0:00:35s
epoch 121| loss: 0.28619 | val_0_rmse: 0.56706 | val_1_rmse: 0.57517 |  0:00:35s
epoch 122| loss: 0.29332 | val_0_rmse: 0.5566  | val_1_rmse: 0.56745 |  0:00:35s
epoch 123| loss: 0.28995 | val_0_rmse: 0.53942 | val_1_rmse: 0.55894 |  0:00:36s
epoch 124| loss: 0.28657 | val_0_rmse: 0.52796 | val_1_rmse: 0.54339 |  0:00:36s

Early stopping occured at epoch 124 with best_epoch = 94 and best_val_1_rmse = 0.52986
Best weights from best epoch are automatically used!
ended training at: 01:48:51
Feature importance:
[('Area', 0.26597520528070334), ('Baths', 0.10964694552148276), ('Beds', 0.04673662381864187), ('Latitude', 0.23235698788127335), ('Longitude', 0.3045090461040474), ('Month', 8.375218183471588e-05), ('Year', 0.04069143921201661)]
Mean squared error is of 2893875638.21333
Mean absolute error:35360.6911101841
MAPE:0.275926917759989
R2 score:0.6694087047887789
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:48:52
epoch 0  | loss: 0.41218 | val_0_rmse: 0.58014 | val_1_rmse: 0.57817 |  0:00:03s
epoch 1  | loss: 0.33802 | val_0_rmse: 0.57712 | val_1_rmse: 0.57916 |  0:00:07s
epoch 2  | loss: 0.33754 | val_0_rmse: 0.56961 | val_1_rmse: 0.57018 |  0:00:11s
epoch 3  | loss: 0.33516 | val_0_rmse: 0.57024 | val_1_rmse: 0.57062 |  0:00:15s
epoch 4  | loss: 0.33353 | val_0_rmse: 0.5645  | val_1_rmse: 0.56387 |  0:00:19s
epoch 5  | loss: 0.32863 | val_0_rmse: 0.5681  | val_1_rmse: 0.56868 |  0:00:23s
epoch 6  | loss: 0.33161 | val_0_rmse: 0.5684  | val_1_rmse: 0.56677 |  0:00:26s
epoch 7  | loss: 0.32521 | val_0_rmse: 0.56206 | val_1_rmse: 0.56185 |  0:00:30s
epoch 8  | loss: 0.32425 | val_0_rmse: 0.55931 | val_1_rmse: 0.55696 |  0:00:34s
epoch 9  | loss: 0.32334 | val_0_rmse: 0.56157 | val_1_rmse: 0.56263 |  0:00:38s
epoch 10 | loss: 0.31935 | val_0_rmse: 0.55984 | val_1_rmse: 0.55881 |  0:00:42s
epoch 11 | loss: 0.31794 | val_0_rmse: 0.61853 | val_1_rmse: 0.57187 |  0:00:45s
epoch 12 | loss: 0.31809 | val_0_rmse: 0.59229 | val_1_rmse: 0.57718 |  0:00:49s
epoch 13 | loss: 0.34243 | val_0_rmse: 0.58608 | val_1_rmse: 0.5875  |  0:00:53s
epoch 14 | loss: 0.33072 | val_0_rmse: 0.5636  | val_1_rmse: 0.56097 |  0:00:57s
epoch 15 | loss: 0.32692 | val_0_rmse: 0.5771  | val_1_rmse: 0.5769  |  0:01:01s
epoch 16 | loss: 0.32543 | val_0_rmse: 0.57338 | val_1_rmse: 0.56328 |  0:01:05s
epoch 17 | loss: 0.31715 | val_0_rmse: 0.56802 | val_1_rmse: 0.56321 |  0:01:08s
epoch 18 | loss: 0.31426 | val_0_rmse: 0.75048 | val_1_rmse: 0.59621 |  0:01:12s
epoch 19 | loss: 0.31008 | val_0_rmse: 0.6009  | val_1_rmse: 0.59343 |  0:01:16s
epoch 20 | loss: 0.31195 | val_0_rmse: 0.57453 | val_1_rmse: 0.57484 |  0:01:20s
epoch 21 | loss: 0.30471 | val_0_rmse: 0.55753 | val_1_rmse: 0.55876 |  0:01:24s
epoch 22 | loss: 0.3056  | val_0_rmse: 0.56909 | val_1_rmse: 0.56873 |  0:01:27s
epoch 23 | loss: 0.29968 | val_0_rmse: 0.55279 | val_1_rmse: 0.55247 |  0:01:31s
epoch 24 | loss: 0.3018  | val_0_rmse: 0.58828 | val_1_rmse: 0.5896  |  0:01:35s
epoch 25 | loss: 0.3001  | val_0_rmse: 0.57556 | val_1_rmse: 0.5761  |  0:01:39s
epoch 26 | loss: 0.29931 | val_0_rmse: 0.63033 | val_1_rmse: 0.6256  |  0:01:43s
epoch 27 | loss: 0.29885 | val_0_rmse: 0.57174 | val_1_rmse: 0.57354 |  0:01:46s
epoch 28 | loss: 0.29812 | val_0_rmse: 0.54527 | val_1_rmse: 0.54582 |  0:01:50s
epoch 29 | loss: 0.2978  | val_0_rmse: 0.56918 | val_1_rmse: 0.57134 |  0:01:54s
epoch 30 | loss: 0.30092 | val_0_rmse: 0.54576 | val_1_rmse: 0.54594 |  0:01:58s
epoch 31 | loss: 0.29824 | val_0_rmse: 0.58432 | val_1_rmse: 0.58597 |  0:02:02s
epoch 32 | loss: 0.29561 | val_0_rmse: 0.57379 | val_1_rmse: 0.57586 |  0:02:05s
epoch 33 | loss: 0.29434 | val_0_rmse: 0.54568 | val_1_rmse: 0.54527 |  0:02:09s
epoch 34 | loss: 0.29581 | val_0_rmse: 0.55065 | val_1_rmse: 0.55005 |  0:02:13s
epoch 35 | loss: 0.29549 | val_0_rmse: 0.54297 | val_1_rmse: 0.54101 |  0:02:17s
epoch 36 | loss: 0.29483 | val_0_rmse: 0.55576 | val_1_rmse: 0.55031 |  0:02:21s
epoch 37 | loss: 0.29315 | val_0_rmse: 0.565   | val_1_rmse: 0.56654 |  0:02:25s
epoch 38 | loss: 0.29527 | val_0_rmse: 0.55305 | val_1_rmse: 0.55435 |  0:02:28s
epoch 39 | loss: 0.29465 | val_0_rmse: 0.55705 | val_1_rmse: 0.55693 |  0:02:32s
epoch 40 | loss: 0.29249 | val_0_rmse: 0.55103 | val_1_rmse: 0.55091 |  0:02:36s
epoch 41 | loss: 0.29173 | val_0_rmse: 0.55173 | val_1_rmse: 0.55111 |  0:02:40s
epoch 42 | loss: 0.29385 | val_0_rmse: 0.58525 | val_1_rmse: 0.58069 |  0:02:44s
epoch 43 | loss: 0.29199 | val_0_rmse: 0.60492 | val_1_rmse: 0.60854 |  0:02:48s
epoch 44 | loss: 0.28945 | val_0_rmse: 0.5537  | val_1_rmse: 0.55354 |  0:02:51s
epoch 45 | loss: 0.29276 | val_0_rmse: 0.61061 | val_1_rmse: 0.61172 |  0:02:55s
epoch 46 | loss: 0.29291 | val_0_rmse: 0.54856 | val_1_rmse: 0.54789 |  0:02:59s
epoch 47 | loss: 0.2891  | val_0_rmse: 0.58625 | val_1_rmse: 0.58832 |  0:03:03s
epoch 48 | loss: 0.29131 | val_0_rmse: 0.54564 | val_1_rmse: 0.54302 |  0:03:07s
epoch 49 | loss: 0.29148 | val_0_rmse: 0.78918 | val_1_rmse: 0.78338 |  0:03:10s
epoch 50 | loss: 0.29692 | val_0_rmse: 0.71693 | val_1_rmse: 0.71852 |  0:03:14s
epoch 51 | loss: 0.29245 | val_0_rmse: 0.62056 | val_1_rmse: 0.62175 |  0:03:18s
epoch 52 | loss: 0.29078 | val_0_rmse: 0.56142 | val_1_rmse: 0.56285 |  0:03:22s
epoch 53 | loss: 0.29089 | val_0_rmse: 0.55914 | val_1_rmse: 0.55744 |  0:03:26s
epoch 54 | loss: 0.29153 | val_0_rmse: 0.55846 | val_1_rmse: 0.55744 |  0:03:30s
epoch 55 | loss: 0.28901 | val_0_rmse: 0.56553 | val_1_rmse: 0.56699 |  0:03:33s
epoch 56 | loss: 0.29031 | val_0_rmse: 0.56947 | val_1_rmse: 0.56434 |  0:03:37s
epoch 57 | loss: 0.28979 | val_0_rmse: 0.58693 | val_1_rmse: 0.58574 |  0:03:41s
epoch 58 | loss: 0.28943 | val_0_rmse: 0.55704 | val_1_rmse: 0.55684 |  0:03:45s
epoch 59 | loss: 0.28877 | val_0_rmse: 0.59771 | val_1_rmse: 0.60059 |  0:03:48s
epoch 60 | loss: 0.28937 | val_0_rmse: 0.56343 | val_1_rmse: 0.56351 |  0:03:52s
epoch 61 | loss: 0.29119 | val_0_rmse: 0.58518 | val_1_rmse: 0.58493 |  0:03:56s
epoch 62 | loss: 0.29249 | val_0_rmse: 0.55274 | val_1_rmse: 0.55217 |  0:04:00s
epoch 63 | loss: 0.28826 | val_0_rmse: 0.5339  | val_1_rmse: 0.5327  |  0:04:04s
epoch 64 | loss: 0.28991 | val_0_rmse: 0.59329 | val_1_rmse: 0.59327 |  0:04:08s
epoch 65 | loss: 0.29758 | val_0_rmse: 0.56619 | val_1_rmse: 0.56125 |  0:04:11s
epoch 66 | loss: 0.29918 | val_0_rmse: 0.56484 | val_1_rmse: 0.56257 |  0:04:15s
epoch 67 | loss: 0.2995  | val_0_rmse: 0.64347 | val_1_rmse: 0.63662 |  0:04:19s
epoch 68 | loss: 0.29777 | val_0_rmse: 0.65599 | val_1_rmse: 0.65658 |  0:04:23s
epoch 69 | loss: 0.29609 | val_0_rmse: 0.56746 | val_1_rmse: 0.56634 |  0:04:27s
epoch 70 | loss: 0.29516 | val_0_rmse: 0.55566 | val_1_rmse: 0.55062 |  0:04:30s
epoch 71 | loss: 0.29827 | val_0_rmse: 0.58538 | val_1_rmse: 0.58414 |  0:04:34s
epoch 72 | loss: 0.29722 | val_0_rmse: 0.54138 | val_1_rmse: 0.53785 |  0:04:38s
epoch 73 | loss: 0.29902 | val_0_rmse: 0.78668 | val_1_rmse: 0.78683 |  0:04:42s
epoch 74 | loss: 0.29788 | val_0_rmse: 0.60489 | val_1_rmse: 0.60499 |  0:04:46s
epoch 75 | loss: 0.29473 | val_0_rmse: 0.54382 | val_1_rmse: 0.54029 |  0:04:49s
epoch 76 | loss: 0.2949  | val_0_rmse: 0.55592 | val_1_rmse: 0.55394 |  0:04:53s
epoch 77 | loss: 0.29729 | val_0_rmse: 0.54267 | val_1_rmse: 0.53882 |  0:04:57s
epoch 78 | loss: 0.29593 | val_0_rmse: 0.56761 | val_1_rmse: 0.56275 |  0:05:01s
epoch 79 | loss: 0.29587 | val_0_rmse: 0.55259 | val_1_rmse: 0.54857 |  0:05:05s
epoch 80 | loss: 0.29274 | val_0_rmse: 0.55939 | val_1_rmse: 0.55843 |  0:05:08s
epoch 81 | loss: 0.29307 | val_0_rmse: 0.56247 | val_1_rmse: 0.55637 |  0:05:12s
epoch 82 | loss: 0.29415 | val_0_rmse: 0.57229 | val_1_rmse: 0.5681  |  0:05:16s
epoch 83 | loss: 0.2953  | val_0_rmse: 0.6389  | val_1_rmse: 0.63863 |  0:05:20s
epoch 84 | loss: 0.29493 | val_0_rmse: 0.54537 | val_1_rmse: 0.54265 |  0:05:24s
epoch 85 | loss: 0.29698 | val_0_rmse: 0.61727 | val_1_rmse: 0.60888 |  0:05:27s
epoch 86 | loss: 0.2986  | val_0_rmse: 0.60146 | val_1_rmse: 0.60186 |  0:05:31s
epoch 87 | loss: 0.29637 | val_0_rmse: 0.54494 | val_1_rmse: 0.54104 |  0:05:35s
epoch 88 | loss: 0.29565 | val_0_rmse: 0.539   | val_1_rmse: 0.53764 |  0:05:39s
epoch 89 | loss: 0.29866 | val_0_rmse: 0.56876 | val_1_rmse: 0.56695 |  0:05:43s
epoch 90 | loss: 0.31253 | val_0_rmse: 0.55625 | val_1_rmse: 0.55518 |  0:05:46s
epoch 91 | loss: 0.30864 | val_0_rmse: 0.55224 | val_1_rmse: 0.55021 |  0:05:50s
epoch 92 | loss: 0.30193 | val_0_rmse: 0.56499 | val_1_rmse: 0.56324 |  0:05:54s
epoch 93 | loss: 0.30268 | val_0_rmse: 0.55966 | val_1_rmse: 0.55838 |  0:05:58s

Early stopping occured at epoch 93 with best_epoch = 63 and best_val_1_rmse = 0.5327
Best weights from best epoch are automatically used!
ended training at: 01:54:51
Feature importance:
[('Area', 0.4498356377106751), ('Baths', 0.10490184224438068), ('Beds', 0.06850900234959034), ('Latitude', 0.19615060126519118), ('Longitude', 0.18060291643016269), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 948363054.0446224
Mean absolute error:21558.318209268138
MAPE:0.3555923650706248
R2 score:0.7200622952391686
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:54:52
epoch 0  | loss: 0.42093 | val_0_rmse: 0.57139 | val_1_rmse: 0.58356 |  0:00:03s
epoch 1  | loss: 0.33217 | val_0_rmse: 0.56427 | val_1_rmse: 0.57716 |  0:00:07s
epoch 2  | loss: 0.32994 | val_0_rmse: 0.56323 | val_1_rmse: 0.57423 |  0:00:11s
epoch 3  | loss: 0.32988 | val_0_rmse: 0.5691  | val_1_rmse: 0.57965 |  0:00:15s
epoch 4  | loss: 0.32765 | val_0_rmse: 0.5737  | val_1_rmse: 0.58818 |  0:00:19s
epoch 5  | loss: 0.32548 | val_0_rmse: 0.56112 | val_1_rmse: 0.57428 |  0:00:22s
epoch 6  | loss: 0.32347 | val_0_rmse: 0.55901 | val_1_rmse: 0.57131 |  0:00:26s
epoch 7  | loss: 0.32219 | val_0_rmse: 0.55899 | val_1_rmse: 0.57269 |  0:00:30s
epoch 8  | loss: 0.32129 | val_0_rmse: 0.55863 | val_1_rmse: 0.57215 |  0:00:34s
epoch 9  | loss: 0.31883 | val_0_rmse: 0.55964 | val_1_rmse: 0.57106 |  0:00:38s
epoch 10 | loss: 0.31756 | val_0_rmse: 0.55991 | val_1_rmse: 0.57343 |  0:00:41s
epoch 11 | loss: 0.31974 | val_0_rmse: 0.56423 | val_1_rmse: 0.57636 |  0:00:45s
epoch 12 | loss: 0.31795 | val_0_rmse: 0.56252 | val_1_rmse: 0.57121 |  0:00:49s
epoch 13 | loss: 0.31843 | val_0_rmse: 0.55806 | val_1_rmse: 0.5695  |  0:00:53s
epoch 14 | loss: 0.3158  | val_0_rmse: 0.55998 | val_1_rmse: 0.57135 |  0:00:57s
epoch 15 | loss: 0.31632 | val_0_rmse: 0.55745 | val_1_rmse: 0.5693  |  0:01:00s
epoch 16 | loss: 0.31339 | val_0_rmse: 0.55853 | val_1_rmse: 0.56969 |  0:01:04s
epoch 17 | loss: 0.31455 | val_0_rmse: 0.56932 | val_1_rmse: 0.58047 |  0:01:08s
epoch 18 | loss: 0.31372 | val_0_rmse: 0.55863 | val_1_rmse: 0.57025 |  0:01:12s
epoch 19 | loss: 0.31354 | val_0_rmse: 0.55832 | val_1_rmse: 0.57295 |  0:01:16s
epoch 20 | loss: 0.30798 | val_0_rmse: 0.57959 | val_1_rmse: 0.59629 |  0:01:19s
epoch 21 | loss: 0.30858 | val_0_rmse: 0.55898 | val_1_rmse: 0.5689  |  0:01:23s
epoch 22 | loss: 0.30546 | val_0_rmse: 0.57236 | val_1_rmse: 0.58806 |  0:01:27s
epoch 23 | loss: 0.30439 | val_0_rmse: 0.58557 | val_1_rmse: 0.60197 |  0:01:31s
epoch 24 | loss: 0.30296 | val_0_rmse: 0.60172 | val_1_rmse: 0.61737 |  0:01:35s
epoch 25 | loss: 0.30142 | val_0_rmse: 0.54757 | val_1_rmse: 0.55887 |  0:01:38s
epoch 26 | loss: 0.30458 | val_0_rmse: 0.6349  | val_1_rmse: 0.64094 |  0:01:42s
epoch 27 | loss: 0.30242 | val_0_rmse: 0.54458 | val_1_rmse: 0.55813 |  0:01:46s
epoch 28 | loss: 0.30307 | val_0_rmse: 0.57052 | val_1_rmse: 0.58398 |  0:01:50s
epoch 29 | loss: 0.30229 | val_0_rmse: 0.54562 | val_1_rmse: 0.55922 |  0:01:54s
epoch 30 | loss: 0.30216 | val_0_rmse: 0.59493 | val_1_rmse: 0.60973 |  0:01:57s
epoch 31 | loss: 0.30092 | val_0_rmse: 0.56482 | val_1_rmse: 0.56792 |  0:02:01s
epoch 32 | loss: 0.29866 | val_0_rmse: 0.56192 | val_1_rmse: 0.55657 |  0:02:05s
epoch 33 | loss: 0.29844 | val_0_rmse: 0.59422 | val_1_rmse: 0.60259 |  0:02:09s
epoch 34 | loss: 0.30161 | val_0_rmse: 0.59786 | val_1_rmse: 0.60621 |  0:02:13s
epoch 35 | loss: 0.30095 | val_0_rmse: 0.57576 | val_1_rmse: 0.58947 |  0:02:16s
epoch 36 | loss: 0.30081 | val_0_rmse: 0.60707 | val_1_rmse: 0.55971 |  0:02:20s
epoch 37 | loss: 0.30037 | val_0_rmse: 0.60921 | val_1_rmse: 0.6089  |  0:02:24s
epoch 38 | loss: 0.29806 | val_0_rmse: 0.59157 | val_1_rmse: 0.56853 |  0:02:28s
epoch 39 | loss: 0.29962 | val_0_rmse: 0.55293 | val_1_rmse: 0.5613  |  0:02:32s
epoch 40 | loss: 0.29706 | val_0_rmse: 0.55855 | val_1_rmse: 0.54999 |  0:02:35s
epoch 41 | loss: 0.29896 | val_0_rmse: 0.55964 | val_1_rmse: 0.55287 |  0:02:39s
epoch 42 | loss: 0.29875 | val_0_rmse: 0.57627 | val_1_rmse: 0.57316 |  0:02:43s
epoch 43 | loss: 0.29929 | val_0_rmse: 0.58296 | val_1_rmse: 0.59063 |  0:02:47s
epoch 44 | loss: 0.2997  | val_0_rmse: 0.61936 | val_1_rmse: 0.58096 |  0:02:51s
epoch 45 | loss: 0.29685 | val_0_rmse: 0.56581 | val_1_rmse: 0.55972 |  0:02:55s
epoch 46 | loss: 0.33297 | val_0_rmse: 0.59414 | val_1_rmse: 0.61166 |  0:02:58s
epoch 47 | loss: 0.34785 | val_0_rmse: 0.58678 | val_1_rmse: 0.60276 |  0:03:02s
epoch 48 | loss: 0.33775 | val_0_rmse: 0.57258 | val_1_rmse: 0.58727 |  0:03:06s
epoch 49 | loss: 0.33167 | val_0_rmse: 0.57229 | val_1_rmse: 0.58651 |  0:03:10s
epoch 50 | loss: 0.32896 | val_0_rmse: 0.57339 | val_1_rmse: 0.58818 |  0:03:14s
epoch 51 | loss: 0.33056 | val_0_rmse: 0.57189 | val_1_rmse: 0.58754 |  0:03:17s
epoch 52 | loss: 0.3273  | val_0_rmse: 0.57043 | val_1_rmse: 0.58569 |  0:03:21s
epoch 53 | loss: 0.32609 | val_0_rmse: 0.56693 | val_1_rmse: 0.58094 |  0:03:25s
epoch 54 | loss: 0.32498 | val_0_rmse: 0.56606 | val_1_rmse: 0.57948 |  0:03:29s
epoch 55 | loss: 0.32505 | val_0_rmse: 0.56845 | val_1_rmse: 0.58287 |  0:03:33s
epoch 56 | loss: 0.32442 | val_0_rmse: 0.56718 | val_1_rmse: 0.58109 |  0:03:36s
epoch 57 | loss: 0.32603 | val_0_rmse: 0.57289 | val_1_rmse: 0.58427 |  0:03:40s
epoch 58 | loss: 0.32402 | val_0_rmse: 0.57021 | val_1_rmse: 0.58514 |  0:03:44s
epoch 59 | loss: 0.3234  | val_0_rmse: 0.56214 | val_1_rmse: 0.57611 |  0:03:48s
epoch 60 | loss: 0.31882 | val_0_rmse: 0.58956 | val_1_rmse: 0.60023 |  0:03:52s
epoch 61 | loss: 0.3177  | val_0_rmse: 0.5835  | val_1_rmse: 0.596   |  0:03:55s
epoch 62 | loss: 0.31071 | val_0_rmse: 0.5617  | val_1_rmse: 0.57709 |  0:03:59s
epoch 63 | loss: 0.30892 | val_0_rmse: 0.58895 | val_1_rmse: 0.60168 |  0:04:03s
epoch 64 | loss: 0.30816 | val_0_rmse: 0.57016 | val_1_rmse: 0.58726 |  0:04:07s
epoch 65 | loss: 0.30754 | val_0_rmse: 0.58234 | val_1_rmse: 0.59843 |  0:04:10s
epoch 66 | loss: 0.30115 | val_0_rmse: 0.63668 | val_1_rmse: 0.64862 |  0:04:14s
epoch 67 | loss: 0.30365 | val_0_rmse: 0.60017 | val_1_rmse: 0.61504 |  0:04:18s
epoch 68 | loss: 0.29598 | val_0_rmse: 0.56289 | val_1_rmse: 0.5749  |  0:04:22s
epoch 69 | loss: 0.28861 | val_0_rmse: 0.67016 | val_1_rmse: 0.68647 |  0:04:26s
epoch 70 | loss: 0.2883  | val_0_rmse: 0.62463 | val_1_rmse: 0.64004 |  0:04:29s

Early stopping occured at epoch 70 with best_epoch = 40 and best_val_1_rmse = 0.54999
Best weights from best epoch are automatically used!
ended training at: 01:59:23
Feature importance:
[('Area', 0.4616646656778677), ('Baths', 0.0706958251695832), ('Beds', 0.14025885578518724), ('Latitude', 0.05101180136987522), ('Longitude', 0.14909525112353234), ('Month', 0.12242331132794108), ('Year', 0.004850289546013225)]
Mean squared error is of 968729014.8747872
Mean absolute error:21437.17283760647
MAPE:0.3358576109535623
R2 score:0.7107127991290693
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:59:24
epoch 0  | loss: 0.43466 | val_0_rmse: 0.5903  | val_1_rmse: 0.58718 |  0:00:03s
epoch 1  | loss: 0.34267 | val_0_rmse: 0.57984 | val_1_rmse: 0.57692 |  0:00:07s
epoch 2  | loss: 0.33914 | val_0_rmse: 0.57133 | val_1_rmse: 0.5674  |  0:00:11s
epoch 3  | loss: 0.33481 | val_0_rmse: 0.56356 | val_1_rmse: 0.55863 |  0:00:15s
epoch 4  | loss: 0.32729 | val_0_rmse: 0.57224 | val_1_rmse: 0.56572 |  0:00:19s
epoch 5  | loss: 0.32724 | val_0_rmse: 0.561   | val_1_rmse: 0.55756 |  0:00:22s
epoch 6  | loss: 0.32302 | val_0_rmse: 0.56165 | val_1_rmse: 0.55705 |  0:00:26s
epoch 7  | loss: 0.32365 | val_0_rmse: 0.62992 | val_1_rmse: 0.62533 |  0:00:30s
epoch 8  | loss: 0.32695 | val_0_rmse: 0.55887 | val_1_rmse: 0.55402 |  0:00:34s
epoch 9  | loss: 0.31663 | val_0_rmse: 0.58653 | val_1_rmse: 0.58535 |  0:00:38s
epoch 10 | loss: 0.31884 | val_0_rmse: 0.60515 | val_1_rmse: 0.60045 |  0:00:41s
epoch 11 | loss: 0.31348 | val_0_rmse: 0.58078 | val_1_rmse: 0.57996 |  0:00:45s
epoch 12 | loss: 0.30319 | val_0_rmse: 0.57682 | val_1_rmse: 0.57589 |  0:00:49s
epoch 13 | loss: 0.30038 | val_0_rmse: 0.54823 | val_1_rmse: 0.544   |  0:00:53s
epoch 14 | loss: 0.29904 | val_0_rmse: 0.53936 | val_1_rmse: 0.53107 |  0:00:56s
epoch 15 | loss: 0.29627 | val_0_rmse: 0.54078 | val_1_rmse: 0.53731 |  0:01:00s
epoch 16 | loss: 0.29631 | val_0_rmse: 0.60364 | val_1_rmse: 0.59553 |  0:01:04s
epoch 17 | loss: 0.29583 | val_0_rmse: 0.60792 | val_1_rmse: 0.60858 |  0:01:08s
epoch 18 | loss: 0.2939  | val_0_rmse: 0.6682  | val_1_rmse: 0.65994 |  0:01:12s
epoch 19 | loss: 0.29459 | val_0_rmse: 0.74901 | val_1_rmse: 0.74593 |  0:01:15s
epoch 20 | loss: 0.29292 | val_0_rmse: 0.55131 | val_1_rmse: 0.54611 |  0:01:19s
epoch 21 | loss: 0.29456 | val_0_rmse: 0.55877 | val_1_rmse: 0.55728 |  0:01:23s
epoch 22 | loss: 0.2974  | val_0_rmse: 0.54385 | val_1_rmse: 0.54263 |  0:01:27s
epoch 23 | loss: 0.29232 | val_0_rmse: 0.54214 | val_1_rmse: 0.53851 |  0:01:31s
epoch 24 | loss: 0.28984 | val_0_rmse: 0.53023 | val_1_rmse: 0.52702 |  0:01:34s
epoch 25 | loss: 0.29158 | val_0_rmse: 0.6245  | val_1_rmse: 0.62967 |  0:01:38s
epoch 26 | loss: 0.29205 | val_0_rmse: 0.75757 | val_1_rmse: 0.75506 |  0:01:42s
epoch 27 | loss: 0.28876 | val_0_rmse: 0.59087 | val_1_rmse: 0.59453 |  0:01:46s
epoch 28 | loss: 0.28772 | val_0_rmse: 0.60382 | val_1_rmse: 0.60023 |  0:01:49s
epoch 29 | loss: 0.28868 | val_0_rmse: 0.52775 | val_1_rmse: 0.52226 |  0:01:53s
epoch 30 | loss: 0.28789 | val_0_rmse: 0.52961 | val_1_rmse: 0.52676 |  0:01:57s
epoch 31 | loss: 0.2895  | val_0_rmse: 0.53005 | val_1_rmse: 0.52376 |  0:02:01s
epoch 32 | loss: 0.28537 | val_0_rmse: 1.25649 | val_1_rmse: 1.25777 |  0:02:05s
epoch 33 | loss: 0.28572 | val_0_rmse: 0.54159 | val_1_rmse: 0.53384 |  0:02:09s
epoch 34 | loss: 0.28552 | val_0_rmse: 0.5401  | val_1_rmse: 0.53731 |  0:02:12s
epoch 35 | loss: 0.28585 | val_0_rmse: 0.60627 | val_1_rmse: 0.60859 |  0:02:16s
epoch 36 | loss: 0.28282 | val_0_rmse: 0.52694 | val_1_rmse: 0.52193 |  0:02:20s
epoch 37 | loss: 0.27985 | val_0_rmse: 0.55099 | val_1_rmse: 0.54954 |  0:02:24s
epoch 38 | loss: 0.27731 | val_0_rmse: 0.66047 | val_1_rmse: 0.66596 |  0:02:28s
epoch 39 | loss: 0.28013 | val_0_rmse: 0.60455 | val_1_rmse: 0.60902 |  0:02:31s
epoch 40 | loss: 0.27849 | val_0_rmse: 0.60873 | val_1_rmse: 0.60905 |  0:02:35s
epoch 41 | loss: 0.28469 | val_0_rmse: 0.52447 | val_1_rmse: 0.51878 |  0:02:39s
epoch 42 | loss: 0.28228 | val_0_rmse: 1.45239 | val_1_rmse: 1.45231 |  0:02:43s
epoch 43 | loss: 0.28418 | val_0_rmse: 1.98357 | val_1_rmse: 1.99454 |  0:02:46s
epoch 44 | loss: 0.28112 | val_0_rmse: 0.57953 | val_1_rmse: 0.58234 |  0:02:50s
epoch 45 | loss: 0.27919 | val_0_rmse: 0.89392 | val_1_rmse: 0.88927 |  0:02:54s
epoch 46 | loss: 0.276   | val_0_rmse: 0.54243 | val_1_rmse: 0.54313 |  0:02:58s
epoch 47 | loss: 0.27657 | val_0_rmse: 0.53021 | val_1_rmse: 0.5282  |  0:03:02s
epoch 48 | loss: 0.27325 | val_0_rmse: 0.51889 | val_1_rmse: 0.51362 |  0:03:06s
epoch 49 | loss: 0.27454 | val_0_rmse: 0.5216  | val_1_rmse: 0.51835 |  0:03:09s
epoch 50 | loss: 0.27405 | val_0_rmse: 0.61481 | val_1_rmse: 0.61814 |  0:03:13s
epoch 51 | loss: 0.2734  | val_0_rmse: 0.51715 | val_1_rmse: 0.51261 |  0:03:17s
epoch 52 | loss: 0.27265 | val_0_rmse: 0.51606 | val_1_rmse: 0.51415 |  0:03:21s
epoch 53 | loss: 0.27139 | val_0_rmse: 0.65478 | val_1_rmse: 0.65868 |  0:03:24s
epoch 54 | loss: 0.27146 | val_0_rmse: 0.53502 | val_1_rmse: 0.53482 |  0:03:28s
epoch 55 | loss: 0.27052 | val_0_rmse: 0.53859 | val_1_rmse: 0.53511 |  0:03:32s
epoch 56 | loss: 0.27026 | val_0_rmse: 0.53648 | val_1_rmse: 0.53489 |  0:03:36s
epoch 57 | loss: 0.2747  | val_0_rmse: 0.51897 | val_1_rmse: 0.51258 |  0:03:40s
epoch 58 | loss: 0.27164 | val_0_rmse: 0.60911 | val_1_rmse: 0.61325 |  0:03:43s
epoch 59 | loss: 0.27323 | val_0_rmse: 0.69606 | val_1_rmse: 0.69871 |  0:03:47s
epoch 60 | loss: 0.27524 | val_0_rmse: 0.62291 | val_1_rmse: 0.62458 |  0:03:51s
epoch 61 | loss: 0.27084 | val_0_rmse: 0.5429  | val_1_rmse: 0.54256 |  0:03:55s
epoch 62 | loss: 0.26984 | val_0_rmse: 0.63089 | val_1_rmse: 0.63526 |  0:03:59s
epoch 63 | loss: 0.26957 | val_0_rmse: 0.6588  | val_1_rmse: 0.65885 |  0:04:02s
epoch 64 | loss: 0.26893 | val_0_rmse: 1.19411 | val_1_rmse: 1.21074 |  0:04:06s
epoch 65 | loss: 0.27437 | val_0_rmse: 0.5362  | val_1_rmse: 0.53319 |  0:04:10s
epoch 66 | loss: 0.27228 | val_0_rmse: 0.5186  | val_1_rmse: 0.51527 |  0:04:14s
epoch 67 | loss: 0.26722 | val_0_rmse: 0.52584 | val_1_rmse: 0.51994 |  0:04:18s
epoch 68 | loss: 0.26837 | val_0_rmse: 0.5194  | val_1_rmse: 0.52042 |  0:04:21s
epoch 69 | loss: 0.26754 | val_0_rmse: 0.55051 | val_1_rmse: 0.55103 |  0:04:25s
epoch 70 | loss: 0.26723 | val_0_rmse: 0.58081 | val_1_rmse: 0.58484 |  0:04:29s
epoch 71 | loss: 0.26585 | val_0_rmse: 1.32156 | val_1_rmse: 1.32799 |  0:04:33s
epoch 72 | loss: 0.27022 | val_0_rmse: 0.51618 | val_1_rmse: 0.51403 |  0:04:37s
epoch 73 | loss: 0.27111 | val_0_rmse: 0.77005 | val_1_rmse: 0.7768  |  0:04:41s
epoch 74 | loss: 0.26608 | val_0_rmse: 0.53739 | val_1_rmse: 0.53539 |  0:04:44s
epoch 75 | loss: 0.26875 | val_0_rmse: 0.56571 | val_1_rmse: 0.56844 |  0:04:48s
epoch 76 | loss: 0.26803 | val_0_rmse: 0.53577 | val_1_rmse: 0.53521 |  0:04:52s
epoch 77 | loss: 0.26756 | val_0_rmse: 0.53334 | val_1_rmse: 0.53278 |  0:04:56s
epoch 78 | loss: 0.26539 | val_0_rmse: 0.90889 | val_1_rmse: 0.92173 |  0:05:00s
epoch 79 | loss: 0.26855 | val_0_rmse: 0.56812 | val_1_rmse: 0.57037 |  0:05:03s
epoch 80 | loss: 0.26955 | val_0_rmse: 0.54425 | val_1_rmse: 0.54449 |  0:05:07s
epoch 81 | loss: 0.26366 | val_0_rmse: 0.67779 | val_1_rmse: 0.67982 |  0:05:11s
epoch 82 | loss: 0.26487 | val_0_rmse: 0.51222 | val_1_rmse: 0.51194 |  0:05:15s
epoch 83 | loss: 0.26622 | val_0_rmse: 0.52428 | val_1_rmse: 0.51534 |  0:05:18s
epoch 84 | loss: 0.26931 | val_0_rmse: 0.61791 | val_1_rmse: 0.62232 |  0:05:22s
epoch 85 | loss: 0.26317 | val_0_rmse: 0.52773 | val_1_rmse: 0.52687 |  0:05:26s
epoch 86 | loss: 0.26158 | val_0_rmse: 0.62054 | val_1_rmse: 0.62424 |  0:05:30s
epoch 87 | loss: 0.26377 | val_0_rmse: 0.58872 | val_1_rmse: 0.59166 |  0:05:34s
epoch 88 | loss: 0.26387 | val_0_rmse: 0.60073 | val_1_rmse: 0.60433 |  0:05:38s
epoch 89 | loss: 0.26547 | val_0_rmse: 0.56789 | val_1_rmse: 0.56689 |  0:05:41s
epoch 90 | loss: 0.2634  | val_0_rmse: 0.54261 | val_1_rmse: 0.54248 |  0:05:45s
epoch 91 | loss: 0.26309 | val_0_rmse: 0.52917 | val_1_rmse: 0.52505 |  0:05:49s
epoch 92 | loss: 0.25897 | val_0_rmse: 0.59828 | val_1_rmse: 0.6035  |  0:05:53s
epoch 93 | loss: 0.25932 | val_0_rmse: 0.53924 | val_1_rmse: 0.5393  |  0:05:57s
epoch 94 | loss: 0.25713 | val_0_rmse: 0.56135 | val_1_rmse: 0.56039 |  0:06:00s
epoch 95 | loss: 0.25913 | val_0_rmse: 0.57852 | val_1_rmse: 0.58242 |  0:06:04s
epoch 96 | loss: 0.25548 | val_0_rmse: 0.56213 | val_1_rmse: 0.56427 |  0:06:08s
epoch 97 | loss: 0.25989 | val_0_rmse: 0.5418  | val_1_rmse: 0.53689 |  0:06:12s
epoch 98 | loss: 0.25898 | val_0_rmse: 0.5221  | val_1_rmse: 0.5208  |  0:06:15s
epoch 99 | loss: 0.25377 | val_0_rmse: 0.5196  | val_1_rmse: 0.51691 |  0:06:20s
epoch 100| loss: 0.25331 | val_0_rmse: 0.52919 | val_1_rmse: 0.52515 |  0:06:23s
epoch 101| loss: 0.25555 | val_0_rmse: 0.68891 | val_1_rmse: 0.69248 |  0:06:28s
epoch 102| loss: 0.25205 | val_0_rmse: 0.63724 | val_1_rmse: 0.64072 |  0:06:32s
epoch 103| loss: 0.25082 | val_0_rmse: 0.53838 | val_1_rmse: 0.53517 |  0:06:36s
epoch 104| loss: 0.25295 | val_0_rmse: 0.56365 | val_1_rmse: 0.56773 |  0:06:40s
epoch 105| loss: 0.25171 | val_0_rmse: 0.5413  | val_1_rmse: 0.54149 |  0:06:44s
epoch 106| loss: 0.25023 | val_0_rmse: 0.53157 | val_1_rmse: 0.53345 |  0:06:48s
epoch 107| loss: 0.25022 | val_0_rmse: 0.59868 | val_1_rmse: 0.59964 |  0:06:51s
epoch 108| loss: 0.24835 | val_0_rmse: 0.60884 | val_1_rmse: 0.6086  |  0:06:55s
epoch 109| loss: 0.24898 | val_0_rmse: 0.56568 | val_1_rmse: 0.57095 |  0:06:59s
epoch 110| loss: 0.24685 | val_0_rmse: 0.53521 | val_1_rmse: 0.53581 |  0:07:03s
epoch 111| loss: 0.24843 | val_0_rmse: 0.59716 | val_1_rmse: 0.59831 |  0:07:07s
epoch 112| loss: 0.24681 | val_0_rmse: 0.5045  | val_1_rmse: 0.50734 |  0:07:11s
epoch 113| loss: 0.24791 | val_0_rmse: 0.49159 | val_1_rmse: 0.49085 |  0:07:15s
epoch 114| loss: 0.2478  | val_0_rmse: 0.56742 | val_1_rmse: 0.56219 |  0:07:19s
epoch 115| loss: 0.24889 | val_0_rmse: 0.57614 | val_1_rmse: 0.5796  |  0:07:23s
epoch 116| loss: 0.24411 | val_0_rmse: 0.52346 | val_1_rmse: 0.52145 |  0:07:27s
epoch 117| loss: 0.24875 | val_0_rmse: 0.52694 | val_1_rmse: 0.52446 |  0:07:31s
epoch 118| loss: 0.24238 | val_0_rmse: 0.49614 | val_1_rmse: 0.49751 |  0:07:35s
epoch 119| loss: 0.24136 | val_0_rmse: 0.51962 | val_1_rmse: 0.52184 |  0:07:39s
epoch 120| loss: 0.24429 | val_0_rmse: 0.52939 | val_1_rmse: 0.52255 |  0:07:43s
epoch 121| loss: 0.24446 | val_0_rmse: 0.66835 | val_1_rmse: 0.67028 |  0:07:47s
epoch 122| loss: 0.24171 | val_0_rmse: 0.54076 | val_1_rmse: 0.53977 |  0:07:51s
epoch 123| loss: 0.24165 | val_0_rmse: 0.5073  | val_1_rmse: 0.51018 |  0:07:55s
epoch 124| loss: 0.24071 | val_0_rmse: 0.57957 | val_1_rmse: 0.58316 |  0:07:59s
epoch 125| loss: 0.24287 | val_0_rmse: 0.56261 | val_1_rmse: 0.56699 |  0:08:03s
epoch 126| loss: 0.24169 | val_0_rmse: 0.55957 | val_1_rmse: 0.56368 |  0:08:07s
epoch 127| loss: 0.23837 | val_0_rmse: 1.87434 | val_1_rmse: 1.88339 |  0:08:11s
epoch 128| loss: 0.23927 | val_0_rmse: 0.59016 | val_1_rmse: 0.59539 |  0:08:15s
epoch 129| loss: 0.2436  | val_0_rmse: 0.67732 | val_1_rmse: 0.68088 |  0:08:19s
epoch 130| loss: 0.2363  | val_0_rmse: 0.54666 | val_1_rmse: 0.54948 |  0:08:23s
epoch 131| loss: 0.23744 | val_0_rmse: 0.57489 | val_1_rmse: 0.57832 |  0:08:27s
epoch 132| loss: 0.24051 | val_0_rmse: 0.50679 | val_1_rmse: 0.50662 |  0:08:30s
epoch 133| loss: 0.2377  | val_0_rmse: 0.49075 | val_1_rmse: 0.49278 |  0:08:34s
epoch 134| loss: 0.23642 | val_0_rmse: 0.58393 | val_1_rmse: 0.58263 |  0:08:38s
epoch 135| loss: 0.23952 | val_0_rmse: 0.51574 | val_1_rmse: 0.51162 |  0:08:42s
epoch 136| loss: 0.23663 | val_0_rmse: 1.07158 | val_1_rmse: 2.26862 |  0:08:46s
epoch 137| loss: 0.23163 | val_0_rmse: 0.54492 | val_1_rmse: 0.54837 |  0:08:49s
epoch 138| loss: 0.2361  | val_0_rmse: 0.55908 | val_1_rmse: 0.55982 |  0:08:53s
epoch 139| loss: 0.23558 | val_0_rmse: 0.56474 | val_1_rmse: 0.54697 |  0:08:57s
epoch 140| loss: 0.23159 | val_0_rmse: 0.54106 | val_1_rmse: 0.5396  |  0:09:01s
epoch 141| loss: 0.23577 | val_0_rmse: 0.64229 | val_1_rmse: 0.64968 |  0:09:05s
epoch 142| loss: 0.22973 | val_0_rmse: 0.68069 | val_1_rmse: 0.68659 |  0:09:08s
epoch 143| loss: 0.23432 | val_0_rmse: 0.49536 | val_1_rmse: 0.49221 |  0:09:12s

Early stopping occured at epoch 143 with best_epoch = 113 and best_val_1_rmse = 0.49085
Best weights from best epoch are automatically used!
ended training at: 02:08:38
Feature importance:
[('Area', 0.4595191381524996), ('Baths', 0.0), ('Beds', 0.1771340014934498), ('Latitude', 0.07248156993129234), ('Longitude', 0.03421717546387171), ('Month', 0.16458296778589332), ('Year', 0.09206514717299326)]
Mean squared error is of 816315364.7318208
Mean absolute error:19573.370220365014
MAPE:0.3152734848752208
R2 score:0.7586433108639437
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:08:38
epoch 0  | loss: 0.42725 | val_0_rmse: 0.5872  | val_1_rmse: 0.58602 |  0:00:03s
epoch 1  | loss: 0.34219 | val_0_rmse: 0.56739 | val_1_rmse: 0.56509 |  0:00:07s
epoch 2  | loss: 0.3298  | val_0_rmse: 0.56645 | val_1_rmse: 0.56512 |  0:00:11s
epoch 3  | loss: 0.32867 | val_0_rmse: 0.56349 | val_1_rmse: 0.56077 |  0:00:15s
epoch 4  | loss: 0.32813 | val_0_rmse: 0.56303 | val_1_rmse: 0.56062 |  0:00:19s
epoch 5  | loss: 0.32825 | val_0_rmse: 0.56965 | val_1_rmse: 0.56662 |  0:00:22s
epoch 6  | loss: 0.32662 | val_0_rmse: 0.56197 | val_1_rmse: 0.55986 |  0:00:26s
epoch 7  | loss: 0.32351 | val_0_rmse: 0.57217 | val_1_rmse: 0.57097 |  0:00:30s
epoch 8  | loss: 0.32247 | val_0_rmse: 0.56192 | val_1_rmse: 0.55926 |  0:00:34s
epoch 9  | loss: 0.31902 | val_0_rmse: 0.57818 | val_1_rmse: 0.57889 |  0:00:38s
epoch 10 | loss: 0.32538 | val_0_rmse: 0.56387 | val_1_rmse: 0.56272 |  0:00:41s
epoch 11 | loss: 0.31939 | val_0_rmse: 0.55749 | val_1_rmse: 0.55797 |  0:00:45s
epoch 12 | loss: 0.31793 | val_0_rmse: 0.55875 | val_1_rmse: 0.55801 |  0:00:49s
epoch 13 | loss: 0.31487 | val_0_rmse: 0.57787 | val_1_rmse: 0.5772  |  0:00:53s
epoch 14 | loss: 0.3083  | val_0_rmse: 0.71308 | val_1_rmse: 0.71003 |  0:00:57s
epoch 15 | loss: 0.30189 | val_0_rmse: 0.564   | val_1_rmse: 0.562   |  0:01:00s
epoch 16 | loss: 0.29753 | val_0_rmse: 0.53854 | val_1_rmse: 0.53912 |  0:01:04s
epoch 17 | loss: 0.2932  | val_0_rmse: 0.61728 | val_1_rmse: 0.61515 |  0:01:08s
epoch 18 | loss: 0.2918  | val_0_rmse: 0.59178 | val_1_rmse: 0.58939 |  0:01:12s
epoch 19 | loss: 0.29316 | val_0_rmse: 0.56156 | val_1_rmse: 0.56001 |  0:01:16s
epoch 20 | loss: 0.29084 | val_0_rmse: 0.54766 | val_1_rmse: 0.54793 |  0:01:19s
epoch 21 | loss: 0.28869 | val_0_rmse: 0.53926 | val_1_rmse: 0.53884 |  0:01:23s
epoch 22 | loss: 0.29035 | val_0_rmse: 0.5524  | val_1_rmse: 0.55137 |  0:01:27s
epoch 23 | loss: 0.28798 | val_0_rmse: 0.57356 | val_1_rmse: 0.5716  |  0:01:31s
epoch 24 | loss: 0.29107 | val_0_rmse: 0.58734 | val_1_rmse: 0.58336 |  0:01:35s
epoch 25 | loss: 0.29219 | val_0_rmse: 0.5871  | val_1_rmse: 0.58577 |  0:01:38s
epoch 26 | loss: 0.30867 | val_0_rmse: 0.76269 | val_1_rmse: 0.75879 |  0:01:42s
epoch 27 | loss: 0.29749 | val_0_rmse: 0.79437 | val_1_rmse: 0.7921  |  0:01:46s
epoch 28 | loss: 0.29283 | val_0_rmse: 0.53569 | val_1_rmse: 0.53569 |  0:01:50s
epoch 29 | loss: 0.28866 | val_0_rmse: 0.5922  | val_1_rmse: 0.59061 |  0:01:54s
epoch 30 | loss: 0.28698 | val_0_rmse: 0.57249 | val_1_rmse: 0.57108 |  0:01:58s
epoch 31 | loss: 0.28782 | val_0_rmse: 0.59726 | val_1_rmse: 0.59555 |  0:02:01s
epoch 32 | loss: 0.28498 | val_0_rmse: 0.55724 | val_1_rmse: 0.55803 |  0:02:05s
epoch 33 | loss: 0.2883  | val_0_rmse: 0.57058 | val_1_rmse: 0.56875 |  0:02:09s
epoch 34 | loss: 0.28803 | val_0_rmse: 0.56126 | val_1_rmse: 0.55951 |  0:02:13s
epoch 35 | loss: 0.28448 | val_0_rmse: 0.58184 | val_1_rmse: 0.57816 |  0:02:17s
epoch 36 | loss: 0.28392 | val_0_rmse: 0.536   | val_1_rmse: 0.53561 |  0:02:20s
epoch 37 | loss: 0.2859  | val_0_rmse: 0.7375  | val_1_rmse: 0.73215 |  0:02:24s
epoch 38 | loss: 0.28276 | val_0_rmse: 0.60514 | val_1_rmse: 0.60322 |  0:02:28s
epoch 39 | loss: 0.28554 | val_0_rmse: 0.59002 | val_1_rmse: 0.58835 |  0:02:32s
epoch 40 | loss: 0.28511 | val_0_rmse: 0.77028 | val_1_rmse: 0.76553 |  0:02:36s
epoch 41 | loss: 0.28278 | val_0_rmse: 0.55685 | val_1_rmse: 0.5554  |  0:02:39s
epoch 42 | loss: 0.28326 | val_0_rmse: 0.60927 | val_1_rmse: 0.60829 |  0:02:43s
epoch 43 | loss: 0.28286 | val_0_rmse: 0.56354 | val_1_rmse: 0.56265 |  0:02:47s
epoch 44 | loss: 0.28217 | val_0_rmse: 0.59035 | val_1_rmse: 0.58901 |  0:02:51s
epoch 45 | loss: 0.28287 | val_0_rmse: 0.59178 | val_1_rmse: 0.59111 |  0:02:55s
epoch 46 | loss: 0.28109 | val_0_rmse: 0.60392 | val_1_rmse: 0.59891 |  0:02:59s
epoch 47 | loss: 0.28245 | val_0_rmse: 0.54666 | val_1_rmse: 0.54544 |  0:03:02s
epoch 48 | loss: 0.28759 | val_0_rmse: 0.60514 | val_1_rmse: 0.59857 |  0:03:06s
epoch 49 | loss: 0.28421 | val_0_rmse: 0.55379 | val_1_rmse: 0.55301 |  0:03:10s
epoch 50 | loss: 0.28177 | val_0_rmse: 0.60316 | val_1_rmse: 0.60185 |  0:03:14s
epoch 51 | loss: 0.2812  | val_0_rmse: 0.59298 | val_1_rmse: 0.5916  |  0:03:18s
epoch 52 | loss: 0.28557 | val_0_rmse: 0.5642  | val_1_rmse: 0.56418 |  0:03:21s
epoch 53 | loss: 0.27837 | val_0_rmse: 0.58482 | val_1_rmse: 0.58444 |  0:03:25s
epoch 54 | loss: 0.27921 | val_0_rmse: 0.54569 | val_1_rmse: 0.54641 |  0:03:29s
epoch 55 | loss: 0.28131 | val_0_rmse: 0.57983 | val_1_rmse: 0.57959 |  0:03:33s
epoch 56 | loss: 0.27921 | val_0_rmse: 0.61425 | val_1_rmse: 0.61351 |  0:03:37s
epoch 57 | loss: 0.27929 | val_0_rmse: 0.58943 | val_1_rmse: 0.58877 |  0:03:40s
epoch 58 | loss: 0.27945 | val_0_rmse: 0.55019 | val_1_rmse: 0.5535  |  0:03:44s
epoch 59 | loss: 0.27762 | val_0_rmse: 0.5987  | val_1_rmse: 0.59806 |  0:03:48s
epoch 60 | loss: 0.27625 | val_0_rmse: 0.53584 | val_1_rmse: 0.53833 |  0:03:52s
epoch 61 | loss: 0.27756 | val_0_rmse: 0.55669 | val_1_rmse: 0.55739 |  0:03:56s
epoch 62 | loss: 0.28107 | val_0_rmse: 0.56606 | val_1_rmse: 0.56397 |  0:04:00s
epoch 63 | loss: 0.27682 | val_0_rmse: 0.56829 | val_1_rmse: 0.5691  |  0:04:03s
epoch 64 | loss: 0.27686 | val_0_rmse: 0.60366 | val_1_rmse: 0.60192 |  0:04:07s
epoch 65 | loss: 0.27946 | val_0_rmse: 0.87227 | val_1_rmse: 0.87091 |  0:04:11s
epoch 66 | loss: 0.27247 | val_0_rmse: 0.58416 | val_1_rmse: 0.58372 |  0:04:15s

Early stopping occured at epoch 66 with best_epoch = 36 and best_val_1_rmse = 0.53561
Best weights from best epoch are automatically used!
ended training at: 02:12:55
Feature importance:
[('Area', 0.5195165031604071), ('Baths', 0.18197892473804017), ('Beds', 0.06487151195907973), ('Latitude', 0.1851659336195337), ('Longitude', 0.016745448171574813), ('Month', 0.031721678351364496), ('Year', 0.0)]
Mean squared error is of 968466582.4302158
Mean absolute error:21625.7311412053
MAPE:0.3671429165615341
R2 score:0.7123950914968469
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:12:55
epoch 0  | loss: 0.43141 | val_0_rmse: 0.58549 | val_1_rmse: 0.5951  |  0:00:03s
epoch 1  | loss: 0.34462 | val_0_rmse: 0.57031 | val_1_rmse: 0.57951 |  0:00:07s
epoch 2  | loss: 0.33053 | val_0_rmse: 0.76468 | val_1_rmse: 0.77588 |  0:00:11s
epoch 3  | loss: 0.31741 | val_0_rmse: 0.62367 | val_1_rmse: 0.62825 |  0:00:15s
epoch 4  | loss: 0.3128  | val_0_rmse: 0.68856 | val_1_rmse: 0.69897 |  0:00:19s
epoch 5  | loss: 0.31106 | val_0_rmse: 0.58872 | val_1_rmse: 0.59772 |  0:00:22s
epoch 6  | loss: 0.30631 | val_0_rmse: 0.55537 | val_1_rmse: 0.5634  |  0:00:26s
epoch 7  | loss: 0.30955 | val_0_rmse: 0.62271 | val_1_rmse: 0.627   |  0:00:30s
epoch 8  | loss: 0.30208 | val_0_rmse: 0.58013 | val_1_rmse: 0.58578 |  0:00:34s
epoch 9  | loss: 0.30394 | val_0_rmse: 0.61596 | val_1_rmse: 0.62057 |  0:00:38s
epoch 10 | loss: 0.30244 | val_0_rmse: 0.57174 | val_1_rmse: 0.57779 |  0:00:41s
epoch 11 | loss: 0.30127 | val_0_rmse: 0.59776 | val_1_rmse: 0.60812 |  0:00:45s
epoch 12 | loss: 0.30145 | val_0_rmse: 0.61092 | val_1_rmse: 0.61583 |  0:00:49s
epoch 13 | loss: 0.30839 | val_0_rmse: 0.60423 | val_1_rmse: 0.60864 |  0:00:53s
epoch 14 | loss: 0.30277 | val_0_rmse: 0.58979 | val_1_rmse: 0.59294 |  0:00:57s
epoch 15 | loss: 0.29917 | val_0_rmse: 0.98826 | val_1_rmse: 1.00104 |  0:01:00s
epoch 16 | loss: 0.29801 | val_0_rmse: 0.54069 | val_1_rmse: 0.54587 |  0:01:04s
epoch 17 | loss: 0.29611 | val_0_rmse: 0.58544 | val_1_rmse: 0.58892 |  0:01:08s
epoch 18 | loss: 0.2954  | val_0_rmse: 0.56081 | val_1_rmse: 0.56944 |  0:01:12s
epoch 19 | loss: 0.2962  | val_0_rmse: 0.54562 | val_1_rmse: 0.55329 |  0:01:16s
epoch 20 | loss: 0.29576 | val_0_rmse: 0.67104 | val_1_rmse: 0.67434 |  0:01:19s
epoch 21 | loss: 0.29497 | val_0_rmse: 0.60571 | val_1_rmse: 0.61484 |  0:01:23s
epoch 22 | loss: 0.29134 | val_0_rmse: 0.61929 | val_1_rmse: 0.62891 |  0:01:27s
epoch 23 | loss: 0.29164 | val_0_rmse: 0.53737 | val_1_rmse: 0.54318 |  0:01:31s
epoch 24 | loss: 0.2903  | val_0_rmse: 0.75517 | val_1_rmse: 0.76716 |  0:01:35s
epoch 25 | loss: 0.29061 | val_0_rmse: 0.65202 | val_1_rmse: 0.65524 |  0:01:38s
epoch 26 | loss: 0.29084 | val_0_rmse: 0.61011 | val_1_rmse: 0.62135 |  0:01:42s
epoch 27 | loss: 0.28715 | val_0_rmse: 0.84551 | val_1_rmse: 0.85916 |  0:01:46s
epoch 28 | loss: 0.29412 | val_0_rmse: 0.54719 | val_1_rmse: 0.55564 |  0:01:50s
epoch 29 | loss: 0.28648 | val_0_rmse: 0.56164 | val_1_rmse: 0.56586 |  0:01:54s
epoch 30 | loss: 0.28626 | val_0_rmse: 0.52716 | val_1_rmse: 0.53422 |  0:01:57s
epoch 31 | loss: 0.28709 | val_0_rmse: 0.55289 | val_1_rmse: 0.55828 |  0:02:01s
epoch 32 | loss: 0.2849  | val_0_rmse: 0.54818 | val_1_rmse: 0.55302 |  0:02:05s
epoch 33 | loss: 0.28305 | val_0_rmse: 1.42969 | val_1_rmse: 1.44613 |  0:02:09s
epoch 34 | loss: 0.28674 | val_0_rmse: 0.56677 | val_1_rmse: 0.57789 |  0:02:13s
epoch 35 | loss: 0.28571 | val_0_rmse: 0.52704 | val_1_rmse: 0.53429 |  0:02:16s
epoch 36 | loss: 0.28515 | val_0_rmse: 0.63489 | val_1_rmse: 0.63951 |  0:02:20s
epoch 37 | loss: 0.28045 | val_0_rmse: 0.54477 | val_1_rmse: 0.54928 |  0:02:24s
epoch 38 | loss: 0.27936 | val_0_rmse: 0.53225 | val_1_rmse: 0.54313 |  0:02:28s
epoch 39 | loss: 0.28107 | val_0_rmse: 0.51966 | val_1_rmse: 0.5269  |  0:02:32s
epoch 40 | loss: 0.28288 | val_0_rmse: 0.62218 | val_1_rmse: 0.62715 |  0:02:36s
epoch 41 | loss: 0.28619 | val_0_rmse: 0.5229  | val_1_rmse: 0.52941 |  0:02:39s
epoch 42 | loss: 0.2764  | val_0_rmse: 0.54532 | val_1_rmse: 0.5484  |  0:02:43s
epoch 43 | loss: 0.27994 | val_0_rmse: 0.52722 | val_1_rmse: 0.53302 |  0:02:47s
epoch 44 | loss: 0.28038 | val_0_rmse: 0.55047 | val_1_rmse: 0.56189 |  0:02:51s
epoch 45 | loss: 0.28618 | val_0_rmse: 0.67883 | val_1_rmse: 0.68409 |  0:02:55s
epoch 46 | loss: 0.28227 | val_0_rmse: 0.64425 | val_1_rmse: 0.65503 |  0:02:59s
epoch 47 | loss: 0.28206 | val_0_rmse: 0.56262 | val_1_rmse: 0.56746 |  0:03:02s
epoch 48 | loss: 0.27922 | val_0_rmse: 0.57826 | val_1_rmse: 0.5822  |  0:03:06s
epoch 49 | loss: 0.28266 | val_0_rmse: 0.56614 | val_1_rmse: 0.56752 |  0:03:10s
epoch 50 | loss: 0.28451 | val_0_rmse: 0.5814  | val_1_rmse: 0.58563 |  0:03:14s
epoch 51 | loss: 0.28076 | val_0_rmse: 0.5455  | val_1_rmse: 0.54901 |  0:03:18s
epoch 52 | loss: 0.28061 | val_0_rmse: 0.75896 | val_1_rmse: 0.77464 |  0:03:21s
epoch 53 | loss: 0.27772 | val_0_rmse: 0.55935 | val_1_rmse: 0.56317 |  0:03:25s
epoch 54 | loss: 0.27989 | val_0_rmse: 0.64754 | val_1_rmse: 0.65287 |  0:03:29s
epoch 55 | loss: 0.28227 | val_0_rmse: 0.54556 | val_1_rmse: 0.55483 |  0:03:33s
epoch 56 | loss: 0.28119 | val_0_rmse: 0.55591 | val_1_rmse: 0.56605 |  0:03:37s
epoch 57 | loss: 0.28135 | val_0_rmse: 0.61285 | val_1_rmse: 0.61786 |  0:03:40s
epoch 58 | loss: 0.28536 | val_0_rmse: 0.53923 | val_1_rmse: 0.54346 |  0:03:44s
epoch 59 | loss: 0.2793  | val_0_rmse: 0.55706 | val_1_rmse: 0.5673  |  0:03:48s
epoch 60 | loss: 0.278   | val_0_rmse: 0.54993 | val_1_rmse: 0.55543 |  0:03:52s
epoch 61 | loss: 0.27907 | val_0_rmse: 0.55141 | val_1_rmse: 0.55576 |  0:03:56s
epoch 62 | loss: 0.28191 | val_0_rmse: 0.55787 | val_1_rmse: 0.56277 |  0:03:59s
epoch 63 | loss: 0.27698 | val_0_rmse: 0.5668  | val_1_rmse: 0.57206 |  0:04:03s
epoch 64 | loss: 0.27822 | val_0_rmse: 0.5552  | val_1_rmse: 0.559   |  0:04:07s
epoch 65 | loss: 0.27758 | val_0_rmse: 0.52774 | val_1_rmse: 0.53667 |  0:04:11s
epoch 66 | loss: 0.27951 | val_0_rmse: 0.52672 | val_1_rmse: 0.53546 |  0:04:15s
epoch 67 | loss: 0.27897 | val_0_rmse: 0.58621 | val_1_rmse: 0.59738 |  0:04:18s
epoch 68 | loss: 0.28055 | val_0_rmse: 0.58588 | val_1_rmse: 0.58978 |  0:04:22s
epoch 69 | loss: 0.27802 | val_0_rmse: 0.73048 | val_1_rmse: 0.74524 |  0:04:26s

Early stopping occured at epoch 69 with best_epoch = 39 and best_val_1_rmse = 0.5269
Best weights from best epoch are automatically used!
ended training at: 02:17:23
Feature importance:
[('Area', 0.5216962361313365), ('Baths', 0.009074224012027416), ('Beds', 0.2342322258686002), ('Latitude', 0.10898871086374101), ('Longitude', 0.1065597478265026), ('Month', 0.0), ('Year', 0.019448855297792195)]
Mean squared error is of 898703663.3773519
Mean absolute error:20972.22278865245
MAPE:0.3494546908135771
R2 score:0.7311544701041282
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:17:24
epoch 0  | loss: 0.38028 | val_0_rmse: 0.57076 | val_1_rmse: 0.57152 |  0:00:17s
epoch 1  | loss: 0.32448 | val_0_rmse: 0.60234 | val_1_rmse: 0.60266 |  0:00:35s
epoch 2  | loss: 0.31972 | val_0_rmse: 0.56059 | val_1_rmse: 0.56107 |  0:00:52s
epoch 3  | loss: 0.31332 | val_0_rmse: 0.54695 | val_1_rmse: 0.54864 |  0:01:10s
epoch 4  | loss: 0.31042 | val_0_rmse: 0.55378 | val_1_rmse: 0.55555 |  0:01:28s
epoch 5  | loss: 0.30732 | val_0_rmse: 0.5435  | val_1_rmse: 0.54428 |  0:01:45s
epoch 6  | loss: 0.30483 | val_0_rmse: 0.5577  | val_1_rmse: 0.55906 |  0:02:03s
epoch 7  | loss: 0.30215 | val_0_rmse: 0.5456  | val_1_rmse: 0.54678 |  0:02:21s
epoch 8  | loss: 0.30047 | val_0_rmse: 0.53858 | val_1_rmse: 0.53899 |  0:02:38s
epoch 9  | loss: 0.30031 | val_0_rmse: 0.54916 | val_1_rmse: 0.55038 |  0:02:56s
epoch 10 | loss: 0.2962  | val_0_rmse: 0.54176 | val_1_rmse: 0.54223 |  0:03:14s
epoch 11 | loss: 0.29811 | val_0_rmse: 0.54663 | val_1_rmse: 0.54758 |  0:03:31s
epoch 12 | loss: 0.29907 | val_0_rmse: 0.53949 | val_1_rmse: 0.54031 |  0:03:49s
epoch 13 | loss: 0.29761 | val_0_rmse: 0.61408 | val_1_rmse: 0.61818 |  0:04:07s
epoch 14 | loss: 0.30346 | val_0_rmse: 0.54143 | val_1_rmse: 0.54219 |  0:04:24s
epoch 15 | loss: 0.29527 | val_0_rmse: 0.59784 | val_1_rmse: 0.59753 |  0:04:42s
epoch 16 | loss: 0.29356 | val_0_rmse: 0.54819 | val_1_rmse: 0.5495  |  0:05:00s
epoch 17 | loss: 0.29257 | val_0_rmse: 0.54104 | val_1_rmse: 0.54306 |  0:05:17s
epoch 18 | loss: 0.2907  | val_0_rmse: 0.53947 | val_1_rmse: 0.54113 |  0:05:35s
epoch 19 | loss: 0.2917  | val_0_rmse: 0.54929 | val_1_rmse: 0.55162 |  0:05:53s
epoch 20 | loss: 0.29288 | val_0_rmse: 0.54459 | val_1_rmse: 0.54598 |  0:06:10s
epoch 21 | loss: 0.29293 | val_0_rmse: 0.54003 | val_1_rmse: 0.54083 |  0:06:28s
epoch 22 | loss: 0.29131 | val_0_rmse: 0.53318 | val_1_rmse: 0.53406 |  0:06:45s
epoch 23 | loss: 0.29287 | val_0_rmse: 0.54132 | val_1_rmse: 0.54314 |  0:07:03s
epoch 24 | loss: 0.28966 | val_0_rmse: 0.5349  | val_1_rmse: 0.5358  |  0:07:20s
epoch 25 | loss: 0.29019 | val_0_rmse: 0.53737 | val_1_rmse: 0.53868 |  0:07:38s
epoch 26 | loss: 0.28809 | val_0_rmse: 0.53728 | val_1_rmse: 0.53854 |  0:07:56s
epoch 27 | loss: 0.29028 | val_0_rmse: 0.53345 | val_1_rmse: 0.53436 |  0:08:13s
epoch 28 | loss: 0.28947 | val_0_rmse: 0.53567 | val_1_rmse: 0.53683 |  0:08:31s
epoch 29 | loss: 0.28959 | val_0_rmse: 0.57035 | val_1_rmse: 0.57103 |  0:08:49s
epoch 30 | loss: 0.28935 | val_0_rmse: 0.53097 | val_1_rmse: 0.53278 |  0:09:06s
epoch 31 | loss: 0.28871 | val_0_rmse: 0.53461 | val_1_rmse: 0.53581 |  0:09:24s
epoch 32 | loss: 0.28999 | val_0_rmse: 0.53198 | val_1_rmse: 0.53345 |  0:09:42s
epoch 33 | loss: 0.29588 | val_0_rmse: 0.54669 | val_1_rmse: 0.54713 |  0:09:59s
epoch 34 | loss: 0.2879  | val_0_rmse: 0.53758 | val_1_rmse: 0.53868 |  0:10:17s
epoch 35 | loss: 0.29209 | val_0_rmse: 0.53402 | val_1_rmse: 0.53483 |  0:10:35s
epoch 36 | loss: 0.28837 | val_0_rmse: 0.53358 | val_1_rmse: 0.53605 |  0:10:52s
epoch 37 | loss: 0.28836 | val_0_rmse: 0.52982 | val_1_rmse: 0.53184 |  0:11:10s
epoch 38 | loss: 0.28807 | val_0_rmse: 0.57716 | val_1_rmse: 0.57799 |  0:11:28s
epoch 39 | loss: 0.2885  | val_0_rmse: 0.53552 | val_1_rmse: 0.53757 |  0:11:45s
epoch 40 | loss: 0.28714 | val_0_rmse: 0.52956 | val_1_rmse: 0.53207 |  0:12:03s
epoch 41 | loss: 0.28701 | val_0_rmse: 0.53757 | val_1_rmse: 0.53996 |  0:12:20s
epoch 42 | loss: 0.29137 | val_0_rmse: 0.53639 | val_1_rmse: 0.5381  |  0:12:38s
epoch 43 | loss: 0.29038 | val_0_rmse: 0.52838 | val_1_rmse: 0.5305  |  0:12:56s
epoch 44 | loss: 0.28798 | val_0_rmse: 0.54446 | val_1_rmse: 0.54612 |  0:13:13s
epoch 45 | loss: 0.2872  | val_0_rmse: 0.52875 | val_1_rmse: 0.53128 |  0:13:31s
epoch 46 | loss: 0.28674 | val_0_rmse: 0.70226 | val_1_rmse: 0.70493 |  0:13:49s
epoch 47 | loss: 0.28622 | val_0_rmse: 0.5268  | val_1_rmse: 0.52962 |  0:14:06s
epoch 48 | loss: 0.28496 | val_0_rmse: 0.53524 | val_1_rmse: 0.53859 |  0:14:24s
epoch 49 | loss: 0.28597 | val_0_rmse: 0.5267  | val_1_rmse: 0.5293  |  0:14:41s
epoch 50 | loss: 0.28566 | val_0_rmse: 0.52899 | val_1_rmse: 0.53185 |  0:14:59s
epoch 51 | loss: 0.2848  | val_0_rmse: 0.53393 | val_1_rmse: 0.53644 |  0:15:17s
epoch 52 | loss: 0.28519 | val_0_rmse: 0.53666 | val_1_rmse: 0.53926 |  0:15:34s
epoch 53 | loss: 0.28523 | val_0_rmse: 0.5373  | val_1_rmse: 0.53953 |  0:15:52s
epoch 54 | loss: 0.28554 | val_0_rmse: 0.55107 | val_1_rmse: 0.55391 |  0:16:09s
epoch 55 | loss: 0.28448 | val_0_rmse: 0.53214 | val_1_rmse: 0.53457 |  0:16:27s
epoch 56 | loss: 0.28439 | val_0_rmse: 0.52734 | val_1_rmse: 0.53007 |  0:16:45s
epoch 57 | loss: 0.28447 | val_0_rmse: 0.5321  | val_1_rmse: 0.53506 |  0:17:02s
epoch 58 | loss: 0.28563 | val_0_rmse: 0.54617 | val_1_rmse: 0.54801 |  0:17:20s
epoch 59 | loss: 0.28483 | val_0_rmse: 0.52889 | val_1_rmse: 0.53133 |  0:17:38s
epoch 60 | loss: 0.28375 | val_0_rmse: 0.53097 | val_1_rmse: 0.53396 |  0:17:55s
epoch 61 | loss: 0.28401 | val_0_rmse: 0.5287  | val_1_rmse: 0.53153 |  0:18:13s
epoch 62 | loss: 0.28417 | val_0_rmse: 0.53488 | val_1_rmse: 0.53716 |  0:18:31s
epoch 63 | loss: 0.28468 | val_0_rmse: 0.52763 | val_1_rmse: 0.53005 |  0:18:48s
epoch 64 | loss: 0.2837  | val_0_rmse: 0.52992 | val_1_rmse: 0.5328  |  0:19:06s
epoch 65 | loss: 0.28483 | val_0_rmse: 0.53529 | val_1_rmse: 0.5389  |  0:19:24s
epoch 66 | loss: 0.28454 | val_0_rmse: 0.53756 | val_1_rmse: 0.54062 |  0:19:41s
epoch 67 | loss: 0.28312 | val_0_rmse: 0.5327  | val_1_rmse: 0.53634 |  0:19:59s
epoch 68 | loss: 0.28377 | val_0_rmse: 0.52716 | val_1_rmse: 0.52984 |  0:20:16s
epoch 69 | loss: 0.28308 | val_0_rmse: 0.53912 | val_1_rmse: 0.54207 |  0:20:34s
epoch 70 | loss: 0.28438 | val_0_rmse: 0.54568 | val_1_rmse: 0.54773 |  0:20:52s
epoch 71 | loss: 0.28375 | val_0_rmse: 0.52604 | val_1_rmse: 0.52961 |  0:21:09s
epoch 72 | loss: 0.28366 | val_0_rmse: 0.54157 | val_1_rmse: 0.54398 |  0:21:27s
epoch 73 | loss: 0.28266 | val_0_rmse: 0.52765 | val_1_rmse: 0.53122 |  0:21:45s
epoch 74 | loss: 0.28265 | val_0_rmse: 0.52991 | val_1_rmse: 0.53369 |  0:22:02s
epoch 75 | loss: 0.28252 | val_0_rmse: 0.52606 | val_1_rmse: 0.52948 |  0:22:20s
epoch 76 | loss: 0.28349 | val_0_rmse: 0.52843 | val_1_rmse: 0.53121 |  0:22:37s
epoch 77 | loss: 0.28255 | val_0_rmse: 0.52864 | val_1_rmse: 0.53189 |  0:22:55s
epoch 78 | loss: 0.28359 | val_0_rmse: 0.53833 | val_1_rmse: 0.54223 |  0:23:13s
epoch 79 | loss: 0.28277 | val_0_rmse: 0.5443  | val_1_rmse: 0.54748 |  0:23:30s

Early stopping occured at epoch 79 with best_epoch = 49 and best_val_1_rmse = 0.5293
Best weights from best epoch are automatically used!
ended training at: 02:41:00
Feature importance:
[('Area', 0.2174431619537439), ('Baths', 0.11734427483080198), ('Beds', 0.04702944764173236), ('Latitude', 0.15887946085162086), ('Longitude', 0.4097581173522005), ('Month', 0.0), ('Year', 0.04954553736990035)]
Mean squared error is of 11047073498.435427
Mean absolute error:66032.58225819109
MAPE:0.4552966490613943
R2 score:0.720869013002959
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:41:03
epoch 0  | loss: 0.38676 | val_0_rmse: 0.56905 | val_1_rmse: 0.56511 |  0:00:17s
epoch 1  | loss: 0.3235  | val_0_rmse: 0.57654 | val_1_rmse: 0.57436 |  0:00:35s
epoch 2  | loss: 0.31995 | val_0_rmse: 0.55878 | val_1_rmse: 0.55497 |  0:00:53s
epoch 3  | loss: 0.31612 | val_0_rmse: 0.55976 | val_1_rmse: 0.55712 |  0:01:10s
epoch 4  | loss: 0.31166 | val_0_rmse: 0.54674 | val_1_rmse: 0.54367 |  0:01:28s
epoch 5  | loss: 0.30985 | val_0_rmse: 0.55091 | val_1_rmse: 0.5486  |  0:01:46s
epoch 6  | loss: 0.30895 | val_0_rmse: 0.54339 | val_1_rmse: 0.5416  |  0:02:03s
epoch 7  | loss: 0.30635 | val_0_rmse: 0.55684 | val_1_rmse: 0.55513 |  0:02:21s
epoch 8  | loss: 0.30764 | val_0_rmse: 0.54253 | val_1_rmse: 0.53999 |  0:02:39s
epoch 9  | loss: 0.30521 | val_0_rmse: 0.54886 | val_1_rmse: 0.54694 |  0:02:56s
epoch 10 | loss: 0.30198 | val_0_rmse: 0.54494 | val_1_rmse: 0.54247 |  0:03:14s
epoch 11 | loss: 0.30466 | val_0_rmse: 0.55425 | val_1_rmse: 0.54986 |  0:03:31s
epoch 12 | loss: 0.30216 | val_0_rmse: 0.54295 | val_1_rmse: 0.54112 |  0:03:49s
epoch 13 | loss: 0.30182 | val_0_rmse: 0.55742 | val_1_rmse: 0.55467 |  0:04:06s
epoch 14 | loss: 0.30354 | val_0_rmse: 0.54975 | val_1_rmse: 0.54714 |  0:04:24s
epoch 15 | loss: 0.30104 | val_0_rmse: 0.54999 | val_1_rmse: 0.54765 |  0:04:41s
epoch 16 | loss: 0.2995  | val_0_rmse: 0.54553 | val_1_rmse: 0.54237 |  0:04:59s
epoch 17 | loss: 0.29952 | val_0_rmse: 0.55162 | val_1_rmse: 0.54942 |  0:05:17s
epoch 18 | loss: 0.30027 | val_0_rmse: 0.54509 | val_1_rmse: 0.54283 |  0:05:34s
epoch 19 | loss: 0.30427 | val_0_rmse: 0.54365 | val_1_rmse: 0.5399  |  0:05:52s
epoch 20 | loss: 0.30235 | val_0_rmse: 0.54819 | val_1_rmse: 0.54589 |  0:06:09s
epoch 21 | loss: 0.31169 | val_0_rmse: 0.55114 | val_1_rmse: 0.54837 |  0:06:27s
epoch 22 | loss: 0.30333 | val_0_rmse: 0.549   | val_1_rmse: 0.54741 |  0:06:45s
epoch 23 | loss: 0.29973 | val_0_rmse: 0.54291 | val_1_rmse: 0.54056 |  0:07:02s
epoch 24 | loss: 0.30578 | val_0_rmse: 0.56171 | val_1_rmse: 0.55891 |  0:07:20s
epoch 25 | loss: 0.3004  | val_0_rmse: 0.54691 | val_1_rmse: 0.54399 |  0:07:38s
epoch 26 | loss: 0.29885 | val_0_rmse: 0.5443  | val_1_rmse: 0.54165 |  0:07:55s
epoch 27 | loss: 0.29599 | val_0_rmse: 0.53679 | val_1_rmse: 0.53527 |  0:08:13s
epoch 28 | loss: 0.29653 | val_0_rmse: 0.53912 | val_1_rmse: 0.53654 |  0:08:31s
epoch 29 | loss: 0.29839 | val_0_rmse: 0.5396  | val_1_rmse: 0.5374  |  0:08:48s
epoch 30 | loss: 0.29781 | val_0_rmse: 0.53996 | val_1_rmse: 0.53749 |  0:09:06s
epoch 31 | loss: 0.29596 | val_0_rmse: 0.5378  | val_1_rmse: 0.53483 |  0:09:23s
epoch 32 | loss: 0.29909 | val_0_rmse: 0.5879  | val_1_rmse: 0.58762 |  0:09:41s
epoch 33 | loss: 0.31873 | val_0_rmse: 0.65007 | val_1_rmse: 0.64776 |  0:09:59s
epoch 34 | loss: 0.3363  | val_0_rmse: 0.55457 | val_1_rmse: 0.55292 |  0:10:16s
epoch 35 | loss: 0.31199 | val_0_rmse: 0.57253 | val_1_rmse: 0.57021 |  0:10:34s
epoch 36 | loss: 0.3061  | val_0_rmse: 0.54774 | val_1_rmse: 0.54619 |  0:10:52s
epoch 37 | loss: 0.31269 | val_0_rmse: 0.5547  | val_1_rmse: 0.5526  |  0:11:09s
epoch 38 | loss: 0.30772 | val_0_rmse: 0.54974 | val_1_rmse: 0.54704 |  0:11:27s
epoch 39 | loss: 0.30952 | val_0_rmse: 0.54577 | val_1_rmse: 0.54299 |  0:11:44s
epoch 40 | loss: 0.30246 | val_0_rmse: 0.5427  | val_1_rmse: 0.54034 |  0:12:02s
epoch 41 | loss: 0.29914 | val_0_rmse: 0.54182 | val_1_rmse: 0.54036 |  0:12:20s
epoch 42 | loss: 0.30084 | val_0_rmse: 0.54068 | val_1_rmse: 0.53788 |  0:12:37s
epoch 43 | loss: 0.29921 | val_0_rmse: 0.53795 | val_1_rmse: 0.53573 |  0:12:55s
epoch 44 | loss: 0.29611 | val_0_rmse: 0.54047 | val_1_rmse: 0.53746 |  0:13:13s
epoch 45 | loss: 0.29727 | val_0_rmse: 0.54109 | val_1_rmse: 0.53938 |  0:13:30s
epoch 46 | loss: 0.30769 | val_0_rmse: 0.53961 | val_1_rmse: 0.53711 |  0:13:48s
epoch 47 | loss: 0.29745 | val_0_rmse: 0.54027 | val_1_rmse: 0.53737 |  0:14:06s
epoch 48 | loss: 0.29584 | val_0_rmse: 0.53614 | val_1_rmse: 0.53406 |  0:14:23s
epoch 49 | loss: 0.29528 | val_0_rmse: 0.53833 | val_1_rmse: 0.5366  |  0:14:41s
epoch 50 | loss: 0.29531 | val_0_rmse: 0.54267 | val_1_rmse: 0.54    |  0:14:58s
epoch 51 | loss: 0.29329 | val_0_rmse: 0.54382 | val_1_rmse: 0.54108 |  0:15:16s
epoch 52 | loss: 0.29312 | val_0_rmse: 0.54238 | val_1_rmse: 0.54053 |  0:15:33s
epoch 53 | loss: 0.29419 | val_0_rmse: 0.53816 | val_1_rmse: 0.53508 |  0:15:51s
epoch 54 | loss: 0.29299 | val_0_rmse: 0.53839 | val_1_rmse: 0.53548 |  0:16:09s
epoch 55 | loss: 0.2931  | val_0_rmse: 0.54308 | val_1_rmse: 0.54111 |  0:16:26s
epoch 56 | loss: 0.29424 | val_0_rmse: 0.5402  | val_1_rmse: 0.53871 |  0:16:44s
epoch 57 | loss: 0.29265 | val_0_rmse: 0.54057 | val_1_rmse: 0.53827 |  0:17:02s
epoch 58 | loss: 0.29194 | val_0_rmse: 0.53705 | val_1_rmse: 0.53529 |  0:17:20s
epoch 59 | loss: 0.29161 | val_0_rmse: 0.54568 | val_1_rmse: 0.54298 |  0:17:37s
epoch 60 | loss: 0.29204 | val_0_rmse: 0.55346 | val_1_rmse: 0.55156 |  0:17:55s
epoch 61 | loss: 0.29073 | val_0_rmse: 0.53511 | val_1_rmse: 0.53266 |  0:18:12s
epoch 62 | loss: 0.29168 | val_0_rmse: 0.54291 | val_1_rmse: 0.5409  |  0:18:30s
epoch 63 | loss: 0.29224 | val_0_rmse: 0.53774 | val_1_rmse: 0.53509 |  0:18:48s
epoch 64 | loss: 0.29367 | val_0_rmse: 0.53582 | val_1_rmse: 0.53363 |  0:19:05s
epoch 65 | loss: 0.29013 | val_0_rmse: 0.53766 | val_1_rmse: 0.53577 |  0:19:23s
epoch 66 | loss: 0.29074 | val_0_rmse: 0.56255 | val_1_rmse: 0.55985 |  0:19:40s
epoch 67 | loss: 0.29013 | val_0_rmse: 0.55125 | val_1_rmse: 0.54871 |  0:19:58s
epoch 68 | loss: 0.29049 | val_0_rmse: 0.54188 | val_1_rmse: 0.5395  |  0:20:15s
epoch 69 | loss: 0.28999 | val_0_rmse: 0.5554  | val_1_rmse: 0.55469 |  0:20:33s
epoch 70 | loss: 0.28904 | val_0_rmse: 0.53896 | val_1_rmse: 0.53742 |  0:20:51s
epoch 71 | loss: 0.28997 | val_0_rmse: 0.53842 | val_1_rmse: 0.53641 |  0:21:08s
epoch 72 | loss: 0.28934 | val_0_rmse: 0.55062 | val_1_rmse: 0.54893 |  0:21:26s
epoch 73 | loss: 0.28998 | val_0_rmse: 0.55957 | val_1_rmse: 0.55891 |  0:21:44s
epoch 74 | loss: 0.2928  | val_0_rmse: 0.53346 | val_1_rmse: 0.53125 |  0:22:01s
epoch 75 | loss: 0.28924 | val_0_rmse: 0.55503 | val_1_rmse: 0.55398 |  0:22:19s
epoch 76 | loss: 0.28929 | val_0_rmse: 0.54091 | val_1_rmse: 0.53954 |  0:22:36s
epoch 77 | loss: 0.28907 | val_0_rmse: 0.53744 | val_1_rmse: 0.5359  |  0:22:54s
epoch 78 | loss: 0.28849 | val_0_rmse: 0.56013 | val_1_rmse: 0.55845 |  0:23:12s
epoch 79 | loss: 0.28892 | val_0_rmse: 0.53445 | val_1_rmse: 0.53278 |  0:23:29s
epoch 80 | loss: 0.28973 | val_0_rmse: 0.54335 | val_1_rmse: 0.54146 |  0:23:47s
epoch 81 | loss: 0.28867 | val_0_rmse: 0.53595 | val_1_rmse: 0.53405 |  0:24:05s
epoch 82 | loss: 0.28901 | val_0_rmse: 0.5389  | val_1_rmse: 0.53709 |  0:24:22s
epoch 83 | loss: 0.30466 | val_0_rmse: 0.56467 | val_1_rmse: 0.56231 |  0:24:40s
epoch 84 | loss: 0.30035 | val_0_rmse: 0.58806 | val_1_rmse: 0.58462 |  0:24:57s
epoch 85 | loss: 0.29383 | val_0_rmse: 0.53419 | val_1_rmse: 0.53228 |  0:25:15s
epoch 86 | loss: 0.29117 | val_0_rmse: 0.54489 | val_1_rmse: 0.54322 |  0:25:33s
epoch 87 | loss: 0.29044 | val_0_rmse: 0.55779 | val_1_rmse: 0.55701 |  0:25:50s
epoch 88 | loss: 0.29088 | val_0_rmse: 0.54789 | val_1_rmse: 0.54715 |  0:26:08s
epoch 89 | loss: 0.28914 | val_0_rmse: 0.53483 | val_1_rmse: 0.53296 |  0:26:25s
epoch 90 | loss: 0.28932 | val_0_rmse: 0.53072 | val_1_rmse: 0.52864 |  0:26:43s
epoch 91 | loss: 0.28909 | val_0_rmse: 0.53849 | val_1_rmse: 0.53594 |  0:27:01s
epoch 92 | loss: 0.28984 | val_0_rmse: 0.55489 | val_1_rmse: 0.55389 |  0:27:18s
epoch 93 | loss: 0.29071 | val_0_rmse: 0.54686 | val_1_rmse: 0.54494 |  0:27:36s
epoch 94 | loss: 0.29049 | val_0_rmse: 0.55969 | val_1_rmse: 0.55786 |  0:27:53s
epoch 95 | loss: 0.28899 | val_0_rmse: 0.53539 | val_1_rmse: 0.53395 |  0:28:11s
epoch 96 | loss: 0.28828 | val_0_rmse: 0.54487 | val_1_rmse: 0.54336 |  0:28:29s
epoch 97 | loss: 0.2886  | val_0_rmse: 0.54101 | val_1_rmse: 0.53976 |  0:28:46s
epoch 98 | loss: 0.28828 | val_0_rmse: 0.54819 | val_1_rmse: 0.54627 |  0:29:04s
epoch 99 | loss: 0.28755 | val_0_rmse: 0.54256 | val_1_rmse: 0.54079 |  0:29:21s
epoch 100| loss: 0.28887 | val_0_rmse: 0.5352  | val_1_rmse: 0.53382 |  0:29:39s
epoch 101| loss: 0.2889  | val_0_rmse: 0.53139 | val_1_rmse: 0.52945 |  0:29:57s
epoch 102| loss: 0.28747 | val_0_rmse: 0.54294 | val_1_rmse: 0.54068 |  0:30:14s
epoch 103| loss: 0.28804 | val_0_rmse: 0.53061 | val_1_rmse: 0.52865 |  0:30:32s
epoch 104| loss: 0.29001 | val_0_rmse: 0.54451 | val_1_rmse: 0.54282 |  0:30:49s
epoch 105| loss: 0.28867 | val_0_rmse: 0.55157 | val_1_rmse: 0.5495  |  0:31:07s
epoch 106| loss: 0.28704 | val_0_rmse: 0.53245 | val_1_rmse: 0.53096 |  0:31:25s
epoch 107| loss: 0.29262 | val_0_rmse: 0.53333 | val_1_rmse: 0.53132 |  0:31:42s
epoch 108| loss: 0.28742 | val_0_rmse: 0.53192 | val_1_rmse: 0.53025 |  0:32:00s
epoch 109| loss: 0.29094 | val_0_rmse: 0.54296 | val_1_rmse: 0.54061 |  0:32:18s
epoch 110| loss: 0.28782 | val_0_rmse: 0.54973 | val_1_rmse: 0.54866 |  0:32:35s
epoch 111| loss: 0.28779 | val_0_rmse: 0.5449  | val_1_rmse: 0.54377 |  0:32:53s
epoch 112| loss: 0.28726 | val_0_rmse: 0.53919 | val_1_rmse: 0.53766 |  0:33:11s
epoch 113| loss: 0.29842 | val_0_rmse: 0.53911 | val_1_rmse: 0.53694 |  0:33:28s
epoch 114| loss: 0.29506 | val_0_rmse: 0.54242 | val_1_rmse: 0.54107 |  0:33:46s
epoch 115| loss: 0.29599 | val_0_rmse: 0.53759 | val_1_rmse: 0.5354  |  0:34:03s
epoch 116| loss: 0.29234 | val_0_rmse: 0.53622 | val_1_rmse: 0.53431 |  0:34:21s
epoch 117| loss: 0.2909  | val_0_rmse: 0.5344  | val_1_rmse: 0.53252 |  0:34:39s
epoch 118| loss: 0.29147 | val_0_rmse: 0.53854 | val_1_rmse: 0.53655 |  0:34:56s
epoch 119| loss: 0.29008 | val_0_rmse: 0.53464 | val_1_rmse: 0.53271 |  0:35:14s
epoch 120| loss: 0.28985 | val_0_rmse: 0.54148 | val_1_rmse: 0.54008 |  0:35:31s

Early stopping occured at epoch 120 with best_epoch = 90 and best_val_1_rmse = 0.52864
Best weights from best epoch are automatically used!
ended training at: 03:16:40
Feature importance:
[('Area', 0.09609744272091385), ('Baths', 0.23139711992604267), ('Beds', 0.142913387871316), ('Latitude', 0.3853543622528502), ('Longitude', 0.09924245237277264), ('Month', 0.0), ('Year', 0.04499523485610468)]
Mean squared error is of 11187280781.816359
Mean absolute error:65989.91218740291
MAPE:0.42137022836120014
R2 score:0.7184797246130239
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:16:42
epoch 0  | loss: 0.38478 | val_0_rmse: 0.59624 | val_1_rmse: 0.5993  |  0:00:17s
epoch 1  | loss: 0.32993 | val_0_rmse: 0.56682 | val_1_rmse: 0.5673  |  0:00:35s
epoch 2  | loss: 0.32419 | val_0_rmse: 0.5585  | val_1_rmse: 0.56152 |  0:00:53s
epoch 3  | loss: 0.31482 | val_0_rmse: 0.59537 | val_1_rmse: 0.59719 |  0:01:10s
epoch 4  | loss: 0.311   | val_0_rmse: 0.54733 | val_1_rmse: 0.54766 |  0:01:28s
epoch 5  | loss: 0.30569 | val_0_rmse: 0.54915 | val_1_rmse: 0.551   |  0:01:45s
epoch 6  | loss: 0.3044  | val_0_rmse: 0.55543 | val_1_rmse: 0.55743 |  0:02:03s
epoch 7  | loss: 0.31333 | val_0_rmse: 0.55067 | val_1_rmse: 0.55036 |  0:02:21s
epoch 8  | loss: 0.3025  | val_0_rmse: 0.55463 | val_1_rmse: 0.55636 |  0:02:38s
epoch 9  | loss: 0.30129 | val_0_rmse: 0.5492  | val_1_rmse: 0.54995 |  0:02:56s
epoch 10 | loss: 0.30236 | val_0_rmse: 0.62077 | val_1_rmse: 0.62336 |  0:03:14s
epoch 11 | loss: 0.29827 | val_0_rmse: 0.5487  | val_1_rmse: 0.55058 |  0:03:31s
epoch 12 | loss: 0.29878 | val_0_rmse: 0.55308 | val_1_rmse: 0.55483 |  0:03:49s
epoch 13 | loss: 0.29753 | val_0_rmse: 0.54147 | val_1_rmse: 0.54291 |  0:04:06s
epoch 14 | loss: 0.29699 | val_0_rmse: 0.55533 | val_1_rmse: 0.55707 |  0:04:24s
epoch 15 | loss: 0.29553 | val_0_rmse: 0.53542 | val_1_rmse: 0.53684 |  0:04:42s
epoch 16 | loss: 0.29498 | val_0_rmse: 0.55294 | val_1_rmse: 0.55473 |  0:04:59s
epoch 17 | loss: 0.29691 | val_0_rmse: 0.54563 | val_1_rmse: 0.54729 |  0:05:17s
epoch 18 | loss: 0.2948  | val_0_rmse: 0.55963 | val_1_rmse: 0.56128 |  0:05:34s
epoch 19 | loss: 0.29432 | val_0_rmse: 0.56407 | val_1_rmse: 0.56514 |  0:05:52s
epoch 20 | loss: 0.29597 | val_0_rmse: 0.54484 | val_1_rmse: 0.54647 |  0:06:10s
epoch 21 | loss: 0.29471 | val_0_rmse: 0.54189 | val_1_rmse: 0.54432 |  0:06:27s
epoch 22 | loss: 0.29428 | val_0_rmse: 0.53534 | val_1_rmse: 0.53777 |  0:06:45s
epoch 23 | loss: 0.29481 | val_0_rmse: 0.54335 | val_1_rmse: 0.54509 |  0:07:03s
epoch 24 | loss: 0.29622 | val_0_rmse: 0.59695 | val_1_rmse: 0.59872 |  0:07:20s
epoch 25 | loss: 0.29379 | val_0_rmse: 0.59011 | val_1_rmse: 0.59194 |  0:07:38s
epoch 26 | loss: 0.29497 | val_0_rmse: 0.53643 | val_1_rmse: 0.53818 |  0:07:55s
epoch 27 | loss: 0.29533 | val_0_rmse: 0.54366 | val_1_rmse: 0.54564 |  0:08:13s
epoch 28 | loss: 0.29084 | val_0_rmse: 0.55109 | val_1_rmse: 0.5522  |  0:08:31s
epoch 29 | loss: 0.29496 | val_0_rmse: 0.5414  | val_1_rmse: 0.54337 |  0:08:48s
epoch 30 | loss: 0.29155 | val_0_rmse: 0.53602 | val_1_rmse: 0.53748 |  0:09:06s
epoch 31 | loss: 0.29249 | val_0_rmse: 0.54992 | val_1_rmse: 0.55187 |  0:09:23s
epoch 32 | loss: 0.29568 | val_0_rmse: 0.54809 | val_1_rmse: 0.54917 |  0:09:41s
epoch 33 | loss: 0.29195 | val_0_rmse: 0.53351 | val_1_rmse: 0.53522 |  0:09:59s
epoch 34 | loss: 0.2942  | val_0_rmse: 0.53327 | val_1_rmse: 0.53459 |  0:10:16s
epoch 35 | loss: 0.29077 | val_0_rmse: 0.53576 | val_1_rmse: 0.53793 |  0:10:34s
epoch 36 | loss: 0.29176 | val_0_rmse: 0.53123 | val_1_rmse: 0.53329 |  0:10:51s
epoch 37 | loss: 0.29016 | val_0_rmse: 0.54209 | val_1_rmse: 0.5437  |  0:11:09s
epoch 38 | loss: 0.29216 | val_0_rmse: 0.54307 | val_1_rmse: 0.54358 |  0:11:27s
epoch 39 | loss: 0.2939  | val_0_rmse: 0.54015 | val_1_rmse: 0.54225 |  0:11:45s
epoch 40 | loss: 0.30677 | val_0_rmse: 0.57532 | val_1_rmse: 0.57814 |  0:12:02s
epoch 41 | loss: 0.35979 | val_0_rmse: 0.61254 | val_1_rmse: 0.61506 |  0:12:20s
epoch 42 | loss: 0.32623 | val_0_rmse: 0.55437 | val_1_rmse: 0.556   |  0:12:38s
epoch 43 | loss: 0.30833 | val_0_rmse: 0.54368 | val_1_rmse: 0.54484 |  0:12:55s
epoch 44 | loss: 0.29912 | val_0_rmse: 0.55233 | val_1_rmse: 0.55271 |  0:13:13s
epoch 45 | loss: 0.29646 | val_0_rmse: 0.5771  | val_1_rmse: 0.57878 |  0:13:30s
epoch 46 | loss: 0.29638 | val_0_rmse: 0.53796 | val_1_rmse: 0.53893 |  0:13:48s
epoch 47 | loss: 0.29293 | val_0_rmse: 0.53627 | val_1_rmse: 0.53761 |  0:14:06s
epoch 48 | loss: 0.29337 | val_0_rmse: 0.56516 | val_1_rmse: 0.56673 |  0:14:23s
epoch 49 | loss: 0.29252 | val_0_rmse: 0.53836 | val_1_rmse: 0.53994 |  0:14:41s
epoch 50 | loss: 0.29171 | val_0_rmse: 0.53136 | val_1_rmse: 0.53292 |  0:14:59s
epoch 51 | loss: 0.29122 | val_0_rmse: 0.55491 | val_1_rmse: 0.55625 |  0:15:16s
epoch 52 | loss: 0.28985 | val_0_rmse: 0.56707 | val_1_rmse: 0.56884 |  0:15:34s
epoch 53 | loss: 0.29079 | val_0_rmse: 0.53468 | val_1_rmse: 0.53576 |  0:15:51s
epoch 54 | loss: 0.29061 | val_0_rmse: 0.53642 | val_1_rmse: 0.53792 |  0:16:09s
epoch 55 | loss: 0.2903  | val_0_rmse: 0.53378 | val_1_rmse: 0.53522 |  0:16:27s
epoch 56 | loss: 0.28926 | val_0_rmse: 0.57937 | val_1_rmse: 0.58077 |  0:16:44s
epoch 57 | loss: 0.2901  | val_0_rmse: 0.54546 | val_1_rmse: 0.54755 |  0:17:02s
epoch 58 | loss: 0.28953 | val_0_rmse: 0.5487  | val_1_rmse: 0.55072 |  0:17:20s
epoch 59 | loss: 0.28821 | val_0_rmse: 0.56659 | val_1_rmse: 0.56868 |  0:17:37s
epoch 60 | loss: 0.28859 | val_0_rmse: 0.58377 | val_1_rmse: 0.58506 |  0:17:55s
epoch 61 | loss: 0.2892  | val_0_rmse: 0.56886 | val_1_rmse: 0.57012 |  0:18:12s
epoch 62 | loss: 0.2882  | val_0_rmse: 0.54513 | val_1_rmse: 0.54651 |  0:18:30s
epoch 63 | loss: 0.28738 | val_0_rmse: 0.575   | val_1_rmse: 0.57745 |  0:18:47s
epoch 64 | loss: 0.28651 | val_0_rmse: 0.56808 | val_1_rmse: 0.57048 |  0:19:05s
epoch 65 | loss: 0.28654 | val_0_rmse: 0.57582 | val_1_rmse: 0.57817 |  0:19:23s
epoch 66 | loss: 0.28627 | val_0_rmse: 0.53401 | val_1_rmse: 0.53538 |  0:19:40s
epoch 67 | loss: 0.28525 | val_0_rmse: 0.56743 | val_1_rmse: 0.56972 |  0:19:58s
epoch 68 | loss: 0.28599 | val_0_rmse: 0.53757 | val_1_rmse: 0.53939 |  0:20:15s
epoch 69 | loss: 0.28463 | val_0_rmse: 0.53623 | val_1_rmse: 0.53756 |  0:20:33s
epoch 70 | loss: 0.28591 | val_0_rmse: 0.54273 | val_1_rmse: 0.54558 |  0:20:51s
epoch 71 | loss: 0.28539 | val_0_rmse: 0.54839 | val_1_rmse: 0.55147 |  0:21:08s
epoch 72 | loss: 0.28348 | val_0_rmse: 0.5772  | val_1_rmse: 0.57914 |  0:21:26s
epoch 73 | loss: 0.286   | val_0_rmse: 0.52829 | val_1_rmse: 0.52988 |  0:21:43s
epoch 74 | loss: 0.2846  | val_0_rmse: 0.55721 | val_1_rmse: 0.56    |  0:22:01s
epoch 75 | loss: 0.28377 | val_0_rmse: 0.57774 | val_1_rmse: 0.57903 |  0:22:19s
epoch 76 | loss: 0.28414 | val_0_rmse: 0.53886 | val_1_rmse: 0.54014 |  0:22:36s
epoch 77 | loss: 0.28422 | val_0_rmse: 0.57808 | val_1_rmse: 0.58018 |  0:22:54s
epoch 78 | loss: 0.28622 | val_0_rmse: 0.57018 | val_1_rmse: 0.57204 |  0:23:12s
epoch 79 | loss: 0.28346 | val_0_rmse: 0.57659 | val_1_rmse: 0.57829 |  0:23:29s
epoch 80 | loss: 0.28549 | val_0_rmse: 0.56441 | val_1_rmse: 0.567   |  0:23:47s
epoch 81 | loss: 0.28374 | val_0_rmse: 0.53692 | val_1_rmse: 0.53998 |  0:24:04s
epoch 82 | loss: 0.28326 | val_0_rmse: 0.55859 | val_1_rmse: 0.56136 |  0:24:22s
epoch 83 | loss: 0.28334 | val_0_rmse: 0.56086 | val_1_rmse: 0.56381 |  0:24:40s
epoch 84 | loss: 0.28235 | val_0_rmse: 0.56077 | val_1_rmse: 0.56273 |  0:24:57s
epoch 85 | loss: 0.28355 | val_0_rmse: 0.56182 | val_1_rmse: 0.56431 |  0:25:15s
epoch 86 | loss: 0.28262 | val_0_rmse: 0.5519  | val_1_rmse: 0.55543 |  0:25:33s
epoch 87 | loss: 0.28391 | val_0_rmse: 0.56621 | val_1_rmse: 0.56667 |  0:25:50s
epoch 88 | loss: 0.28278 | val_0_rmse: 0.55423 | val_1_rmse: 0.55617 |  0:26:08s
epoch 89 | loss: 0.28255 | val_0_rmse: 0.56174 | val_1_rmse: 0.56313 |  0:26:25s
epoch 90 | loss: 0.28442 | val_0_rmse: 0.58002 | val_1_rmse: 0.58216 |  0:26:43s
epoch 91 | loss: 0.28234 | val_0_rmse: 0.59867 | val_1_rmse: 0.6006  |  0:27:01s
epoch 92 | loss: 0.28275 | val_0_rmse: 0.56627 | val_1_rmse: 0.56793 |  0:27:18s
epoch 93 | loss: 0.28234 | val_0_rmse: 0.54486 | val_1_rmse: 0.54626 |  0:27:36s
epoch 94 | loss: 0.28288 | val_0_rmse: 0.53522 | val_1_rmse: 0.53722 |  0:27:53s
epoch 95 | loss: 0.28262 | val_0_rmse: 0.56519 | val_1_rmse: 0.56808 |  0:28:11s
epoch 96 | loss: 0.28277 | val_0_rmse: 0.54139 | val_1_rmse: 0.54299 |  0:28:29s
epoch 97 | loss: 0.2819  | val_0_rmse: 0.55546 | val_1_rmse: 0.5585  |  0:28:46s
epoch 98 | loss: 0.28222 | val_0_rmse: 0.56243 | val_1_rmse: 0.56487 |  0:29:04s
epoch 99 | loss: 0.28169 | val_0_rmse: 0.53151 | val_1_rmse: 0.53392 |  0:29:22s
epoch 100| loss: 0.28218 | val_0_rmse: 0.57292 | val_1_rmse: 0.57483 |  0:29:39s
epoch 101| loss: 0.281   | val_0_rmse: 0.57848 | val_1_rmse: 0.58135 |  0:29:57s
epoch 102| loss: 0.2832  | val_0_rmse: 0.56514 | val_1_rmse: 0.56732 |  0:30:14s
epoch 103| loss: 0.28176 | val_0_rmse: 0.5677  | val_1_rmse: 0.57065 |  0:30:32s

Early stopping occured at epoch 103 with best_epoch = 73 and best_val_1_rmse = 0.52988
Best weights from best epoch are automatically used!
ended training at: 03:47:20
Feature importance:
[('Area', 0.09132914567552447), ('Baths', 0.0), ('Beds', 0.0), ('Latitude', 0.5012685186427325), ('Longitude', 0.057486633145044486), ('Month', 0.10948675922456992), ('Year', 0.24042894331212863)]
Mean squared error is of 10997093242.362782
Mean absolute error:64936.527861407296
MAPE:0.4126045280924393
R2 score:0.7187369930852523
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:47:22
epoch 0  | loss: 0.3687  | val_0_rmse: 0.58908 | val_1_rmse: 0.58948 |  0:00:17s
epoch 1  | loss: 0.31846 | val_0_rmse: 0.55379 | val_1_rmse: 0.55571 |  0:00:35s
epoch 2  | loss: 0.31447 | val_0_rmse: 0.55681 | val_1_rmse: 0.56054 |  0:00:53s
epoch 3  | loss: 0.30797 | val_0_rmse: 0.54154 | val_1_rmse: 0.54429 |  0:01:10s
epoch 4  | loss: 0.30933 | val_0_rmse: 0.56241 | val_1_rmse: 0.56461 |  0:01:28s
epoch 5  | loss: 0.30428 | val_0_rmse: 0.54548 | val_1_rmse: 0.54891 |  0:01:46s
epoch 6  | loss: 0.30349 | val_0_rmse: 0.54404 | val_1_rmse: 0.54646 |  0:02:03s
epoch 7  | loss: 0.29889 | val_0_rmse: 0.55574 | val_1_rmse: 0.55876 |  0:02:21s
epoch 8  | loss: 0.29783 | val_0_rmse: 0.54313 | val_1_rmse: 0.54514 |  0:02:38s
epoch 9  | loss: 0.29622 | val_0_rmse: 0.53772 | val_1_rmse: 0.54216 |  0:02:56s
epoch 10 | loss: 0.29577 | val_0_rmse: 0.53774 | val_1_rmse: 0.54084 |  0:03:14s
epoch 11 | loss: 0.29836 | val_0_rmse: 0.547   | val_1_rmse: 0.54885 |  0:03:31s
epoch 12 | loss: 0.2961  | val_0_rmse: 0.56816 | val_1_rmse: 0.57056 |  0:03:49s
epoch 13 | loss: 0.29405 | val_0_rmse: 0.53676 | val_1_rmse: 0.5393  |  0:04:07s
epoch 14 | loss: 0.30674 | val_0_rmse: 0.53973 | val_1_rmse: 0.54251 |  0:04:24s
epoch 15 | loss: 0.29745 | val_0_rmse: 0.53787 | val_1_rmse: 0.54092 |  0:04:42s
epoch 16 | loss: 0.29262 | val_0_rmse: 0.56644 | val_1_rmse: 0.56891 |  0:05:00s
epoch 17 | loss: 0.29219 | val_0_rmse: 0.54256 | val_1_rmse: 0.54468 |  0:05:17s
epoch 18 | loss: 0.2903  | val_0_rmse: 0.53769 | val_1_rmse: 0.54215 |  0:05:35s
epoch 19 | loss: 0.29123 | val_0_rmse: 0.546   | val_1_rmse: 0.54894 |  0:05:52s
epoch 20 | loss: 0.29123 | val_0_rmse: 0.53342 | val_1_rmse: 0.53734 |  0:06:10s
epoch 21 | loss: 0.29265 | val_0_rmse: 0.54588 | val_1_rmse: 0.54878 |  0:06:28s
epoch 22 | loss: 0.28972 | val_0_rmse: 0.54987 | val_1_rmse: 0.55157 |  0:06:45s
epoch 23 | loss: 0.28891 | val_0_rmse: 0.53503 | val_1_rmse: 0.53881 |  0:07:03s
epoch 24 | loss: 0.28872 | val_0_rmse: 0.71686 | val_1_rmse: 0.72057 |  0:07:20s
epoch 25 | loss: 0.28889 | val_0_rmse: 0.53463 | val_1_rmse: 0.53693 |  0:07:38s
epoch 26 | loss: 0.28938 | val_0_rmse: 0.53649 | val_1_rmse: 0.54131 |  0:07:56s
epoch 27 | loss: 0.28898 | val_0_rmse: 0.5496  | val_1_rmse: 0.55345 |  0:08:13s
epoch 28 | loss: 0.28912 | val_0_rmse: 0.55666 | val_1_rmse: 0.55941 |  0:08:31s
epoch 29 | loss: 0.29051 | val_0_rmse: 0.55473 | val_1_rmse: 0.55671 |  0:08:49s
epoch 30 | loss: 0.29026 | val_0_rmse: 0.55134 | val_1_rmse: 0.55346 |  0:09:06s
epoch 31 | loss: 0.28766 | val_0_rmse: 0.55123 | val_1_rmse: 0.5548  |  0:09:24s
epoch 32 | loss: 0.28863 | val_0_rmse: 0.54584 | val_1_rmse: 0.54993 |  0:09:41s
epoch 33 | loss: 0.28879 | val_0_rmse: 0.52991 | val_1_rmse: 0.53317 |  0:09:59s
epoch 34 | loss: 0.2881  | val_0_rmse: 0.55151 | val_1_rmse: 0.55554 |  0:10:17s
epoch 35 | loss: 0.28604 | val_0_rmse: 0.53247 | val_1_rmse: 0.53665 |  0:10:34s
epoch 36 | loss: 0.28678 | val_0_rmse: 0.53336 | val_1_rmse: 0.53867 |  0:10:52s
epoch 37 | loss: 0.28671 | val_0_rmse: 0.53326 | val_1_rmse: 0.53828 |  0:11:10s
epoch 38 | loss: 0.28573 | val_0_rmse: 0.53574 | val_1_rmse: 0.53902 |  0:11:27s
epoch 39 | loss: 0.28545 | val_0_rmse: 0.53969 | val_1_rmse: 0.54365 |  0:11:45s
epoch 40 | loss: 0.28517 | val_0_rmse: 0.53137 | val_1_rmse: 0.53512 |  0:12:03s
epoch 41 | loss: 0.28674 | val_0_rmse: 0.5374  | val_1_rmse: 0.54214 |  0:12:20s
epoch 42 | loss: 0.29858 | val_0_rmse: 0.54117 | val_1_rmse: 0.54283 |  0:12:38s
epoch 43 | loss: 0.29165 | val_0_rmse: 0.54398 | val_1_rmse: 0.5454  |  0:12:56s
epoch 44 | loss: 0.28864 | val_0_rmse: 0.5374  | val_1_rmse: 0.53909 |  0:13:13s
epoch 45 | loss: 0.28898 | val_0_rmse: 0.54756 | val_1_rmse: 0.55014 |  0:13:31s
epoch 46 | loss: 0.28726 | val_0_rmse: 0.54535 | val_1_rmse: 0.5486  |  0:13:49s
epoch 47 | loss: 0.29401 | val_0_rmse: 0.53591 | val_1_rmse: 0.53834 |  0:14:06s
epoch 48 | loss: 0.28701 | val_0_rmse: 0.54778 | val_1_rmse: 0.55371 |  0:14:24s
epoch 49 | loss: 0.28565 | val_0_rmse: 0.53489 | val_1_rmse: 0.5383  |  0:14:42s
epoch 50 | loss: 0.28464 | val_0_rmse: 0.54462 | val_1_rmse: 0.54871 |  0:14:59s
epoch 51 | loss: 0.28445 | val_0_rmse: 0.53372 | val_1_rmse: 0.53652 |  0:15:17s
epoch 52 | loss: 0.28388 | val_0_rmse: 0.53    | val_1_rmse: 0.53514 |  0:15:34s
epoch 53 | loss: 0.28379 | val_0_rmse: 0.53259 | val_1_rmse: 0.5347  |  0:15:52s
epoch 54 | loss: 0.28448 | val_0_rmse: 0.53394 | val_1_rmse: 0.53794 |  0:16:10s
epoch 55 | loss: 0.28865 | val_0_rmse: 0.52929 | val_1_rmse: 0.5321  |  0:16:27s
epoch 56 | loss: 0.28342 | val_0_rmse: 0.53872 | val_1_rmse: 0.5407  |  0:16:45s
epoch 57 | loss: 0.28346 | val_0_rmse: 0.52997 | val_1_rmse: 0.53347 |  0:17:02s
epoch 58 | loss: 0.28296 | val_0_rmse: 0.53059 | val_1_rmse: 0.53405 |  0:17:20s
epoch 59 | loss: 0.28305 | val_0_rmse: 0.53512 | val_1_rmse: 0.53825 |  0:17:38s
epoch 60 | loss: 0.28234 | val_0_rmse: 0.52864 | val_1_rmse: 0.53286 |  0:17:55s
epoch 61 | loss: 0.28229 | val_0_rmse: 0.53167 | val_1_rmse: 0.53377 |  0:18:13s
epoch 62 | loss: 0.28255 | val_0_rmse: 0.52917 | val_1_rmse: 0.53313 |  0:18:31s
epoch 63 | loss: 0.2819  | val_0_rmse: 0.52915 | val_1_rmse: 0.53266 |  0:18:48s
epoch 64 | loss: 0.28073 | val_0_rmse: 0.53335 | val_1_rmse: 0.53862 |  0:19:06s
epoch 65 | loss: 0.28283 | val_0_rmse: 0.53332 | val_1_rmse: 0.53752 |  0:19:24s
epoch 66 | loss: 0.28184 | val_0_rmse: 0.53269 | val_1_rmse: 0.53715 |  0:19:41s
epoch 67 | loss: 0.28547 | val_0_rmse: 0.54121 | val_1_rmse: 0.54474 |  0:19:59s
epoch 68 | loss: 0.28888 | val_0_rmse: 0.54847 | val_1_rmse: 0.55165 |  0:20:17s
epoch 69 | loss: 0.28288 | val_0_rmse: 0.55034 | val_1_rmse: 0.55372 |  0:20:34s
epoch 70 | loss: 0.28126 | val_0_rmse: 0.53337 | val_1_rmse: 0.53917 |  0:20:52s
epoch 71 | loss: 0.28243 | val_0_rmse: 0.53485 | val_1_rmse: 0.53865 |  0:21:09s
epoch 72 | loss: 0.28051 | val_0_rmse: 0.52675 | val_1_rmse: 0.53205 |  0:21:27s
epoch 73 | loss: 0.28    | val_0_rmse: 0.54818 | val_1_rmse: 0.55316 |  0:21:45s
epoch 74 | loss: 0.28615 | val_0_rmse: 0.52653 | val_1_rmse: 0.53171 |  0:22:02s
epoch 75 | loss: 0.28368 | val_0_rmse: 0.53033 | val_1_rmse: 0.53358 |  0:22:20s
epoch 76 | loss: 0.28329 | val_0_rmse: 0.53712 | val_1_rmse: 0.5418  |  0:22:37s
epoch 77 | loss: 0.28162 | val_0_rmse: 0.5288  | val_1_rmse: 0.5332  |  0:22:55s
epoch 78 | loss: 0.28213 | val_0_rmse: 0.53556 | val_1_rmse: 0.53953 |  0:23:13s
epoch 79 | loss: 0.28178 | val_0_rmse: 0.53088 | val_1_rmse: 0.53504 |  0:23:30s
epoch 80 | loss: 0.28123 | val_0_rmse: 0.53664 | val_1_rmse: 0.54092 |  0:23:48s
epoch 81 | loss: 0.28025 | val_0_rmse: 0.52938 | val_1_rmse: 0.5352  |  0:24:06s
epoch 82 | loss: 0.28012 | val_0_rmse: 0.53619 | val_1_rmse: 0.5393  |  0:24:23s
epoch 83 | loss: 0.28065 | val_0_rmse: 0.54472 | val_1_rmse: 0.5486  |  0:24:41s
epoch 84 | loss: 0.28019 | val_0_rmse: 0.52761 | val_1_rmse: 0.53205 |  0:24:58s
epoch 85 | loss: 0.28021 | val_0_rmse: 0.53249 | val_1_rmse: 0.53643 |  0:25:16s
epoch 86 | loss: 0.28099 | val_0_rmse: 0.53463 | val_1_rmse: 0.53935 |  0:25:34s
epoch 87 | loss: 0.29535 | val_0_rmse: 0.54763 | val_1_rmse: 0.55204 |  0:25:51s
epoch 88 | loss: 0.28315 | val_0_rmse: 0.5411  | val_1_rmse: 0.54626 |  0:26:09s
epoch 89 | loss: 0.28067 | val_0_rmse: 0.52834 | val_1_rmse: 0.53371 |  0:26:27s
epoch 90 | loss: 0.27991 | val_0_rmse: 0.5388  | val_1_rmse: 0.54449 |  0:26:44s
epoch 91 | loss: 0.28444 | val_0_rmse: 0.53278 | val_1_rmse: 0.53536 |  0:27:02s
epoch 92 | loss: 0.28185 | val_0_rmse: 0.52823 | val_1_rmse: 0.53256 |  0:27:20s
epoch 93 | loss: 0.27929 | val_0_rmse: 0.52545 | val_1_rmse: 0.53003 |  0:27:37s
epoch 94 | loss: 0.27998 | val_0_rmse: 0.53092 | val_1_rmse: 0.53429 |  0:27:55s
epoch 95 | loss: 0.28117 | val_0_rmse: 0.52799 | val_1_rmse: 0.53406 |  0:28:13s
epoch 96 | loss: 0.27977 | val_0_rmse: 0.53648 | val_1_rmse: 0.54071 |  0:28:30s
epoch 97 | loss: 0.27857 | val_0_rmse: 0.52801 | val_1_rmse: 0.53323 |  0:28:48s
epoch 98 | loss: 0.27984 | val_0_rmse: 0.5251  | val_1_rmse: 0.53047 |  0:29:06s
epoch 99 | loss: 0.279   | val_0_rmse: 0.53307 | val_1_rmse: 0.53696 |  0:29:23s
epoch 100| loss: 0.27965 | val_0_rmse: 0.54011 | val_1_rmse: 0.54575 |  0:29:41s
epoch 101| loss: 0.27912 | val_0_rmse: 0.52876 | val_1_rmse: 0.53335 |  0:29:58s
epoch 102| loss: 0.2788  | val_0_rmse: 0.53343 | val_1_rmse: 0.53666 |  0:30:16s
epoch 103| loss: 0.28098 | val_0_rmse: 0.54261 | val_1_rmse: 0.54877 |  0:30:34s
epoch 104| loss: 0.27994 | val_0_rmse: 0.5285  | val_1_rmse: 0.53384 |  0:30:51s
epoch 105| loss: 0.2807  | val_0_rmse: 0.53361 | val_1_rmse: 0.53827 |  0:31:09s
epoch 106| loss: 0.27907 | val_0_rmse: 0.52937 | val_1_rmse: 0.53345 |  0:31:27s
epoch 107| loss: 0.27781 | val_0_rmse: 0.52258 | val_1_rmse: 0.52858 |  0:31:44s
epoch 108| loss: 0.27844 | val_0_rmse: 0.52811 | val_1_rmse: 0.53432 |  0:32:02s
epoch 109| loss: 0.27908 | val_0_rmse: 0.52817 | val_1_rmse: 0.53557 |  0:32:20s
epoch 110| loss: 0.27852 | val_0_rmse: 0.52379 | val_1_rmse: 0.52935 |  0:32:37s
epoch 111| loss: 0.27868 | val_0_rmse: 0.52756 | val_1_rmse: 0.53296 |  0:32:55s
epoch 112| loss: 0.27742 | val_0_rmse: 0.52591 | val_1_rmse: 0.53113 |  0:33:12s
epoch 113| loss: 0.27926 | val_0_rmse: 0.52771 | val_1_rmse: 0.53296 |  0:33:30s
epoch 114| loss: 0.27826 | val_0_rmse: 0.52668 | val_1_rmse: 0.53176 |  0:33:48s
epoch 115| loss: 0.28813 | val_0_rmse: 0.52782 | val_1_rmse: 0.53234 |  0:34:05s
epoch 116| loss: 0.28121 | val_0_rmse: 0.53458 | val_1_rmse: 0.54111 |  0:34:23s
epoch 117| loss: 0.2788  | val_0_rmse: 0.52772 | val_1_rmse: 0.5343  |  0:34:41s
epoch 118| loss: 0.27818 | val_0_rmse: 0.53626 | val_1_rmse: 0.54007 |  0:34:59s
epoch 119| loss: 0.27854 | val_0_rmse: 0.52453 | val_1_rmse: 0.52994 |  0:35:18s
epoch 120| loss: 0.278   | val_0_rmse: 0.53515 | val_1_rmse: 0.53953 |  0:35:37s
epoch 121| loss: 0.27745 | val_0_rmse: 0.53089 | val_1_rmse: 0.53584 |  0:35:56s
epoch 122| loss: 0.2773  | val_0_rmse: 0.52826 | val_1_rmse: 0.53363 |  0:36:13s
epoch 123| loss: 0.28006 | val_0_rmse: 0.53152 | val_1_rmse: 0.53563 |  0:36:31s
epoch 124| loss: 0.27829 | val_0_rmse: 0.52979 | val_1_rmse: 0.53363 |  0:36:48s
epoch 125| loss: 0.27828 | val_0_rmse: 0.53289 | val_1_rmse: 0.53732 |  0:37:06s
epoch 126| loss: 0.27773 | val_0_rmse: 0.53142 | val_1_rmse: 0.53702 |  0:37:24s
epoch 127| loss: 0.2777  | val_0_rmse: 0.52154 | val_1_rmse: 0.52791 |  0:37:41s
epoch 128| loss: 0.27909 | val_0_rmse: 0.52658 | val_1_rmse: 0.5324  |  0:37:59s
epoch 129| loss: 0.2775  | val_0_rmse: 0.53832 | val_1_rmse: 0.54381 |  0:38:17s
epoch 130| loss: 0.28119 | val_0_rmse: 0.52971 | val_1_rmse: 0.53287 |  0:38:34s
epoch 131| loss: 0.27859 | val_0_rmse: 0.53001 | val_1_rmse: 0.53567 |  0:38:52s
epoch 132| loss: 0.27678 | val_0_rmse: 0.52702 | val_1_rmse: 0.53059 |  0:39:09s
epoch 133| loss: 0.2774  | val_0_rmse: 0.52489 | val_1_rmse: 0.53007 |  0:39:27s
epoch 134| loss: 0.2773  | val_0_rmse: 0.52495 | val_1_rmse: 0.5313  |  0:39:44s
epoch 135| loss: 0.27763 | val_0_rmse: 0.53272 | val_1_rmse: 0.5383  |  0:40:02s
epoch 136| loss: 0.27682 | val_0_rmse: 0.52905 | val_1_rmse: 0.53637 |  0:40:20s
epoch 137| loss: 0.27734 | val_0_rmse: 0.52579 | val_1_rmse: 0.53179 |  0:40:38s
epoch 138| loss: 0.27664 | val_0_rmse: 0.52584 | val_1_rmse: 0.53126 |  0:40:55s
epoch 139| loss: 0.27739 | val_0_rmse: 0.53571 | val_1_rmse: 0.5409  |  0:41:13s
epoch 140| loss: 0.2775  | val_0_rmse: 0.54898 | val_1_rmse: 0.55564 |  0:41:31s
epoch 141| loss: 0.27759 | val_0_rmse: 0.60216 | val_1_rmse: 0.60797 |  0:41:48s
epoch 142| loss: 0.27764 | val_0_rmse: 0.52531 | val_1_rmse: 0.53079 |  0:42:06s
epoch 143| loss: 0.2773  | val_0_rmse: 0.52274 | val_1_rmse: 0.52881 |  0:42:23s
epoch 144| loss: 0.27647 | val_0_rmse: 0.52149 | val_1_rmse: 0.52794 |  0:42:41s
epoch 145| loss: 0.27638 | val_0_rmse: 0.52977 | val_1_rmse: 0.53531 |  0:42:59s
epoch 146| loss: 0.27609 | val_0_rmse: 0.52638 | val_1_rmse: 0.53073 |  0:43:16s
epoch 147| loss: 0.27697 | val_0_rmse: 0.52862 | val_1_rmse: 0.5352  |  0:43:34s
epoch 148| loss: 0.27626 | val_0_rmse: 0.52833 | val_1_rmse: 0.53424 |  0:43:51s
epoch 149| loss: 0.28744 | val_0_rmse: 0.53605 | val_1_rmse: 0.53969 |  0:44:09s
Stop training because you reached max_epochs = 150 with best_epoch = 127 and best_val_1_rmse = 0.52791
Best weights from best epoch are automatically used!
ended training at: 04:31:37
Feature importance:
[('Area', 0.23475942336161268), ('Baths', 0.06032622918244331), ('Beds', 0.0), ('Latitude', 0.31924879962874336), ('Longitude', 0.3856655478272007), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 10914548296.044079
Mean absolute error:64115.52662993129
MAPE:0.4189040677003241
R2 score:0.7231637653909556
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:31:39
epoch 0  | loss: 0.38502 | val_0_rmse: 0.64499 | val_1_rmse: 0.64556 |  0:00:17s
epoch 1  | loss: 0.33735 | val_0_rmse: 0.57574 | val_1_rmse: 0.57517 |  0:00:35s
epoch 2  | loss: 0.33518 | val_0_rmse: 0.58701 | val_1_rmse: 0.58676 |  0:00:53s
epoch 3  | loss: 0.33086 | val_0_rmse: 0.57289 | val_1_rmse: 0.57354 |  0:01:10s
epoch 4  | loss: 0.3242  | val_0_rmse: 0.55851 | val_1_rmse: 0.55844 |  0:01:28s
epoch 5  | loss: 0.3275  | val_0_rmse: 0.58863 | val_1_rmse: 0.58885 |  0:01:46s
epoch 6  | loss: 0.32916 | val_0_rmse: 0.63979 | val_1_rmse: 0.63919 |  0:02:03s
epoch 7  | loss: 0.32075 | val_0_rmse: 0.558   | val_1_rmse: 0.55844 |  0:02:21s
epoch 8  | loss: 0.31592 | val_0_rmse: 0.60611 | val_1_rmse: 0.60622 |  0:02:39s
epoch 9  | loss: 0.32737 | val_0_rmse: 0.61472 | val_1_rmse: 0.61705 |  0:02:56s
epoch 10 | loss: 0.31699 | val_0_rmse: 0.56045 | val_1_rmse: 0.56038 |  0:03:14s
epoch 11 | loss: 0.31531 | val_0_rmse: 0.57885 | val_1_rmse: 0.57855 |  0:03:32s
epoch 12 | loss: 0.30771 | val_0_rmse: 0.55281 | val_1_rmse: 0.55129 |  0:03:49s
epoch 13 | loss: 0.30772 | val_0_rmse: 0.5417  | val_1_rmse: 0.54045 |  0:04:07s
epoch 14 | loss: 0.30488 | val_0_rmse: 0.5457  | val_1_rmse: 0.5444  |  0:04:24s
epoch 15 | loss: 0.30772 | val_0_rmse: 0.64042 | val_1_rmse: 0.64221 |  0:04:42s
epoch 16 | loss: 0.33058 | val_0_rmse: 0.60195 | val_1_rmse: 0.60437 |  0:05:00s
epoch 17 | loss: 0.3239  | val_0_rmse: 0.56465 | val_1_rmse: 0.56565 |  0:05:17s
epoch 18 | loss: 0.32288 | val_0_rmse: 0.59503 | val_1_rmse: 0.59772 |  0:05:35s
epoch 19 | loss: 0.34359 | val_0_rmse: 0.60762 | val_1_rmse: 0.60915 |  0:05:53s
epoch 20 | loss: 0.34912 | val_0_rmse: 0.58466 | val_1_rmse: 0.58417 |  0:06:10s
epoch 21 | loss: 0.32425 | val_0_rmse: 0.5653  | val_1_rmse: 0.56391 |  0:06:28s
epoch 22 | loss: 0.31298 | val_0_rmse: 0.5699  | val_1_rmse: 0.56536 |  0:06:45s
epoch 23 | loss: 0.3129  | val_0_rmse: 0.5595  | val_1_rmse: 0.55835 |  0:07:03s
epoch 24 | loss: 0.30698 | val_0_rmse: 0.54344 | val_1_rmse: 0.541   |  0:07:21s
epoch 25 | loss: 0.31443 | val_0_rmse: 0.57424 | val_1_rmse: 0.57349 |  0:07:38s
epoch 26 | loss: 0.31444 | val_0_rmse: 0.55885 | val_1_rmse: 0.5578  |  0:07:56s
epoch 27 | loss: 0.3133  | val_0_rmse: 0.55483 | val_1_rmse: 0.55204 |  0:08:13s
epoch 28 | loss: 0.31703 | val_0_rmse: 0.55181 | val_1_rmse: 0.5497  |  0:08:31s
epoch 29 | loss: 0.30589 | val_0_rmse: 0.54843 | val_1_rmse: 0.54664 |  0:08:49s
epoch 30 | loss: 0.3079  | val_0_rmse: 0.56514 | val_1_rmse: 0.56352 |  0:09:06s
epoch 31 | loss: 0.30368 | val_0_rmse: 0.54261 | val_1_rmse: 0.54063 |  0:09:24s
epoch 32 | loss: 0.30663 | val_0_rmse: 0.5451  | val_1_rmse: 0.54266 |  0:09:42s
epoch 33 | loss: 0.30195 | val_0_rmse: 0.53892 | val_1_rmse: 0.53671 |  0:10:01s
epoch 34 | loss: 0.29974 | val_0_rmse: 0.53731 | val_1_rmse: 0.53595 |  0:10:19s
epoch 35 | loss: 0.29838 | val_0_rmse: 0.5392  | val_1_rmse: 0.53753 |  0:10:37s
epoch 36 | loss: 0.29782 | val_0_rmse: 0.56195 | val_1_rmse: 0.55823 |  0:10:54s
epoch 37 | loss: 0.29527 | val_0_rmse: 0.5355  | val_1_rmse: 0.53393 |  0:11:12s
epoch 38 | loss: 0.29688 | val_0_rmse: 0.5351  | val_1_rmse: 0.53326 |  0:11:30s
epoch 39 | loss: 0.29663 | val_0_rmse: 0.53514 | val_1_rmse: 0.53369 |  0:11:47s
epoch 40 | loss: 0.29648 | val_0_rmse: 0.53892 | val_1_rmse: 0.53852 |  0:12:05s
epoch 41 | loss: 0.29851 | val_0_rmse: 0.57189 | val_1_rmse: 0.57058 |  0:12:23s
epoch 42 | loss: 0.29562 | val_0_rmse: 0.53437 | val_1_rmse: 0.53206 |  0:12:40s
epoch 43 | loss: 0.29376 | val_0_rmse: 0.54978 | val_1_rmse: 0.54942 |  0:12:58s
epoch 44 | loss: 0.29265 | val_0_rmse: 0.57929 | val_1_rmse: 0.5782  |  0:13:15s
epoch 45 | loss: 0.29299 | val_0_rmse: 0.54365 | val_1_rmse: 0.54268 |  0:13:33s
epoch 46 | loss: 0.29332 | val_0_rmse: 0.53329 | val_1_rmse: 0.5311  |  0:13:51s
epoch 47 | loss: 0.29294 | val_0_rmse: 0.53739 | val_1_rmse: 0.53617 |  0:14:08s
epoch 48 | loss: 0.29201 | val_0_rmse: 0.54043 | val_1_rmse: 0.53883 |  0:14:26s
epoch 49 | loss: 0.29232 | val_0_rmse: 0.5337  | val_1_rmse: 0.53096 |  0:14:44s
epoch 50 | loss: 0.29142 | val_0_rmse: 0.54735 | val_1_rmse: 0.54511 |  0:15:01s
epoch 51 | loss: 0.29135 | val_0_rmse: 0.53614 | val_1_rmse: 0.53341 |  0:15:19s
epoch 52 | loss: 0.29262 | val_0_rmse: 0.53772 | val_1_rmse: 0.53759 |  0:15:36s
epoch 53 | loss: 0.29081 | val_0_rmse: 0.53686 | val_1_rmse: 0.5344  |  0:15:54s
epoch 54 | loss: 0.29227 | val_0_rmse: 0.53329 | val_1_rmse: 0.53158 |  0:16:12s
epoch 55 | loss: 0.29163 | val_0_rmse: 0.53401 | val_1_rmse: 0.53192 |  0:16:29s
epoch 56 | loss: 0.29026 | val_0_rmse: 0.54742 | val_1_rmse: 0.54602 |  0:16:47s
epoch 57 | loss: 0.29076 | val_0_rmse: 0.54178 | val_1_rmse: 0.53967 |  0:17:05s
epoch 58 | loss: 0.29212 | val_0_rmse: 0.53054 | val_1_rmse: 0.52939 |  0:17:22s
epoch 59 | loss: 0.29198 | val_0_rmse: 0.54295 | val_1_rmse: 0.54297 |  0:17:40s
epoch 60 | loss: 0.29017 | val_0_rmse: 0.54082 | val_1_rmse: 0.53868 |  0:17:58s
epoch 61 | loss: 0.29003 | val_0_rmse: 0.53651 | val_1_rmse: 0.53513 |  0:18:15s
epoch 62 | loss: 0.28998 | val_0_rmse: 0.54827 | val_1_rmse: 0.54647 |  0:18:33s
epoch 63 | loss: 0.29013 | val_0_rmse: 0.532   | val_1_rmse: 0.53118 |  0:18:50s
epoch 64 | loss: 0.28826 | val_0_rmse: 0.5366  | val_1_rmse: 0.53594 |  0:19:08s
epoch 65 | loss: 0.29003 | val_0_rmse: 0.53179 | val_1_rmse: 0.53006 |  0:19:26s
epoch 66 | loss: 0.28901 | val_0_rmse: 0.53613 | val_1_rmse: 0.53469 |  0:19:43s
epoch 67 | loss: 0.28935 | val_0_rmse: 0.54397 | val_1_rmse: 0.54316 |  0:20:01s
epoch 68 | loss: 0.28919 | val_0_rmse: 0.5395  | val_1_rmse: 0.53921 |  0:20:19s
epoch 69 | loss: 0.28857 | val_0_rmse: 0.5355  | val_1_rmse: 0.53374 |  0:20:36s
epoch 70 | loss: 0.28834 | val_0_rmse: 0.53493 | val_1_rmse: 0.53428 |  0:20:54s
epoch 71 | loss: 0.28869 | val_0_rmse: 0.52886 | val_1_rmse: 0.52792 |  0:21:12s
epoch 72 | loss: 0.28903 | val_0_rmse: 0.53481 | val_1_rmse: 0.53448 |  0:21:29s
epoch 73 | loss: 0.28779 | val_0_rmse: 0.53321 | val_1_rmse: 0.533   |  0:21:47s
epoch 74 | loss: 0.28882 | val_0_rmse: 0.53308 | val_1_rmse: 0.53175 |  0:22:04s
epoch 75 | loss: 0.28809 | val_0_rmse: 0.53209 | val_1_rmse: 0.53081 |  0:22:22s
epoch 76 | loss: 0.28759 | val_0_rmse: 0.53152 | val_1_rmse: 0.53156 |  0:22:40s
epoch 77 | loss: 0.28683 | val_0_rmse: 0.52973 | val_1_rmse: 0.52977 |  0:22:57s
epoch 78 | loss: 0.28725 | val_0_rmse: 0.5314  | val_1_rmse: 0.5312  |  0:23:15s
epoch 79 | loss: 0.28756 | val_0_rmse: 0.54001 | val_1_rmse: 0.53871 |  0:23:33s
epoch 80 | loss: 0.29377 | val_0_rmse: 0.53406 | val_1_rmse: 0.53265 |  0:23:50s
epoch 81 | loss: 0.28772 | val_0_rmse: 0.53953 | val_1_rmse: 0.53959 |  0:24:08s
epoch 82 | loss: 0.28676 | val_0_rmse: 0.53557 | val_1_rmse: 0.53405 |  0:24:25s
epoch 83 | loss: 0.28703 | val_0_rmse: 0.53384 | val_1_rmse: 0.53293 |  0:24:43s
epoch 84 | loss: 0.28708 | val_0_rmse: 0.54064 | val_1_rmse: 0.54036 |  0:25:01s
epoch 85 | loss: 0.28777 | val_0_rmse: 0.53089 | val_1_rmse: 0.53066 |  0:25:18s
epoch 86 | loss: 0.28754 | val_0_rmse: 0.53878 | val_1_rmse: 0.53869 |  0:25:36s
epoch 87 | loss: 0.29104 | val_0_rmse: 0.54617 | val_1_rmse: 0.54505 |  0:25:54s
epoch 88 | loss: 0.28899 | val_0_rmse: 0.53514 | val_1_rmse: 0.53534 |  0:26:11s
epoch 89 | loss: 0.28689 | val_0_rmse: 0.53215 | val_1_rmse: 0.53177 |  0:26:29s
epoch 90 | loss: 0.28643 | val_0_rmse: 0.5292  | val_1_rmse: 0.52857 |  0:26:47s
epoch 91 | loss: 0.28582 | val_0_rmse: 0.53252 | val_1_rmse: 0.53187 |  0:27:04s
epoch 92 | loss: 0.28661 | val_0_rmse: 0.52921 | val_1_rmse: 0.5286  |  0:27:22s
epoch 93 | loss: 0.28656 | val_0_rmse: 0.52811 | val_1_rmse: 0.52783 |  0:27:39s
epoch 94 | loss: 0.28545 | val_0_rmse: 0.52949 | val_1_rmse: 0.52865 |  0:27:57s
epoch 95 | loss: 0.28559 | val_0_rmse: 0.53027 | val_1_rmse: 0.52987 |  0:28:15s
epoch 96 | loss: 0.28548 | val_0_rmse: 0.53277 | val_1_rmse: 0.53319 |  0:28:32s
epoch 97 | loss: 0.28602 | val_0_rmse: 0.53536 | val_1_rmse: 0.53491 |  0:28:50s
epoch 98 | loss: 0.28616 | val_0_rmse: 0.52859 | val_1_rmse: 0.52862 |  0:29:07s
epoch 99 | loss: 0.28533 | val_0_rmse: 0.52752 | val_1_rmse: 0.52758 |  0:29:25s
epoch 100| loss: 0.28488 | val_0_rmse: 0.5295  | val_1_rmse: 0.53048 |  0:29:43s
epoch 101| loss: 0.28561 | val_0_rmse: 0.58118 | val_1_rmse: 0.58141 |  0:30:01s
epoch 102| loss: 0.28557 | val_0_rmse: 0.52649 | val_1_rmse: 0.52689 |  0:30:18s
epoch 103| loss: 0.28665 | val_0_rmse: 0.52625 | val_1_rmse: 0.52592 |  0:30:36s
epoch 104| loss: 0.28496 | val_0_rmse: 0.53846 | val_1_rmse: 0.53858 |  0:30:53s
epoch 105| loss: 0.28485 | val_0_rmse: 0.52999 | val_1_rmse: 0.53002 |  0:31:11s
epoch 106| loss: 0.28521 | val_0_rmse: 0.53504 | val_1_rmse: 0.53471 |  0:31:29s
epoch 107| loss: 0.28496 | val_0_rmse: 0.52951 | val_1_rmse: 0.52961 |  0:31:46s
epoch 108| loss: 0.28602 | val_0_rmse: 0.53709 | val_1_rmse: 0.53756 |  0:32:04s
epoch 109| loss: 0.2847  | val_0_rmse: 0.53695 | val_1_rmse: 0.53689 |  0:32:22s
epoch 110| loss: 0.28572 | val_0_rmse: 0.52813 | val_1_rmse: 0.52764 |  0:32:39s
epoch 111| loss: 0.28493 | val_0_rmse: 0.53111 | val_1_rmse: 0.53187 |  0:32:57s
epoch 112| loss: 0.28365 | val_0_rmse: 0.53059 | val_1_rmse: 0.53061 |  0:33:15s
epoch 113| loss: 0.28499 | val_0_rmse: 0.5481  | val_1_rmse: 0.54872 |  0:33:32s
epoch 114| loss: 0.28473 | val_0_rmse: 0.52736 | val_1_rmse: 0.52781 |  0:33:50s
epoch 115| loss: 0.28415 | val_0_rmse: 0.52925 | val_1_rmse: 0.52926 |  0:34:08s
epoch 116| loss: 0.28414 | val_0_rmse: 0.5616  | val_1_rmse: 0.56195 |  0:34:25s
epoch 117| loss: 0.28433 | val_0_rmse: 0.56306 | val_1_rmse: 0.56386 |  0:34:43s
epoch 118| loss: 0.28371 | val_0_rmse: 0.53739 | val_1_rmse: 0.53847 |  0:35:00s
epoch 119| loss: 0.28432 | val_0_rmse: 0.55577 | val_1_rmse: 0.55666 |  0:35:18s
epoch 120| loss: 0.28309 | val_0_rmse: 0.5312  | val_1_rmse: 0.53239 |  0:35:35s
epoch 121| loss: 0.28372 | val_0_rmse: 0.52754 | val_1_rmse: 0.52806 |  0:35:53s
epoch 122| loss: 0.28332 | val_0_rmse: 0.53633 | val_1_rmse: 0.53679 |  0:36:11s
epoch 123| loss: 0.28347 | val_0_rmse: 0.54625 | val_1_rmse: 0.54758 |  0:36:28s
epoch 124| loss: 0.28439 | val_0_rmse: 0.53072 | val_1_rmse: 0.53109 |  0:36:46s
epoch 125| loss: 0.28367 | val_0_rmse: 0.53485 | val_1_rmse: 0.53569 |  0:37:04s
epoch 126| loss: 0.28379 | val_0_rmse: 0.55154 | val_1_rmse: 0.55265 |  0:37:21s
epoch 127| loss: 0.28319 | val_0_rmse: 0.54402 | val_1_rmse: 0.54599 |  0:37:39s
epoch 128| loss: 0.2837  | val_0_rmse: 0.53221 | val_1_rmse: 0.53241 |  0:37:57s
epoch 129| loss: 0.28385 | val_0_rmse: 0.55699 | val_1_rmse: 0.55741 |  0:38:14s
epoch 130| loss: 0.28389 | val_0_rmse: 0.52885 | val_1_rmse: 0.52937 |  0:38:32s
epoch 131| loss: 0.28375 | val_0_rmse: 0.52948 | val_1_rmse: 0.53013 |  0:38:49s
epoch 132| loss: 0.28291 | val_0_rmse: 0.54154 | val_1_rmse: 0.54263 |  0:39:07s
epoch 133| loss: 0.28337 | val_0_rmse: 0.53101 | val_1_rmse: 0.53185 |  0:39:25s

Early stopping occured at epoch 133 with best_epoch = 103 and best_val_1_rmse = 0.52592
Best weights from best epoch are automatically used!
ended training at: 05:11:10
Feature importance:
[('Area', 0.12202489383311929), ('Baths', 0.1858340723863536), ('Beds', 0.0), ('Latitude', 0.34502848059020624), ('Longitude', 0.0005917524044782538), ('Month', 0.0), ('Year', 0.3465208007858426)]
Mean squared error is of 10832254059.62877
Mean absolute error:65555.79494805101
MAPE:0.4293781192693572
R2 score:0.7225257918255963
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\SSNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
