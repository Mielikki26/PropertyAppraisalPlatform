TabNet Logs:

Saving copy of script...
In this script all datasets are decreased in size down to the size of the smallest dataset by sampling random rows and deleting them from the dataThis is done to test the possibility that the variance in datasets sizes is decreasing performanceBy evening out the sizes its excepted that the model achieves better performance
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:06:41
epoch 0  | loss: 0.88062 | val_0_rmse: 1.00224 | val_1_rmse: 0.99793 |  0:00:02s
epoch 1  | loss: 0.58353 | val_0_rmse: 0.75218 | val_1_rmse: 0.74746 |  0:00:03s
epoch 2  | loss: 0.51055 | val_0_rmse: 0.68023 | val_1_rmse: 0.68591 |  0:00:04s
epoch 3  | loss: 0.47025 | val_0_rmse: 0.65826 | val_1_rmse: 0.65522 |  0:00:05s
epoch 4  | loss: 0.44479 | val_0_rmse: 0.65268 | val_1_rmse: 0.65829 |  0:00:05s
epoch 5  | loss: 0.44177 | val_0_rmse: 0.65817 | val_1_rmse: 0.65913 |  0:00:06s
epoch 6  | loss: 0.43387 | val_0_rmse: 0.63189 | val_1_rmse: 0.63032 |  0:00:07s
epoch 7  | loss: 0.42319 | val_0_rmse: 0.63708 | val_1_rmse: 0.62912 |  0:00:08s
epoch 8  | loss: 0.41927 | val_0_rmse: 0.62793 | val_1_rmse: 0.62976 |  0:00:08s
epoch 9  | loss: 0.4186  | val_0_rmse: 0.6253  | val_1_rmse: 0.63184 |  0:00:09s
epoch 10 | loss: 0.42236 | val_0_rmse: 0.65055 | val_1_rmse: 0.6505  |  0:00:10s
epoch 11 | loss: 0.41978 | val_0_rmse: 0.61331 | val_1_rmse: 0.61705 |  0:00:10s
epoch 12 | loss: 0.41148 | val_0_rmse: 0.62837 | val_1_rmse: 0.62201 |  0:00:11s
epoch 13 | loss: 0.40874 | val_0_rmse: 0.61433 | val_1_rmse: 0.61669 |  0:00:12s
epoch 14 | loss: 0.40264 | val_0_rmse: 0.60699 | val_1_rmse: 0.60429 |  0:00:13s
epoch 15 | loss: 0.3891  | val_0_rmse: 0.60804 | val_1_rmse: 0.6131  |  0:00:13s
epoch 16 | loss: 0.39228 | val_0_rmse: 0.62782 | val_1_rmse: 0.63493 |  0:00:14s
epoch 17 | loss: 0.39575 | val_0_rmse: 0.61145 | val_1_rmse: 0.61707 |  0:00:15s
epoch 18 | loss: 0.39519 | val_0_rmse: 0.61421 | val_1_rmse: 0.61458 |  0:00:16s
epoch 19 | loss: 0.39524 | val_0_rmse: 0.62046 | val_1_rmse: 0.63045 |  0:00:16s
epoch 20 | loss: 0.40236 | val_0_rmse: 0.62816 | val_1_rmse: 0.62994 |  0:00:17s
epoch 21 | loss: 0.38993 | val_0_rmse: 0.63063 | val_1_rmse: 0.64339 |  0:00:18s
epoch 22 | loss: 0.37911 | val_0_rmse: 0.58673 | val_1_rmse: 0.5947  |  0:00:19s
epoch 23 | loss: 0.36897 | val_0_rmse: 0.59796 | val_1_rmse: 0.60604 |  0:00:19s
epoch 24 | loss: 0.36173 | val_0_rmse: 0.58511 | val_1_rmse: 0.59455 |  0:00:20s
epoch 25 | loss: 0.35842 | val_0_rmse: 0.57706 | val_1_rmse: 0.58383 |  0:00:21s
epoch 26 | loss: 0.3571  | val_0_rmse: 0.5808  | val_1_rmse: 0.58126 |  0:00:22s
epoch 27 | loss: 0.35079 | val_0_rmse: 0.57074 | val_1_rmse: 0.5718  |  0:00:22s
epoch 28 | loss: 0.34823 | val_0_rmse: 0.58672 | val_1_rmse: 0.59411 |  0:00:23s
epoch 29 | loss: 0.37403 | val_0_rmse: 0.59024 | val_1_rmse: 0.5898  |  0:00:24s
epoch 30 | loss: 0.36161 | val_0_rmse: 0.574   | val_1_rmse: 0.5773  |  0:00:25s
epoch 31 | loss: 0.35266 | val_0_rmse: 0.56699 | val_1_rmse: 0.57279 |  0:00:25s
epoch 32 | loss: 0.35185 | val_0_rmse: 0.56441 | val_1_rmse: 0.56694 |  0:00:26s
epoch 33 | loss: 0.34754 | val_0_rmse: 0.56709 | val_1_rmse: 0.56777 |  0:00:27s
epoch 34 | loss: 0.35398 | val_0_rmse: 0.56872 | val_1_rmse: 0.57446 |  0:00:27s
epoch 35 | loss: 0.34129 | val_0_rmse: 0.5645  | val_1_rmse: 0.57271 |  0:00:28s
epoch 36 | loss: 0.34282 | val_0_rmse: 0.5836  | val_1_rmse: 0.58656 |  0:00:29s
epoch 37 | loss: 0.34849 | val_0_rmse: 0.56583 | val_1_rmse: 0.58021 |  0:00:30s
epoch 38 | loss: 0.34342 | val_0_rmse: 0.56962 | val_1_rmse: 0.57111 |  0:00:30s
epoch 39 | loss: 0.34176 | val_0_rmse: 0.57335 | val_1_rmse: 0.578   |  0:00:31s
epoch 40 | loss: 0.33849 | val_0_rmse: 0.56067 | val_1_rmse: 0.57354 |  0:00:32s
epoch 41 | loss: 0.33464 | val_0_rmse: 0.56113 | val_1_rmse: 0.57094 |  0:00:33s
epoch 42 | loss: 0.33367 | val_0_rmse: 0.56047 | val_1_rmse: 0.56843 |  0:00:33s
epoch 43 | loss: 0.33212 | val_0_rmse: 0.57899 | val_1_rmse: 0.59401 |  0:00:34s
epoch 44 | loss: 0.34117 | val_0_rmse: 0.56363 | val_1_rmse: 0.57594 |  0:00:35s
epoch 45 | loss: 0.36238 | val_0_rmse: 0.59774 | val_1_rmse: 0.6056  |  0:00:36s
epoch 46 | loss: 0.36324 | val_0_rmse: 0.57949 | val_1_rmse: 0.59554 |  0:00:36s
epoch 47 | loss: 0.34166 | val_0_rmse: 0.55656 | val_1_rmse: 0.56946 |  0:00:37s
epoch 48 | loss: 0.34469 | val_0_rmse: 0.56737 | val_1_rmse: 0.58134 |  0:00:38s
epoch 49 | loss: 0.3408  | val_0_rmse: 0.56202 | val_1_rmse: 0.57642 |  0:00:38s
epoch 50 | loss: 0.33439 | val_0_rmse: 0.55529 | val_1_rmse: 0.56891 |  0:00:39s
epoch 51 | loss: 0.33615 | val_0_rmse: 0.56349 | val_1_rmse: 0.56939 |  0:00:40s
epoch 52 | loss: 0.32481 | val_0_rmse: 0.54681 | val_1_rmse: 0.56089 |  0:00:41s
epoch 53 | loss: 0.33413 | val_0_rmse: 0.55344 | val_1_rmse: 0.57205 |  0:00:41s
epoch 54 | loss: 0.33976 | val_0_rmse: 0.54702 | val_1_rmse: 0.56153 |  0:00:42s
epoch 55 | loss: 0.33021 | val_0_rmse: 0.55719 | val_1_rmse: 0.57506 |  0:00:43s
epoch 56 | loss: 0.32803 | val_0_rmse: 0.55861 | val_1_rmse: 0.57359 |  0:00:44s
epoch 57 | loss: 0.33763 | val_0_rmse: 0.56369 | val_1_rmse: 0.58191 |  0:00:44s
epoch 58 | loss: 0.33347 | val_0_rmse: 0.55859 | val_1_rmse: 0.57098 |  0:00:45s
epoch 59 | loss: 0.32814 | val_0_rmse: 0.5478  | val_1_rmse: 0.56555 |  0:00:46s
epoch 60 | loss: 0.32732 | val_0_rmse: 0.54224 | val_1_rmse: 0.55905 |  0:00:47s
epoch 61 | loss: 0.31912 | val_0_rmse: 0.54251 | val_1_rmse: 0.55476 |  0:00:47s
epoch 62 | loss: 0.32327 | val_0_rmse: 0.55447 | val_1_rmse: 0.57613 |  0:00:48s
epoch 63 | loss: 0.32438 | val_0_rmse: 0.55461 | val_1_rmse: 0.56554 |  0:00:49s
epoch 64 | loss: 0.31954 | val_0_rmse: 0.55001 | val_1_rmse: 0.57495 |  0:00:50s
epoch 65 | loss: 0.3299  | val_0_rmse: 0.54228 | val_1_rmse: 0.55414 |  0:00:50s
epoch 66 | loss: 0.31707 | val_0_rmse: 0.5472  | val_1_rmse: 0.56592 |  0:00:51s
epoch 67 | loss: 0.3185  | val_0_rmse: 0.53669 | val_1_rmse: 0.55429 |  0:00:52s
epoch 68 | loss: 0.31964 | val_0_rmse: 0.53997 | val_1_rmse: 0.55919 |  0:00:52s
epoch 69 | loss: 0.324   | val_0_rmse: 0.54913 | val_1_rmse: 0.57034 |  0:00:53s
epoch 70 | loss: 0.31819 | val_0_rmse: 0.53756 | val_1_rmse: 0.5544  |  0:00:54s
epoch 71 | loss: 0.30533 | val_0_rmse: 0.53701 | val_1_rmse: 0.55597 |  0:00:55s
epoch 72 | loss: 0.31234 | val_0_rmse: 0.56939 | val_1_rmse: 0.58783 |  0:00:55s
epoch 73 | loss: 0.32325 | val_0_rmse: 0.54382 | val_1_rmse: 0.56898 |  0:00:56s
epoch 74 | loss: 0.31855 | val_0_rmse: 0.5404  | val_1_rmse: 0.55659 |  0:00:57s
epoch 75 | loss: 0.312   | val_0_rmse: 0.54627 | val_1_rmse: 0.56532 |  0:00:58s
epoch 76 | loss: 0.31223 | val_0_rmse: 0.53903 | val_1_rmse: 0.55881 |  0:00:58s
epoch 77 | loss: 0.31996 | val_0_rmse: 0.57913 | val_1_rmse: 0.59287 |  0:00:59s
epoch 78 | loss: 0.32914 | val_0_rmse: 0.53895 | val_1_rmse: 0.56145 |  0:01:00s
epoch 79 | loss: 0.31546 | val_0_rmse: 0.53429 | val_1_rmse: 0.55649 |  0:01:01s
epoch 80 | loss: 0.31636 | val_0_rmse: 0.53834 | val_1_rmse: 0.56095 |  0:01:01s
epoch 81 | loss: 0.31382 | val_0_rmse: 0.52823 | val_1_rmse: 0.55032 |  0:01:02s
epoch 82 | loss: 0.30764 | val_0_rmse: 0.55178 | val_1_rmse: 0.56944 |  0:01:03s
epoch 83 | loss: 0.31791 | val_0_rmse: 0.53735 | val_1_rmse: 0.56403 |  0:01:03s
epoch 84 | loss: 0.31947 | val_0_rmse: 0.53831 | val_1_rmse: 0.56256 |  0:01:04s
epoch 85 | loss: 0.32108 | val_0_rmse: 0.54384 | val_1_rmse: 0.5671  |  0:01:05s
epoch 86 | loss: 0.3051  | val_0_rmse: 0.52952 | val_1_rmse: 0.5563  |  0:01:06s
epoch 87 | loss: 0.31436 | val_0_rmse: 0.53142 | val_1_rmse: 0.55874 |  0:01:06s
epoch 88 | loss: 0.31321 | val_0_rmse: 0.53448 | val_1_rmse: 0.56132 |  0:01:07s
epoch 89 | loss: 0.30731 | val_0_rmse: 0.53836 | val_1_rmse: 0.56192 |  0:01:08s
epoch 90 | loss: 0.31029 | val_0_rmse: 0.5295  | val_1_rmse: 0.55422 |  0:01:09s
epoch 91 | loss: 0.32125 | val_0_rmse: 0.54468 | val_1_rmse: 0.57051 |  0:01:09s
epoch 92 | loss: 0.30789 | val_0_rmse: 0.52811 | val_1_rmse: 0.54942 |  0:01:10s
epoch 93 | loss: 0.30607 | val_0_rmse: 0.54435 | val_1_rmse: 0.56772 |  0:01:11s
epoch 94 | loss: 0.31381 | val_0_rmse: 0.54346 | val_1_rmse: 0.57504 |  0:01:12s
epoch 95 | loss: 0.31282 | val_0_rmse: 0.54626 | val_1_rmse: 0.57053 |  0:01:12s
epoch 96 | loss: 0.31464 | val_0_rmse: 0.55791 | val_1_rmse: 0.58699 |  0:01:13s
epoch 97 | loss: 0.32191 | val_0_rmse: 0.53767 | val_1_rmse: 0.5602  |  0:01:14s
epoch 98 | loss: 0.32562 | val_0_rmse: 0.53587 | val_1_rmse: 0.56222 |  0:01:14s
epoch 99 | loss: 0.30917 | val_0_rmse: 0.52339 | val_1_rmse: 0.55235 |  0:01:15s
epoch 100| loss: 0.30555 | val_0_rmse: 0.54253 | val_1_rmse: 0.56883 |  0:01:16s
epoch 101| loss: 0.31668 | val_0_rmse: 0.52878 | val_1_rmse: 0.55661 |  0:01:17s
epoch 102| loss: 0.30249 | val_0_rmse: 0.51812 | val_1_rmse: 0.54869 |  0:01:17s
epoch 103| loss: 0.30003 | val_0_rmse: 0.52061 | val_1_rmse: 0.55531 |  0:01:18s
epoch 104| loss: 0.29905 | val_0_rmse: 0.52113 | val_1_rmse: 0.54945 |  0:01:19s
epoch 105| loss: 0.30004 | val_0_rmse: 0.52339 | val_1_rmse: 0.55022 |  0:01:20s
epoch 106| loss: 0.29294 | val_0_rmse: 0.52995 | val_1_rmse: 0.57365 |  0:01:20s
epoch 107| loss: 0.29234 | val_0_rmse: 0.52783 | val_1_rmse: 0.55982 |  0:01:21s
epoch 108| loss: 0.3061  | val_0_rmse: 0.53293 | val_1_rmse: 0.56397 |  0:01:22s
epoch 109| loss: 0.30075 | val_0_rmse: 0.52092 | val_1_rmse: 0.55498 |  0:01:23s
epoch 110| loss: 0.29834 | val_0_rmse: 0.52704 | val_1_rmse: 0.56373 |  0:01:23s
epoch 111| loss: 0.30547 | val_0_rmse: 0.52343 | val_1_rmse: 0.55424 |  0:01:24s
epoch 112| loss: 0.3028  | val_0_rmse: 0.52855 | val_1_rmse: 0.56387 |  0:01:25s
epoch 113| loss: 0.29513 | val_0_rmse: 0.52343 | val_1_rmse: 0.55897 |  0:01:25s
epoch 114| loss: 0.29671 | val_0_rmse: 0.52157 | val_1_rmse: 0.55677 |  0:01:26s
epoch 115| loss: 0.29736 | val_0_rmse: 0.51906 | val_1_rmse: 0.559   |  0:01:27s
epoch 116| loss: 0.29663 | val_0_rmse: 0.5139  | val_1_rmse: 0.54762 |  0:01:28s
epoch 117| loss: 0.28909 | val_0_rmse: 0.51065 | val_1_rmse: 0.54925 |  0:01:28s
epoch 118| loss: 0.30014 | val_0_rmse: 0.52427 | val_1_rmse: 0.56256 |  0:01:29s
epoch 119| loss: 0.30283 | val_0_rmse: 0.5271  | val_1_rmse: 0.56712 |  0:01:30s
epoch 120| loss: 0.29574 | val_0_rmse: 0.52115 | val_1_rmse: 0.55492 |  0:01:31s
epoch 121| loss: 0.29473 | val_0_rmse: 0.51522 | val_1_rmse: 0.55735 |  0:01:31s
epoch 122| loss: 0.28986 | val_0_rmse: 0.51347 | val_1_rmse: 0.55305 |  0:01:32s
epoch 123| loss: 0.29435 | val_0_rmse: 0.5271  | val_1_rmse: 0.56361 |  0:01:33s
epoch 124| loss: 0.2872  | val_0_rmse: 0.51631 | val_1_rmse: 0.5597  |  0:01:34s
epoch 125| loss: 0.28608 | val_0_rmse: 0.51216 | val_1_rmse: 0.55934 |  0:01:34s
epoch 126| loss: 0.29228 | val_0_rmse: 0.50144 | val_1_rmse: 0.55751 |  0:01:35s
epoch 127| loss: 0.29095 | val_0_rmse: 0.51686 | val_1_rmse: 0.56404 |  0:01:36s
epoch 128| loss: 0.28656 | val_0_rmse: 0.50311 | val_1_rmse: 0.55288 |  0:01:36s
epoch 129| loss: 0.28347 | val_0_rmse: 0.50559 | val_1_rmse: 0.55075 |  0:01:37s
epoch 130| loss: 0.28938 | val_0_rmse: 0.51794 | val_1_rmse: 0.56436 |  0:01:38s
epoch 131| loss: 0.28781 | val_0_rmse: 0.50908 | val_1_rmse: 0.54837 |  0:01:39s
epoch 132| loss: 0.28771 | val_0_rmse: 0.50598 | val_1_rmse: 0.54846 |  0:01:39s
epoch 133| loss: 0.28306 | val_0_rmse: 0.50525 | val_1_rmse: 0.55781 |  0:01:40s
epoch 134| loss: 0.28574 | val_0_rmse: 0.52096 | val_1_rmse: 0.55655 |  0:01:41s
epoch 135| loss: 0.29374 | val_0_rmse: 0.51495 | val_1_rmse: 0.56487 |  0:01:42s
epoch 136| loss: 0.28556 | val_0_rmse: 0.51547 | val_1_rmse: 0.56412 |  0:01:42s
epoch 137| loss: 0.29575 | val_0_rmse: 0.51361 | val_1_rmse: 0.56721 |  0:01:43s
epoch 138| loss: 0.29832 | val_0_rmse: 0.51204 | val_1_rmse: 0.55479 |  0:01:44s
epoch 139| loss: 0.28772 | val_0_rmse: 0.50525 | val_1_rmse: 0.55338 |  0:01:45s
epoch 140| loss: 0.27903 | val_0_rmse: 0.50673 | val_1_rmse: 0.55848 |  0:01:45s
epoch 141| loss: 0.27621 | val_0_rmse: 0.49868 | val_1_rmse: 0.55324 |  0:01:46s
epoch 142| loss: 0.27806 | val_0_rmse: 0.51677 | val_1_rmse: 0.56375 |  0:01:47s
epoch 143| loss: 0.2935  | val_0_rmse: 0.52535 | val_1_rmse: 0.57014 |  0:01:47s
epoch 144| loss: 0.29002 | val_0_rmse: 0.52842 | val_1_rmse: 0.58648 |  0:01:48s
epoch 145| loss: 0.28698 | val_0_rmse: 0.51028 | val_1_rmse: 0.56006 |  0:01:49s
epoch 146| loss: 0.2852  | val_0_rmse: 0.50254 | val_1_rmse: 0.5582  |  0:01:50s

Early stopping occured at epoch 146 with best_epoch = 116 and best_val_1_rmse = 0.54762
Best weights from best epoch are automatically used!
ended training at: 13:08:32
Feature importance:
[('Area', 0.24105440953993118), ('Baths', 0.048088488312179906), ('Beds', 0.0), ('Latitude', 0.387759210920922), ('Longitude', 0.28413633794029863), ('Month', 0.019423353141117483), ('Year', 0.019538200145550744)]
Mean squared error is of 7249702414.278035
Mean absolute error:59049.334601709255
MAPE:0.19414405762113993
R2 score:0.6798516050983394
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:08:32
epoch 0  | loss: 0.94296 | val_0_rmse: 0.88943 | val_1_rmse: 0.86919 |  0:00:00s
epoch 1  | loss: 0.6458  | val_0_rmse: 0.76822 | val_1_rmse: 0.76244 |  0:00:01s
epoch 2  | loss: 0.54842 | val_0_rmse: 0.74938 | val_1_rmse: 0.7457  |  0:00:02s
epoch 3  | loss: 0.49452 | val_0_rmse: 0.72765 | val_1_rmse: 0.74087 |  0:00:02s
epoch 4  | loss: 0.4454  | val_0_rmse: 0.65965 | val_1_rmse: 0.66824 |  0:00:03s
epoch 5  | loss: 0.43586 | val_0_rmse: 0.63771 | val_1_rmse: 0.64128 |  0:00:04s
epoch 6  | loss: 0.41951 | val_0_rmse: 0.65307 | val_1_rmse: 0.66608 |  0:00:05s
epoch 7  | loss: 0.4089  | val_0_rmse: 0.62555 | val_1_rmse: 0.63288 |  0:00:05s
epoch 8  | loss: 0.40085 | val_0_rmse: 0.6149  | val_1_rmse: 0.61514 |  0:00:06s
epoch 9  | loss: 0.39161 | val_0_rmse: 0.61785 | val_1_rmse: 0.62212 |  0:00:07s
epoch 10 | loss: 0.39421 | val_0_rmse: 0.61533 | val_1_rmse: 0.63582 |  0:00:08s
epoch 11 | loss: 0.40346 | val_0_rmse: 0.60359 | val_1_rmse: 0.61297 |  0:00:08s
epoch 12 | loss: 0.38857 | val_0_rmse: 0.60721 | val_1_rmse: 0.61438 |  0:00:09s
epoch 13 | loss: 0.38006 | val_0_rmse: 0.62882 | val_1_rmse: 0.63933 |  0:00:10s
epoch 14 | loss: 0.38511 | val_0_rmse: 0.60876 | val_1_rmse: 0.61803 |  0:00:11s
epoch 15 | loss: 0.3796  | val_0_rmse: 0.61333 | val_1_rmse: 0.62844 |  0:00:11s
epoch 16 | loss: 0.36831 | val_0_rmse: 0.58765 | val_1_rmse: 0.60264 |  0:00:12s
epoch 17 | loss: 0.37162 | val_0_rmse: 0.58411 | val_1_rmse: 0.59402 |  0:00:13s
epoch 18 | loss: 0.36747 | val_0_rmse: 0.60648 | val_1_rmse: 0.6091  |  0:00:14s
epoch 19 | loss: 0.3616  | val_0_rmse: 0.58293 | val_1_rmse: 0.60014 |  0:00:14s
epoch 20 | loss: 0.35554 | val_0_rmse: 0.57637 | val_1_rmse: 0.59289 |  0:00:15s
epoch 21 | loss: 0.35403 | val_0_rmse: 0.57776 | val_1_rmse: 0.59144 |  0:00:16s
epoch 22 | loss: 0.35107 | val_0_rmse: 0.56863 | val_1_rmse: 0.58051 |  0:00:16s
epoch 23 | loss: 0.34943 | val_0_rmse: 0.59111 | val_1_rmse: 0.60439 |  0:00:17s
epoch 24 | loss: 0.36088 | val_0_rmse: 0.60984 | val_1_rmse: 0.61992 |  0:00:18s
epoch 25 | loss: 0.35667 | val_0_rmse: 0.56501 | val_1_rmse: 0.56974 |  0:00:19s
epoch 26 | loss: 0.35906 | val_0_rmse: 0.58    | val_1_rmse: 0.58667 |  0:00:19s
epoch 27 | loss: 0.34612 | val_0_rmse: 0.57771 | val_1_rmse: 0.59175 |  0:00:20s
epoch 28 | loss: 0.33681 | val_0_rmse: 0.57061 | val_1_rmse: 0.57863 |  0:00:21s
epoch 29 | loss: 0.34514 | val_0_rmse: 0.5645  | val_1_rmse: 0.58384 |  0:00:22s
epoch 30 | loss: 0.34048 | val_0_rmse: 0.5743  | val_1_rmse: 0.58086 |  0:00:22s
epoch 31 | loss: 0.33613 | val_0_rmse: 0.55545 | val_1_rmse: 0.56356 |  0:00:23s
epoch 32 | loss: 0.33703 | val_0_rmse: 0.5529  | val_1_rmse: 0.55907 |  0:00:24s
epoch 33 | loss: 0.33525 | val_0_rmse: 0.55422 | val_1_rmse: 0.55735 |  0:00:25s
epoch 34 | loss: 0.33516 | val_0_rmse: 0.56113 | val_1_rmse: 0.57309 |  0:00:25s
epoch 35 | loss: 0.33877 | val_0_rmse: 0.55998 | val_1_rmse: 0.5696  |  0:00:26s
epoch 36 | loss: 0.32519 | val_0_rmse: 0.54421 | val_1_rmse: 0.56002 |  0:00:27s
epoch 37 | loss: 0.32633 | val_0_rmse: 0.55059 | val_1_rmse: 0.56861 |  0:00:28s
epoch 38 | loss: 0.31968 | val_0_rmse: 0.54521 | val_1_rmse: 0.5614  |  0:00:28s
epoch 39 | loss: 0.32585 | val_0_rmse: 0.55105 | val_1_rmse: 0.55957 |  0:00:29s
epoch 40 | loss: 0.32818 | val_0_rmse: 0.54928 | val_1_rmse: 0.5563  |  0:00:30s
epoch 41 | loss: 0.32529 | val_0_rmse: 0.54827 | val_1_rmse: 0.56381 |  0:00:31s
epoch 42 | loss: 0.33217 | val_0_rmse: 0.55624 | val_1_rmse: 0.57126 |  0:00:31s
epoch 43 | loss: 0.32508 | val_0_rmse: 0.55701 | val_1_rmse: 0.5683  |  0:00:32s
epoch 44 | loss: 0.32785 | val_0_rmse: 0.54713 | val_1_rmse: 0.56337 |  0:00:33s
epoch 45 | loss: 0.32507 | val_0_rmse: 0.54078 | val_1_rmse: 0.55221 |  0:00:33s
epoch 46 | loss: 0.32204 | val_0_rmse: 0.5413  | val_1_rmse: 0.55578 |  0:00:34s
epoch 47 | loss: 0.32305 | val_0_rmse: 0.55915 | val_1_rmse: 0.578   |  0:00:35s
epoch 48 | loss: 0.32279 | val_0_rmse: 0.53243 | val_1_rmse: 0.54582 |  0:00:36s
epoch 49 | loss: 0.31694 | val_0_rmse: 0.54198 | val_1_rmse: 0.56428 |  0:00:36s
epoch 50 | loss: 0.31789 | val_0_rmse: 0.53966 | val_1_rmse: 0.55301 |  0:00:37s
epoch 51 | loss: 0.31939 | val_0_rmse: 0.53402 | val_1_rmse: 0.55432 |  0:00:38s
epoch 52 | loss: 0.31338 | val_0_rmse: 0.57634 | val_1_rmse: 0.58988 |  0:00:39s
epoch 53 | loss: 0.32367 | val_0_rmse: 0.54524 | val_1_rmse: 0.56867 |  0:00:39s
epoch 54 | loss: 0.3097  | val_0_rmse: 0.53785 | val_1_rmse: 0.55265 |  0:00:40s
epoch 55 | loss: 0.31631 | val_0_rmse: 0.53864 | val_1_rmse: 0.55038 |  0:00:41s
epoch 56 | loss: 0.31239 | val_0_rmse: 0.54657 | val_1_rmse: 0.57307 |  0:00:42s
epoch 57 | loss: 0.3157  | val_0_rmse: 0.55129 | val_1_rmse: 0.56641 |  0:00:42s
epoch 58 | loss: 0.3257  | val_0_rmse: 0.54332 | val_1_rmse: 0.55524 |  0:00:43s
epoch 59 | loss: 0.31422 | val_0_rmse: 0.54531 | val_1_rmse: 0.56191 |  0:00:44s
epoch 60 | loss: 0.31013 | val_0_rmse: 0.5241  | val_1_rmse: 0.54402 |  0:00:45s
epoch 61 | loss: 0.30384 | val_0_rmse: 0.5267  | val_1_rmse: 0.55579 |  0:00:45s
epoch 62 | loss: 0.30705 | val_0_rmse: 0.52763 | val_1_rmse: 0.55647 |  0:00:46s
epoch 63 | loss: 0.29577 | val_0_rmse: 0.52259 | val_1_rmse: 0.54948 |  0:00:47s
epoch 64 | loss: 0.30163 | val_0_rmse: 0.52968 | val_1_rmse: 0.54986 |  0:00:47s
epoch 65 | loss: 0.30492 | val_0_rmse: 0.53163 | val_1_rmse: 0.55229 |  0:00:48s
epoch 66 | loss: 0.29656 | val_0_rmse: 0.51862 | val_1_rmse: 0.54033 |  0:00:49s
epoch 67 | loss: 0.30009 | val_0_rmse: 0.52555 | val_1_rmse: 0.55053 |  0:00:50s
epoch 68 | loss: 0.30351 | val_0_rmse: 0.52089 | val_1_rmse: 0.54508 |  0:00:50s
epoch 69 | loss: 0.29038 | val_0_rmse: 0.51404 | val_1_rmse: 0.54417 |  0:00:51s
epoch 70 | loss: 0.29742 | val_0_rmse: 0.529   | val_1_rmse: 0.56284 |  0:00:52s
epoch 71 | loss: 0.29784 | val_0_rmse: 0.53291 | val_1_rmse: 0.55478 |  0:00:53s
epoch 72 | loss: 0.3071  | val_0_rmse: 0.52756 | val_1_rmse: 0.55642 |  0:00:53s
epoch 73 | loss: 0.30806 | val_0_rmse: 0.51811 | val_1_rmse: 0.54944 |  0:00:54s
epoch 74 | loss: 0.30543 | val_0_rmse: 0.52421 | val_1_rmse: 0.55126 |  0:00:55s
epoch 75 | loss: 0.30005 | val_0_rmse: 0.52102 | val_1_rmse: 0.55016 |  0:00:56s
epoch 76 | loss: 0.30354 | val_0_rmse: 0.52174 | val_1_rmse: 0.55408 |  0:00:56s
epoch 77 | loss: 0.30364 | val_0_rmse: 0.5351  | val_1_rmse: 0.56418 |  0:00:57s
epoch 78 | loss: 0.30526 | val_0_rmse: 0.51449 | val_1_rmse: 0.5409  |  0:00:58s
epoch 79 | loss: 0.2979  | val_0_rmse: 0.52974 | val_1_rmse: 0.55782 |  0:00:58s
epoch 80 | loss: 0.30202 | val_0_rmse: 0.51887 | val_1_rmse: 0.54944 |  0:00:59s
epoch 81 | loss: 0.28972 | val_0_rmse: 0.51296 | val_1_rmse: 0.54362 |  0:01:00s
epoch 82 | loss: 0.29876 | val_0_rmse: 0.5108  | val_1_rmse: 0.54491 |  0:01:01s
epoch 83 | loss: 0.28911 | val_0_rmse: 0.52715 | val_1_rmse: 0.55469 |  0:01:01s
epoch 84 | loss: 0.29435 | val_0_rmse: 0.5156  | val_1_rmse: 0.54806 |  0:01:02s
epoch 85 | loss: 0.29254 | val_0_rmse: 0.51592 | val_1_rmse: 0.54773 |  0:01:03s
epoch 86 | loss: 0.2927  | val_0_rmse: 0.50867 | val_1_rmse: 0.54501 |  0:01:04s
epoch 87 | loss: 0.29467 | val_0_rmse: 0.50258 | val_1_rmse: 0.53526 |  0:01:04s
epoch 88 | loss: 0.28588 | val_0_rmse: 0.50933 | val_1_rmse: 0.5421  |  0:01:05s
epoch 89 | loss: 0.28231 | val_0_rmse: 0.52044 | val_1_rmse: 0.54662 |  0:01:06s
epoch 90 | loss: 0.28607 | val_0_rmse: 0.50622 | val_1_rmse: 0.55081 |  0:01:07s
epoch 91 | loss: 0.28481 | val_0_rmse: 0.51154 | val_1_rmse: 0.54518 |  0:01:07s
epoch 92 | loss: 0.29288 | val_0_rmse: 0.51251 | val_1_rmse: 0.55284 |  0:01:08s
epoch 93 | loss: 0.28647 | val_0_rmse: 0.50567 | val_1_rmse: 0.54379 |  0:01:09s
epoch 94 | loss: 0.28636 | val_0_rmse: 0.51815 | val_1_rmse: 0.55077 |  0:01:10s
epoch 95 | loss: 0.29323 | val_0_rmse: 0.52972 | val_1_rmse: 0.55905 |  0:01:10s
epoch 96 | loss: 0.29597 | val_0_rmse: 0.51957 | val_1_rmse: 0.55487 |  0:01:11s
epoch 97 | loss: 0.29817 | val_0_rmse: 0.51202 | val_1_rmse: 0.54005 |  0:01:12s
epoch 98 | loss: 0.29163 | val_0_rmse: 0.51457 | val_1_rmse: 0.54363 |  0:01:13s
epoch 99 | loss: 0.28735 | val_0_rmse: 0.51196 | val_1_rmse: 0.54575 |  0:01:13s
epoch 100| loss: 0.28795 | val_0_rmse: 0.5021  | val_1_rmse: 0.53639 |  0:01:14s
epoch 101| loss: 0.29179 | val_0_rmse: 0.54685 | val_1_rmse: 0.56881 |  0:01:15s
epoch 102| loss: 0.30248 | val_0_rmse: 0.53453 | val_1_rmse: 0.57052 |  0:01:15s
epoch 103| loss: 0.28311 | val_0_rmse: 0.50973 | val_1_rmse: 0.54509 |  0:01:16s
epoch 104| loss: 0.28172 | val_0_rmse: 0.50854 | val_1_rmse: 0.54731 |  0:01:17s
epoch 105| loss: 0.28754 | val_0_rmse: 0.50918 | val_1_rmse: 0.54321 |  0:01:18s
epoch 106| loss: 0.28484 | val_0_rmse: 0.50401 | val_1_rmse: 0.54275 |  0:01:18s
epoch 107| loss: 0.27908 | val_0_rmse: 0.51699 | val_1_rmse: 0.54803 |  0:01:19s
epoch 108| loss: 0.29142 | val_0_rmse: 0.50997 | val_1_rmse: 0.54063 |  0:01:20s
epoch 109| loss: 0.2895  | val_0_rmse: 0.50744 | val_1_rmse: 0.54141 |  0:01:21s
epoch 110| loss: 0.29142 | val_0_rmse: 0.51064 | val_1_rmse: 0.5483  |  0:01:21s
epoch 111| loss: 0.29457 | val_0_rmse: 0.52304 | val_1_rmse: 0.56586 |  0:01:22s
epoch 112| loss: 0.29703 | val_0_rmse: 0.51145 | val_1_rmse: 0.55499 |  0:01:23s
epoch 113| loss: 0.28972 | val_0_rmse: 0.53094 | val_1_rmse: 0.56591 |  0:01:23s
epoch 114| loss: 0.28727 | val_0_rmse: 0.51447 | val_1_rmse: 0.54551 |  0:01:24s
epoch 115| loss: 0.29759 | val_0_rmse: 0.51976 | val_1_rmse: 0.54736 |  0:01:25s
epoch 116| loss: 0.30974 | val_0_rmse: 0.53445 | val_1_rmse: 0.55724 |  0:01:26s
epoch 117| loss: 0.29546 | val_0_rmse: 0.51389 | val_1_rmse: 0.55005 |  0:01:26s

Early stopping occured at epoch 117 with best_epoch = 87 and best_val_1_rmse = 0.53526
Best weights from best epoch are automatically used!
ended training at: 13:10:00
Feature importance:
[('Area', 0.33014640495364905), ('Baths', 0.036090873213402805), ('Beds', 0.014384448982571036), ('Latitude', 0.2063025770790433), ('Longitude', 0.23867466413985078), ('Month', 0.008409874149915323), ('Year', 0.16599115748156773)]
Mean squared error is of 6382608730.814237
Mean absolute error:55035.78481732798
MAPE:0.18046178858173811
R2 score:0.706207079852929
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:10:00
epoch 0  | loss: 0.95422 | val_0_rmse: 0.84224 | val_1_rmse: 0.8617  |  0:00:00s
epoch 1  | loss: 0.61389 | val_0_rmse: 0.84107 | val_1_rmse: 0.86719 |  0:00:01s
epoch 2  | loss: 0.55058 | val_0_rmse: 0.7419  | val_1_rmse: 0.76015 |  0:00:02s
epoch 3  | loss: 0.47832 | val_0_rmse: 0.66524 | val_1_rmse: 0.67865 |  0:00:03s
epoch 4  | loss: 0.43549 | val_0_rmse: 0.6544  | val_1_rmse: 0.65627 |  0:00:03s
epoch 5  | loss: 0.41006 | val_0_rmse: 0.62367 | val_1_rmse: 0.63768 |  0:00:04s
epoch 6  | loss: 0.40716 | val_0_rmse: 0.61827 | val_1_rmse: 0.63081 |  0:00:05s
epoch 7  | loss: 0.39685 | val_0_rmse: 0.60284 | val_1_rmse: 0.60239 |  0:00:06s
epoch 8  | loss: 0.39831 | val_0_rmse: 0.60947 | val_1_rmse: 0.61698 |  0:00:06s
epoch 9  | loss: 0.39557 | val_0_rmse: 0.59457 | val_1_rmse: 0.60106 |  0:00:07s
epoch 10 | loss: 0.39034 | val_0_rmse: 0.60261 | val_1_rmse: 0.60707 |  0:00:08s
epoch 11 | loss: 0.38543 | val_0_rmse: 0.59928 | val_1_rmse: 0.60743 |  0:00:09s
epoch 12 | loss: 0.375   | val_0_rmse: 0.60182 | val_1_rmse: 0.59888 |  0:00:09s
epoch 13 | loss: 0.37508 | val_0_rmse: 0.60575 | val_1_rmse: 0.61001 |  0:00:10s
epoch 14 | loss: 0.38377 | val_0_rmse: 0.59107 | val_1_rmse: 0.59565 |  0:00:11s
epoch 15 | loss: 0.36508 | val_0_rmse: 0.60182 | val_1_rmse: 0.60685 |  0:00:12s
epoch 16 | loss: 0.37552 | val_0_rmse: 0.60788 | val_1_rmse: 0.6118  |  0:00:12s
epoch 17 | loss: 0.37103 | val_0_rmse: 0.58938 | val_1_rmse: 0.59313 |  0:00:13s
epoch 18 | loss: 0.35618 | val_0_rmse: 0.58462 | val_1_rmse: 0.59214 |  0:00:14s
epoch 19 | loss: 0.35813 | val_0_rmse: 0.57035 | val_1_rmse: 0.57366 |  0:00:15s
epoch 20 | loss: 0.35565 | val_0_rmse: 0.56511 | val_1_rmse: 0.56928 |  0:00:15s
epoch 21 | loss: 0.3603  | val_0_rmse: 0.59035 | val_1_rmse: 0.60352 |  0:00:16s
epoch 22 | loss: 0.3638  | val_0_rmse: 0.58196 | val_1_rmse: 0.58715 |  0:00:17s
epoch 23 | loss: 0.34596 | val_0_rmse: 0.57157 | val_1_rmse: 0.57987 |  0:00:18s
epoch 24 | loss: 0.34426 | val_0_rmse: 0.57674 | val_1_rmse: 0.58065 |  0:00:18s
epoch 25 | loss: 0.34455 | val_0_rmse: 0.55743 | val_1_rmse: 0.5624  |  0:00:19s
epoch 26 | loss: 0.34112 | val_0_rmse: 0.55825 | val_1_rmse: 0.57065 |  0:00:20s
epoch 27 | loss: 0.33925 | val_0_rmse: 0.5593  | val_1_rmse: 0.56906 |  0:00:20s
epoch 28 | loss: 0.33634 | val_0_rmse: 0.54732 | val_1_rmse: 0.54878 |  0:00:21s
epoch 29 | loss: 0.32789 | val_0_rmse: 0.54479 | val_1_rmse: 0.54997 |  0:00:22s
epoch 30 | loss: 0.33831 | val_0_rmse: 0.57772 | val_1_rmse: 0.5847  |  0:00:23s
epoch 31 | loss: 0.38731 | val_0_rmse: 0.60029 | val_1_rmse: 0.60541 |  0:00:23s
epoch 32 | loss: 0.37301 | val_0_rmse: 0.61177 | val_1_rmse: 0.60327 |  0:00:24s
epoch 33 | loss: 0.36949 | val_0_rmse: 0.61061 | val_1_rmse: 0.61762 |  0:00:25s
epoch 34 | loss: 0.3523  | val_0_rmse: 0.57569 | val_1_rmse: 0.57493 |  0:00:26s
epoch 35 | loss: 0.36398 | val_0_rmse: 0.60105 | val_1_rmse: 0.61212 |  0:00:26s
epoch 36 | loss: 0.36704 | val_0_rmse: 0.58442 | val_1_rmse: 0.57821 |  0:00:27s
epoch 37 | loss: 0.35752 | val_0_rmse: 0.5698  | val_1_rmse: 0.56872 |  0:00:28s
epoch 38 | loss: 0.35781 | val_0_rmse: 0.57372 | val_1_rmse: 0.56895 |  0:00:29s
epoch 39 | loss: 0.35173 | val_0_rmse: 0.56711 | val_1_rmse: 0.56687 |  0:00:29s
epoch 40 | loss: 0.3534  | val_0_rmse: 0.58341 | val_1_rmse: 0.57842 |  0:00:30s
epoch 41 | loss: 0.35129 | val_0_rmse: 0.58744 | val_1_rmse: 0.59024 |  0:00:31s
epoch 42 | loss: 0.35524 | val_0_rmse: 0.56864 | val_1_rmse: 0.56962 |  0:00:32s
epoch 43 | loss: 0.34839 | val_0_rmse: 0.56757 | val_1_rmse: 0.57465 |  0:00:32s
epoch 44 | loss: 0.34141 | val_0_rmse: 0.56067 | val_1_rmse: 0.5618  |  0:00:33s
epoch 45 | loss: 0.34965 | val_0_rmse: 0.60777 | val_1_rmse: 0.61225 |  0:00:34s
epoch 46 | loss: 0.35597 | val_0_rmse: 0.59515 | val_1_rmse: 0.60696 |  0:00:34s
epoch 47 | loss: 0.34131 | val_0_rmse: 0.56316 | val_1_rmse: 0.56771 |  0:00:35s
epoch 48 | loss: 0.33799 | val_0_rmse: 0.58724 | val_1_rmse: 0.58982 |  0:00:36s
epoch 49 | loss: 0.36432 | val_0_rmse: 0.57192 | val_1_rmse: 0.57714 |  0:00:37s
epoch 50 | loss: 0.34616 | val_0_rmse: 0.5549  | val_1_rmse: 0.56152 |  0:00:37s
epoch 51 | loss: 0.35042 | val_0_rmse: 0.55941 | val_1_rmse: 0.56339 |  0:00:38s
epoch 52 | loss: 0.33827 | val_0_rmse: 0.59255 | val_1_rmse: 0.59307 |  0:00:39s
epoch 53 | loss: 0.33894 | val_0_rmse: 0.54919 | val_1_rmse: 0.55599 |  0:00:40s
epoch 54 | loss: 0.33575 | val_0_rmse: 0.58752 | val_1_rmse: 0.59785 |  0:00:40s
epoch 55 | loss: 0.33629 | val_0_rmse: 0.54824 | val_1_rmse: 0.55957 |  0:00:41s
epoch 56 | loss: 0.31748 | val_0_rmse: 0.5423  | val_1_rmse: 0.55026 |  0:00:42s
epoch 57 | loss: 0.3186  | val_0_rmse: 0.53628 | val_1_rmse: 0.54514 |  0:00:43s
epoch 58 | loss: 0.31998 | val_0_rmse: 0.53468 | val_1_rmse: 0.54338 |  0:00:43s
epoch 59 | loss: 0.31493 | val_0_rmse: 0.53821 | val_1_rmse: 0.54798 |  0:00:44s
epoch 60 | loss: 0.31097 | val_0_rmse: 0.54416 | val_1_rmse: 0.55354 |  0:00:45s
epoch 61 | loss: 0.31537 | val_0_rmse: 0.53494 | val_1_rmse: 0.54336 |  0:00:45s
epoch 62 | loss: 0.31601 | val_0_rmse: 0.55172 | val_1_rmse: 0.56464 |  0:00:46s
epoch 63 | loss: 0.32952 | val_0_rmse: 0.55401 | val_1_rmse: 0.56639 |  0:00:47s
epoch 64 | loss: 0.32375 | val_0_rmse: 0.53859 | val_1_rmse: 0.54765 |  0:00:48s
epoch 65 | loss: 0.32803 | val_0_rmse: 0.55114 | val_1_rmse: 0.56006 |  0:00:48s
epoch 66 | loss: 0.31466 | val_0_rmse: 0.54099 | val_1_rmse: 0.54904 |  0:00:49s
epoch 67 | loss: 0.32488 | val_0_rmse: 0.55244 | val_1_rmse: 0.55794 |  0:00:50s
epoch 68 | loss: 0.31508 | val_0_rmse: 0.54799 | val_1_rmse: 0.5577  |  0:00:51s
epoch 69 | loss: 0.3259  | val_0_rmse: 0.5484  | val_1_rmse: 0.55762 |  0:00:51s
epoch 70 | loss: 0.32411 | val_0_rmse: 0.55523 | val_1_rmse: 0.56072 |  0:00:52s
epoch 71 | loss: 0.32795 | val_0_rmse: 0.54206 | val_1_rmse: 0.55141 |  0:00:53s
epoch 72 | loss: 0.3285  | val_0_rmse: 0.57101 | val_1_rmse: 0.57941 |  0:00:54s
epoch 73 | loss: 0.31473 | val_0_rmse: 0.53489 | val_1_rmse: 0.54664 |  0:00:54s
epoch 74 | loss: 0.31073 | val_0_rmse: 0.53322 | val_1_rmse: 0.54292 |  0:00:55s
epoch 75 | loss: 0.31088 | val_0_rmse: 0.53501 | val_1_rmse: 0.54289 |  0:00:56s
epoch 76 | loss: 0.30644 | val_0_rmse: 0.53513 | val_1_rmse: 0.54674 |  0:00:57s
epoch 77 | loss: 0.31391 | val_0_rmse: 0.57126 | val_1_rmse: 0.59205 |  0:00:57s
epoch 78 | loss: 0.31774 | val_0_rmse: 0.53952 | val_1_rmse: 0.55324 |  0:00:58s
epoch 79 | loss: 0.31317 | val_0_rmse: 0.54425 | val_1_rmse: 0.55435 |  0:00:59s
epoch 80 | loss: 0.31484 | val_0_rmse: 0.54381 | val_1_rmse: 0.55728 |  0:00:59s
epoch 81 | loss: 0.31386 | val_0_rmse: 0.53885 | val_1_rmse: 0.54699 |  0:01:00s
epoch 82 | loss: 0.30651 | val_0_rmse: 0.53632 | val_1_rmse: 0.54376 |  0:01:01s
epoch 83 | loss: 0.31148 | val_0_rmse: 0.5258  | val_1_rmse: 0.54003 |  0:01:02s
epoch 84 | loss: 0.31186 | val_0_rmse: 0.55148 | val_1_rmse: 0.56032 |  0:01:02s
epoch 85 | loss: 0.31469 | val_0_rmse: 0.52714 | val_1_rmse: 0.5453  |  0:01:03s
epoch 86 | loss: 0.31008 | val_0_rmse: 0.53526 | val_1_rmse: 0.54769 |  0:01:04s
epoch 87 | loss: 0.30529 | val_0_rmse: 0.5756  | val_1_rmse: 0.5817  |  0:01:05s
epoch 88 | loss: 0.31667 | val_0_rmse: 0.56707 | val_1_rmse: 0.58246 |  0:01:05s
epoch 89 | loss: 0.31047 | val_0_rmse: 0.54158 | val_1_rmse: 0.55345 |  0:01:06s
epoch 90 | loss: 0.30475 | val_0_rmse: 0.52045 | val_1_rmse: 0.52948 |  0:01:07s
epoch 91 | loss: 0.30242 | val_0_rmse: 0.5324  | val_1_rmse: 0.5453  |  0:01:08s
epoch 92 | loss: 0.30855 | val_0_rmse: 0.53661 | val_1_rmse: 0.54542 |  0:01:08s
epoch 93 | loss: 0.30159 | val_0_rmse: 0.52562 | val_1_rmse: 0.53991 |  0:01:09s
epoch 94 | loss: 0.29654 | val_0_rmse: 0.52614 | val_1_rmse: 0.54251 |  0:01:10s
epoch 95 | loss: 0.30133 | val_0_rmse: 0.53621 | val_1_rmse: 0.54648 |  0:01:10s
epoch 96 | loss: 0.30904 | val_0_rmse: 0.53532 | val_1_rmse: 0.54798 |  0:01:11s
epoch 97 | loss: 0.31075 | val_0_rmse: 0.54331 | val_1_rmse: 0.55919 |  0:01:12s
epoch 98 | loss: 0.3116  | val_0_rmse: 0.53519 | val_1_rmse: 0.54532 |  0:01:13s
epoch 99 | loss: 0.30416 | val_0_rmse: 0.52855 | val_1_rmse: 0.54268 |  0:01:13s
epoch 100| loss: 0.30455 | val_0_rmse: 0.52029 | val_1_rmse: 0.53288 |  0:01:14s
epoch 101| loss: 0.29436 | val_0_rmse: 0.53656 | val_1_rmse: 0.55234 |  0:01:15s
epoch 102| loss: 0.29447 | val_0_rmse: 0.52264 | val_1_rmse: 0.53467 |  0:01:16s
epoch 103| loss: 0.2937  | val_0_rmse: 0.52305 | val_1_rmse: 0.54054 |  0:01:16s
epoch 104| loss: 0.30018 | val_0_rmse: 0.55411 | val_1_rmse: 0.5666  |  0:01:17s
epoch 105| loss: 0.30327 | val_0_rmse: 0.52191 | val_1_rmse: 0.54283 |  0:01:18s
epoch 106| loss: 0.30092 | val_0_rmse: 0.52166 | val_1_rmse: 0.53395 |  0:01:19s
epoch 107| loss: 0.30295 | val_0_rmse: 0.52263 | val_1_rmse: 0.53556 |  0:01:19s
epoch 108| loss: 0.30093 | val_0_rmse: 0.52671 | val_1_rmse: 0.54202 |  0:01:20s
epoch 109| loss: 0.3091  | val_0_rmse: 0.53619 | val_1_rmse: 0.54732 |  0:01:21s
epoch 110| loss: 0.30666 | val_0_rmse: 0.54509 | val_1_rmse: 0.5578  |  0:01:22s
epoch 111| loss: 0.30745 | val_0_rmse: 0.53705 | val_1_rmse: 0.55047 |  0:01:22s
epoch 112| loss: 0.3105  | val_0_rmse: 0.54403 | val_1_rmse: 0.56209 |  0:01:23s
epoch 113| loss: 0.3016  | val_0_rmse: 0.52421 | val_1_rmse: 0.54077 |  0:01:24s
epoch 114| loss: 0.31426 | val_0_rmse: 0.52259 | val_1_rmse: 0.53452 |  0:01:24s
epoch 115| loss: 0.30265 | val_0_rmse: 0.53859 | val_1_rmse: 0.5566  |  0:01:25s
epoch 116| loss: 0.29954 | val_0_rmse: 0.52647 | val_1_rmse: 0.53924 |  0:01:26s
epoch 117| loss: 0.30108 | val_0_rmse: 0.52557 | val_1_rmse: 0.539   |  0:01:27s
epoch 118| loss: 0.28769 | val_0_rmse: 0.51655 | val_1_rmse: 0.52736 |  0:01:27s
epoch 119| loss: 0.29195 | val_0_rmse: 0.51282 | val_1_rmse: 0.53469 |  0:01:28s
epoch 120| loss: 0.29005 | val_0_rmse: 0.51549 | val_1_rmse: 0.53176 |  0:01:29s
epoch 121| loss: 0.29077 | val_0_rmse: 0.53714 | val_1_rmse: 0.54496 |  0:01:30s
epoch 122| loss: 0.30171 | val_0_rmse: 0.5228  | val_1_rmse: 0.54002 |  0:01:30s
epoch 123| loss: 0.29917 | val_0_rmse: 0.52553 | val_1_rmse: 0.54367 |  0:01:31s
epoch 124| loss: 0.30965 | val_0_rmse: 0.5278  | val_1_rmse: 0.54007 |  0:01:32s
epoch 125| loss: 0.30137 | val_0_rmse: 0.5241  | val_1_rmse: 0.53232 |  0:01:33s
epoch 126| loss: 0.30158 | val_0_rmse: 0.52777 | val_1_rmse: 0.54245 |  0:01:33s
epoch 127| loss: 0.2969  | val_0_rmse: 0.53184 | val_1_rmse: 0.54717 |  0:01:34s
epoch 128| loss: 0.29985 | val_0_rmse: 0.53863 | val_1_rmse: 0.55074 |  0:01:35s
epoch 129| loss: 0.30623 | val_0_rmse: 0.52696 | val_1_rmse: 0.54818 |  0:01:35s
epoch 130| loss: 0.29729 | val_0_rmse: 0.54045 | val_1_rmse: 0.55099 |  0:01:36s
epoch 131| loss: 0.29437 | val_0_rmse: 0.53747 | val_1_rmse: 0.54951 |  0:01:37s
epoch 132| loss: 0.29476 | val_0_rmse: 0.52731 | val_1_rmse: 0.54486 |  0:01:38s
epoch 133| loss: 0.2959  | val_0_rmse: 0.51514 | val_1_rmse: 0.53496 |  0:01:38s
epoch 134| loss: 0.2934  | val_0_rmse: 0.51608 | val_1_rmse: 0.53657 |  0:01:39s
epoch 135| loss: 0.29181 | val_0_rmse: 0.51695 | val_1_rmse: 0.53376 |  0:01:40s
epoch 136| loss: 0.29392 | val_0_rmse: 0.51812 | val_1_rmse: 0.54046 |  0:01:41s
epoch 137| loss: 0.29065 | val_0_rmse: 0.52434 | val_1_rmse: 0.53755 |  0:01:41s
epoch 138| loss: 0.29822 | val_0_rmse: 0.54179 | val_1_rmse: 0.56416 |  0:01:42s
epoch 139| loss: 0.30225 | val_0_rmse: 0.51165 | val_1_rmse: 0.53275 |  0:01:43s
epoch 140| loss: 0.29669 | val_0_rmse: 0.52553 | val_1_rmse: 0.54789 |  0:01:44s
epoch 141| loss: 0.29956 | val_0_rmse: 0.51954 | val_1_rmse: 0.53496 |  0:01:44s
epoch 142| loss: 0.29101 | val_0_rmse: 0.51879 | val_1_rmse: 0.53972 |  0:01:45s
epoch 143| loss: 0.28887 | val_0_rmse: 0.53095 | val_1_rmse: 0.5498  |  0:01:46s
epoch 144| loss: 0.30206 | val_0_rmse: 0.55506 | val_1_rmse: 0.56822 |  0:01:46s
epoch 145| loss: 0.30388 | val_0_rmse: 0.51772 | val_1_rmse: 0.54519 |  0:01:47s
epoch 146| loss: 0.28691 | val_0_rmse: 0.51264 | val_1_rmse: 0.53087 |  0:01:48s
epoch 147| loss: 0.28845 | val_0_rmse: 0.51184 | val_1_rmse: 0.52961 |  0:01:49s
epoch 148| loss: 0.28098 | val_0_rmse: 0.5045  | val_1_rmse: 0.52501 |  0:01:49s
epoch 149| loss: 0.28053 | val_0_rmse: 0.51067 | val_1_rmse: 0.53145 |  0:01:50s
Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_val_1_rmse = 0.52501
Best weights from best epoch are automatically used!
ended training at: 13:11:51
Feature importance:
[('Area', 0.25974824928964907), ('Baths', 0.08168962846901409), ('Beds', 0.03091713636824803), ('Latitude', 0.27069952755988536), ('Longitude', 0.3569454583132034), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 6390377966.602105
Mean absolute error:54538.9102587431
MAPE:0.17749789623773662
R2 score:0.7085455594594294
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:11:51
epoch 0  | loss: 0.88642 | val_0_rmse: 1.00446 | val_1_rmse: 1.00946 |  0:00:00s
epoch 1  | loss: 0.61053 | val_0_rmse: 0.80252 | val_1_rmse: 0.8222  |  0:00:01s
epoch 2  | loss: 0.56571 | val_0_rmse: 0.72311 | val_1_rmse: 0.73559 |  0:00:02s
epoch 3  | loss: 0.48108 | val_0_rmse: 0.6904  | val_1_rmse: 0.70851 |  0:00:02s
epoch 4  | loss: 0.43959 | val_0_rmse: 0.63765 | val_1_rmse: 0.6534  |  0:00:03s
epoch 5  | loss: 0.41723 | val_0_rmse: 0.65518 | val_1_rmse: 0.67047 |  0:00:04s
epoch 6  | loss: 0.41677 | val_0_rmse: 0.607   | val_1_rmse: 0.62385 |  0:00:05s
epoch 7  | loss: 0.38129 | val_0_rmse: 0.58945 | val_1_rmse: 0.60645 |  0:00:05s
epoch 8  | loss: 0.3758  | val_0_rmse: 0.59351 | val_1_rmse: 0.60487 |  0:00:06s
epoch 9  | loss: 0.37051 | val_0_rmse: 0.60033 | val_1_rmse: 0.61743 |  0:00:07s
epoch 10 | loss: 0.36738 | val_0_rmse: 0.61054 | val_1_rmse: 0.62937 |  0:00:08s
epoch 11 | loss: 0.37372 | val_0_rmse: 0.59778 | val_1_rmse: 0.62307 |  0:00:08s
epoch 12 | loss: 0.38632 | val_0_rmse: 0.64984 | val_1_rmse: 0.67037 |  0:00:09s
epoch 13 | loss: 0.41255 | val_0_rmse: 0.6397  | val_1_rmse: 0.65835 |  0:00:10s
epoch 14 | loss: 0.39361 | val_0_rmse: 0.6021  | val_1_rmse: 0.61803 |  0:00:11s
epoch 15 | loss: 0.37735 | val_0_rmse: 0.59639 | val_1_rmse: 0.60835 |  0:00:11s
epoch 16 | loss: 0.36418 | val_0_rmse: 0.57782 | val_1_rmse: 0.59605 |  0:00:12s
epoch 17 | loss: 0.35294 | val_0_rmse: 0.57595 | val_1_rmse: 0.58997 |  0:00:13s
epoch 18 | loss: 0.35213 | val_0_rmse: 0.5703  | val_1_rmse: 0.58735 |  0:00:14s
epoch 19 | loss: 0.34785 | val_0_rmse: 0.57987 | val_1_rmse: 0.59867 |  0:00:14s
epoch 20 | loss: 0.33858 | val_0_rmse: 0.56658 | val_1_rmse: 0.58556 |  0:00:15s
epoch 21 | loss: 0.36042 | val_0_rmse: 0.57361 | val_1_rmse: 0.59301 |  0:00:16s
epoch 22 | loss: 0.35903 | val_0_rmse: 0.5865  | val_1_rmse: 0.60157 |  0:00:17s
epoch 23 | loss: 0.34871 | val_0_rmse: 0.57337 | val_1_rmse: 0.58759 |  0:00:17s
epoch 24 | loss: 0.34339 | val_0_rmse: 0.55121 | val_1_rmse: 0.57186 |  0:00:18s
epoch 25 | loss: 0.33102 | val_0_rmse: 0.56013 | val_1_rmse: 0.5806  |  0:00:19s
epoch 26 | loss: 0.34608 | val_0_rmse: 0.56084 | val_1_rmse: 0.5759  |  0:00:20s
epoch 27 | loss: 0.33455 | val_0_rmse: 0.54838 | val_1_rmse: 0.56871 |  0:00:20s
epoch 28 | loss: 0.32967 | val_0_rmse: 0.56476 | val_1_rmse: 0.58905 |  0:00:21s
epoch 29 | loss: 0.32602 | val_0_rmse: 0.54657 | val_1_rmse: 0.56951 |  0:00:22s
epoch 30 | loss: 0.32508 | val_0_rmse: 0.54377 | val_1_rmse: 0.56254 |  0:00:23s
epoch 31 | loss: 0.31494 | val_0_rmse: 0.54805 | val_1_rmse: 0.56323 |  0:00:23s
epoch 32 | loss: 0.32054 | val_0_rmse: 0.53729 | val_1_rmse: 0.55771 |  0:00:24s
epoch 33 | loss: 0.32048 | val_0_rmse: 0.55056 | val_1_rmse: 0.57505 |  0:00:25s
epoch 34 | loss: 0.31589 | val_0_rmse: 0.54501 | val_1_rmse: 0.56996 |  0:00:25s
epoch 35 | loss: 0.33157 | val_0_rmse: 0.54669 | val_1_rmse: 0.56937 |  0:00:26s
epoch 36 | loss: 0.3156  | val_0_rmse: 0.54408 | val_1_rmse: 0.5698  |  0:00:27s
epoch 37 | loss: 0.32026 | val_0_rmse: 0.53918 | val_1_rmse: 0.57064 |  0:00:28s
epoch 38 | loss: 0.3179  | val_0_rmse: 0.53824 | val_1_rmse: 0.56171 |  0:00:28s
epoch 39 | loss: 0.31078 | val_0_rmse: 0.53801 | val_1_rmse: 0.56752 |  0:00:29s
epoch 40 | loss: 0.31718 | val_0_rmse: 0.5409  | val_1_rmse: 0.56599 |  0:00:30s
epoch 41 | loss: 0.31257 | val_0_rmse: 0.55448 | val_1_rmse: 0.57862 |  0:00:31s
epoch 42 | loss: 0.31074 | val_0_rmse: 0.54529 | val_1_rmse: 0.57073 |  0:00:31s
epoch 43 | loss: 0.30674 | val_0_rmse: 0.54213 | val_1_rmse: 0.56877 |  0:00:32s
epoch 44 | loss: 0.316   | val_0_rmse: 0.53647 | val_1_rmse: 0.55704 |  0:00:33s
epoch 45 | loss: 0.30417 | val_0_rmse: 0.54224 | val_1_rmse: 0.57138 |  0:00:34s
epoch 46 | loss: 0.30905 | val_0_rmse: 0.54634 | val_1_rmse: 0.57272 |  0:00:34s
epoch 47 | loss: 0.31695 | val_0_rmse: 0.53499 | val_1_rmse: 0.56024 |  0:00:35s
epoch 48 | loss: 0.31249 | val_0_rmse: 0.53233 | val_1_rmse: 0.55345 |  0:00:36s
epoch 49 | loss: 0.30698 | val_0_rmse: 0.53661 | val_1_rmse: 0.56352 |  0:00:37s
epoch 50 | loss: 0.30664 | val_0_rmse: 0.52485 | val_1_rmse: 0.55384 |  0:00:37s
epoch 51 | loss: 0.30357 | val_0_rmse: 0.52788 | val_1_rmse: 0.54942 |  0:00:38s
epoch 52 | loss: 0.30604 | val_0_rmse: 0.52821 | val_1_rmse: 0.5591  |  0:00:39s
epoch 53 | loss: 0.30568 | val_0_rmse: 0.52551 | val_1_rmse: 0.5541  |  0:00:40s
epoch 54 | loss: 0.30054 | val_0_rmse: 0.52985 | val_1_rmse: 0.56151 |  0:00:40s
epoch 55 | loss: 0.30475 | val_0_rmse: 0.54195 | val_1_rmse: 0.56617 |  0:00:41s
epoch 56 | loss: 0.30563 | val_0_rmse: 0.52899 | val_1_rmse: 0.55641 |  0:00:42s
epoch 57 | loss: 0.30309 | val_0_rmse: 0.51888 | val_1_rmse: 0.5447  |  0:00:42s
epoch 58 | loss: 0.29951 | val_0_rmse: 0.51793 | val_1_rmse: 0.55038 |  0:00:43s
epoch 59 | loss: 0.30223 | val_0_rmse: 0.5328  | val_1_rmse: 0.56694 |  0:00:44s
epoch 60 | loss: 0.31798 | val_0_rmse: 0.52853 | val_1_rmse: 0.55674 |  0:00:45s
epoch 61 | loss: 0.30423 | val_0_rmse: 0.52569 | val_1_rmse: 0.55799 |  0:00:45s
epoch 62 | loss: 0.29621 | val_0_rmse: 0.51535 | val_1_rmse: 0.54333 |  0:00:46s
epoch 63 | loss: 0.29391 | val_0_rmse: 0.51613 | val_1_rmse: 0.54466 |  0:00:47s
epoch 64 | loss: 0.29891 | val_0_rmse: 0.52506 | val_1_rmse: 0.55437 |  0:00:48s
epoch 65 | loss: 0.30187 | val_0_rmse: 0.53547 | val_1_rmse: 0.56454 |  0:00:48s
epoch 66 | loss: 0.29879 | val_0_rmse: 0.54966 | val_1_rmse: 0.58114 |  0:00:49s
epoch 67 | loss: 0.29399 | val_0_rmse: 0.52545 | val_1_rmse: 0.55364 |  0:00:50s
epoch 68 | loss: 0.2939  | val_0_rmse: 0.51733 | val_1_rmse: 0.5464  |  0:00:51s
epoch 69 | loss: 0.29031 | val_0_rmse: 0.5232  | val_1_rmse: 0.55131 |  0:00:51s
epoch 70 | loss: 0.29312 | val_0_rmse: 0.52882 | val_1_rmse: 0.55561 |  0:00:52s
epoch 71 | loss: 0.30858 | val_0_rmse: 0.53223 | val_1_rmse: 0.56094 |  0:00:53s
epoch 72 | loss: 0.30781 | val_0_rmse: 0.52439 | val_1_rmse: 0.54651 |  0:00:53s
epoch 73 | loss: 0.30076 | val_0_rmse: 0.54431 | val_1_rmse: 0.57525 |  0:00:54s
epoch 74 | loss: 0.29975 | val_0_rmse: 0.52189 | val_1_rmse: 0.55521 |  0:00:55s
epoch 75 | loss: 0.3015  | val_0_rmse: 0.53226 | val_1_rmse: 0.56744 |  0:00:56s
epoch 76 | loss: 0.29178 | val_0_rmse: 0.53289 | val_1_rmse: 0.56927 |  0:00:56s
epoch 77 | loss: 0.3033  | val_0_rmse: 0.54974 | val_1_rmse: 0.58043 |  0:00:57s
epoch 78 | loss: 0.29323 | val_0_rmse: 0.52206 | val_1_rmse: 0.56422 |  0:00:58s
epoch 79 | loss: 0.29601 | val_0_rmse: 0.51665 | val_1_rmse: 0.54908 |  0:00:59s
epoch 80 | loss: 0.29244 | val_0_rmse: 0.51159 | val_1_rmse: 0.54977 |  0:00:59s
epoch 81 | loss: 0.29015 | val_0_rmse: 0.51417 | val_1_rmse: 0.551   |  0:01:00s
epoch 82 | loss: 0.2964  | val_0_rmse: 0.55803 | val_1_rmse: 0.5831  |  0:01:01s
epoch 83 | loss: 0.32226 | val_0_rmse: 0.54437 | val_1_rmse: 0.58629 |  0:01:02s
epoch 84 | loss: 0.29717 | val_0_rmse: 0.54977 | val_1_rmse: 0.58717 |  0:01:02s
epoch 85 | loss: 0.30503 | val_0_rmse: 0.5259  | val_1_rmse: 0.5681  |  0:01:03s
epoch 86 | loss: 0.29719 | val_0_rmse: 0.54112 | val_1_rmse: 0.57436 |  0:01:04s
epoch 87 | loss: 0.3003  | val_0_rmse: 0.50996 | val_1_rmse: 0.54142 |  0:01:05s
epoch 88 | loss: 0.29972 | val_0_rmse: 0.51413 | val_1_rmse: 0.55343 |  0:01:05s
epoch 89 | loss: 0.28974 | val_0_rmse: 0.51424 | val_1_rmse: 0.55155 |  0:01:06s
epoch 90 | loss: 0.28138 | val_0_rmse: 0.5231  | val_1_rmse: 0.56045 |  0:01:07s
epoch 91 | loss: 0.28728 | val_0_rmse: 0.50892 | val_1_rmse: 0.55041 |  0:01:08s
epoch 92 | loss: 0.28013 | val_0_rmse: 0.50783 | val_1_rmse: 0.54432 |  0:01:08s
epoch 93 | loss: 0.28765 | val_0_rmse: 0.50282 | val_1_rmse: 0.53799 |  0:01:09s
epoch 94 | loss: 0.28547 | val_0_rmse: 0.52005 | val_1_rmse: 0.5585  |  0:01:10s
epoch 95 | loss: 0.28143 | val_0_rmse: 0.51372 | val_1_rmse: 0.54831 |  0:01:10s
epoch 96 | loss: 0.29124 | val_0_rmse: 0.55414 | val_1_rmse: 0.58442 |  0:01:11s
epoch 97 | loss: 0.29254 | val_0_rmse: 0.51534 | val_1_rmse: 0.54302 |  0:01:12s
epoch 98 | loss: 0.28424 | val_0_rmse: 0.51191 | val_1_rmse: 0.54782 |  0:01:13s
epoch 99 | loss: 0.28621 | val_0_rmse: 0.50041 | val_1_rmse: 0.5362  |  0:01:13s
epoch 100| loss: 0.28124 | val_0_rmse: 0.51272 | val_1_rmse: 0.55704 |  0:01:14s
epoch 101| loss: 0.27765 | val_0_rmse: 0.50961 | val_1_rmse: 0.54259 |  0:01:15s
epoch 102| loss: 0.29122 | val_0_rmse: 0.53333 | val_1_rmse: 0.57427 |  0:01:16s
epoch 103| loss: 0.28786 | val_0_rmse: 0.51353 | val_1_rmse: 0.54537 |  0:01:16s
epoch 104| loss: 0.28379 | val_0_rmse: 0.50565 | val_1_rmse: 0.54272 |  0:01:17s
epoch 105| loss: 0.27927 | val_0_rmse: 0.5151  | val_1_rmse: 0.55617 |  0:01:18s
epoch 106| loss: 0.29816 | val_0_rmse: 0.52601 | val_1_rmse: 0.56384 |  0:01:19s
epoch 107| loss: 0.28039 | val_0_rmse: 0.50394 | val_1_rmse: 0.54398 |  0:01:19s
epoch 108| loss: 0.28579 | val_0_rmse: 0.51821 | val_1_rmse: 0.55496 |  0:01:20s
epoch 109| loss: 0.28408 | val_0_rmse: 0.51846 | val_1_rmse: 0.55258 |  0:01:21s
epoch 110| loss: 0.28973 | val_0_rmse: 0.51797 | val_1_rmse: 0.55091 |  0:01:22s
epoch 111| loss: 0.28792 | val_0_rmse: 0.51557 | val_1_rmse: 0.5554  |  0:01:22s
epoch 112| loss: 0.28386 | val_0_rmse: 0.51015 | val_1_rmse: 0.54518 |  0:01:23s
epoch 113| loss: 0.2962  | val_0_rmse: 0.53673 | val_1_rmse: 0.58307 |  0:01:24s
epoch 114| loss: 0.28687 | val_0_rmse: 0.51451 | val_1_rmse: 0.5544  |  0:01:25s
epoch 115| loss: 0.26883 | val_0_rmse: 0.50056 | val_1_rmse: 0.54245 |  0:01:25s
epoch 116| loss: 0.28543 | val_0_rmse: 0.50685 | val_1_rmse: 0.54129 |  0:01:26s
epoch 117| loss: 0.28434 | val_0_rmse: 0.53133 | val_1_rmse: 0.56378 |  0:01:27s
epoch 118| loss: 0.28177 | val_0_rmse: 0.50122 | val_1_rmse: 0.54327 |  0:01:27s
epoch 119| loss: 0.28174 | val_0_rmse: 0.50414 | val_1_rmse: 0.54918 |  0:01:28s
epoch 120| loss: 0.28422 | val_0_rmse: 0.50619 | val_1_rmse: 0.54764 |  0:01:29s
epoch 121| loss: 0.28894 | val_0_rmse: 0.52803 | val_1_rmse: 0.57278 |  0:01:30s
epoch 122| loss: 0.28137 | val_0_rmse: 0.49935 | val_1_rmse: 0.54008 |  0:01:30s
epoch 123| loss: 0.27312 | val_0_rmse: 0.50498 | val_1_rmse: 0.53763 |  0:01:31s
epoch 124| loss: 0.28855 | val_0_rmse: 0.52001 | val_1_rmse: 0.56421 |  0:01:32s
epoch 125| loss: 0.28801 | val_0_rmse: 0.53927 | val_1_rmse: 0.57039 |  0:01:33s
epoch 126| loss: 0.2945  | val_0_rmse: 0.51656 | val_1_rmse: 0.55184 |  0:01:33s
epoch 127| loss: 0.28209 | val_0_rmse: 0.53579 | val_1_rmse: 0.56926 |  0:01:34s
epoch 128| loss: 0.283   | val_0_rmse: 0.50044 | val_1_rmse: 0.5356  |  0:01:35s
epoch 129| loss: 0.27897 | val_0_rmse: 0.53735 | val_1_rmse: 0.58297 |  0:01:36s
epoch 130| loss: 0.28829 | val_0_rmse: 0.5164  | val_1_rmse: 0.555   |  0:01:36s
epoch 131| loss: 0.28775 | val_0_rmse: 0.51087 | val_1_rmse: 0.55712 |  0:01:37s
epoch 132| loss: 0.27705 | val_0_rmse: 0.50578 | val_1_rmse: 0.54318 |  0:01:38s
epoch 133| loss: 0.27575 | val_0_rmse: 0.50641 | val_1_rmse: 0.54608 |  0:01:39s
epoch 134| loss: 0.27139 | val_0_rmse: 0.51697 | val_1_rmse: 0.55209 |  0:01:39s
epoch 135| loss: 0.27916 | val_0_rmse: 0.5024  | val_1_rmse: 0.54178 |  0:01:40s
epoch 136| loss: 0.27267 | val_0_rmse: 0.49907 | val_1_rmse: 0.53851 |  0:01:41s
epoch 137| loss: 0.28117 | val_0_rmse: 0.51357 | val_1_rmse: 0.54742 |  0:01:41s
epoch 138| loss: 0.27958 | val_0_rmse: 0.50301 | val_1_rmse: 0.55579 |  0:01:42s
epoch 139| loss: 0.27873 | val_0_rmse: 0.52943 | val_1_rmse: 0.57305 |  0:01:43s
epoch 140| loss: 0.28343 | val_0_rmse: 0.50354 | val_1_rmse: 0.55028 |  0:01:44s
epoch 141| loss: 0.2798  | val_0_rmse: 0.51047 | val_1_rmse: 0.55927 |  0:01:44s
epoch 142| loss: 0.28466 | val_0_rmse: 0.51871 | val_1_rmse: 0.55512 |  0:01:45s
epoch 143| loss: 0.28486 | val_0_rmse: 0.51342 | val_1_rmse: 0.55305 |  0:01:46s
epoch 144| loss: 0.2823  | val_0_rmse: 0.51213 | val_1_rmse: 0.54934 |  0:01:47s
epoch 145| loss: 0.27574 | val_0_rmse: 0.50787 | val_1_rmse: 0.54675 |  0:01:47s
epoch 146| loss: 0.28444 | val_0_rmse: 0.51465 | val_1_rmse: 0.55532 |  0:01:48s
epoch 147| loss: 0.27158 | val_0_rmse: 0.49421 | val_1_rmse: 0.54201 |  0:01:49s
epoch 148| loss: 0.28051 | val_0_rmse: 0.51273 | val_1_rmse: 0.5585  |  0:01:50s
epoch 149| loss: 0.27656 | val_0_rmse: 0.50587 | val_1_rmse: 0.55255 |  0:01:50s
Stop training because you reached max_epochs = 150 with best_epoch = 128 and best_val_1_rmse = 0.5356
Best weights from best epoch are automatically used!
ended training at: 13:13:42
Feature importance:
[('Area', 0.3408738691871266), ('Baths', 0.0), ('Beds', 0.07238432765765103), ('Latitude', 0.1945207944293861), ('Longitude', 0.22538769628537425), ('Month', 0.03315579908503758), ('Year', 0.13367751335542444)]
Mean squared error is of 6010660911.8365965
Mean absolute error:53182.14280103863
MAPE:0.1742408374523375
R2 score:0.7305628391936068
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:13:42
epoch 0  | loss: 0.89988 | val_0_rmse: 0.86716 | val_1_rmse: 0.88329 |  0:00:00s
epoch 1  | loss: 0.59739 | val_0_rmse: 0.76773 | val_1_rmse: 0.77016 |  0:00:01s
epoch 2  | loss: 0.51339 | val_0_rmse: 0.68936 | val_1_rmse: 0.68153 |  0:00:02s
epoch 3  | loss: 0.48943 | val_0_rmse: 0.70606 | val_1_rmse: 0.70551 |  0:00:03s
epoch 4  | loss: 0.46503 | val_0_rmse: 0.68769 | val_1_rmse: 0.683   |  0:00:03s
epoch 5  | loss: 0.46418 | val_0_rmse: 0.67502 | val_1_rmse: 0.67351 |  0:00:04s
epoch 6  | loss: 0.43637 | val_0_rmse: 0.64381 | val_1_rmse: 0.64338 |  0:00:05s
epoch 7  | loss: 0.43181 | val_0_rmse: 0.62797 | val_1_rmse: 0.63662 |  0:00:05s
epoch 8  | loss: 0.39859 | val_0_rmse: 0.61603 | val_1_rmse: 0.61613 |  0:00:06s
epoch 9  | loss: 0.38994 | val_0_rmse: 0.60661 | val_1_rmse: 0.60865 |  0:00:07s
epoch 10 | loss: 0.38957 | val_0_rmse: 0.60585 | val_1_rmse: 0.61373 |  0:00:08s
epoch 11 | loss: 0.38954 | val_0_rmse: 0.5977  | val_1_rmse: 0.59865 |  0:00:08s
epoch 12 | loss: 0.37337 | val_0_rmse: 0.60115 | val_1_rmse: 0.60639 |  0:00:09s
epoch 13 | loss: 0.36773 | val_0_rmse: 0.60339 | val_1_rmse: 0.61586 |  0:00:10s
epoch 14 | loss: 0.36964 | val_0_rmse: 0.61242 | val_1_rmse: 0.62606 |  0:00:11s
epoch 15 | loss: 0.35728 | val_0_rmse: 0.58371 | val_1_rmse: 0.59601 |  0:00:11s
epoch 16 | loss: 0.34322 | val_0_rmse: 0.56191 | val_1_rmse: 0.57545 |  0:00:12s
epoch 17 | loss: 0.34018 | val_0_rmse: 0.56271 | val_1_rmse: 0.57397 |  0:00:13s
epoch 18 | loss: 0.33851 | val_0_rmse: 0.57968 | val_1_rmse: 0.5908  |  0:00:14s
epoch 19 | loss: 0.3419  | val_0_rmse: 0.56004 | val_1_rmse: 0.56974 |  0:00:14s
epoch 20 | loss: 0.34061 | val_0_rmse: 0.58463 | val_1_rmse: 0.5975  |  0:00:15s
epoch 21 | loss: 0.33241 | val_0_rmse: 0.55329 | val_1_rmse: 0.57242 |  0:00:16s
epoch 22 | loss: 0.3327  | val_0_rmse: 0.54765 | val_1_rmse: 0.56586 |  0:00:17s
epoch 23 | loss: 0.32458 | val_0_rmse: 0.55134 | val_1_rmse: 0.5716  |  0:00:17s
epoch 24 | loss: 0.33468 | val_0_rmse: 0.55078 | val_1_rmse: 0.56841 |  0:00:18s
epoch 25 | loss: 0.33548 | val_0_rmse: 0.59439 | val_1_rmse: 0.61324 |  0:00:19s
epoch 26 | loss: 0.33054 | val_0_rmse: 0.55817 | val_1_rmse: 0.57555 |  0:00:20s
epoch 27 | loss: 0.3238  | val_0_rmse: 0.55092 | val_1_rmse: 0.56828 |  0:00:20s
epoch 28 | loss: 0.32584 | val_0_rmse: 0.54445 | val_1_rmse: 0.5613  |  0:00:21s
epoch 29 | loss: 0.33557 | val_0_rmse: 0.54636 | val_1_rmse: 0.56383 |  0:00:22s
epoch 30 | loss: 0.32128 | val_0_rmse: 0.55657 | val_1_rmse: 0.57135 |  0:00:22s
epoch 31 | loss: 0.32294 | val_0_rmse: 0.57368 | val_1_rmse: 0.58905 |  0:00:23s
epoch 32 | loss: 0.32825 | val_0_rmse: 0.55281 | val_1_rmse: 0.572   |  0:00:24s
epoch 33 | loss: 0.32828 | val_0_rmse: 0.54724 | val_1_rmse: 0.57114 |  0:00:25s
epoch 34 | loss: 0.33166 | val_0_rmse: 0.55227 | val_1_rmse: 0.56339 |  0:00:25s
epoch 35 | loss: 0.32015 | val_0_rmse: 0.54146 | val_1_rmse: 0.56183 |  0:00:26s
epoch 36 | loss: 0.31461 | val_0_rmse: 0.55609 | val_1_rmse: 0.56385 |  0:00:27s
epoch 37 | loss: 0.32187 | val_0_rmse: 0.54905 | val_1_rmse: 0.56333 |  0:00:28s
epoch 38 | loss: 0.31519 | val_0_rmse: 0.5472  | val_1_rmse: 0.56056 |  0:00:28s
epoch 39 | loss: 0.31163 | val_0_rmse: 0.53534 | val_1_rmse: 0.56235 |  0:00:29s
epoch 40 | loss: 0.31049 | val_0_rmse: 0.53535 | val_1_rmse: 0.55194 |  0:00:30s
epoch 41 | loss: 0.30566 | val_0_rmse: 0.53676 | val_1_rmse: 0.55919 |  0:00:31s
epoch 42 | loss: 0.31086 | val_0_rmse: 0.53596 | val_1_rmse: 0.55186 |  0:00:31s
epoch 43 | loss: 0.30685 | val_0_rmse: 0.52729 | val_1_rmse: 0.54021 |  0:00:32s
epoch 44 | loss: 0.30794 | val_0_rmse: 0.5415  | val_1_rmse: 0.55552 |  0:00:33s
epoch 45 | loss: 0.31623 | val_0_rmse: 0.55667 | val_1_rmse: 0.57395 |  0:00:34s
epoch 46 | loss: 0.31574 | val_0_rmse: 0.56666 | val_1_rmse: 0.5781  |  0:00:34s
epoch 47 | loss: 0.31462 | val_0_rmse: 0.53499 | val_1_rmse: 0.55115 |  0:00:35s
epoch 48 | loss: 0.32156 | val_0_rmse: 0.54644 | val_1_rmse: 0.56657 |  0:00:36s
epoch 49 | loss: 0.30609 | val_0_rmse: 0.53107 | val_1_rmse: 0.54627 |  0:00:37s
epoch 50 | loss: 0.31076 | val_0_rmse: 0.54214 | val_1_rmse: 0.55573 |  0:00:37s
epoch 51 | loss: 0.31379 | val_0_rmse: 0.53201 | val_1_rmse: 0.54903 |  0:00:38s
epoch 52 | loss: 0.31039 | val_0_rmse: 0.53457 | val_1_rmse: 0.55402 |  0:00:39s
epoch 53 | loss: 0.30754 | val_0_rmse: 0.53698 | val_1_rmse: 0.55425 |  0:00:39s
epoch 54 | loss: 0.30951 | val_0_rmse: 0.53298 | val_1_rmse: 0.54784 |  0:00:40s
epoch 55 | loss: 0.30452 | val_0_rmse: 0.52417 | val_1_rmse: 0.54152 |  0:00:41s
epoch 56 | loss: 0.30367 | val_0_rmse: 0.54802 | val_1_rmse: 0.56429 |  0:00:42s
epoch 57 | loss: 0.30907 | val_0_rmse: 0.52686 | val_1_rmse: 0.54912 |  0:00:42s
epoch 58 | loss: 0.30379 | val_0_rmse: 0.56343 | val_1_rmse: 0.58446 |  0:00:43s
epoch 59 | loss: 0.30323 | val_0_rmse: 0.52394 | val_1_rmse: 0.54507 |  0:00:44s
epoch 60 | loss: 0.30013 | val_0_rmse: 0.53662 | val_1_rmse: 0.56035 |  0:00:45s
epoch 61 | loss: 0.30372 | val_0_rmse: 0.52387 | val_1_rmse: 0.54266 |  0:00:45s
epoch 62 | loss: 0.30745 | val_0_rmse: 0.54291 | val_1_rmse: 0.5701  |  0:00:46s
epoch 63 | loss: 0.30167 | val_0_rmse: 0.53257 | val_1_rmse: 0.54656 |  0:00:47s
epoch 64 | loss: 0.29531 | val_0_rmse: 0.51989 | val_1_rmse: 0.5406  |  0:00:48s
epoch 65 | loss: 0.30084 | val_0_rmse: 0.55409 | val_1_rmse: 0.5799  |  0:00:48s
epoch 66 | loss: 0.32828 | val_0_rmse: 0.54454 | val_1_rmse: 0.56783 |  0:00:49s
epoch 67 | loss: 0.31305 | val_0_rmse: 0.53593 | val_1_rmse: 0.55913 |  0:00:50s
epoch 68 | loss: 0.31051 | val_0_rmse: 0.54645 | val_1_rmse: 0.56005 |  0:00:51s
epoch 69 | loss: 0.31273 | val_0_rmse: 0.71542 | val_1_rmse: 0.70929 |  0:00:51s
epoch 70 | loss: 0.32728 | val_0_rmse: 0.55472 | val_1_rmse: 0.57484 |  0:00:52s
epoch 71 | loss: 0.32295 | val_0_rmse: 0.55562 | val_1_rmse: 0.56667 |  0:00:53s
epoch 72 | loss: 0.32771 | val_0_rmse: 0.54334 | val_1_rmse: 0.55771 |  0:00:53s
epoch 73 | loss: 0.32881 | val_0_rmse: 0.55241 | val_1_rmse: 0.55698 |  0:00:54s

Early stopping occured at epoch 73 with best_epoch = 43 and best_val_1_rmse = 0.54021
Best weights from best epoch are automatically used!
ended training at: 13:14:37
Feature importance:
[('Area', 0.3585910802170353), ('Baths', 0.0796787009573335), ('Beds', 0.01108299315114506), ('Latitude', 0.22006209429036366), ('Longitude', 0.20086384964348833), ('Month', 2.4853802377779572e-05), ('Year', 0.12969642793825636)]
Mean squared error is of 7112447428.201347
Mean absolute error:58875.12332391795
MAPE:0.19749245196954945
R2 score:0.6852055254603824
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:14:37
epoch 0  | loss: 0.6462  | val_0_rmse: 0.73836 | val_1_rmse: 0.74088 |  0:00:02s
epoch 1  | loss: 0.54553 | val_0_rmse: 0.734   | val_1_rmse: 0.73093 |  0:00:05s
epoch 2  | loss: 0.527   | val_0_rmse: 0.73452 | val_1_rmse: 0.7325  |  0:00:08s
epoch 3  | loss: 0.50601 | val_0_rmse: 0.69933 | val_1_rmse: 0.69633 |  0:00:11s
epoch 4  | loss: 0.47995 | val_0_rmse: 0.67697 | val_1_rmse: 0.67736 |  0:00:13s
epoch 5  | loss: 0.46683 | val_0_rmse: 0.65847 | val_1_rmse: 0.6617  |  0:00:16s
epoch 6  | loss: 0.42148 | val_0_rmse: 0.67216 | val_1_rmse: 0.6718  |  0:00:19s
epoch 7  | loss: 0.41563 | val_0_rmse: 0.65549 | val_1_rmse: 0.66257 |  0:00:22s
epoch 8  | loss: 0.39878 | val_0_rmse: 0.65385 | val_1_rmse: 0.65909 |  0:00:24s
epoch 9  | loss: 0.38333 | val_0_rmse: 0.667   | val_1_rmse: 0.67086 |  0:00:27s
epoch 10 | loss: 0.38743 | val_0_rmse: 0.72282 | val_1_rmse: 0.72385 |  0:00:30s
epoch 11 | loss: 0.37986 | val_0_rmse: 0.69483 | val_1_rmse: 0.70011 |  0:00:33s
epoch 12 | loss: 0.37708 | val_0_rmse: 0.61493 | val_1_rmse: 0.61814 |  0:00:36s
epoch 13 | loss: 0.36865 | val_0_rmse: 0.62687 | val_1_rmse: 0.63515 |  0:00:38s
epoch 14 | loss: 0.36948 | val_0_rmse: 0.66243 | val_1_rmse: 0.66661 |  0:00:41s
epoch 15 | loss: 0.36222 | val_0_rmse: 0.60847 | val_1_rmse: 0.61283 |  0:00:44s
epoch 16 | loss: 0.36148 | val_0_rmse: 0.61474 | val_1_rmse: 0.61807 |  0:00:47s
epoch 17 | loss: 0.36146 | val_0_rmse: 0.60116 | val_1_rmse: 0.60704 |  0:00:50s
epoch 18 | loss: 0.35995 | val_0_rmse: 0.63374 | val_1_rmse: 0.6375  |  0:00:52s
epoch 19 | loss: 0.35907 | val_0_rmse: 0.64637 | val_1_rmse: 0.64937 |  0:00:55s
epoch 20 | loss: 0.37075 | val_0_rmse: 0.61121 | val_1_rmse: 0.61529 |  0:00:58s
epoch 21 | loss: 0.36431 | val_0_rmse: 0.58903 | val_1_rmse: 0.59261 |  0:01:01s
epoch 22 | loss: 0.35735 | val_0_rmse: 0.62723 | val_1_rmse: 0.6315  |  0:01:03s
epoch 23 | loss: 0.36171 | val_0_rmse: 0.62331 | val_1_rmse: 0.62542 |  0:01:06s
epoch 24 | loss: 0.36046 | val_0_rmse: 0.61578 | val_1_rmse: 0.62054 |  0:01:09s
epoch 25 | loss: 0.3537  | val_0_rmse: 0.59775 | val_1_rmse: 0.60092 |  0:01:12s
epoch 26 | loss: 0.35404 | val_0_rmse: 0.58715 | val_1_rmse: 0.59063 |  0:01:14s
epoch 27 | loss: 0.35473 | val_0_rmse: 0.61166 | val_1_rmse: 0.61537 |  0:01:17s
epoch 28 | loss: 0.40452 | val_0_rmse: 0.66483 | val_1_rmse: 0.66769 |  0:01:20s
epoch 29 | loss: 0.39463 | val_0_rmse: 0.6948  | val_1_rmse: 0.69721 |  0:01:23s
epoch 30 | loss: 0.3747  | val_0_rmse: 0.61106 | val_1_rmse: 0.61694 |  0:01:26s
epoch 31 | loss: 0.36264 | val_0_rmse: 0.58887 | val_1_rmse: 0.59336 |  0:01:28s
epoch 32 | loss: 0.36804 | val_0_rmse: 0.59689 | val_1_rmse: 0.60294 |  0:01:31s
epoch 33 | loss: 0.36086 | val_0_rmse: 0.63673 | val_1_rmse: 0.64043 |  0:01:34s
epoch 34 | loss: 0.35734 | val_0_rmse: 0.67155 | val_1_rmse: 0.67637 |  0:01:37s
epoch 35 | loss: 0.35388 | val_0_rmse: 0.62419 | val_1_rmse: 0.62795 |  0:01:39s
epoch 36 | loss: 0.35541 | val_0_rmse: 0.60459 | val_1_rmse: 0.60877 |  0:01:42s
epoch 37 | loss: 0.35632 | val_0_rmse: 0.64012 | val_1_rmse: 0.64097 |  0:01:45s
epoch 38 | loss: 0.34999 | val_0_rmse: 0.64253 | val_1_rmse: 0.64507 |  0:01:48s
epoch 39 | loss: 0.35139 | val_0_rmse: 0.60508 | val_1_rmse: 0.61146 |  0:01:51s
epoch 40 | loss: 0.35333 | val_0_rmse: 0.70904 | val_1_rmse: 0.70945 |  0:01:53s
epoch 41 | loss: 0.35207 | val_0_rmse: 0.58177 | val_1_rmse: 0.58465 |  0:01:56s
epoch 42 | loss: 0.34816 | val_0_rmse: 0.61047 | val_1_rmse: 0.61375 |  0:01:59s
epoch 43 | loss: 0.34773 | val_0_rmse: 0.63507 | val_1_rmse: 0.63845 |  0:02:02s
epoch 44 | loss: 0.35343 | val_0_rmse: 0.68626 | val_1_rmse: 0.6907  |  0:02:04s
epoch 45 | loss: 0.36075 | val_0_rmse: 0.59115 | val_1_rmse: 0.59521 |  0:02:07s
epoch 46 | loss: 0.35833 | val_0_rmse: 0.64823 | val_1_rmse: 0.65076 |  0:02:10s
epoch 47 | loss: 0.35954 | val_0_rmse: 0.63719 | val_1_rmse: 0.64316 |  0:02:13s
epoch 48 | loss: 0.34971 | val_0_rmse: 0.61598 | val_1_rmse: 0.62103 |  0:02:15s
epoch 49 | loss: 0.34809 | val_0_rmse: 0.57892 | val_1_rmse: 0.58327 |  0:02:18s
epoch 50 | loss: 0.34657 | val_0_rmse: 0.5854  | val_1_rmse: 0.59151 |  0:02:21s
epoch 51 | loss: 0.34789 | val_0_rmse: 0.68321 | val_1_rmse: 0.68664 |  0:02:24s
epoch 52 | loss: 0.34516 | val_0_rmse: 0.65751 | val_1_rmse: 0.65843 |  0:02:26s
epoch 53 | loss: 0.34721 | val_0_rmse: 0.62909 | val_1_rmse: 0.63372 |  0:02:29s
epoch 54 | loss: 0.35218 | val_0_rmse: 0.62047 | val_1_rmse: 0.62552 |  0:02:32s
epoch 55 | loss: 0.34392 | val_0_rmse: 0.59917 | val_1_rmse: 0.60443 |  0:02:35s
epoch 56 | loss: 0.34871 | val_0_rmse: 0.67024 | val_1_rmse: 0.6744  |  0:02:37s
epoch 57 | loss: 0.34483 | val_0_rmse: 0.63895 | val_1_rmse: 0.64349 |  0:02:40s
epoch 58 | loss: 0.34603 | val_0_rmse: 0.64017 | val_1_rmse: 0.64342 |  0:02:43s
epoch 59 | loss: 0.34752 | val_0_rmse: 0.57361 | val_1_rmse: 0.57997 |  0:02:46s
epoch 60 | loss: 0.34713 | val_0_rmse: 0.5894  | val_1_rmse: 0.5904  |  0:02:49s
epoch 61 | loss: 0.34993 | val_0_rmse: 0.69679 | val_1_rmse: 0.69947 |  0:02:51s
epoch 62 | loss: 0.34293 | val_0_rmse: 0.6257  | val_1_rmse: 0.62992 |  0:02:54s
epoch 63 | loss: 0.34255 | val_0_rmse: 0.73314 | val_1_rmse: 0.73611 |  0:02:57s
epoch 64 | loss: 0.34488 | val_0_rmse: 0.64372 | val_1_rmse: 0.64587 |  0:03:00s
epoch 65 | loss: 0.3404  | val_0_rmse: 0.70717 | val_1_rmse: 0.70955 |  0:03:02s
epoch 66 | loss: 0.33962 | val_0_rmse: 0.61866 | val_1_rmse: 0.62397 |  0:03:05s
epoch 67 | loss: 0.34244 | val_0_rmse: 0.62522 | val_1_rmse: 0.62695 |  0:03:08s
epoch 68 | loss: 0.34525 | val_0_rmse: 0.59697 | val_1_rmse: 0.60116 |  0:03:11s
epoch 69 | loss: 0.34541 | val_0_rmse: 0.65833 | val_1_rmse: 0.66089 |  0:03:14s
epoch 70 | loss: 0.34096 | val_0_rmse: 0.599   | val_1_rmse: 0.6024  |  0:03:16s
epoch 71 | loss: 0.34099 | val_0_rmse: 0.57704 | val_1_rmse: 0.58459 |  0:03:19s
epoch 72 | loss: 0.34022 | val_0_rmse: 0.65933 | val_1_rmse: 0.66433 |  0:03:22s
epoch 73 | loss: 0.34037 | val_0_rmse: 0.57462 | val_1_rmse: 0.58173 |  0:03:25s
epoch 74 | loss: 0.34416 | val_0_rmse: 0.60405 | val_1_rmse: 0.60928 |  0:03:27s
epoch 75 | loss: 0.33937 | val_0_rmse: 0.68995 | val_1_rmse: 0.69312 |  0:03:30s
epoch 76 | loss: 0.33823 | val_0_rmse: 0.67323 | val_1_rmse: 0.67662 |  0:03:33s
epoch 77 | loss: 0.3407  | val_0_rmse: 0.63575 | val_1_rmse: 0.63842 |  0:03:36s
epoch 78 | loss: 0.34615 | val_0_rmse: 0.61036 | val_1_rmse: 0.61337 |  0:03:38s
epoch 79 | loss: 0.34533 | val_0_rmse: 0.60487 | val_1_rmse: 0.61143 |  0:03:41s
epoch 80 | loss: 0.34997 | val_0_rmse: 0.62138 | val_1_rmse: 0.62528 |  0:03:44s
epoch 81 | loss: 0.33964 | val_0_rmse: 0.65025 | val_1_rmse: 0.65529 |  0:03:47s
epoch 82 | loss: 0.34137 | val_0_rmse: 0.58342 | val_1_rmse: 0.59004 |  0:03:50s
epoch 83 | loss: 0.33968 | val_0_rmse: 0.65105 | val_1_rmse: 0.65463 |  0:03:52s
epoch 84 | loss: 0.343   | val_0_rmse: 0.5743  | val_1_rmse: 0.57721 |  0:03:55s
epoch 85 | loss: 0.33998 | val_0_rmse: 0.64578 | val_1_rmse: 0.64915 |  0:03:58s
epoch 86 | loss: 0.3366  | val_0_rmse: 0.61659 | val_1_rmse: 0.62279 |  0:04:01s
epoch 87 | loss: 0.34001 | val_0_rmse: 0.58043 | val_1_rmse: 0.58481 |  0:04:03s
epoch 88 | loss: 0.33624 | val_0_rmse: 0.61761 | val_1_rmse: 0.62274 |  0:04:06s
epoch 89 | loss: 0.34012 | val_0_rmse: 0.64777 | val_1_rmse: 0.64888 |  0:04:09s
epoch 90 | loss: 0.342   | val_0_rmse: 0.6458  | val_1_rmse: 0.64994 |  0:04:12s
epoch 91 | loss: 0.33998 | val_0_rmse: 0.60084 | val_1_rmse: 0.60576 |  0:04:14s
epoch 92 | loss: 0.33942 | val_0_rmse: 0.63488 | val_1_rmse: 0.63958 |  0:04:17s
epoch 93 | loss: 0.33855 | val_0_rmse: 0.58099 | val_1_rmse: 0.58371 |  0:04:20s
epoch 94 | loss: 0.33955 | val_0_rmse: 0.69392 | val_1_rmse: 0.70013 |  0:04:23s
epoch 95 | loss: 0.33926 | val_0_rmse: 0.63803 | val_1_rmse: 0.63772 |  0:04:25s
epoch 96 | loss: 0.33608 | val_0_rmse: 0.5741  | val_1_rmse: 0.58317 |  0:04:28s
epoch 97 | loss: 0.33813 | val_0_rmse: 0.60038 | val_1_rmse: 0.60506 |  0:04:31s
epoch 98 | loss: 0.33772 | val_0_rmse: 0.67948 | val_1_rmse: 0.68206 |  0:04:34s
epoch 99 | loss: 0.34054 | val_0_rmse: 0.58029 | val_1_rmse: 0.58826 |  0:04:37s
epoch 100| loss: 0.3428  | val_0_rmse: 0.60214 | val_1_rmse: 0.60685 |  0:04:39s
epoch 101| loss: 0.33886 | val_0_rmse: 0.66462 | val_1_rmse: 0.67039 |  0:04:42s
epoch 102| loss: 0.3339  | val_0_rmse: 0.6729  | val_1_rmse: 0.67755 |  0:04:45s
epoch 103| loss: 0.33895 | val_0_rmse: 0.61707 | val_1_rmse: 0.62226 |  0:04:48s
epoch 104| loss: 0.3401  | val_0_rmse: 0.6593  | val_1_rmse: 0.66001 |  0:04:50s
epoch 105| loss: 0.33485 | val_0_rmse: 0.59621 | val_1_rmse: 0.60481 |  0:04:53s
epoch 106| loss: 0.33807 | val_0_rmse: 0.61407 | val_1_rmse: 0.61573 |  0:04:56s
epoch 107| loss: 0.34248 | val_0_rmse: 0.62426 | val_1_rmse: 0.63168 |  0:04:59s
epoch 108| loss: 0.33838 | val_0_rmse: 0.65686 | val_1_rmse: 0.65977 |  0:05:02s
epoch 109| loss: 0.33617 | val_0_rmse: 0.61889 | val_1_rmse: 0.62408 |  0:05:04s
epoch 110| loss: 0.33672 | val_0_rmse: 0.61947 | val_1_rmse: 0.62275 |  0:05:07s
epoch 111| loss: 0.35242 | val_0_rmse: 0.76344 | val_1_rmse: 0.76804 |  0:05:10s
epoch 112| loss: 0.33938 | val_0_rmse: 0.62925 | val_1_rmse: 0.63602 |  0:05:13s
epoch 113| loss: 0.34247 | val_0_rmse: 0.78847 | val_1_rmse: 0.78305 |  0:05:15s
epoch 114| loss: 0.33879 | val_0_rmse: 0.57749 | val_1_rmse: 0.58217 |  0:05:18s

Early stopping occured at epoch 114 with best_epoch = 84 and best_val_1_rmse = 0.57721
Best weights from best epoch are automatically used!
ended training at: 13:19:56
Feature importance:
[('Area', 0.3452712894534237), ('Baths', 0.14125670666033457), ('Beds', 0.0006849266773635944), ('Latitude', 0.2486720279052246), ('Longitude', 0.2501204623781642), ('Month', 0.01399458692548927), ('Year', 0.0)]
Mean squared error is of 2273073248.77121
Mean absolute error:34464.222774316244
MAPE:0.34489441256736614
R2 score:0.6657403479280892
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:19:57
epoch 0  | loss: 0.64748 | val_0_rmse: 0.73028 | val_1_rmse: 0.7226  |  0:00:02s
epoch 1  | loss: 0.5247  | val_0_rmse: 0.71971 | val_1_rmse: 0.71354 |  0:00:05s
epoch 2  | loss: 0.50678 | val_0_rmse: 0.71328 | val_1_rmse: 0.70881 |  0:00:08s
epoch 3  | loss: 0.46487 | val_0_rmse: 0.70451 | val_1_rmse: 0.70059 |  0:00:11s
epoch 4  | loss: 0.44409 | val_0_rmse: 0.65055 | val_1_rmse: 0.64608 |  0:00:14s
epoch 5  | loss: 0.42101 | val_0_rmse: 0.62492 | val_1_rmse: 0.62078 |  0:00:16s
epoch 6  | loss: 0.41382 | val_0_rmse: 0.64258 | val_1_rmse: 0.6417  |  0:00:19s
epoch 7  | loss: 0.40638 | val_0_rmse: 0.67279 | val_1_rmse: 0.66578 |  0:00:22s
epoch 8  | loss: 0.40096 | val_0_rmse: 0.64725 | val_1_rmse: 0.6444  |  0:00:25s
epoch 9  | loss: 0.39022 | val_0_rmse: 0.61788 | val_1_rmse: 0.61169 |  0:00:27s
epoch 10 | loss: 0.3775  | val_0_rmse: 0.60908 | val_1_rmse: 0.60726 |  0:00:30s
epoch 11 | loss: 0.39163 | val_0_rmse: 0.61535 | val_1_rmse: 0.60946 |  0:00:33s
epoch 12 | loss: 0.37863 | val_0_rmse: 0.61285 | val_1_rmse: 0.60593 |  0:00:36s
epoch 13 | loss: 0.37775 | val_0_rmse: 0.62534 | val_1_rmse: 0.62197 |  0:00:39s
epoch 14 | loss: 0.36881 | val_0_rmse: 0.7508  | val_1_rmse: 0.75577 |  0:00:41s
epoch 15 | loss: 0.37694 | val_0_rmse: 0.65927 | val_1_rmse: 0.65975 |  0:00:44s
epoch 16 | loss: 0.37271 | val_0_rmse: 0.62453 | val_1_rmse: 0.61645 |  0:00:47s
epoch 17 | loss: 0.35835 | val_0_rmse: 0.60134 | val_1_rmse: 0.60224 |  0:00:50s
epoch 18 | loss: 0.36139 | val_0_rmse: 0.63749 | val_1_rmse: 0.63541 |  0:00:52s
epoch 19 | loss: 0.35913 | val_0_rmse: 0.65072 | val_1_rmse: 0.65451 |  0:00:55s
epoch 20 | loss: 0.36146 | val_0_rmse: 0.60181 | val_1_rmse: 0.60204 |  0:00:58s
epoch 21 | loss: 0.36207 | val_0_rmse: 0.59373 | val_1_rmse: 0.59417 |  0:01:01s
epoch 22 | loss: 0.36222 | val_0_rmse: 0.63219 | val_1_rmse: 0.63549 |  0:01:03s
epoch 23 | loss: 0.36667 | val_0_rmse: 0.6086  | val_1_rmse: 0.60532 |  0:01:06s
epoch 24 | loss: 0.36075 | val_0_rmse: 0.59829 | val_1_rmse: 0.60138 |  0:01:09s
epoch 25 | loss: 0.35903 | val_0_rmse: 0.62002 | val_1_rmse: 0.6239  |  0:01:12s
epoch 26 | loss: 0.35103 | val_0_rmse: 0.62346 | val_1_rmse: 0.6272  |  0:01:15s
epoch 27 | loss: 0.35262 | val_0_rmse: 0.64453 | val_1_rmse: 0.63414 |  0:01:17s
epoch 28 | loss: 0.35284 | val_0_rmse: 0.589   | val_1_rmse: 0.58685 |  0:01:20s
epoch 29 | loss: 0.36146 | val_0_rmse: 0.65501 | val_1_rmse: 0.64652 |  0:01:23s
epoch 30 | loss: 0.35105 | val_0_rmse: 0.7161  | val_1_rmse: 0.70943 |  0:01:26s
epoch 31 | loss: 0.35179 | val_0_rmse: 0.6315  | val_1_rmse: 0.63434 |  0:01:29s
epoch 32 | loss: 0.35759 | val_0_rmse: 0.5963  | val_1_rmse: 0.5958  |  0:01:31s
epoch 33 | loss: 0.35305 | val_0_rmse: 0.62694 | val_1_rmse: 0.62883 |  0:01:34s
epoch 34 | loss: 0.35937 | val_0_rmse: 0.63836 | val_1_rmse: 0.63596 |  0:01:37s
epoch 35 | loss: 0.35336 | val_0_rmse: 0.60657 | val_1_rmse: 0.59937 |  0:01:40s
epoch 36 | loss: 0.35078 | val_0_rmse: 0.62118 | val_1_rmse: 0.61745 |  0:01:42s
epoch 37 | loss: 0.34936 | val_0_rmse: 0.61555 | val_1_rmse: 0.60931 |  0:01:45s
epoch 38 | loss: 0.34788 | val_0_rmse: 0.58148 | val_1_rmse: 0.57835 |  0:01:48s
epoch 39 | loss: 0.35016 | val_0_rmse: 0.65688 | val_1_rmse: 0.65548 |  0:01:51s
epoch 40 | loss: 0.3471  | val_0_rmse: 0.62018 | val_1_rmse: 0.6239  |  0:01:54s
epoch 41 | loss: 0.34884 | val_0_rmse: 0.67245 | val_1_rmse: 0.67466 |  0:01:56s
epoch 42 | loss: 0.34891 | val_0_rmse: 0.62673 | val_1_rmse: 0.61832 |  0:01:59s
epoch 43 | loss: 0.35353 | val_0_rmse: 0.58136 | val_1_rmse: 0.57891 |  0:02:02s
epoch 44 | loss: 0.35034 | val_0_rmse: 0.60589 | val_1_rmse: 0.59997 |  0:02:05s
epoch 45 | loss: 0.35194 | val_0_rmse: 0.64188 | val_1_rmse: 0.63542 |  0:02:08s
epoch 46 | loss: 0.35355 | val_0_rmse: 0.65233 | val_1_rmse: 0.65787 |  0:02:10s
epoch 47 | loss: 0.34812 | val_0_rmse: 0.60312 | val_1_rmse: 0.6     |  0:02:13s
epoch 48 | loss: 0.35031 | val_0_rmse: 0.58221 | val_1_rmse: 0.58095 |  0:02:16s
epoch 49 | loss: 0.35944 | val_0_rmse: 0.60283 | val_1_rmse: 0.60291 |  0:02:19s
epoch 50 | loss: 0.35156 | val_0_rmse: 0.65142 | val_1_rmse: 0.65048 |  0:02:21s
epoch 51 | loss: 0.35262 | val_0_rmse: 0.67223 | val_1_rmse: 0.67311 |  0:02:24s
epoch 52 | loss: 0.35223 | val_0_rmse: 0.65261 | val_1_rmse: 0.65157 |  0:02:27s
epoch 53 | loss: 0.35111 | val_0_rmse: 0.61597 | val_1_rmse: 0.61784 |  0:02:30s
epoch 54 | loss: 0.34945 | val_0_rmse: 0.60271 | val_1_rmse: 0.60151 |  0:02:32s
epoch 55 | loss: 0.34969 | val_0_rmse: 0.64471 | val_1_rmse: 0.6356  |  0:02:35s
epoch 56 | loss: 0.35279 | val_0_rmse: 0.73173 | val_1_rmse: 0.73454 |  0:02:38s
epoch 57 | loss: 0.35576 | val_0_rmse: 0.61559 | val_1_rmse: 0.61065 |  0:02:41s
epoch 58 | loss: 0.35786 | val_0_rmse: 0.69506 | val_1_rmse: 0.70159 |  0:02:44s
epoch 59 | loss: 0.3489  | val_0_rmse: 0.60714 | val_1_rmse: 0.60247 |  0:02:46s
epoch 60 | loss: 0.34765 | val_0_rmse: 0.6119  | val_1_rmse: 0.61146 |  0:02:49s
epoch 61 | loss: 0.34621 | val_0_rmse: 0.62898 | val_1_rmse: 0.6349  |  0:02:52s
epoch 62 | loss: 0.34577 | val_0_rmse: 0.63516 | val_1_rmse: 0.64026 |  0:02:55s
epoch 63 | loss: 0.34362 | val_0_rmse: 0.6253  | val_1_rmse: 0.62976 |  0:02:57s
epoch 64 | loss: 0.34771 | val_0_rmse: 0.60077 | val_1_rmse: 0.59323 |  0:03:00s
epoch 65 | loss: 0.34334 | val_0_rmse: 0.58144 | val_1_rmse: 0.58022 |  0:03:03s
epoch 66 | loss: 0.34637 | val_0_rmse: 0.59576 | val_1_rmse: 0.59333 |  0:03:06s
epoch 67 | loss: 0.3444  | val_0_rmse: 0.67092 | val_1_rmse: 0.6728  |  0:03:09s
epoch 68 | loss: 0.34561 | val_0_rmse: 0.64527 | val_1_rmse: 0.64691 |  0:03:11s

Early stopping occured at epoch 68 with best_epoch = 38 and best_val_1_rmse = 0.57835
Best weights from best epoch are automatically used!
ended training at: 13:23:10
Feature importance:
[('Area', 0.2466288785953617), ('Baths', 0.19602940311867897), ('Beds', 0.0), ('Latitude', 0.3438197000017067), ('Longitude', 0.21266503939899745), ('Month', 0.000856978885255198), ('Year', 0.0)]
Mean squared error is of 2392031548.995959
Mean absolute error:35324.386835836616
MAPE:0.33657874683917754
R2 score:0.6583188483583845
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:23:10
epoch 0  | loss: 0.6619  | val_0_rmse: 0.74274 | val_1_rmse: 0.74562 |  0:00:02s
epoch 1  | loss: 0.53619 | val_0_rmse: 0.72766 | val_1_rmse: 0.73185 |  0:00:05s
epoch 2  | loss: 0.51383 | val_0_rmse: 0.70363 | val_1_rmse: 0.70401 |  0:00:08s
epoch 3  | loss: 0.45496 | val_0_rmse: 0.7097  | val_1_rmse: 0.70764 |  0:00:11s
epoch 4  | loss: 0.43228 | val_0_rmse: 0.68102 | val_1_rmse: 0.67867 |  0:00:14s
epoch 5  | loss: 0.41128 | val_0_rmse: 0.77126 | val_1_rmse: 0.77489 |  0:00:16s
epoch 6  | loss: 0.39739 | val_0_rmse: 0.63569 | val_1_rmse: 0.63401 |  0:00:19s
epoch 7  | loss: 0.3947  | val_0_rmse: 0.71027 | val_1_rmse: 0.71097 |  0:00:22s
epoch 8  | loss: 0.3925  | val_0_rmse: 0.63706 | val_1_rmse: 0.63383 |  0:00:25s
epoch 9  | loss: 0.39444 | val_0_rmse: 0.69996 | val_1_rmse: 0.70319 |  0:00:27s
epoch 10 | loss: 0.38635 | val_0_rmse: 0.63774 | val_1_rmse: 0.63962 |  0:00:30s
epoch 11 | loss: 0.3816  | val_0_rmse: 0.6284  | val_1_rmse: 0.63196 |  0:00:33s
epoch 12 | loss: 0.38357 | val_0_rmse: 0.7088  | val_1_rmse: 0.70731 |  0:00:36s
epoch 13 | loss: 0.38534 | val_0_rmse: 0.63074 | val_1_rmse: 0.63155 |  0:00:38s
epoch 14 | loss: 0.38002 | val_0_rmse: 0.68124 | val_1_rmse: 0.68033 |  0:00:41s
epoch 15 | loss: 0.39038 | val_0_rmse: 0.66004 | val_1_rmse: 0.65858 |  0:00:44s
epoch 16 | loss: 0.38675 | val_0_rmse: 0.61545 | val_1_rmse: 0.61654 |  0:00:47s
epoch 17 | loss: 0.38418 | val_0_rmse: 0.75234 | val_1_rmse: 0.74934 |  0:00:50s
epoch 18 | loss: 0.37819 | val_0_rmse: 0.66301 | val_1_rmse: 0.66106 |  0:00:52s
epoch 19 | loss: 0.37402 | val_0_rmse: 0.89833 | val_1_rmse: 0.90264 |  0:00:55s
epoch 20 | loss: 0.37823 | val_0_rmse: 0.68502 | val_1_rmse: 0.69008 |  0:00:58s
epoch 21 | loss: 0.37869 | val_0_rmse: 0.68336 | val_1_rmse: 0.68123 |  0:01:01s
epoch 22 | loss: 0.37623 | val_0_rmse: 0.73588 | val_1_rmse: 0.73884 |  0:01:03s
epoch 23 | loss: 0.37514 | val_0_rmse: 0.62101 | val_1_rmse: 0.62342 |  0:01:06s
epoch 24 | loss: 0.37106 | val_0_rmse: 0.64304 | val_1_rmse: 0.64085 |  0:01:09s
epoch 25 | loss: 0.38019 | val_0_rmse: 0.64734 | val_1_rmse: 0.64846 |  0:01:12s
epoch 26 | loss: 0.37495 | val_0_rmse: 0.63053 | val_1_rmse: 0.63077 |  0:01:15s
epoch 27 | loss: 0.3772  | val_0_rmse: 0.6594  | val_1_rmse: 0.66234 |  0:01:17s
epoch 28 | loss: 0.37328 | val_0_rmse: 0.64144 | val_1_rmse: 0.63795 |  0:01:20s
epoch 29 | loss: 0.37222 | val_0_rmse: 0.61036 | val_1_rmse: 0.61118 |  0:01:23s
epoch 30 | loss: 0.37267 | val_0_rmse: 0.64397 | val_1_rmse: 0.64002 |  0:01:26s
epoch 31 | loss: 0.37186 | val_0_rmse: 0.81802 | val_1_rmse: 0.8173  |  0:01:29s
epoch 32 | loss: 0.36967 | val_0_rmse: 0.67196 | val_1_rmse: 0.66865 |  0:01:31s
epoch 33 | loss: 0.37129 | val_0_rmse: 0.69604 | val_1_rmse: 0.69271 |  0:01:34s
epoch 34 | loss: 0.36859 | val_0_rmse: 0.79526 | val_1_rmse: 0.79197 |  0:01:37s
epoch 35 | loss: 0.37115 | val_0_rmse: 0.63277 | val_1_rmse: 0.63378 |  0:01:40s
epoch 36 | loss: 0.36625 | val_0_rmse: 0.64652 | val_1_rmse: 0.64771 |  0:01:42s
epoch 37 | loss: 0.36704 | val_0_rmse: 0.64126 | val_1_rmse: 0.63746 |  0:01:45s
epoch 38 | loss: 0.3725  | val_0_rmse: 0.60957 | val_1_rmse: 0.60715 |  0:01:48s
epoch 39 | loss: 0.36477 | val_0_rmse: 0.63812 | val_1_rmse: 0.63586 |  0:01:51s
epoch 40 | loss: 0.36465 | val_0_rmse: 0.64703 | val_1_rmse: 0.64564 |  0:01:54s
epoch 41 | loss: 0.36951 | val_0_rmse: 0.63904 | val_1_rmse: 0.6407  |  0:01:56s
epoch 42 | loss: 0.36649 | val_0_rmse: 0.62078 | val_1_rmse: 0.6199  |  0:01:59s
epoch 43 | loss: 0.36397 | val_0_rmse: 0.59323 | val_1_rmse: 0.5963  |  0:02:02s
epoch 44 | loss: 0.36776 | val_0_rmse: 0.62377 | val_1_rmse: 0.62093 |  0:02:05s
epoch 45 | loss: 0.36495 | val_0_rmse: 0.62971 | val_1_rmse: 0.63108 |  0:02:08s
epoch 46 | loss: 0.37078 | val_0_rmse: 0.62792 | val_1_rmse: 0.62991 |  0:02:10s
epoch 47 | loss: 0.3674  | val_0_rmse: 0.6596  | val_1_rmse: 0.65666 |  0:02:13s
epoch 48 | loss: 0.36761 | val_0_rmse: 0.6661  | val_1_rmse: 0.66139 |  0:02:16s
epoch 49 | loss: 0.36847 | val_0_rmse: 0.63426 | val_1_rmse: 0.63197 |  0:02:19s
epoch 50 | loss: 0.36459 | val_0_rmse: 0.67844 | val_1_rmse: 0.67569 |  0:02:21s
epoch 51 | loss: 0.36914 | val_0_rmse: 0.73522 | val_1_rmse: 0.74163 |  0:02:24s
epoch 52 | loss: 0.36057 | val_0_rmse: 0.62928 | val_1_rmse: 0.62892 |  0:02:27s
epoch 53 | loss: 0.36786 | val_0_rmse: 0.62767 | val_1_rmse: 0.63077 |  0:02:30s
epoch 54 | loss: 0.37116 | val_0_rmse: 0.79473 | val_1_rmse: 0.79138 |  0:02:33s
epoch 55 | loss: 0.36619 | val_0_rmse: 0.63311 | val_1_rmse: 0.63201 |  0:02:35s
epoch 56 | loss: 0.36447 | val_0_rmse: 0.61436 | val_1_rmse: 0.61422 |  0:02:38s
epoch 57 | loss: 0.36075 | val_0_rmse: 0.59816 | val_1_rmse: 0.60019 |  0:02:41s
epoch 58 | loss: 0.36787 | val_0_rmse: 0.65216 | val_1_rmse: 0.65547 |  0:02:44s
epoch 59 | loss: 0.36579 | val_0_rmse: 0.66789 | val_1_rmse: 0.67167 |  0:02:46s
epoch 60 | loss: 0.36459 | val_0_rmse: 0.65489 | val_1_rmse: 0.65801 |  0:02:49s
epoch 61 | loss: 0.36231 | val_0_rmse: 0.72393 | val_1_rmse: 0.71908 |  0:02:52s
epoch 62 | loss: 0.36626 | val_0_rmse: 0.62091 | val_1_rmse: 0.61848 |  0:02:55s
epoch 63 | loss: 0.36537 | val_0_rmse: 0.63309 | val_1_rmse: 0.63106 |  0:02:58s
epoch 64 | loss: 0.36313 | val_0_rmse: 0.79652 | val_1_rmse: 0.80121 |  0:03:00s
epoch 65 | loss: 0.36308 | val_0_rmse: 0.63493 | val_1_rmse: 0.63789 |  0:03:03s
epoch 66 | loss: 0.35824 | val_0_rmse: 0.61838 | val_1_rmse: 0.62231 |  0:03:06s
epoch 67 | loss: 0.36593 | val_0_rmse: 0.65062 | val_1_rmse: 0.65544 |  0:03:09s
epoch 68 | loss: 0.36298 | val_0_rmse: 0.63302 | val_1_rmse: 0.63068 |  0:03:12s
epoch 69 | loss: 0.36123 | val_0_rmse: 0.64657 | val_1_rmse: 0.64659 |  0:03:14s
epoch 70 | loss: 0.36683 | val_0_rmse: 0.5947  | val_1_rmse: 0.59663 |  0:03:17s
epoch 71 | loss: 0.35933 | val_0_rmse: 0.59249 | val_1_rmse: 0.5941  |  0:03:20s
epoch 72 | loss: 0.36298 | val_0_rmse: 0.6079  | val_1_rmse: 0.6088  |  0:03:23s
epoch 73 | loss: 0.35925 | val_0_rmse: 0.65076 | val_1_rmse: 0.64968 |  0:03:25s
epoch 74 | loss: 0.36283 | val_0_rmse: 0.71237 | val_1_rmse: 0.717   |  0:03:28s
epoch 75 | loss: 0.36156 | val_0_rmse: 0.61582 | val_1_rmse: 0.61319 |  0:03:31s
epoch 76 | loss: 0.36093 | val_0_rmse: 0.59768 | val_1_rmse: 0.59878 |  0:03:34s
epoch 77 | loss: 0.3614  | val_0_rmse: 0.63007 | val_1_rmse: 0.6306  |  0:03:37s
epoch 78 | loss: 0.36376 | val_0_rmse: 0.60194 | val_1_rmse: 0.60457 |  0:03:39s
epoch 79 | loss: 0.3557  | val_0_rmse: 0.69831 | val_1_rmse: 0.7024  |  0:03:42s
epoch 80 | loss: 0.35903 | val_0_rmse: 0.60788 | val_1_rmse: 0.61166 |  0:03:45s
epoch 81 | loss: 0.35965 | val_0_rmse: 0.6617  | val_1_rmse: 0.66397 |  0:03:48s
epoch 82 | loss: 0.36115 | val_0_rmse: 0.64785 | val_1_rmse: 0.64924 |  0:03:50s
epoch 83 | loss: 0.36116 | val_0_rmse: 0.71374 | val_1_rmse: 0.71719 |  0:03:53s
epoch 84 | loss: 0.35904 | val_0_rmse: 0.66119 | val_1_rmse: 0.65806 |  0:03:56s
epoch 85 | loss: 0.36184 | val_0_rmse: 0.61739 | val_1_rmse: 0.61711 |  0:03:59s
epoch 86 | loss: 0.35819 | val_0_rmse: 0.6483  | val_1_rmse: 0.6467  |  0:04:02s
epoch 87 | loss: 0.35811 | val_0_rmse: 0.59189 | val_1_rmse: 0.59395 |  0:04:04s
epoch 88 | loss: 0.3585  | val_0_rmse: 0.64409 | val_1_rmse: 0.64295 |  0:04:07s
epoch 89 | loss: 0.35679 | val_0_rmse: 0.66396 | val_1_rmse: 0.66714 |  0:04:10s
epoch 90 | loss: 0.35667 | val_0_rmse: 0.62688 | val_1_rmse: 0.62379 |  0:04:13s
epoch 91 | loss: 0.3629  | val_0_rmse: 0.64165 | val_1_rmse: 0.64521 |  0:04:15s
epoch 92 | loss: 0.36004 | val_0_rmse: 0.67769 | val_1_rmse: 0.68281 |  0:04:18s
epoch 93 | loss: 0.3569  | val_0_rmse: 0.59636 | val_1_rmse: 0.59572 |  0:04:21s
epoch 94 | loss: 0.35945 | val_0_rmse: 0.74359 | val_1_rmse: 0.74209 |  0:04:24s
epoch 95 | loss: 0.3574  | val_0_rmse: 0.59512 | val_1_rmse: 0.59389 |  0:04:27s
epoch 96 | loss: 0.35942 | val_0_rmse: 0.63181 | val_1_rmse: 0.63331 |  0:04:29s
epoch 97 | loss: 0.36965 | val_0_rmse: 0.61811 | val_1_rmse: 0.61802 |  0:04:32s
epoch 98 | loss: 0.36194 | val_0_rmse: 0.77889 | val_1_rmse: 0.77712 |  0:04:35s
epoch 99 | loss: 0.36279 | val_0_rmse: 0.6149  | val_1_rmse: 0.61847 |  0:04:38s
epoch 100| loss: 0.35814 | val_0_rmse: 0.63046 | val_1_rmse: 0.62945 |  0:04:40s
epoch 101| loss: 0.36126 | val_0_rmse: 0.64357 | val_1_rmse: 0.64438 |  0:04:43s
epoch 102| loss: 0.35988 | val_0_rmse: 0.6065  | val_1_rmse: 0.60847 |  0:04:46s
epoch 103| loss: 0.361   | val_0_rmse: 0.63454 | val_1_rmse: 0.63291 |  0:04:49s
epoch 104| loss: 0.35418 | val_0_rmse: 0.60556 | val_1_rmse: 0.60757 |  0:04:52s
epoch 105| loss: 0.35738 | val_0_rmse: 0.5927  | val_1_rmse: 0.59479 |  0:04:54s
epoch 106| loss: 0.3575  | val_0_rmse: 0.62434 | val_1_rmse: 0.62353 |  0:04:57s
epoch 107| loss: 0.35967 | val_0_rmse: 0.75141 | val_1_rmse: 0.74737 |  0:05:00s
epoch 108| loss: 0.35609 | val_0_rmse: 0.65909 | val_1_rmse: 0.66079 |  0:05:03s
epoch 109| loss: 0.35663 | val_0_rmse: 0.67588 | val_1_rmse: 0.67987 |  0:05:05s
epoch 110| loss: 0.35749 | val_0_rmse: 0.61896 | val_1_rmse: 0.62042 |  0:05:08s
epoch 111| loss: 0.35615 | val_0_rmse: 0.72265 | val_1_rmse: 0.7221  |  0:05:11s
epoch 112| loss: 0.35485 | val_0_rmse: 0.63741 | val_1_rmse: 0.64153 |  0:05:14s
epoch 113| loss: 0.35505 | val_0_rmse: 0.61949 | val_1_rmse: 0.62048 |  0:05:17s
epoch 114| loss: 0.35811 | val_0_rmse: 0.60663 | val_1_rmse: 0.6105  |  0:05:19s
epoch 115| loss: 0.35498 | val_0_rmse: 0.65543 | val_1_rmse: 0.65557 |  0:05:22s
epoch 116| loss: 0.36344 | val_0_rmse: 1.0218  | val_1_rmse: 1.02591 |  0:05:25s
epoch 117| loss: 0.36733 | val_0_rmse: 0.67318 | val_1_rmse: 0.67414 |  0:05:28s
epoch 118| loss: 0.3574  | val_0_rmse: 0.61703 | val_1_rmse: 0.62084 |  0:05:31s
epoch 119| loss: 0.36108 | val_0_rmse: 0.63079 | val_1_rmse: 0.63537 |  0:05:33s
epoch 120| loss: 0.35571 | val_0_rmse: 0.70291 | val_1_rmse: 0.70555 |  0:05:36s
epoch 121| loss: 0.36092 | val_0_rmse: 0.85015 | val_1_rmse: 0.84812 |  0:05:39s
epoch 122| loss: 0.35673 | val_0_rmse: 0.74723 | val_1_rmse: 0.74513 |  0:05:42s
epoch 123| loss: 0.35579 | val_0_rmse: 0.79232 | val_1_rmse: 0.79585 |  0:05:44s
epoch 124| loss: 0.35352 | val_0_rmse: 0.65364 | val_1_rmse: 0.65241 |  0:05:47s
epoch 125| loss: 0.36056 | val_0_rmse: 0.64611 | val_1_rmse: 0.64321 |  0:05:50s

Early stopping occured at epoch 125 with best_epoch = 95 and best_val_1_rmse = 0.59389
Best weights from best epoch are automatically used!
ended training at: 13:29:02
Feature importance:
[('Area', 0.3756101857576404), ('Baths', 0.22226707837317922), ('Beds', 0.0), ('Latitude', 0.177124817467937), ('Longitude', 0.2249979184012434), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 2402591736.4148884
Mean absolute error:35436.57532812251
MAPE:0.34361904348112476
R2 score:0.6422445933183221
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:29:02
epoch 0  | loss: 0.64474 | val_0_rmse: 0.73467 | val_1_rmse: 0.73028 |  0:00:02s
epoch 1  | loss: 0.52681 | val_0_rmse: 0.71771 | val_1_rmse: 0.71877 |  0:00:05s
epoch 2  | loss: 0.52152 | val_0_rmse: 0.72542 | val_1_rmse: 0.72839 |  0:00:08s
epoch 3  | loss: 0.49912 | val_0_rmse: 0.69202 | val_1_rmse: 0.69414 |  0:00:11s
epoch 4  | loss: 0.4728  | val_0_rmse: 0.7057  | val_1_rmse: 0.70966 |  0:00:13s
epoch 5  | loss: 0.45296 | val_0_rmse: 0.67287 | val_1_rmse: 0.68095 |  0:00:16s
epoch 6  | loss: 0.42698 | val_0_rmse: 0.65899 | val_1_rmse: 0.66096 |  0:00:19s
epoch 7  | loss: 0.42028 | val_0_rmse: 0.64283 | val_1_rmse: 0.64814 |  0:00:22s
epoch 8  | loss: 0.403   | val_0_rmse: 0.65224 | val_1_rmse: 0.66031 |  0:00:25s
epoch 9  | loss: 0.40412 | val_0_rmse: 0.66025 | val_1_rmse: 0.66508 |  0:00:27s
epoch 10 | loss: 0.41088 | val_0_rmse: 0.70519 | val_1_rmse: 0.70756 |  0:00:30s
epoch 11 | loss: 0.41259 | val_0_rmse: 0.81743 | val_1_rmse: 0.82008 |  0:00:33s
epoch 12 | loss: 0.42022 | val_0_rmse: 0.65077 | val_1_rmse: 0.65334 |  0:00:36s
epoch 13 | loss: 0.4077  | val_0_rmse: 0.6754  | val_1_rmse: 0.67341 |  0:00:39s
epoch 14 | loss: 0.39821 | val_0_rmse: 0.63185 | val_1_rmse: 0.63115 |  0:00:41s
epoch 15 | loss: 0.38676 | val_0_rmse: 0.63526 | val_1_rmse: 0.63616 |  0:00:44s
epoch 16 | loss: 0.38549 | val_0_rmse: 0.65931 | val_1_rmse: 0.659   |  0:00:47s
epoch 17 | loss: 0.39234 | val_0_rmse: 0.64624 | val_1_rmse: 0.64652 |  0:00:50s
epoch 18 | loss: 0.38725 | val_0_rmse: 0.61258 | val_1_rmse: 0.61494 |  0:00:52s
epoch 19 | loss: 0.37814 | val_0_rmse: 0.61579 | val_1_rmse: 0.61716 |  0:00:55s
epoch 20 | loss: 0.37322 | val_0_rmse: 0.62128 | val_1_rmse: 0.63512 |  0:00:58s
epoch 21 | loss: 0.3728  | val_0_rmse: 0.6298  | val_1_rmse: 0.6352  |  0:01:01s
epoch 22 | loss: 0.38043 | val_0_rmse: 0.66894 | val_1_rmse: 0.67403 |  0:01:04s
epoch 23 | loss: 0.3771  | val_0_rmse: 0.59888 | val_1_rmse: 0.60563 |  0:01:06s
epoch 24 | loss: 0.36675 | val_0_rmse: 0.61826 | val_1_rmse: 0.62284 |  0:01:09s
epoch 25 | loss: 0.3715  | val_0_rmse: 0.62933 | val_1_rmse: 0.63122 |  0:01:12s
epoch 26 | loss: 0.3674  | val_0_rmse: 0.63909 | val_1_rmse: 0.63798 |  0:01:15s
epoch 27 | loss: 0.36715 | val_0_rmse: 0.74307 | val_1_rmse: 0.7473  |  0:01:18s
epoch 28 | loss: 0.37179 | val_0_rmse: 0.63339 | val_1_rmse: 0.63246 |  0:01:20s
epoch 29 | loss: 0.36329 | val_0_rmse: 0.76065 | val_1_rmse: 0.76339 |  0:01:23s
epoch 30 | loss: 0.36394 | val_0_rmse: 0.64032 | val_1_rmse: 0.6424  |  0:01:26s
epoch 31 | loss: 0.36758 | val_0_rmse: 0.65981 | val_1_rmse: 0.66441 |  0:01:29s
epoch 32 | loss: 0.38245 | val_0_rmse: 0.6507  | val_1_rmse: 0.64789 |  0:01:31s
epoch 33 | loss: 0.36852 | val_0_rmse: 0.63739 | val_1_rmse: 0.63579 |  0:01:34s
epoch 34 | loss: 0.36293 | val_0_rmse: 0.62063 | val_1_rmse: 0.62168 |  0:01:37s
epoch 35 | loss: 0.36635 | val_0_rmse: 0.62622 | val_1_rmse: 0.63112 |  0:01:40s
epoch 36 | loss: 0.37052 | val_0_rmse: 0.64298 | val_1_rmse: 0.64795 |  0:01:43s
epoch 37 | loss: 0.36241 | val_0_rmse: 0.62111 | val_1_rmse: 0.62313 |  0:01:45s
epoch 38 | loss: 0.36359 | val_0_rmse: 0.59042 | val_1_rmse: 0.59239 |  0:01:48s
epoch 39 | loss: 0.35809 | val_0_rmse: 0.64986 | val_1_rmse: 0.65139 |  0:01:51s
epoch 40 | loss: 0.35746 | val_0_rmse: 0.60465 | val_1_rmse: 0.61055 |  0:01:54s
epoch 41 | loss: 0.35944 | val_0_rmse: 0.63171 | val_1_rmse: 0.63309 |  0:01:56s
epoch 42 | loss: 0.35883 | val_0_rmse: 0.63482 | val_1_rmse: 0.63692 |  0:01:59s
epoch 43 | loss: 0.35683 | val_0_rmse: 0.59781 | val_1_rmse: 0.59887 |  0:02:02s
epoch 44 | loss: 0.35494 | val_0_rmse: 0.60129 | val_1_rmse: 0.6065  |  0:02:05s
epoch 45 | loss: 0.35151 | val_0_rmse: 0.69697 | val_1_rmse: 0.7005  |  0:02:08s
epoch 46 | loss: 0.359   | val_0_rmse: 0.62902 | val_1_rmse: 0.63297 |  0:02:10s
epoch 47 | loss: 0.35756 | val_0_rmse: 0.60677 | val_1_rmse: 0.61259 |  0:02:13s
epoch 48 | loss: 0.35365 | val_0_rmse: 0.61358 | val_1_rmse: 0.62405 |  0:02:16s
epoch 49 | loss: 0.35396 | val_0_rmse: 0.67309 | val_1_rmse: 0.67879 |  0:02:19s
epoch 50 | loss: 0.35822 | val_0_rmse: 0.63519 | val_1_rmse: 0.63948 |  0:02:22s
epoch 51 | loss: 0.35    | val_0_rmse: 0.66062 | val_1_rmse: 0.66305 |  0:02:24s
epoch 52 | loss: 0.35557 | val_0_rmse: 0.6591  | val_1_rmse: 0.66245 |  0:02:27s
epoch 53 | loss: 0.35416 | val_0_rmse: 0.6321  | val_1_rmse: 0.63737 |  0:02:30s
epoch 54 | loss: 0.35328 | val_0_rmse: 0.65686 | val_1_rmse: 0.66065 |  0:02:33s
epoch 55 | loss: 0.36144 | val_0_rmse: 0.67822 | val_1_rmse: 0.69218 |  0:02:35s
epoch 56 | loss: 0.37579 | val_0_rmse: 0.73762 | val_1_rmse: 0.73984 |  0:02:38s
epoch 57 | loss: 0.36159 | val_0_rmse: 0.61058 | val_1_rmse: 0.61726 |  0:02:41s
epoch 58 | loss: 0.35705 | val_0_rmse: 0.63805 | val_1_rmse: 0.63912 |  0:02:44s
epoch 59 | loss: 0.35091 | val_0_rmse: 0.61826 | val_1_rmse: 0.62163 |  0:02:47s
epoch 60 | loss: 0.35559 | val_0_rmse: 0.73802 | val_1_rmse: 0.74149 |  0:02:49s
epoch 61 | loss: 0.35089 | val_0_rmse: 0.63282 | val_1_rmse: 0.63463 |  0:02:52s
epoch 62 | loss: 0.35001 | val_0_rmse: 0.66655 | val_1_rmse: 0.66877 |  0:02:55s
epoch 63 | loss: 0.34717 | val_0_rmse: 0.65508 | val_1_rmse: 0.65865 |  0:02:58s
epoch 64 | loss: 0.35039 | val_0_rmse: 0.6181  | val_1_rmse: 0.62148 |  0:03:00s
epoch 65 | loss: 0.34935 | val_0_rmse: 0.63258 | val_1_rmse: 0.63851 |  0:03:03s
epoch 66 | loss: 0.3502  | val_0_rmse: 0.66458 | val_1_rmse: 0.66233 |  0:03:06s
epoch 67 | loss: 0.34929 | val_0_rmse: 0.60689 | val_1_rmse: 0.60854 |  0:03:09s
epoch 68 | loss: 0.3471  | val_0_rmse: 0.63635 | val_1_rmse: 0.6402  |  0:03:12s

Early stopping occured at epoch 68 with best_epoch = 38 and best_val_1_rmse = 0.59239
Best weights from best epoch are automatically used!
ended training at: 13:32:15
Feature importance:
[('Area', 0.2346969784956081), ('Baths', 0.15779933219055536), ('Beds', 0.049475949143489115), ('Latitude', 0.27493074483284713), ('Longitude', 0.2687095875108218), ('Month', 0.00014280263213454767), ('Year', 0.014244605194543921)]
Mean squared error is of 2436346138.085708
Mean absolute error:36146.94986992802
MAPE:0.3515614338474895
R2 score:0.6436752646363628
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:32:15
epoch 0  | loss: 0.67    | val_0_rmse: 0.73822 | val_1_rmse: 0.73175 |  0:00:02s
epoch 1  | loss: 0.53751 | val_0_rmse: 0.72609 | val_1_rmse: 0.7224  |  0:00:05s
epoch 2  | loss: 0.51933 | val_0_rmse: 0.72505 | val_1_rmse: 0.7266  |  0:00:08s
epoch 3  | loss: 0.48386 | val_0_rmse: 0.66253 | val_1_rmse: 0.65613 |  0:00:11s
epoch 4  | loss: 0.43098 | val_0_rmse: 0.72375 | val_1_rmse: 0.72696 |  0:00:13s
epoch 5  | loss: 0.42763 | val_0_rmse: 0.70537 | val_1_rmse: 0.71189 |  0:00:16s
epoch 6  | loss: 0.40758 | val_0_rmse: 0.69083 | val_1_rmse: 0.69875 |  0:00:19s
epoch 7  | loss: 0.39357 | val_0_rmse: 0.63181 | val_1_rmse: 0.62735 |  0:00:22s
epoch 8  | loss: 0.38318 | val_0_rmse: 0.60802 | val_1_rmse: 0.60787 |  0:00:25s
epoch 9  | loss: 0.38273 | val_0_rmse: 0.628   | val_1_rmse: 0.62175 |  0:00:27s
epoch 10 | loss: 0.38457 | val_0_rmse: 0.76662 | val_1_rmse: 0.75686 |  0:00:30s
epoch 11 | loss: 0.38119 | val_0_rmse: 0.76894 | val_1_rmse: 0.77786 |  0:00:33s
epoch 12 | loss: 0.37218 | val_0_rmse: 0.66238 | val_1_rmse: 0.6569  |  0:00:36s
epoch 13 | loss: 0.37105 | val_0_rmse: 0.60893 | val_1_rmse: 0.60428 |  0:00:39s
epoch 14 | loss: 0.37583 | val_0_rmse: 0.67099 | val_1_rmse: 0.67966 |  0:00:41s
epoch 15 | loss: 0.37242 | val_0_rmse: 0.65555 | val_1_rmse: 0.64882 |  0:00:44s
epoch 16 | loss: 0.36788 | val_0_rmse: 0.59655 | val_1_rmse: 0.59815 |  0:00:47s
epoch 17 | loss: 0.36751 | val_0_rmse: 0.64287 | val_1_rmse: 0.63341 |  0:00:50s
epoch 18 | loss: 0.3681  | val_0_rmse: 0.59846 | val_1_rmse: 0.59994 |  0:00:52s
epoch 19 | loss: 0.37376 | val_0_rmse: 0.58885 | val_1_rmse: 0.59211 |  0:00:55s
epoch 20 | loss: 0.3624  | val_0_rmse: 0.62204 | val_1_rmse: 0.62778 |  0:00:58s
epoch 21 | loss: 0.36123 | val_0_rmse: 0.69234 | val_1_rmse: 0.70318 |  0:01:01s
epoch 22 | loss: 0.36112 | val_0_rmse: 0.6428  | val_1_rmse: 0.64811 |  0:01:04s
epoch 23 | loss: 0.3631  | val_0_rmse: 0.60744 | val_1_rmse: 0.60325 |  0:01:06s
epoch 24 | loss: 0.36483 | val_0_rmse: 0.65103 | val_1_rmse: 0.64405 |  0:01:09s
epoch 25 | loss: 0.36801 | val_0_rmse: 0.78158 | val_1_rmse: 0.76878 |  0:01:12s
epoch 26 | loss: 0.36275 | val_0_rmse: 0.67124 | val_1_rmse: 0.67438 |  0:01:15s
epoch 27 | loss: 0.36113 | val_0_rmse: 0.66164 | val_1_rmse: 0.66715 |  0:01:18s
epoch 28 | loss: 0.35855 | val_0_rmse: 0.58273 | val_1_rmse: 0.58414 |  0:01:20s
epoch 29 | loss: 0.358   | val_0_rmse: 0.59476 | val_1_rmse: 0.5961  |  0:01:23s
epoch 30 | loss: 0.3585  | val_0_rmse: 0.63591 | val_1_rmse: 0.63262 |  0:01:26s
epoch 31 | loss: 0.35694 | val_0_rmse: 0.64691 | val_1_rmse: 0.64446 |  0:01:29s
epoch 32 | loss: 0.36293 | val_0_rmse: 0.62501 | val_1_rmse: 0.62129 |  0:01:32s
epoch 33 | loss: 0.3668  | val_0_rmse: 0.64487 | val_1_rmse: 0.64008 |  0:01:34s
epoch 34 | loss: 0.35857 | val_0_rmse: 0.63439 | val_1_rmse: 0.63329 |  0:01:37s
epoch 35 | loss: 0.35613 | val_0_rmse: 0.60604 | val_1_rmse: 0.61075 |  0:01:40s
epoch 36 | loss: 0.35247 | val_0_rmse: 0.58542 | val_1_rmse: 0.58764 |  0:01:43s
epoch 37 | loss: 0.35687 | val_0_rmse: 0.65982 | val_1_rmse: 0.65934 |  0:01:45s
epoch 38 | loss: 0.35893 | val_0_rmse: 0.64742 | val_1_rmse: 0.64229 |  0:01:48s
epoch 39 | loss: 0.36553 | val_0_rmse: 0.59733 | val_1_rmse: 0.60028 |  0:01:51s
epoch 40 | loss: 0.35452 | val_0_rmse: 0.58569 | val_1_rmse: 0.5874  |  0:01:54s
epoch 41 | loss: 0.35228 | val_0_rmse: 0.6985  | val_1_rmse: 0.70709 |  0:01:56s
epoch 42 | loss: 0.3514  | val_0_rmse: 0.59399 | val_1_rmse: 0.59545 |  0:01:59s
epoch 43 | loss: 0.35093 | val_0_rmse: 0.60999 | val_1_rmse: 0.60721 |  0:02:02s
epoch 44 | loss: 0.35049 | val_0_rmse: 0.6422  | val_1_rmse: 0.64153 |  0:02:05s
epoch 45 | loss: 0.35333 | val_0_rmse: 0.64517 | val_1_rmse: 0.64182 |  0:02:08s
epoch 46 | loss: 0.35511 | val_0_rmse: 0.59285 | val_1_rmse: 0.59069 |  0:02:10s
epoch 47 | loss: 0.35444 | val_0_rmse: 0.64519 | val_1_rmse: 0.65146 |  0:02:13s
epoch 48 | loss: 0.35277 | val_0_rmse: 0.72163 | val_1_rmse: 0.70843 |  0:02:16s
epoch 49 | loss: 0.35363 | val_0_rmse: 0.65168 | val_1_rmse: 0.65979 |  0:02:19s
epoch 50 | loss: 0.35647 | val_0_rmse: 0.61771 | val_1_rmse: 0.61405 |  0:02:22s
epoch 51 | loss: 0.35534 | val_0_rmse: 0.72519 | val_1_rmse: 0.71646 |  0:02:24s
epoch 52 | loss: 0.34511 | val_0_rmse: 0.62524 | val_1_rmse: 0.63097 |  0:02:27s
epoch 53 | loss: 0.34796 | val_0_rmse: 0.61364 | val_1_rmse: 0.62062 |  0:02:30s
epoch 54 | loss: 0.35075 | val_0_rmse: 0.59473 | val_1_rmse: 0.5969  |  0:02:33s
epoch 55 | loss: 0.34608 | val_0_rmse: 0.66531 | val_1_rmse: 0.65908 |  0:02:36s
epoch 56 | loss: 0.34573 | val_0_rmse: 0.69532 | val_1_rmse: 0.6897  |  0:02:38s
epoch 57 | loss: 0.34833 | val_0_rmse: 0.5976  | val_1_rmse: 0.59991 |  0:02:41s
epoch 58 | loss: 0.34926 | val_0_rmse: 0.59563 | val_1_rmse: 0.59407 |  0:02:44s

Early stopping occured at epoch 58 with best_epoch = 28 and best_val_1_rmse = 0.58414
Best weights from best epoch are automatically used!
ended training at: 13:35:01
Feature importance:
[('Area', 0.4143238846361459), ('Baths', 0.2004916008174785), ('Beds', 0.002631665100140324), ('Latitude', 0.19851090490859824), ('Longitude', 0.11541287921460638), ('Month', 0.0), ('Year', 0.06862906532303067)]
Mean squared error is of 2277723892.8438563
Mean absolute error:34674.655873722506
MAPE:0.34174138677232163
R2 score:0.6660584922277295
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:35:01
epoch 0  | loss: 0.68994 | val_0_rmse: 0.76581 | val_1_rmse: 0.76572 |  0:00:01s
epoch 1  | loss: 0.37743 | val_0_rmse: 0.62024 | val_1_rmse: 0.6282  |  0:00:02s
epoch 2  | loss: 0.35061 | val_0_rmse: 0.58303 | val_1_rmse: 0.58727 |  0:00:03s
epoch 3  | loss: 0.3462  | val_0_rmse: 0.60963 | val_1_rmse: 0.61522 |  0:00:04s
epoch 4  | loss: 0.33919 | val_0_rmse: 0.57842 | val_1_rmse: 0.58228 |  0:00:05s
epoch 5  | loss: 0.33639 | val_0_rmse: 0.5559  | val_1_rmse: 0.56038 |  0:00:06s
epoch 6  | loss: 0.31839 | val_0_rmse: 0.552   | val_1_rmse: 0.56086 |  0:00:07s
epoch 7  | loss: 0.31139 | val_0_rmse: 0.55188 | val_1_rmse: 0.56183 |  0:00:08s
epoch 8  | loss: 0.30785 | val_0_rmse: 0.54447 | val_1_rmse: 0.55369 |  0:00:09s
epoch 9  | loss: 0.30355 | val_0_rmse: 0.54233 | val_1_rmse: 0.54577 |  0:00:10s
epoch 10 | loss: 0.30182 | val_0_rmse: 0.54164 | val_1_rmse: 0.55105 |  0:00:11s
epoch 11 | loss: 0.29923 | val_0_rmse: 0.52916 | val_1_rmse: 0.53867 |  0:00:12s
epoch 12 | loss: 0.29519 | val_0_rmse: 0.52702 | val_1_rmse: 0.53541 |  0:00:13s
epoch 13 | loss: 0.29249 | val_0_rmse: 0.53134 | val_1_rmse: 0.54096 |  0:00:14s
epoch 14 | loss: 0.29657 | val_0_rmse: 0.53336 | val_1_rmse: 0.5384  |  0:00:15s
epoch 15 | loss: 0.28848 | val_0_rmse: 0.57596 | val_1_rmse: 0.58614 |  0:00:16s
epoch 16 | loss: 0.30579 | val_0_rmse: 0.5663  | val_1_rmse: 0.57622 |  0:00:17s
epoch 17 | loss: 0.29902 | val_0_rmse: 0.53762 | val_1_rmse: 0.54755 |  0:00:18s
epoch 18 | loss: 0.29293 | val_0_rmse: 0.54088 | val_1_rmse: 0.55284 |  0:00:19s
epoch 19 | loss: 0.2937  | val_0_rmse: 0.53071 | val_1_rmse: 0.54183 |  0:00:20s
epoch 20 | loss: 0.28571 | val_0_rmse: 0.53496 | val_1_rmse: 0.54216 |  0:00:21s
epoch 21 | loss: 0.29077 | val_0_rmse: 0.52377 | val_1_rmse: 0.53413 |  0:00:22s
epoch 22 | loss: 0.28755 | val_0_rmse: 0.52523 | val_1_rmse: 0.53676 |  0:00:23s
epoch 23 | loss: 0.28277 | val_0_rmse: 0.51846 | val_1_rmse: 0.53124 |  0:00:24s
epoch 24 | loss: 0.28218 | val_0_rmse: 0.52921 | val_1_rmse: 0.54423 |  0:00:25s
epoch 25 | loss: 0.27987 | val_0_rmse: 0.51806 | val_1_rmse: 0.52944 |  0:00:26s
epoch 26 | loss: 0.28362 | val_0_rmse: 0.52794 | val_1_rmse: 0.54002 |  0:00:27s
epoch 27 | loss: 0.28592 | val_0_rmse: 0.5247  | val_1_rmse: 0.53836 |  0:00:28s
epoch 28 | loss: 0.2881  | val_0_rmse: 0.54772 | val_1_rmse: 0.55639 |  0:00:29s
epoch 29 | loss: 0.28524 | val_0_rmse: 0.51903 | val_1_rmse: 0.5289  |  0:00:30s
epoch 30 | loss: 0.2823  | val_0_rmse: 0.52798 | val_1_rmse: 0.53822 |  0:00:31s
epoch 31 | loss: 0.28475 | val_0_rmse: 0.51925 | val_1_rmse: 0.53244 |  0:00:32s
epoch 32 | loss: 0.27455 | val_0_rmse: 0.53205 | val_1_rmse: 0.54057 |  0:00:33s
epoch 33 | loss: 0.27762 | val_0_rmse: 0.5933  | val_1_rmse: 0.59839 |  0:00:34s
epoch 34 | loss: 0.27681 | val_0_rmse: 0.50998 | val_1_rmse: 0.52167 |  0:00:36s
epoch 35 | loss: 0.27248 | val_0_rmse: 0.63007 | val_1_rmse: 0.63828 |  0:00:37s
epoch 36 | loss: 0.26822 | val_0_rmse: 0.52843 | val_1_rmse: 0.54098 |  0:00:38s
epoch 37 | loss: 0.27448 | val_0_rmse: 0.51519 | val_1_rmse: 0.52637 |  0:00:39s
epoch 38 | loss: 0.27412 | val_0_rmse: 0.50189 | val_1_rmse: 0.50989 |  0:00:40s
epoch 39 | loss: 0.27083 | val_0_rmse: 0.52462 | val_1_rmse: 0.53165 |  0:00:41s
epoch 40 | loss: 0.26695 | val_0_rmse: 0.50101 | val_1_rmse: 0.5095  |  0:00:42s
epoch 41 | loss: 0.26319 | val_0_rmse: 0.5067  | val_1_rmse: 0.51996 |  0:00:43s
epoch 42 | loss: 0.26913 | val_0_rmse: 0.53819 | val_1_rmse: 0.55036 |  0:00:44s
epoch 43 | loss: 0.26807 | val_0_rmse: 0.54268 | val_1_rmse: 0.54974 |  0:00:45s
epoch 44 | loss: 0.26694 | val_0_rmse: 0.52445 | val_1_rmse: 0.53095 |  0:00:46s
epoch 45 | loss: 0.26121 | val_0_rmse: 0.5287  | val_1_rmse: 0.53425 |  0:00:47s
epoch 46 | loss: 0.26124 | val_0_rmse: 0.5084  | val_1_rmse: 0.51463 |  0:00:48s
epoch 47 | loss: 0.25893 | val_0_rmse: 0.58555 | val_1_rmse: 0.59344 |  0:00:49s
epoch 48 | loss: 0.27378 | val_0_rmse: 0.50861 | val_1_rmse: 0.51731 |  0:00:50s
epoch 49 | loss: 0.26646 | val_0_rmse: 0.50243 | val_1_rmse: 0.50998 |  0:00:51s
epoch 50 | loss: 0.26251 | val_0_rmse: 0.5161  | val_1_rmse: 0.5184  |  0:00:52s
epoch 51 | loss: 0.26236 | val_0_rmse: 0.50614 | val_1_rmse: 0.51406 |  0:00:53s
epoch 52 | loss: 0.26358 | val_0_rmse: 0.49928 | val_1_rmse: 0.50497 |  0:00:54s
epoch 53 | loss: 0.26191 | val_0_rmse: 0.50986 | val_1_rmse: 0.51341 |  0:00:55s
epoch 54 | loss: 0.25918 | val_0_rmse: 0.52513 | val_1_rmse: 0.52839 |  0:00:56s
epoch 55 | loss: 0.25604 | val_0_rmse: 0.48771 | val_1_rmse: 0.49537 |  0:00:57s
epoch 56 | loss: 0.25943 | val_0_rmse: 0.50193 | val_1_rmse: 0.50914 |  0:00:58s
epoch 57 | loss: 0.26916 | val_0_rmse: 0.54338 | val_1_rmse: 0.54883 |  0:00:59s
epoch 58 | loss: 0.25984 | val_0_rmse: 0.50852 | val_1_rmse: 0.51391 |  0:01:00s
epoch 59 | loss: 0.25667 | val_0_rmse: 0.53784 | val_1_rmse: 0.55429 |  0:01:01s
epoch 60 | loss: 0.25605 | val_0_rmse: 0.50315 | val_1_rmse: 0.51124 |  0:01:02s
epoch 61 | loss: 0.25209 | val_0_rmse: 0.48951 | val_1_rmse: 0.49661 |  0:01:03s
epoch 62 | loss: 0.2547  | val_0_rmse: 0.50758 | val_1_rmse: 0.51423 |  0:01:04s
epoch 63 | loss: 0.25891 | val_0_rmse: 0.49688 | val_1_rmse: 0.50477 |  0:01:05s
epoch 64 | loss: 0.25658 | val_0_rmse: 0.5381  | val_1_rmse: 0.54711 |  0:01:06s
epoch 65 | loss: 0.25688 | val_0_rmse: 0.58639 | val_1_rmse: 0.59407 |  0:01:07s
epoch 66 | loss: 0.26068 | val_0_rmse: 0.53546 | val_1_rmse: 0.54161 |  0:01:08s
epoch 67 | loss: 0.26009 | val_0_rmse: 0.53247 | val_1_rmse: 0.54197 |  0:01:09s
epoch 68 | loss: 0.25262 | val_0_rmse: 0.53768 | val_1_rmse: 0.54971 |  0:01:10s
epoch 69 | loss: 0.26023 | val_0_rmse: 0.52229 | val_1_rmse: 0.53132 |  0:01:11s
epoch 70 | loss: 0.2535  | val_0_rmse: 0.49887 | val_1_rmse: 0.5028  |  0:01:12s
epoch 71 | loss: 0.24793 | val_0_rmse: 0.53808 | val_1_rmse: 0.54535 |  0:01:13s
epoch 72 | loss: 0.25051 | val_0_rmse: 0.5008  | val_1_rmse: 0.51044 |  0:01:14s
epoch 73 | loss: 0.25197 | val_0_rmse: 0.51805 | val_1_rmse: 0.52113 |  0:01:16s
epoch 74 | loss: 0.24903 | val_0_rmse: 0.50768 | val_1_rmse: 0.51337 |  0:01:17s
epoch 75 | loss: 0.25305 | val_0_rmse: 0.53601 | val_1_rmse: 0.54531 |  0:01:18s
epoch 76 | loss: 0.25413 | val_0_rmse: 0.55517 | val_1_rmse: 0.56403 |  0:01:19s
epoch 77 | loss: 0.24954 | val_0_rmse: 0.53246 | val_1_rmse: 0.54989 |  0:01:20s
epoch 78 | loss: 0.25833 | val_0_rmse: 0.50154 | val_1_rmse: 0.51222 |  0:01:21s
epoch 79 | loss: 0.25021 | val_0_rmse: 0.48474 | val_1_rmse: 0.48724 |  0:01:22s
epoch 80 | loss: 0.25266 | val_0_rmse: 0.50835 | val_1_rmse: 0.51276 |  0:01:23s
epoch 81 | loss: 0.25258 | val_0_rmse: 0.49077 | val_1_rmse: 0.4946  |  0:01:24s
epoch 82 | loss: 0.25112 | val_0_rmse: 0.61667 | val_1_rmse: 0.62172 |  0:01:25s
epoch 83 | loss: 0.25794 | val_0_rmse: 0.54317 | val_1_rmse: 0.561   |  0:01:26s
epoch 84 | loss: 0.26012 | val_0_rmse: 0.5181  | val_1_rmse: 0.52342 |  0:01:27s
epoch 85 | loss: 0.25415 | val_0_rmse: 0.50204 | val_1_rmse: 0.51091 |  0:01:28s
epoch 86 | loss: 0.24664 | val_0_rmse: 0.49901 | val_1_rmse: 0.50371 |  0:01:29s
epoch 87 | loss: 0.25397 | val_0_rmse: 0.5113  | val_1_rmse: 0.52096 |  0:01:30s
epoch 88 | loss: 0.25621 | val_0_rmse: 0.52273 | val_1_rmse: 0.52654 |  0:01:31s
epoch 89 | loss: 0.25009 | val_0_rmse: 0.50987 | val_1_rmse: 0.51447 |  0:01:32s
epoch 90 | loss: 0.25578 | val_0_rmse: 0.51774 | val_1_rmse: 0.51955 |  0:01:33s
epoch 91 | loss: 0.25532 | val_0_rmse: 0.50477 | val_1_rmse: 0.51841 |  0:01:34s
epoch 92 | loss: 0.25127 | val_0_rmse: 0.51207 | val_1_rmse: 0.51761 |  0:01:35s
epoch 93 | loss: 0.24711 | val_0_rmse: 0.50999 | val_1_rmse: 0.51943 |  0:01:36s
epoch 94 | loss: 0.24709 | val_0_rmse: 0.48755 | val_1_rmse: 0.49231 |  0:01:37s
epoch 95 | loss: 0.24798 | val_0_rmse: 0.56104 | val_1_rmse: 0.57178 |  0:01:38s
epoch 96 | loss: 0.24028 | val_0_rmse: 0.51145 | val_1_rmse: 0.52112 |  0:01:39s
epoch 97 | loss: 0.24348 | val_0_rmse: 0.52058 | val_1_rmse: 0.53315 |  0:01:40s
epoch 98 | loss: 0.23841 | val_0_rmse: 0.53624 | val_1_rmse: 0.55423 |  0:01:41s
epoch 99 | loss: 0.24702 | val_0_rmse: 0.49967 | val_1_rmse: 0.50805 |  0:01:42s
epoch 100| loss: 0.249   | val_0_rmse: 0.50275 | val_1_rmse: 0.51261 |  0:01:43s
epoch 101| loss: 0.2468  | val_0_rmse: 0.53336 | val_1_rmse: 0.54766 |  0:01:44s
epoch 102| loss: 0.24649 | val_0_rmse: 0.50411 | val_1_rmse: 0.51284 |  0:01:45s
epoch 103| loss: 0.24815 | val_0_rmse: 0.49007 | val_1_rmse: 0.5001  |  0:01:46s
epoch 104| loss: 0.24917 | val_0_rmse: 0.51124 | val_1_rmse: 0.51771 |  0:01:47s
epoch 105| loss: 0.25147 | val_0_rmse: 0.57358 | val_1_rmse: 0.57997 |  0:01:48s
epoch 106| loss: 0.25597 | val_0_rmse: 0.50094 | val_1_rmse: 0.50619 |  0:01:49s
epoch 107| loss: 0.25961 | val_0_rmse: 0.5443  | val_1_rmse: 0.55253 |  0:01:50s
epoch 108| loss: 0.2533  | val_0_rmse: 0.53346 | val_1_rmse: 0.5423  |  0:01:51s
epoch 109| loss: 0.2551  | val_0_rmse: 0.52516 | val_1_rmse: 0.52806 |  0:01:52s

Early stopping occured at epoch 109 with best_epoch = 79 and best_val_1_rmse = 0.48724
Best weights from best epoch are automatically used!
ended training at: 13:36:54
Feature importance:
[('Area', 0.33537888838773483), ('Baths', 0.05825419155057363), ('Beds', 0.11958167945820988), ('Latitude', 0.015348864610580595), ('Longitude', 0.2330776590726718), ('Month', 0.11067109354067638), ('Year', 0.1276876233795529)]
Mean squared error is of 989433453.470114
Mean absolute error:21401.640215463187
MAPE:0.25608310484761876
R2 score:0.7515927706812481
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:36:55
epoch 0  | loss: 0.70768 | val_0_rmse: 0.67993 | val_1_rmse: 0.68686 |  0:00:01s
epoch 1  | loss: 0.40391 | val_0_rmse: 0.63844 | val_1_rmse: 0.64182 |  0:00:02s
epoch 2  | loss: 0.38532 | val_0_rmse: 0.6353  | val_1_rmse: 0.64009 |  0:00:03s
epoch 3  | loss: 0.37219 | val_0_rmse: 0.59251 | val_1_rmse: 0.59273 |  0:00:04s
epoch 4  | loss: 0.35127 | val_0_rmse: 0.58542 | val_1_rmse: 0.58243 |  0:00:05s
epoch 5  | loss: 0.34393 | val_0_rmse: 0.5713  | val_1_rmse: 0.57046 |  0:00:06s
epoch 6  | loss: 0.3383  | val_0_rmse: 0.57245 | val_1_rmse: 0.57278 |  0:00:07s
epoch 7  | loss: 0.34292 | val_0_rmse: 0.59648 | val_1_rmse: 0.60215 |  0:00:08s
epoch 8  | loss: 0.32464 | val_0_rmse: 0.56522 | val_1_rmse: 0.56399 |  0:00:09s
epoch 9  | loss: 0.32312 | val_0_rmse: 0.56972 | val_1_rmse: 0.56707 |  0:00:10s
epoch 10 | loss: 0.31554 | val_0_rmse: 0.56154 | val_1_rmse: 0.55618 |  0:00:11s
epoch 11 | loss: 0.31698 | val_0_rmse: 0.57051 | val_1_rmse: 0.56949 |  0:00:12s
epoch 12 | loss: 0.32871 | val_0_rmse: 0.57447 | val_1_rmse: 0.5802  |  0:00:13s
epoch 13 | loss: 0.33452 | val_0_rmse: 0.6148  | val_1_rmse: 0.61174 |  0:00:14s
epoch 14 | loss: 0.33247 | val_0_rmse: 0.55781 | val_1_rmse: 0.55579 |  0:00:15s
epoch 15 | loss: 0.3119  | val_0_rmse: 0.55356 | val_1_rmse: 0.55539 |  0:00:16s
epoch 16 | loss: 0.30746 | val_0_rmse: 0.54921 | val_1_rmse: 0.54767 |  0:00:17s
epoch 17 | loss: 0.30562 | val_0_rmse: 0.54129 | val_1_rmse: 0.53968 |  0:00:18s
epoch 18 | loss: 0.30532 | val_0_rmse: 0.54298 | val_1_rmse: 0.5425  |  0:00:19s
epoch 19 | loss: 0.30302 | val_0_rmse: 0.54258 | val_1_rmse: 0.54153 |  0:00:20s
epoch 20 | loss: 0.30197 | val_0_rmse: 0.53791 | val_1_rmse: 0.53789 |  0:00:21s
epoch 21 | loss: 0.30356 | val_0_rmse: 0.54762 | val_1_rmse: 0.5535  |  0:00:22s
epoch 22 | loss: 0.3032  | val_0_rmse: 0.53707 | val_1_rmse: 0.53711 |  0:00:23s
epoch 23 | loss: 0.30284 | val_0_rmse: 0.55199 | val_1_rmse: 0.55048 |  0:00:24s
epoch 24 | loss: 0.30781 | val_0_rmse: 0.56935 | val_1_rmse: 0.57198 |  0:00:25s
epoch 25 | loss: 0.30448 | val_0_rmse: 0.53487 | val_1_rmse: 0.53405 |  0:00:26s
epoch 26 | loss: 0.29991 | val_0_rmse: 0.53235 | val_1_rmse: 0.53251 |  0:00:27s
epoch 27 | loss: 0.30238 | val_0_rmse: 0.53877 | val_1_rmse: 0.53892 |  0:00:28s
epoch 28 | loss: 0.29884 | val_0_rmse: 0.54997 | val_1_rmse: 0.54586 |  0:00:29s
epoch 29 | loss: 0.30797 | val_0_rmse: 0.54967 | val_1_rmse: 0.55328 |  0:00:30s
epoch 30 | loss: 0.30043 | val_0_rmse: 0.55126 | val_1_rmse: 0.5553  |  0:00:31s
epoch 31 | loss: 0.2993  | val_0_rmse: 0.54229 | val_1_rmse: 0.53659 |  0:00:32s
epoch 32 | loss: 0.29488 | val_0_rmse: 0.53217 | val_1_rmse: 0.53348 |  0:00:33s
epoch 33 | loss: 0.30159 | val_0_rmse: 0.53524 | val_1_rmse: 0.53719 |  0:00:34s
epoch 34 | loss: 0.29591 | val_0_rmse: 0.53139 | val_1_rmse: 0.53194 |  0:00:35s
epoch 35 | loss: 0.28983 | val_0_rmse: 0.53827 | val_1_rmse: 0.53211 |  0:00:37s
epoch 36 | loss: 0.2964  | val_0_rmse: 0.5381  | val_1_rmse: 0.53841 |  0:00:38s
epoch 37 | loss: 0.29111 | val_0_rmse: 0.53296 | val_1_rmse: 0.53117 |  0:00:39s
epoch 38 | loss: 0.29569 | val_0_rmse: 0.53732 | val_1_rmse: 0.54057 |  0:00:40s
epoch 39 | loss: 0.29703 | val_0_rmse: 0.53511 | val_1_rmse: 0.5332  |  0:00:41s
epoch 40 | loss: 0.29335 | val_0_rmse: 0.53494 | val_1_rmse: 0.53846 |  0:00:42s
epoch 41 | loss: 0.29407 | val_0_rmse: 0.52818 | val_1_rmse: 0.52863 |  0:00:43s
epoch 42 | loss: 0.2973  | val_0_rmse: 0.52698 | val_1_rmse: 0.52599 |  0:00:44s
epoch 43 | loss: 0.28644 | val_0_rmse: 0.5288  | val_1_rmse: 0.53118 |  0:00:45s
epoch 44 | loss: 0.28224 | val_0_rmse: 0.52846 | val_1_rmse: 0.52734 |  0:00:46s
epoch 45 | loss: 0.28471 | val_0_rmse: 0.53601 | val_1_rmse: 0.53542 |  0:00:47s
epoch 46 | loss: 0.28579 | val_0_rmse: 0.53271 | val_1_rmse: 0.53239 |  0:00:48s
epoch 47 | loss: 0.29388 | val_0_rmse: 0.53298 | val_1_rmse: 0.53123 |  0:00:49s
epoch 48 | loss: 0.2936  | val_0_rmse: 0.54409 | val_1_rmse: 0.53993 |  0:00:50s
epoch 49 | loss: 0.28807 | val_0_rmse: 0.52698 | val_1_rmse: 0.52883 |  0:00:51s
epoch 50 | loss: 0.29181 | val_0_rmse: 0.52667 | val_1_rmse: 0.52948 |  0:00:52s
epoch 51 | loss: 0.29842 | val_0_rmse: 0.54228 | val_1_rmse: 0.54307 |  0:00:53s
epoch 52 | loss: 0.30232 | val_0_rmse: 0.5458  | val_1_rmse: 0.54981 |  0:00:54s
epoch 53 | loss: 0.29969 | val_0_rmse: 0.54358 | val_1_rmse: 0.54838 |  0:00:55s
epoch 54 | loss: 0.30237 | val_0_rmse: 0.54012 | val_1_rmse: 0.54285 |  0:00:56s
epoch 55 | loss: 0.29846 | val_0_rmse: 0.52501 | val_1_rmse: 0.53091 |  0:00:57s
epoch 56 | loss: 0.29109 | val_0_rmse: 0.53996 | val_1_rmse: 0.54433 |  0:00:58s
epoch 57 | loss: 0.28926 | val_0_rmse: 0.54282 | val_1_rmse: 0.54197 |  0:00:59s
epoch 58 | loss: 0.29673 | val_0_rmse: 0.52669 | val_1_rmse: 0.52941 |  0:01:00s
epoch 59 | loss: 0.2873  | val_0_rmse: 0.54287 | val_1_rmse: 0.54091 |  0:01:01s
epoch 60 | loss: 0.28691 | val_0_rmse: 0.52166 | val_1_rmse: 0.51937 |  0:01:02s
epoch 61 | loss: 0.28327 | val_0_rmse: 0.5192  | val_1_rmse: 0.52021 |  0:01:03s
epoch 62 | loss: 0.28646 | val_0_rmse: 0.52537 | val_1_rmse: 0.5266  |  0:01:04s
epoch 63 | loss: 0.2853  | val_0_rmse: 0.54714 | val_1_rmse: 0.55548 |  0:01:05s
epoch 64 | loss: 0.28834 | val_0_rmse: 0.52368 | val_1_rmse: 0.5281  |  0:01:06s
epoch 65 | loss: 0.28838 | val_0_rmse: 0.5414  | val_1_rmse: 0.54402 |  0:01:07s
epoch 66 | loss: 0.28591 | val_0_rmse: 0.54294 | val_1_rmse: 0.54703 |  0:01:08s
epoch 67 | loss: 0.28403 | val_0_rmse: 0.53178 | val_1_rmse: 0.53817 |  0:01:09s
epoch 68 | loss: 0.28167 | val_0_rmse: 0.53028 | val_1_rmse: 0.5352  |  0:01:10s
epoch 69 | loss: 0.28397 | val_0_rmse: 0.52424 | val_1_rmse: 0.52628 |  0:01:12s
epoch 70 | loss: 0.28195 | val_0_rmse: 0.52119 | val_1_rmse: 0.52053 |  0:01:13s
epoch 71 | loss: 0.28218 | val_0_rmse: 0.5163  | val_1_rmse: 0.51596 |  0:01:14s
epoch 72 | loss: 0.27803 | val_0_rmse: 0.617   | val_1_rmse: 0.62437 |  0:01:15s
epoch 73 | loss: 0.27451 | val_0_rmse: 0.5178  | val_1_rmse: 0.51992 |  0:01:16s
epoch 74 | loss: 0.27273 | val_0_rmse: 0.51551 | val_1_rmse: 0.50976 |  0:01:17s
epoch 75 | loss: 0.27357 | val_0_rmse: 0.51626 | val_1_rmse: 0.51217 |  0:01:18s
epoch 76 | loss: 0.26335 | val_0_rmse: 0.59564 | val_1_rmse: 0.60609 |  0:01:19s
epoch 77 | loss: 0.26439 | val_0_rmse: 0.76211 | val_1_rmse: 0.75689 |  0:01:20s
epoch 78 | loss: 0.26305 | val_0_rmse: 0.57053 | val_1_rmse: 0.58065 |  0:01:21s
epoch 79 | loss: 0.25532 | val_0_rmse: 0.52399 | val_1_rmse: 0.52846 |  0:01:22s
epoch 80 | loss: 0.25438 | val_0_rmse: 0.53242 | val_1_rmse: 0.53891 |  0:01:23s
epoch 81 | loss: 0.25626 | val_0_rmse: 0.59496 | val_1_rmse: 0.60172 |  0:01:24s
epoch 82 | loss: 0.25261 | val_0_rmse: 0.5064  | val_1_rmse: 0.50871 |  0:01:25s
epoch 83 | loss: 0.2559  | val_0_rmse: 0.51084 | val_1_rmse: 0.5195  |  0:01:26s
epoch 84 | loss: 0.25819 | val_0_rmse: 0.56085 | val_1_rmse: 0.57394 |  0:01:27s
epoch 85 | loss: 0.25795 | val_0_rmse: 0.5067  | val_1_rmse: 0.51277 |  0:01:28s
epoch 86 | loss: 0.25921 | val_0_rmse: 0.56393 | val_1_rmse: 0.57301 |  0:01:29s
epoch 87 | loss: 0.26004 | val_0_rmse: 0.61056 | val_1_rmse: 0.60396 |  0:01:30s
epoch 88 | loss: 0.25548 | val_0_rmse: 0.55676 | val_1_rmse: 0.56091 |  0:01:31s
epoch 89 | loss: 0.25364 | val_0_rmse: 0.50321 | val_1_rmse: 0.50561 |  0:01:32s
epoch 90 | loss: 0.25312 | val_0_rmse: 0.50409 | val_1_rmse: 0.50661 |  0:01:33s
epoch 91 | loss: 0.25381 | val_0_rmse: 0.55265 | val_1_rmse: 0.56012 |  0:01:34s
epoch 92 | loss: 0.25268 | val_0_rmse: 0.52055 | val_1_rmse: 0.52829 |  0:01:35s
epoch 93 | loss: 0.25282 | val_0_rmse: 0.57406 | val_1_rmse: 0.56497 |  0:01:36s
epoch 94 | loss: 0.24909 | val_0_rmse: 0.59224 | val_1_rmse: 0.60571 |  0:01:37s
epoch 95 | loss: 0.25013 | val_0_rmse: 0.49525 | val_1_rmse: 0.49964 |  0:01:38s
epoch 96 | loss: 0.24696 | val_0_rmse: 0.51439 | val_1_rmse: 0.52179 |  0:01:39s
epoch 97 | loss: 0.24411 | val_0_rmse: 0.53931 | val_1_rmse: 0.54592 |  0:01:40s
epoch 98 | loss: 0.24352 | val_0_rmse: 0.5158  | val_1_rmse: 0.52431 |  0:01:41s
epoch 99 | loss: 0.24643 | val_0_rmse: 0.54828 | val_1_rmse: 0.55535 |  0:01:42s
epoch 100| loss: 0.24226 | val_0_rmse: 0.53522 | val_1_rmse: 0.54596 |  0:01:43s
epoch 101| loss: 0.24249 | val_0_rmse: 0.49635 | val_1_rmse: 0.50367 |  0:01:44s
epoch 102| loss: 0.25018 | val_0_rmse: 0.52484 | val_1_rmse: 0.53128 |  0:01:45s
epoch 103| loss: 0.24273 | val_0_rmse: 0.52954 | val_1_rmse: 0.54158 |  0:01:47s
epoch 104| loss: 0.24273 | val_0_rmse: 0.50864 | val_1_rmse: 0.51397 |  0:01:48s
epoch 105| loss: 0.24132 | val_0_rmse: 0.52825 | val_1_rmse: 0.52698 |  0:01:49s
epoch 106| loss: 0.24226 | val_0_rmse: 0.57796 | val_1_rmse: 0.59064 |  0:01:50s
epoch 107| loss: 0.24913 | val_0_rmse: 0.51707 | val_1_rmse: 0.51883 |  0:01:51s
epoch 108| loss: 0.24184 | val_0_rmse: 0.60365 | val_1_rmse: 0.61302 |  0:01:52s
epoch 109| loss: 0.2439  | val_0_rmse: 0.54341 | val_1_rmse: 0.55323 |  0:01:53s
epoch 110| loss: 0.24284 | val_0_rmse: 0.51136 | val_1_rmse: 0.51186 |  0:01:54s
epoch 111| loss: 0.24073 | val_0_rmse: 0.49394 | val_1_rmse: 0.50336 |  0:01:55s
epoch 112| loss: 0.24351 | val_0_rmse: 0.4826  | val_1_rmse: 0.49275 |  0:01:56s
epoch 113| loss: 0.24064 | val_0_rmse: 0.48446 | val_1_rmse: 0.49121 |  0:01:57s
epoch 114| loss: 0.24164 | val_0_rmse: 0.61416 | val_1_rmse: 0.622   |  0:01:58s
epoch 115| loss: 0.24592 | val_0_rmse: 0.57017 | val_1_rmse: 0.57715 |  0:01:59s
epoch 116| loss: 0.24466 | val_0_rmse: 0.57143 | val_1_rmse: 0.58709 |  0:02:00s
epoch 117| loss: 0.24224 | val_0_rmse: 0.52344 | val_1_rmse: 0.53274 |  0:02:01s
epoch 118| loss: 0.24286 | val_0_rmse: 0.51699 | val_1_rmse: 0.53062 |  0:02:02s
epoch 119| loss: 0.23975 | val_0_rmse: 0.56177 | val_1_rmse: 0.57322 |  0:02:03s
epoch 120| loss: 0.24383 | val_0_rmse: 0.48945 | val_1_rmse: 0.4973  |  0:02:04s
epoch 121| loss: 0.23563 | val_0_rmse: 0.5933  | val_1_rmse: 0.60761 |  0:02:05s
epoch 122| loss: 0.23642 | val_0_rmse: 0.50144 | val_1_rmse: 0.50669 |  0:02:06s
epoch 123| loss: 0.24349 | val_0_rmse: 0.55762 | val_1_rmse: 0.57306 |  0:02:07s
epoch 124| loss: 0.23851 | val_0_rmse: 0.51588 | val_1_rmse: 0.52618 |  0:02:08s
epoch 125| loss: 0.23962 | val_0_rmse: 0.48569 | val_1_rmse: 0.49307 |  0:02:09s
epoch 126| loss: 0.24542 | val_0_rmse: 0.48305 | val_1_rmse: 0.49213 |  0:02:10s
epoch 127| loss: 0.2428  | val_0_rmse: 0.55572 | val_1_rmse: 0.56565 |  0:02:11s
epoch 128| loss: 0.2441  | val_0_rmse: 0.53969 | val_1_rmse: 0.55055 |  0:02:12s
epoch 129| loss: 0.24146 | val_0_rmse: 0.57266 | val_1_rmse: 0.58679 |  0:02:13s
epoch 130| loss: 0.24052 | val_0_rmse: 0.49994 | val_1_rmse: 0.50878 |  0:02:14s
epoch 131| loss: 0.23815 | val_0_rmse: 0.51783 | val_1_rmse: 0.52776 |  0:02:15s
epoch 132| loss: 0.23716 | val_0_rmse: 0.5504  | val_1_rmse: 0.55402 |  0:02:16s
epoch 133| loss: 0.24773 | val_0_rmse: 0.54699 | val_1_rmse: 0.54906 |  0:02:17s
epoch 134| loss: 0.24631 | val_0_rmse: 0.49662 | val_1_rmse: 0.50336 |  0:02:18s
epoch 135| loss: 0.23812 | val_0_rmse: 0.60714 | val_1_rmse: 0.62358 |  0:02:19s
epoch 136| loss: 0.237   | val_0_rmse: 0.60145 | val_1_rmse: 0.61556 |  0:02:20s
epoch 137| loss: 0.23573 | val_0_rmse: 0.58721 | val_1_rmse: 0.60832 |  0:02:21s
epoch 138| loss: 0.23574 | val_0_rmse: 0.58767 | val_1_rmse: 0.59843 |  0:02:23s
epoch 139| loss: 0.24171 | val_0_rmse: 0.55177 | val_1_rmse: 0.56418 |  0:02:24s
epoch 140| loss: 0.23597 | val_0_rmse: 0.52023 | val_1_rmse: 0.53286 |  0:02:25s
epoch 141| loss: 0.23481 | val_0_rmse: 0.52805 | val_1_rmse: 0.54205 |  0:02:26s
epoch 142| loss: 0.23635 | val_0_rmse: 0.59391 | val_1_rmse: 0.60781 |  0:02:27s
epoch 143| loss: 0.23836 | val_0_rmse: 0.49666 | val_1_rmse: 0.50729 |  0:02:28s

Early stopping occured at epoch 143 with best_epoch = 113 and best_val_1_rmse = 0.49121
Best weights from best epoch are automatically used!
ended training at: 13:39:23
Feature importance:
[('Area', 0.44955001527408905), ('Baths', 0.08976593165397444), ('Beds', 0.14103801740644464), ('Latitude', 0.17431735261252615), ('Longitude', 0.14189691344874367), ('Month', 0.0), ('Year', 0.003431769604222054)]
Mean squared error is of 962802455.0839511
Mean absolute error:21726.248861025728
MAPE:0.2832625640099381
R2 score:0.7522235294557039
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:39:23
epoch 0  | loss: 0.71868 | val_0_rmse: 0.92924 | val_1_rmse: 0.93619 |  0:00:01s
epoch 1  | loss: 0.39147 | val_0_rmse: 0.63279 | val_1_rmse: 0.62987 |  0:00:02s
epoch 2  | loss: 0.3664  | val_0_rmse: 0.58148 | val_1_rmse: 0.57102 |  0:00:03s
epoch 3  | loss: 0.34283 | val_0_rmse: 0.55794 | val_1_rmse: 0.54729 |  0:00:04s
epoch 4  | loss: 0.32834 | val_0_rmse: 0.55144 | val_1_rmse: 0.54128 |  0:00:05s
epoch 5  | loss: 0.31775 | val_0_rmse: 0.55255 | val_1_rmse: 0.54541 |  0:00:06s
epoch 6  | loss: 0.32217 | val_0_rmse: 0.553   | val_1_rmse: 0.54088 |  0:00:07s
epoch 7  | loss: 0.31588 | val_0_rmse: 0.5436  | val_1_rmse: 0.53462 |  0:00:08s
epoch 8  | loss: 0.30863 | val_0_rmse: 0.55849 | val_1_rmse: 0.5553  |  0:00:09s
epoch 9  | loss: 0.31855 | val_0_rmse: 0.54037 | val_1_rmse: 0.52692 |  0:00:10s
epoch 10 | loss: 0.32237 | val_0_rmse: 0.58082 | val_1_rmse: 0.58088 |  0:00:11s
epoch 11 | loss: 0.33544 | val_0_rmse: 0.5661  | val_1_rmse: 0.55494 |  0:00:12s
epoch 12 | loss: 0.31242 | val_0_rmse: 0.56218 | val_1_rmse: 0.54968 |  0:00:13s
epoch 13 | loss: 0.31017 | val_0_rmse: 0.54311 | val_1_rmse: 0.53364 |  0:00:14s
epoch 14 | loss: 0.31059 | val_0_rmse: 0.54141 | val_1_rmse: 0.53013 |  0:00:15s
epoch 15 | loss: 0.30101 | val_0_rmse: 0.54044 | val_1_rmse: 0.52611 |  0:00:16s
epoch 16 | loss: 0.3084  | val_0_rmse: 0.5593  | val_1_rmse: 0.55237 |  0:00:17s
epoch 17 | loss: 0.3093  | val_0_rmse: 0.53769 | val_1_rmse: 0.52089 |  0:00:18s
epoch 18 | loss: 0.29801 | val_0_rmse: 0.52938 | val_1_rmse: 0.51624 |  0:00:19s
epoch 19 | loss: 0.29335 | val_0_rmse: 0.56263 | val_1_rmse: 0.54326 |  0:00:20s
epoch 20 | loss: 0.31757 | val_0_rmse: 0.70191 | val_1_rmse: 0.67984 |  0:00:21s
epoch 21 | loss: 0.31228 | val_0_rmse: 0.57759 | val_1_rmse: 0.56309 |  0:00:22s
epoch 22 | loss: 0.30639 | val_0_rmse: 0.53241 | val_1_rmse: 0.51339 |  0:00:23s
epoch 23 | loss: 0.29552 | val_0_rmse: 0.53064 | val_1_rmse: 0.51251 |  0:00:24s
epoch 24 | loss: 0.28825 | val_0_rmse: 0.53093 | val_1_rmse: 0.51215 |  0:00:25s
epoch 25 | loss: 0.28714 | val_0_rmse: 0.53254 | val_1_rmse: 0.51775 |  0:00:26s
epoch 26 | loss: 0.28367 | val_0_rmse: 0.55037 | val_1_rmse: 0.53587 |  0:00:27s
epoch 27 | loss: 0.28855 | val_0_rmse: 0.52457 | val_1_rmse: 0.50715 |  0:00:28s
epoch 28 | loss: 0.28476 | val_0_rmse: 0.52984 | val_1_rmse: 0.51223 |  0:00:30s
epoch 29 | loss: 0.28625 | val_0_rmse: 0.52291 | val_1_rmse: 0.50722 |  0:00:31s
epoch 30 | loss: 0.28183 | val_0_rmse: 0.53567 | val_1_rmse: 0.515   |  0:00:32s
epoch 31 | loss: 0.28267 | val_0_rmse: 0.52971 | val_1_rmse: 0.51031 |  0:00:33s
epoch 32 | loss: 0.28021 | val_0_rmse: 0.58502 | val_1_rmse: 0.569   |  0:00:34s
epoch 33 | loss: 0.27583 | val_0_rmse: 0.52824 | val_1_rmse: 0.50914 |  0:00:35s
epoch 34 | loss: 0.27965 | val_0_rmse: 0.51838 | val_1_rmse: 0.5022  |  0:00:36s
epoch 35 | loss: 0.27651 | val_0_rmse: 0.62434 | val_1_rmse: 0.60681 |  0:00:37s
epoch 36 | loss: 0.27104 | val_0_rmse: 0.54964 | val_1_rmse: 0.53459 |  0:00:38s
epoch 37 | loss: 0.27036 | val_0_rmse: 0.51721 | val_1_rmse: 0.50126 |  0:00:39s
epoch 38 | loss: 0.2746  | val_0_rmse: 0.53768 | val_1_rmse: 0.51929 |  0:00:40s
epoch 39 | loss: 0.27326 | val_0_rmse: 0.54809 | val_1_rmse: 0.5327  |  0:00:41s
epoch 40 | loss: 0.27834 | val_0_rmse: 0.6331  | val_1_rmse: 0.61534 |  0:00:42s
epoch 41 | loss: 0.28917 | val_0_rmse: 0.65348 | val_1_rmse: 0.64004 |  0:00:43s
epoch 42 | loss: 0.28182 | val_0_rmse: 0.53681 | val_1_rmse: 0.51816 |  0:00:44s
epoch 43 | loss: 0.27154 | val_0_rmse: 0.61263 | val_1_rmse: 0.59571 |  0:00:45s
epoch 44 | loss: 0.27806 | val_0_rmse: 0.53794 | val_1_rmse: 0.51821 |  0:00:46s
epoch 45 | loss: 0.28336 | val_0_rmse: 0.53902 | val_1_rmse: 0.52133 |  0:00:47s
epoch 46 | loss: 0.26986 | val_0_rmse: 0.507   | val_1_rmse: 0.48602 |  0:00:48s
epoch 47 | loss: 0.27979 | val_0_rmse: 0.54063 | val_1_rmse: 0.52057 |  0:00:49s
epoch 48 | loss: 0.29419 | val_0_rmse: 0.59306 | val_1_rmse: 0.57311 |  0:00:50s
epoch 49 | loss: 0.27472 | val_0_rmse: 0.53823 | val_1_rmse: 0.51631 |  0:00:51s
epoch 50 | loss: 0.27931 | val_0_rmse: 0.55856 | val_1_rmse: 0.54213 |  0:00:52s
epoch 51 | loss: 0.27414 | val_0_rmse: 0.53159 | val_1_rmse: 0.51472 |  0:00:53s
epoch 52 | loss: 0.26733 | val_0_rmse: 0.53185 | val_1_rmse: 0.51265 |  0:00:54s
epoch 53 | loss: 0.26823 | val_0_rmse: 0.5131  | val_1_rmse: 0.49321 |  0:00:55s
epoch 54 | loss: 0.26873 | val_0_rmse: 0.54663 | val_1_rmse: 0.52632 |  0:00:56s
epoch 55 | loss: 0.26403 | val_0_rmse: 0.50253 | val_1_rmse: 0.4845  |  0:00:57s
epoch 56 | loss: 0.2728  | val_0_rmse: 0.51465 | val_1_rmse: 0.49497 |  0:00:58s
epoch 57 | loss: 0.27863 | val_0_rmse: 0.53414 | val_1_rmse: 0.51397 |  0:00:59s
epoch 58 | loss: 0.27331 | val_0_rmse: 0.53353 | val_1_rmse: 0.51537 |  0:01:00s
epoch 59 | loss: 0.26679 | val_0_rmse: 0.50555 | val_1_rmse: 0.4856  |  0:01:01s
epoch 60 | loss: 0.2594  | val_0_rmse: 0.51572 | val_1_rmse: 0.49608 |  0:01:02s
epoch 61 | loss: 0.2612  | val_0_rmse: 0.52233 | val_1_rmse: 0.50238 |  0:01:03s
epoch 62 | loss: 0.2637  | val_0_rmse: 0.49839 | val_1_rmse: 0.47893 |  0:01:04s
epoch 63 | loss: 0.26749 | val_0_rmse: 0.54379 | val_1_rmse: 0.52413 |  0:01:05s
epoch 64 | loss: 0.25835 | val_0_rmse: 0.56131 | val_1_rmse: 0.54277 |  0:01:06s
epoch 65 | loss: 0.25867 | val_0_rmse: 0.53954 | val_1_rmse: 0.52258 |  0:01:07s
epoch 66 | loss: 0.26159 | val_0_rmse: 0.52319 | val_1_rmse: 0.49962 |  0:01:08s
epoch 67 | loss: 0.27082 | val_0_rmse: 0.53058 | val_1_rmse: 0.51312 |  0:01:09s
epoch 68 | loss: 0.26372 | val_0_rmse: 0.54224 | val_1_rmse: 0.52089 |  0:01:10s
epoch 69 | loss: 0.25837 | val_0_rmse: 0.54286 | val_1_rmse: 0.52622 |  0:01:11s
epoch 70 | loss: 0.26319 | val_0_rmse: 0.58452 | val_1_rmse: 0.56762 |  0:01:13s
epoch 71 | loss: 0.26489 | val_0_rmse: 0.51315 | val_1_rmse: 0.49441 |  0:01:14s
epoch 72 | loss: 0.27053 | val_0_rmse: 0.50893 | val_1_rmse: 0.49116 |  0:01:15s
epoch 73 | loss: 0.2594  | val_0_rmse: 0.58029 | val_1_rmse: 0.5632  |  0:01:16s
epoch 74 | loss: 0.26402 | val_0_rmse: 0.51787 | val_1_rmse: 0.49984 |  0:01:17s
epoch 75 | loss: 0.26023 | val_0_rmse: 0.60373 | val_1_rmse: 0.58714 |  0:01:18s
epoch 76 | loss: 0.25782 | val_0_rmse: 0.55282 | val_1_rmse: 0.53762 |  0:01:19s
epoch 77 | loss: 0.26363 | val_0_rmse: 0.54523 | val_1_rmse: 0.52322 |  0:01:20s
epoch 78 | loss: 0.25956 | val_0_rmse: 0.49953 | val_1_rmse: 0.48126 |  0:01:21s
epoch 79 | loss: 0.25497 | val_0_rmse: 0.5469  | val_1_rmse: 0.53118 |  0:01:22s
epoch 80 | loss: 0.26497 | val_0_rmse: 0.4987  | val_1_rmse: 0.48017 |  0:01:23s
epoch 81 | loss: 0.25955 | val_0_rmse: 0.52848 | val_1_rmse: 0.50808 |  0:01:24s
epoch 82 | loss: 0.26196 | val_0_rmse: 0.67028 | val_1_rmse: 0.65238 |  0:01:25s
epoch 83 | loss: 0.26536 | val_0_rmse: 0.50694 | val_1_rmse: 0.48743 |  0:01:26s
epoch 84 | loss: 0.26379 | val_0_rmse: 0.56342 | val_1_rmse: 0.54371 |  0:01:27s
epoch 85 | loss: 0.25798 | val_0_rmse: 0.54308 | val_1_rmse: 0.52356 |  0:01:28s
epoch 86 | loss: 0.2585  | val_0_rmse: 0.54903 | val_1_rmse: 0.53404 |  0:01:29s
epoch 87 | loss: 0.25761 | val_0_rmse: 0.54075 | val_1_rmse: 0.52375 |  0:01:30s
epoch 88 | loss: 0.2575  | val_0_rmse: 0.54008 | val_1_rmse: 0.52571 |  0:01:31s
epoch 89 | loss: 0.25909 | val_0_rmse: 0.51679 | val_1_rmse: 0.50168 |  0:01:32s
epoch 90 | loss: 0.25366 | val_0_rmse: 0.50802 | val_1_rmse: 0.49003 |  0:01:33s
epoch 91 | loss: 0.25915 | val_0_rmse: 0.52705 | val_1_rmse: 0.50955 |  0:01:34s
epoch 92 | loss: 0.26257 | val_0_rmse: 0.51095 | val_1_rmse: 0.49383 |  0:01:35s

Early stopping occured at epoch 92 with best_epoch = 62 and best_val_1_rmse = 0.47893
Best weights from best epoch are automatically used!
ended training at: 13:40:59
Feature importance:
[('Area', 0.4517587090220872), ('Baths', 0.042636669979856275), ('Beds', 0.0), ('Latitude', 0.19476116672322535), ('Longitude', 0.2543892671070427), ('Month', 0.025012617356254707), ('Year', 0.03144156981153377)]
Mean squared error is of 1005490574.9138803
Mean absolute error:21576.910680452642
MAPE:0.2597250100056784
R2 score:0.7469078029890388
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:40:59
epoch 0  | loss: 0.69853 | val_0_rmse: 0.77983 | val_1_rmse: 0.77734 |  0:00:01s
epoch 1  | loss: 0.38131 | val_0_rmse: 0.61925 | val_1_rmse: 0.61906 |  0:00:02s
epoch 2  | loss: 0.35496 | val_0_rmse: 0.59444 | val_1_rmse: 0.59248 |  0:00:03s
epoch 3  | loss: 0.33838 | val_0_rmse: 0.57415 | val_1_rmse: 0.58123 |  0:00:04s
epoch 4  | loss: 0.33288 | val_0_rmse: 0.55659 | val_1_rmse: 0.56775 |  0:00:05s
epoch 5  | loss: 0.32033 | val_0_rmse: 0.56064 | val_1_rmse: 0.57012 |  0:00:06s
epoch 6  | loss: 0.33402 | val_0_rmse: 0.5532  | val_1_rmse: 0.56589 |  0:00:07s
epoch 7  | loss: 0.31879 | val_0_rmse: 0.54395 | val_1_rmse: 0.55107 |  0:00:08s
epoch 8  | loss: 0.31618 | val_0_rmse: 0.5645  | val_1_rmse: 0.5782  |  0:00:09s
epoch 9  | loss: 0.31161 | val_0_rmse: 0.54127 | val_1_rmse: 0.55159 |  0:00:10s
epoch 10 | loss: 0.30363 | val_0_rmse: 0.53786 | val_1_rmse: 0.54577 |  0:00:11s
epoch 11 | loss: 0.30058 | val_0_rmse: 0.5393  | val_1_rmse: 0.55016 |  0:00:12s
epoch 12 | loss: 0.30006 | val_0_rmse: 0.53813 | val_1_rmse: 0.54461 |  0:00:13s
epoch 13 | loss: 0.30562 | val_0_rmse: 0.54553 | val_1_rmse: 0.55856 |  0:00:14s
epoch 14 | loss: 0.30602 | val_0_rmse: 0.55137 | val_1_rmse: 0.56036 |  0:00:15s
epoch 15 | loss: 0.3039  | val_0_rmse: 0.53539 | val_1_rmse: 0.53955 |  0:00:16s
epoch 16 | loss: 0.30186 | val_0_rmse: 0.53976 | val_1_rmse: 0.55018 |  0:00:17s
epoch 17 | loss: 0.30101 | val_0_rmse: 0.53886 | val_1_rmse: 0.5473  |  0:00:18s
epoch 18 | loss: 0.29735 | val_0_rmse: 0.52939 | val_1_rmse: 0.53828 |  0:00:19s
epoch 19 | loss: 0.29165 | val_0_rmse: 0.52926 | val_1_rmse: 0.53778 |  0:00:20s
epoch 20 | loss: 0.29272 | val_0_rmse: 0.53499 | val_1_rmse: 0.53935 |  0:00:21s
epoch 21 | loss: 0.30338 | val_0_rmse: 0.54292 | val_1_rmse: 0.55205 |  0:00:22s
epoch 22 | loss: 0.29392 | val_0_rmse: 0.53445 | val_1_rmse: 0.53717 |  0:00:23s
epoch 23 | loss: 0.29931 | val_0_rmse: 0.54519 | val_1_rmse: 0.55879 |  0:00:24s
epoch 24 | loss: 0.29587 | val_0_rmse: 0.54514 | val_1_rmse: 0.5518  |  0:00:25s
epoch 25 | loss: 0.29563 | val_0_rmse: 0.52571 | val_1_rmse: 0.53165 |  0:00:26s
epoch 26 | loss: 0.28968 | val_0_rmse: 0.52433 | val_1_rmse: 0.52562 |  0:00:27s
epoch 27 | loss: 0.28696 | val_0_rmse: 0.52499 | val_1_rmse: 0.53276 |  0:00:28s
epoch 28 | loss: 0.2861  | val_0_rmse: 0.5272  | val_1_rmse: 0.53192 |  0:00:29s
epoch 29 | loss: 0.28401 | val_0_rmse: 0.51886 | val_1_rmse: 0.52452 |  0:00:30s
epoch 30 | loss: 0.27972 | val_0_rmse: 0.53303 | val_1_rmse: 0.54011 |  0:00:31s
epoch 31 | loss: 0.28653 | val_0_rmse: 0.52065 | val_1_rmse: 0.52824 |  0:00:32s
epoch 32 | loss: 0.28695 | val_0_rmse: 0.53769 | val_1_rmse: 0.54923 |  0:00:34s
epoch 33 | loss: 0.28382 | val_0_rmse: 0.52535 | val_1_rmse: 0.53578 |  0:00:35s
epoch 34 | loss: 0.27965 | val_0_rmse: 0.58515 | val_1_rmse: 0.58769 |  0:00:36s
epoch 35 | loss: 0.29356 | val_0_rmse: 0.53218 | val_1_rmse: 0.53628 |  0:00:37s
epoch 36 | loss: 0.28466 | val_0_rmse: 0.55476 | val_1_rmse: 0.55974 |  0:00:38s
epoch 37 | loss: 0.2991  | val_0_rmse: 0.54102 | val_1_rmse: 0.55719 |  0:00:39s
epoch 38 | loss: 0.28357 | val_0_rmse: 0.52882 | val_1_rmse: 0.53303 |  0:00:40s
epoch 39 | loss: 0.28538 | val_0_rmse: 0.55855 | val_1_rmse: 0.5691  |  0:00:41s
epoch 40 | loss: 0.2912  | val_0_rmse: 0.54867 | val_1_rmse: 0.56133 |  0:00:42s
epoch 41 | loss: 0.28773 | val_0_rmse: 0.51831 | val_1_rmse: 0.5224  |  0:00:43s
epoch 42 | loss: 0.27577 | val_0_rmse: 0.52182 | val_1_rmse: 0.52924 |  0:00:44s
epoch 43 | loss: 0.26784 | val_0_rmse: 0.54535 | val_1_rmse: 0.55392 |  0:00:45s
epoch 44 | loss: 0.27459 | val_0_rmse: 0.55826 | val_1_rmse: 0.57422 |  0:00:46s
epoch 45 | loss: 0.27034 | val_0_rmse: 0.54501 | val_1_rmse: 0.5518  |  0:00:47s
epoch 46 | loss: 0.27489 | val_0_rmse: 0.53368 | val_1_rmse: 0.54632 |  0:00:48s
epoch 47 | loss: 0.27499 | val_0_rmse: 0.55416 | val_1_rmse: 0.57075 |  0:00:49s
epoch 48 | loss: 0.27313 | val_0_rmse: 0.53986 | val_1_rmse: 0.53914 |  0:00:50s
epoch 49 | loss: 0.28293 | val_0_rmse: 0.51918 | val_1_rmse: 0.52208 |  0:00:51s
epoch 50 | loss: 0.26837 | val_0_rmse: 0.53384 | val_1_rmse: 0.55597 |  0:00:52s
epoch 51 | loss: 0.27325 | val_0_rmse: 0.512   | val_1_rmse: 0.52671 |  0:00:53s
epoch 52 | loss: 0.27465 | val_0_rmse: 0.53948 | val_1_rmse: 0.5485  |  0:00:54s
epoch 53 | loss: 0.27112 | val_0_rmse: 0.5694  | val_1_rmse: 0.57131 |  0:00:55s
epoch 54 | loss: 0.27595 | val_0_rmse: 0.5484  | val_1_rmse: 0.55682 |  0:00:56s
epoch 55 | loss: 0.26918 | val_0_rmse: 0.5191  | val_1_rmse: 0.52058 |  0:00:57s
epoch 56 | loss: 0.26865 | val_0_rmse: 0.51444 | val_1_rmse: 0.52958 |  0:00:58s
epoch 57 | loss: 0.26889 | val_0_rmse: 0.57413 | val_1_rmse: 0.59311 |  0:00:59s
epoch 58 | loss: 0.27204 | val_0_rmse: 0.54065 | val_1_rmse: 0.53996 |  0:01:00s
epoch 59 | loss: 0.26489 | val_0_rmse: 0.53905 | val_1_rmse: 0.55438 |  0:01:01s
epoch 60 | loss: 0.26323 | val_0_rmse: 0.5746  | val_1_rmse: 0.58068 |  0:01:02s
epoch 61 | loss: 0.2658  | val_0_rmse: 0.54024 | val_1_rmse: 0.54753 |  0:01:03s
epoch 62 | loss: 0.26023 | val_0_rmse: 0.55183 | val_1_rmse: 0.55162 |  0:01:04s
epoch 63 | loss: 0.26418 | val_0_rmse: 0.51025 | val_1_rmse: 0.5161  |  0:01:05s
epoch 64 | loss: 0.26313 | val_0_rmse: 0.50348 | val_1_rmse: 0.51513 |  0:01:06s
epoch 65 | loss: 0.25878 | val_0_rmse: 0.49469 | val_1_rmse: 0.50898 |  0:01:08s
epoch 66 | loss: 0.26362 | val_0_rmse: 0.49418 | val_1_rmse: 0.50733 |  0:01:09s
epoch 67 | loss: 0.25685 | val_0_rmse: 0.56399 | val_1_rmse: 0.57572 |  0:01:10s
epoch 68 | loss: 0.25427 | val_0_rmse: 0.53382 | val_1_rmse: 0.53519 |  0:01:11s
epoch 69 | loss: 0.26139 | val_0_rmse: 0.53709 | val_1_rmse: 0.54412 |  0:01:12s
epoch 70 | loss: 0.26126 | val_0_rmse: 0.56282 | val_1_rmse: 0.56833 |  0:01:13s
epoch 71 | loss: 0.26831 | val_0_rmse: 0.52437 | val_1_rmse: 0.53698 |  0:01:14s
epoch 72 | loss: 0.2641  | val_0_rmse: 0.54281 | val_1_rmse: 0.5546  |  0:01:15s
epoch 73 | loss: 0.26123 | val_0_rmse: 0.52204 | val_1_rmse: 0.53214 |  0:01:16s
epoch 74 | loss: 0.26999 | val_0_rmse: 0.56596 | val_1_rmse: 0.57018 |  0:01:17s
epoch 75 | loss: 0.2757  | val_0_rmse: 0.54064 | val_1_rmse: 0.54857 |  0:01:18s
epoch 76 | loss: 0.26718 | val_0_rmse: 0.52267 | val_1_rmse: 0.53068 |  0:01:19s
epoch 77 | loss: 0.27109 | val_0_rmse: 0.58806 | val_1_rmse: 0.59447 |  0:01:20s
epoch 78 | loss: 0.25867 | val_0_rmse: 0.51659 | val_1_rmse: 0.53383 |  0:01:21s
epoch 79 | loss: 0.25786 | val_0_rmse: 0.50025 | val_1_rmse: 0.51051 |  0:01:22s
epoch 80 | loss: 0.25506 | val_0_rmse: 0.53569 | val_1_rmse: 0.54533 |  0:01:23s
epoch 81 | loss: 0.26188 | val_0_rmse: 0.54206 | val_1_rmse: 0.54481 |  0:01:24s
epoch 82 | loss: 0.25644 | val_0_rmse: 0.54255 | val_1_rmse: 0.56177 |  0:01:25s
epoch 83 | loss: 0.25651 | val_0_rmse: 0.57064 | val_1_rmse: 0.5856  |  0:01:26s
epoch 84 | loss: 0.25746 | val_0_rmse: 0.50643 | val_1_rmse: 0.52098 |  0:01:27s
epoch 85 | loss: 0.26135 | val_0_rmse: 0.49899 | val_1_rmse: 0.5066  |  0:01:28s
epoch 86 | loss: 0.25857 | val_0_rmse: 0.50889 | val_1_rmse: 0.51763 |  0:01:29s
epoch 87 | loss: 0.25721 | val_0_rmse: 0.54312 | val_1_rmse: 0.55823 |  0:01:30s
epoch 88 | loss: 0.2522  | val_0_rmse: 0.50939 | val_1_rmse: 0.5171  |  0:01:31s
epoch 89 | loss: 0.25118 | val_0_rmse: 0.56504 | val_1_rmse: 0.57821 |  0:01:32s
epoch 90 | loss: 0.25195 | val_0_rmse: 0.49335 | val_1_rmse: 0.51256 |  0:01:33s
epoch 91 | loss: 0.25064 | val_0_rmse: 0.55415 | val_1_rmse: 0.57284 |  0:01:34s
epoch 92 | loss: 0.25469 | val_0_rmse: 0.50552 | val_1_rmse: 0.51401 |  0:01:35s
epoch 93 | loss: 0.2561  | val_0_rmse: 0.53097 | val_1_rmse: 0.54019 |  0:01:36s
epoch 94 | loss: 0.25943 | val_0_rmse: 0.52929 | val_1_rmse: 0.54002 |  0:01:37s
epoch 95 | loss: 0.27345 | val_0_rmse: 0.54553 | val_1_rmse: 0.54695 |  0:01:39s
epoch 96 | loss: 0.26334 | val_0_rmse: 0.49995 | val_1_rmse: 0.50573 |  0:01:40s
epoch 97 | loss: 0.25326 | val_0_rmse: 0.50292 | val_1_rmse: 0.51445 |  0:01:41s
epoch 98 | loss: 0.26059 | val_0_rmse: 0.56822 | val_1_rmse: 0.58142 |  0:01:42s
epoch 99 | loss: 0.25374 | val_0_rmse: 0.51491 | val_1_rmse: 0.53013 |  0:01:43s
epoch 100| loss: 0.25038 | val_0_rmse: 0.51586 | val_1_rmse: 0.52308 |  0:01:44s
epoch 101| loss: 0.26037 | val_0_rmse: 0.50528 | val_1_rmse: 0.52427 |  0:01:45s
epoch 102| loss: 0.25035 | val_0_rmse: 0.524   | val_1_rmse: 0.53283 |  0:01:46s
epoch 103| loss: 0.24745 | val_0_rmse: 0.5168  | val_1_rmse: 0.53158 |  0:01:47s
epoch 104| loss: 0.24829 | val_0_rmse: 0.49795 | val_1_rmse: 0.51359 |  0:01:48s
epoch 105| loss: 0.24944 | val_0_rmse: 0.52782 | val_1_rmse: 0.54007 |  0:01:49s
epoch 106| loss: 0.25114 | val_0_rmse: 0.62309 | val_1_rmse: 0.64105 |  0:01:50s
epoch 107| loss: 0.26479 | val_0_rmse: 0.5582  | val_1_rmse: 0.56526 |  0:01:51s
epoch 108| loss: 0.24799 | val_0_rmse: 0.53    | val_1_rmse: 0.54866 |  0:01:52s
epoch 109| loss: 0.24782 | val_0_rmse: 0.50662 | val_1_rmse: 0.52252 |  0:01:53s
epoch 110| loss: 0.25246 | val_0_rmse: 0.50679 | val_1_rmse: 0.51642 |  0:01:54s
epoch 111| loss: 0.24899 | val_0_rmse: 0.50866 | val_1_rmse: 0.51854 |  0:01:55s
epoch 112| loss: 0.24603 | val_0_rmse: 0.53156 | val_1_rmse: 0.54709 |  0:01:56s
epoch 113| loss: 0.24534 | val_0_rmse: 0.51047 | val_1_rmse: 0.51966 |  0:01:57s
epoch 114| loss: 0.24311 | val_0_rmse: 0.50782 | val_1_rmse: 0.51051 |  0:01:58s
epoch 115| loss: 0.24427 | val_0_rmse: 0.50605 | val_1_rmse: 0.52225 |  0:01:59s
epoch 116| loss: 0.25413 | val_0_rmse: 0.48629 | val_1_rmse: 0.50592 |  0:02:00s
epoch 117| loss: 0.24526 | val_0_rmse: 0.54753 | val_1_rmse: 0.54812 |  0:02:01s
epoch 118| loss: 0.25165 | val_0_rmse: 0.53055 | val_1_rmse: 0.52959 |  0:02:02s
epoch 119| loss: 0.24543 | val_0_rmse: 0.47952 | val_1_rmse: 0.49325 |  0:02:03s
epoch 120| loss: 0.24079 | val_0_rmse: 0.48798 | val_1_rmse: 0.50319 |  0:02:04s
epoch 121| loss: 0.28382 | val_0_rmse: 0.55641 | val_1_rmse: 0.55786 |  0:02:05s
epoch 122| loss: 0.2713  | val_0_rmse: 0.52925 | val_1_rmse: 0.54505 |  0:02:06s
epoch 123| loss: 0.26572 | val_0_rmse: 0.51015 | val_1_rmse: 0.52378 |  0:02:07s
epoch 124| loss: 0.26085 | val_0_rmse: 0.62042 | val_1_rmse: 0.6331  |  0:02:08s
epoch 125| loss: 0.25673 | val_0_rmse: 0.50327 | val_1_rmse: 0.51365 |  0:02:09s
epoch 126| loss: 0.2506  | val_0_rmse: 0.58098 | val_1_rmse: 0.59508 |  0:02:10s
epoch 127| loss: 0.25208 | val_0_rmse: 0.48817 | val_1_rmse: 0.49843 |  0:02:11s
epoch 128| loss: 0.25109 | val_0_rmse: 0.54897 | val_1_rmse: 0.55774 |  0:02:12s
epoch 129| loss: 0.2537  | val_0_rmse: 0.545   | val_1_rmse: 0.56539 |  0:02:13s
epoch 130| loss: 0.25119 | val_0_rmse: 0.5465  | val_1_rmse: 0.55784 |  0:02:15s
epoch 131| loss: 0.25811 | val_0_rmse: 0.57704 | val_1_rmse: 0.58502 |  0:02:16s
epoch 132| loss: 0.26166 | val_0_rmse: 0.49867 | val_1_rmse: 0.5144  |  0:02:17s
epoch 133| loss: 0.24899 | val_0_rmse: 0.51901 | val_1_rmse: 0.52977 |  0:02:18s
epoch 134| loss: 0.24445 | val_0_rmse: 0.51354 | val_1_rmse: 0.52984 |  0:02:19s
epoch 135| loss: 0.25672 | val_0_rmse: 0.49035 | val_1_rmse: 0.50373 |  0:02:20s
epoch 136| loss: 0.24824 | val_0_rmse: 0.49132 | val_1_rmse: 0.50238 |  0:02:21s
epoch 137| loss: 0.24448 | val_0_rmse: 0.51214 | val_1_rmse: 0.52574 |  0:02:22s
epoch 138| loss: 0.24616 | val_0_rmse: 0.49132 | val_1_rmse: 0.5076  |  0:02:23s
epoch 139| loss: 0.24909 | val_0_rmse: 0.5124  | val_1_rmse: 0.52383 |  0:02:24s
epoch 140| loss: 0.26787 | val_0_rmse: 0.57403 | val_1_rmse: 0.59009 |  0:02:25s
epoch 141| loss: 0.26383 | val_0_rmse: 0.5899  | val_1_rmse: 0.59379 |  0:02:26s
epoch 142| loss: 0.25512 | val_0_rmse: 0.52322 | val_1_rmse: 0.52398 |  0:02:27s
epoch 143| loss: 0.25553 | val_0_rmse: 0.58346 | val_1_rmse: 0.59605 |  0:02:28s
epoch 144| loss: 0.25689 | val_0_rmse: 0.50994 | val_1_rmse: 0.51335 |  0:02:29s
epoch 145| loss: 0.25152 | val_0_rmse: 0.49444 | val_1_rmse: 0.49816 |  0:02:30s
epoch 146| loss: 0.25126 | val_0_rmse: 0.51773 | val_1_rmse: 0.5179  |  0:02:31s
epoch 147| loss: 0.25682 | val_0_rmse: 0.54032 | val_1_rmse: 0.55479 |  0:02:32s
epoch 148| loss: 0.25164 | val_0_rmse: 0.52199 | val_1_rmse: 0.53534 |  0:02:33s
epoch 149| loss: 0.24685 | val_0_rmse: 0.49812 | val_1_rmse: 0.51268 |  0:02:34s

Early stopping occured at epoch 149 with best_epoch = 119 and best_val_1_rmse = 0.49325
Best weights from best epoch are automatically used!
ended training at: 13:43:34
Feature importance:
[('Area', 0.32753756254987537), ('Baths', 0.25550993975431896), ('Beds', 0.036925126596216704), ('Latitude', 0.06021839709372121), ('Longitude', 0.26453363139844044), ('Month', 0.055208690281800914), ('Year', 6.665232562638963e-05)]
Mean squared error is of 927562734.2472428
Mean absolute error:20866.848799090618
MAPE:0.25265467849189616
R2 score:0.7683204927689646
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:43:34
epoch 0  | loss: 0.72066 | val_0_rmse: 0.72953 | val_1_rmse: 0.74023 |  0:00:01s
epoch 1  | loss: 0.40555 | val_0_rmse: 0.61828 | val_1_rmse: 0.61592 |  0:00:02s
epoch 2  | loss: 0.36376 | val_0_rmse: 0.5955  | val_1_rmse: 0.58885 |  0:00:03s
epoch 3  | loss: 0.34938 | val_0_rmse: 0.56666 | val_1_rmse: 0.56302 |  0:00:04s
epoch 4  | loss: 0.33487 | val_0_rmse: 0.56582 | val_1_rmse: 0.55998 |  0:00:05s
epoch 5  | loss: 0.33001 | val_0_rmse: 0.55478 | val_1_rmse: 0.54618 |  0:00:06s
epoch 6  | loss: 0.31973 | val_0_rmse: 0.55447 | val_1_rmse: 0.54978 |  0:00:07s
epoch 7  | loss: 0.31782 | val_0_rmse: 0.55039 | val_1_rmse: 0.54082 |  0:00:08s
epoch 8  | loss: 0.31355 | val_0_rmse: 0.57358 | val_1_rmse: 0.57142 |  0:00:09s
epoch 9  | loss: 0.316   | val_0_rmse: 0.55867 | val_1_rmse: 0.55551 |  0:00:10s
epoch 10 | loss: 0.30439 | val_0_rmse: 0.53957 | val_1_rmse: 0.53414 |  0:00:11s
epoch 11 | loss: 0.30292 | val_0_rmse: 0.53528 | val_1_rmse: 0.53396 |  0:00:12s
epoch 12 | loss: 0.30371 | val_0_rmse: 0.53464 | val_1_rmse: 0.53195 |  0:00:13s
epoch 13 | loss: 0.30702 | val_0_rmse: 0.53694 | val_1_rmse: 0.53364 |  0:00:14s
epoch 14 | loss: 0.30405 | val_0_rmse: 0.54046 | val_1_rmse: 0.53849 |  0:00:15s
epoch 15 | loss: 0.30267 | val_0_rmse: 0.54887 | val_1_rmse: 0.54588 |  0:00:16s
epoch 16 | loss: 0.31462 | val_0_rmse: 0.53698 | val_1_rmse: 0.53634 |  0:00:17s
epoch 17 | loss: 0.3047  | val_0_rmse: 0.55133 | val_1_rmse: 0.55218 |  0:00:18s
epoch 18 | loss: 0.30238 | val_0_rmse: 0.53312 | val_1_rmse: 0.5337  |  0:00:19s
epoch 19 | loss: 0.2902  | val_0_rmse: 0.52317 | val_1_rmse: 0.52233 |  0:00:20s
epoch 20 | loss: 0.29156 | val_0_rmse: 0.54085 | val_1_rmse: 0.54008 |  0:00:21s
epoch 21 | loss: 0.29551 | val_0_rmse: 0.52113 | val_1_rmse: 0.52193 |  0:00:22s
epoch 22 | loss: 0.28662 | val_0_rmse: 0.51982 | val_1_rmse: 0.52106 |  0:00:23s
epoch 23 | loss: 0.28321 | val_0_rmse: 0.54215 | val_1_rmse: 0.5417  |  0:00:24s
epoch 24 | loss: 0.29452 | val_0_rmse: 0.5481  | val_1_rmse: 0.54512 |  0:00:25s
epoch 25 | loss: 0.29195 | val_0_rmse: 0.53661 | val_1_rmse: 0.53673 |  0:00:26s
epoch 26 | loss: 0.29719 | val_0_rmse: 0.5276  | val_1_rmse: 0.52205 |  0:00:27s
epoch 27 | loss: 0.28654 | val_0_rmse: 0.56634 | val_1_rmse: 0.56541 |  0:00:28s
epoch 28 | loss: 0.28232 | val_0_rmse: 0.52287 | val_1_rmse: 0.52009 |  0:00:29s
epoch 29 | loss: 0.28457 | val_0_rmse: 0.51697 | val_1_rmse: 0.51909 |  0:00:30s
epoch 30 | loss: 0.28435 | val_0_rmse: 0.53086 | val_1_rmse: 0.5292  |  0:00:31s
epoch 31 | loss: 0.27856 | val_0_rmse: 0.52939 | val_1_rmse: 0.53467 |  0:00:33s
epoch 32 | loss: 0.27569 | val_0_rmse: 0.50808 | val_1_rmse: 0.50651 |  0:00:34s
epoch 33 | loss: 0.27633 | val_0_rmse: 0.51736 | val_1_rmse: 0.51504 |  0:00:35s
epoch 34 | loss: 0.28489 | val_0_rmse: 0.55613 | val_1_rmse: 0.55575 |  0:00:36s
epoch 35 | loss: 0.27907 | val_0_rmse: 0.52792 | val_1_rmse: 0.52851 |  0:00:37s
epoch 36 | loss: 0.27592 | val_0_rmse: 0.51744 | val_1_rmse: 0.5157  |  0:00:38s
epoch 37 | loss: 0.27588 | val_0_rmse: 0.51565 | val_1_rmse: 0.51483 |  0:00:39s
epoch 38 | loss: 0.2767  | val_0_rmse: 0.51645 | val_1_rmse: 0.5148  |  0:00:40s
epoch 39 | loss: 0.27437 | val_0_rmse: 0.54627 | val_1_rmse: 0.54522 |  0:00:41s
epoch 40 | loss: 0.27783 | val_0_rmse: 0.53053 | val_1_rmse: 0.5324  |  0:00:42s
epoch 41 | loss: 0.27794 | val_0_rmse: 0.5339  | val_1_rmse: 0.53785 |  0:00:43s
epoch 42 | loss: 0.27254 | val_0_rmse: 0.55651 | val_1_rmse: 0.55199 |  0:00:44s
epoch 43 | loss: 0.27472 | val_0_rmse: 0.51207 | val_1_rmse: 0.51537 |  0:00:45s
epoch 44 | loss: 0.27182 | val_0_rmse: 0.60527 | val_1_rmse: 0.60067 |  0:00:46s
epoch 45 | loss: 0.2722  | val_0_rmse: 0.51491 | val_1_rmse: 0.51245 |  0:00:47s
epoch 46 | loss: 0.2683  | val_0_rmse: 0.49887 | val_1_rmse: 0.49755 |  0:00:48s
epoch 47 | loss: 0.26166 | val_0_rmse: 0.54286 | val_1_rmse: 0.54272 |  0:00:49s
epoch 48 | loss: 0.26338 | val_0_rmse: 0.53687 | val_1_rmse: 0.53512 |  0:00:50s
epoch 49 | loss: 0.26537 | val_0_rmse: 0.50463 | val_1_rmse: 0.50074 |  0:00:51s
epoch 50 | loss: 0.26327 | val_0_rmse: 0.51347 | val_1_rmse: 0.51023 |  0:00:52s
epoch 51 | loss: 0.26755 | val_0_rmse: 0.55911 | val_1_rmse: 0.56368 |  0:00:53s
epoch 52 | loss: 0.26624 | val_0_rmse: 0.52029 | val_1_rmse: 0.52399 |  0:00:54s
epoch 53 | loss: 0.27283 | val_0_rmse: 0.56996 | val_1_rmse: 0.56749 |  0:00:55s
epoch 54 | loss: 0.27719 | val_0_rmse: 0.51538 | val_1_rmse: 0.51661 |  0:00:56s
epoch 55 | loss: 0.26852 | val_0_rmse: 0.52656 | val_1_rmse: 0.52991 |  0:00:57s
epoch 56 | loss: 0.26352 | val_0_rmse: 0.50717 | val_1_rmse: 0.50592 |  0:00:58s
epoch 57 | loss: 0.26515 | val_0_rmse: 0.49873 | val_1_rmse: 0.5     |  0:00:59s
epoch 58 | loss: 0.25862 | val_0_rmse: 0.54254 | val_1_rmse: 0.54159 |  0:01:00s
epoch 59 | loss: 0.2619  | val_0_rmse: 0.49486 | val_1_rmse: 0.49376 |  0:01:01s
epoch 60 | loss: 0.25851 | val_0_rmse: 0.54101 | val_1_rmse: 0.54039 |  0:01:02s
epoch 61 | loss: 0.26463 | val_0_rmse: 0.54447 | val_1_rmse: 0.54397 |  0:01:03s
epoch 62 | loss: 0.25919 | val_0_rmse: 0.51156 | val_1_rmse: 0.51018 |  0:01:04s
epoch 63 | loss: 0.257   | val_0_rmse: 0.5206  | val_1_rmse: 0.52078 |  0:01:05s
epoch 64 | loss: 0.26055 | val_0_rmse: 0.50405 | val_1_rmse: 0.5048  |  0:01:06s
epoch 65 | loss: 0.26262 | val_0_rmse: 0.52212 | val_1_rmse: 0.5206  |  0:01:07s
epoch 66 | loss: 0.26098 | val_0_rmse: 0.56198 | val_1_rmse: 0.56273 |  0:01:09s
epoch 67 | loss: 0.25967 | val_0_rmse: 0.54689 | val_1_rmse: 0.54364 |  0:01:10s
epoch 68 | loss: 0.25507 | val_0_rmse: 0.55309 | val_1_rmse: 0.54942 |  0:01:11s
epoch 69 | loss: 0.25389 | val_0_rmse: 0.52349 | val_1_rmse: 0.52633 |  0:01:12s
epoch 70 | loss: 0.26121 | val_0_rmse: 0.51272 | val_1_rmse: 0.51511 |  0:01:13s
epoch 71 | loss: 0.25888 | val_0_rmse: 0.52326 | val_1_rmse: 0.5233  |  0:01:14s
epoch 72 | loss: 0.25434 | val_0_rmse: 0.53191 | val_1_rmse: 0.53558 |  0:01:15s
epoch 73 | loss: 0.26159 | val_0_rmse: 0.51471 | val_1_rmse: 0.52051 |  0:01:16s
epoch 74 | loss: 0.25503 | val_0_rmse: 0.51422 | val_1_rmse: 0.51139 |  0:01:17s
epoch 75 | loss: 0.25853 | val_0_rmse: 0.5506  | val_1_rmse: 0.55429 |  0:01:18s
epoch 76 | loss: 0.27013 | val_0_rmse: 0.51917 | val_1_rmse: 0.5214  |  0:01:19s
epoch 77 | loss: 0.25994 | val_0_rmse: 0.51021 | val_1_rmse: 0.51312 |  0:01:20s
epoch 78 | loss: 0.25708 | val_0_rmse: 0.49829 | val_1_rmse: 0.4995  |  0:01:21s
epoch 79 | loss: 0.25291 | val_0_rmse: 0.55373 | val_1_rmse: 0.55474 |  0:01:22s
epoch 80 | loss: 0.25926 | val_0_rmse: 0.51421 | val_1_rmse: 0.51729 |  0:01:23s
epoch 81 | loss: 0.25402 | val_0_rmse: 0.49111 | val_1_rmse: 0.49384 |  0:01:24s
epoch 82 | loss: 0.25676 | val_0_rmse: 0.50617 | val_1_rmse: 0.5065  |  0:01:25s
epoch 83 | loss: 0.25946 | val_0_rmse: 0.54441 | val_1_rmse: 0.54334 |  0:01:26s
epoch 84 | loss: 0.26    | val_0_rmse: 0.50928 | val_1_rmse: 0.51348 |  0:01:27s
epoch 85 | loss: 0.2551  | val_0_rmse: 0.56093 | val_1_rmse: 0.56165 |  0:01:28s
epoch 86 | loss: 0.25885 | val_0_rmse: 0.52799 | val_1_rmse: 0.52951 |  0:01:29s
epoch 87 | loss: 0.25409 | val_0_rmse: 0.53241 | val_1_rmse: 0.53393 |  0:01:30s
epoch 88 | loss: 0.2537  | val_0_rmse: 0.51212 | val_1_rmse: 0.51211 |  0:01:31s
epoch 89 | loss: 0.25686 | val_0_rmse: 0.57324 | val_1_rmse: 0.57626 |  0:01:32s

Early stopping occured at epoch 89 with best_epoch = 59 and best_val_1_rmse = 0.49376
Best weights from best epoch are automatically used!
ended training at: 13:45:07
Feature importance:
[('Area', 0.23858806536516675), ('Baths', 0.17887032734577932), ('Beds', 0.11721854622259513), ('Latitude', 0.1512172222364707), ('Longitude', 0.229937723820649), ('Month', 0.0), ('Year', 0.08416811500933914)]
Mean squared error is of 1028248621.6128117
Mean absolute error:21570.604605605306
MAPE:0.2569930381548925
R2 score:0.7474341898512789
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:45:07
epoch 0  | loss: 0.53079 | val_0_rmse: 0.62422 | val_1_rmse: 0.62533 |  0:00:01s
epoch 1  | loss: 0.35723 | val_0_rmse: 0.57513 | val_1_rmse: 0.5724  |  0:00:03s
epoch 2  | loss: 0.29807 | val_0_rmse: 0.51118 | val_1_rmse: 0.50314 |  0:00:04s
epoch 3  | loss: 0.2782  | val_0_rmse: 0.49761 | val_1_rmse: 0.48868 |  0:00:06s
epoch 4  | loss: 0.26398 | val_0_rmse: 0.49599 | val_1_rmse: 0.48636 |  0:00:08s
epoch 5  | loss: 0.24852 | val_0_rmse: 0.47089 | val_1_rmse: 0.46389 |  0:00:09s
epoch 6  | loss: 0.24304 | val_0_rmse: 0.49996 | val_1_rmse: 0.49725 |  0:00:11s
epoch 7  | loss: 0.24087 | val_0_rmse: 0.45965 | val_1_rmse: 0.45517 |  0:00:13s
epoch 8  | loss: 0.23003 | val_0_rmse: 0.46657 | val_1_rmse: 0.45961 |  0:00:14s
epoch 9  | loss: 0.23444 | val_0_rmse: 0.45766 | val_1_rmse: 0.45166 |  0:00:16s
epoch 10 | loss: 0.22813 | val_0_rmse: 0.4564  | val_1_rmse: 0.4525  |  0:00:18s
epoch 11 | loss: 0.23016 | val_0_rmse: 0.45455 | val_1_rmse: 0.44941 |  0:00:19s
epoch 12 | loss: 0.22683 | val_0_rmse: 0.45317 | val_1_rmse: 0.44811 |  0:00:21s
epoch 13 | loss: 0.22314 | val_0_rmse: 0.45924 | val_1_rmse: 0.45274 |  0:00:22s
epoch 14 | loss: 0.21886 | val_0_rmse: 0.44994 | val_1_rmse: 0.44404 |  0:00:24s
epoch 15 | loss: 0.21693 | val_0_rmse: 0.44704 | val_1_rmse: 0.44167 |  0:00:26s
epoch 16 | loss: 0.21866 | val_0_rmse: 0.45009 | val_1_rmse: 0.44327 |  0:00:27s
epoch 17 | loss: 0.22334 | val_0_rmse: 0.47696 | val_1_rmse: 0.46985 |  0:00:29s
epoch 18 | loss: 0.21955 | val_0_rmse: 0.44809 | val_1_rmse: 0.44316 |  0:00:31s
epoch 19 | loss: 0.21687 | val_0_rmse: 0.44975 | val_1_rmse: 0.4419  |  0:00:32s
epoch 20 | loss: 0.21678 | val_0_rmse: 0.44929 | val_1_rmse: 0.44547 |  0:00:34s
epoch 21 | loss: 0.21196 | val_0_rmse: 0.43809 | val_1_rmse: 0.4354  |  0:00:35s
epoch 22 | loss: 0.21642 | val_0_rmse: 0.47428 | val_1_rmse: 0.46769 |  0:00:37s
epoch 23 | loss: 0.22633 | val_0_rmse: 0.44291 | val_1_rmse: 0.4357  |  0:00:39s
epoch 24 | loss: 0.21511 | val_0_rmse: 0.44443 | val_1_rmse: 0.43997 |  0:00:40s
epoch 25 | loss: 0.21498 | val_0_rmse: 0.44428 | val_1_rmse: 0.44097 |  0:00:42s
epoch 26 | loss: 0.21083 | val_0_rmse: 0.43723 | val_1_rmse: 0.43219 |  0:00:44s
epoch 27 | loss: 0.20854 | val_0_rmse: 0.46744 | val_1_rmse: 0.46558 |  0:00:45s
epoch 28 | loss: 0.21406 | val_0_rmse: 0.44292 | val_1_rmse: 0.43957 |  0:00:47s
epoch 29 | loss: 0.20778 | val_0_rmse: 0.43471 | val_1_rmse: 0.42821 |  0:00:48s
epoch 30 | loss: 0.20621 | val_0_rmse: 0.44433 | val_1_rmse: 0.44034 |  0:00:50s
epoch 31 | loss: 0.20657 | val_0_rmse: 0.43274 | val_1_rmse: 0.42771 |  0:00:52s
epoch 32 | loss: 0.20944 | val_0_rmse: 0.44712 | val_1_rmse: 0.44348 |  0:00:53s
epoch 33 | loss: 0.20931 | val_0_rmse: 0.43495 | val_1_rmse: 0.43177 |  0:00:55s
epoch 34 | loss: 0.20663 | val_0_rmse: 0.43016 | val_1_rmse: 0.42814 |  0:00:57s
epoch 35 | loss: 0.20349 | val_0_rmse: 0.43563 | val_1_rmse: 0.43372 |  0:00:58s
epoch 36 | loss: 0.20707 | val_0_rmse: 0.42719 | val_1_rmse: 0.42502 |  0:01:00s
epoch 37 | loss: 0.2108  | val_0_rmse: 0.43477 | val_1_rmse: 0.43007 |  0:01:02s
epoch 38 | loss: 0.21082 | val_0_rmse: 0.43387 | val_1_rmse: 0.42872 |  0:01:03s
epoch 39 | loss: 0.2056  | val_0_rmse: 0.45074 | val_1_rmse: 0.44564 |  0:01:05s
epoch 40 | loss: 0.20541 | val_0_rmse: 0.42803 | val_1_rmse: 0.42302 |  0:01:06s
epoch 41 | loss: 0.20064 | val_0_rmse: 0.43371 | val_1_rmse: 0.43264 |  0:01:08s
epoch 42 | loss: 0.20218 | val_0_rmse: 0.42616 | val_1_rmse: 0.42117 |  0:01:10s
epoch 43 | loss: 0.20254 | val_0_rmse: 0.45251 | val_1_rmse: 0.44764 |  0:01:11s
epoch 44 | loss: 0.21047 | val_0_rmse: 0.44928 | val_1_rmse: 0.4439  |  0:01:13s
epoch 45 | loss: 0.20203 | val_0_rmse: 0.42368 | val_1_rmse: 0.42093 |  0:01:14s
epoch 46 | loss: 0.20274 | val_0_rmse: 0.43378 | val_1_rmse: 0.42877 |  0:01:16s
epoch 47 | loss: 0.1999  | val_0_rmse: 0.42638 | val_1_rmse: 0.42491 |  0:01:18s
epoch 48 | loss: 0.20443 | val_0_rmse: 0.42769 | val_1_rmse: 0.42577 |  0:01:19s
epoch 49 | loss: 0.19807 | val_0_rmse: 0.43684 | val_1_rmse: 0.43507 |  0:01:21s
epoch 50 | loss: 0.20092 | val_0_rmse: 0.42799 | val_1_rmse: 0.42943 |  0:01:23s
epoch 51 | loss: 0.19694 | val_0_rmse: 0.42764 | val_1_rmse: 0.42717 |  0:01:24s
epoch 52 | loss: 0.19777 | val_0_rmse: 0.43351 | val_1_rmse: 0.43263 |  0:01:26s
epoch 53 | loss: 0.20121 | val_0_rmse: 0.42893 | val_1_rmse: 0.42818 |  0:01:27s
epoch 54 | loss: 0.19671 | val_0_rmse: 0.41847 | val_1_rmse: 0.42013 |  0:01:29s
epoch 55 | loss: 0.19726 | val_0_rmse: 0.42586 | val_1_rmse: 0.42415 |  0:01:31s
epoch 56 | loss: 0.20168 | val_0_rmse: 0.43163 | val_1_rmse: 0.42801 |  0:01:32s
epoch 57 | loss: 0.19877 | val_0_rmse: 0.42175 | val_1_rmse: 0.4192  |  0:01:34s
epoch 58 | loss: 0.19605 | val_0_rmse: 0.42459 | val_1_rmse: 0.4263  |  0:01:36s
epoch 59 | loss: 0.1992  | val_0_rmse: 0.42936 | val_1_rmse: 0.42696 |  0:01:37s
epoch 60 | loss: 0.19593 | val_0_rmse: 0.43052 | val_1_rmse: 0.42729 |  0:01:39s
epoch 61 | loss: 0.20419 | val_0_rmse: 0.43409 | val_1_rmse: 0.43002 |  0:01:41s
epoch 62 | loss: 0.2009  | val_0_rmse: 0.43507 | val_1_rmse: 0.43693 |  0:01:42s
epoch 63 | loss: 0.1973  | val_0_rmse: 0.42393 | val_1_rmse: 0.42389 |  0:01:44s
epoch 64 | loss: 0.19708 | val_0_rmse: 0.41958 | val_1_rmse: 0.41976 |  0:01:45s
epoch 65 | loss: 0.19738 | val_0_rmse: 0.42152 | val_1_rmse: 0.4211  |  0:01:47s
epoch 66 | loss: 0.19984 | val_0_rmse: 0.42384 | val_1_rmse: 0.42527 |  0:01:49s
epoch 67 | loss: 0.19685 | val_0_rmse: 0.41717 | val_1_rmse: 0.41641 |  0:01:50s
epoch 68 | loss: 0.19687 | val_0_rmse: 0.42591 | val_1_rmse: 0.42836 |  0:01:52s
epoch 69 | loss: 0.19436 | val_0_rmse: 0.41917 | val_1_rmse: 0.41795 |  0:01:54s
epoch 70 | loss: 0.19276 | val_0_rmse: 0.42172 | val_1_rmse: 0.42054 |  0:01:55s
epoch 71 | loss: 0.19066 | val_0_rmse: 0.41992 | val_1_rmse: 0.42174 |  0:01:57s
epoch 72 | loss: 0.19246 | val_0_rmse: 0.41935 | val_1_rmse: 0.41887 |  0:01:58s
epoch 73 | loss: 0.19237 | val_0_rmse: 0.41693 | val_1_rmse: 0.41853 |  0:02:00s
epoch 74 | loss: 0.19015 | val_0_rmse: 0.4256  | val_1_rmse: 0.42631 |  0:02:02s
epoch 75 | loss: 0.19393 | val_0_rmse: 0.42602 | val_1_rmse: 0.42746 |  0:02:03s
epoch 76 | loss: 0.1955  | val_0_rmse: 0.43302 | val_1_rmse: 0.43427 |  0:02:05s
epoch 77 | loss: 0.19177 | val_0_rmse: 0.41418 | val_1_rmse: 0.41474 |  0:02:07s
epoch 78 | loss: 0.1929  | val_0_rmse: 0.41586 | val_1_rmse: 0.41662 |  0:02:08s
epoch 79 | loss: 0.19138 | val_0_rmse: 0.41312 | val_1_rmse: 0.41583 |  0:02:10s
epoch 80 | loss: 0.19298 | val_0_rmse: 0.42028 | val_1_rmse: 0.42229 |  0:02:11s
epoch 81 | loss: 0.19626 | val_0_rmse: 0.41784 | val_1_rmse: 0.42181 |  0:02:13s
epoch 82 | loss: 0.1954  | val_0_rmse: 0.42911 | val_1_rmse: 0.43064 |  0:02:15s
epoch 83 | loss: 0.1942  | val_0_rmse: 0.4141  | val_1_rmse: 0.41776 |  0:02:16s
epoch 84 | loss: 0.19362 | val_0_rmse: 0.43795 | val_1_rmse: 0.44291 |  0:02:18s
epoch 85 | loss: 0.192   | val_0_rmse: 0.42215 | val_1_rmse: 0.42318 |  0:02:20s
epoch 86 | loss: 0.19032 | val_0_rmse: 0.43029 | val_1_rmse: 0.43102 |  0:02:21s
epoch 87 | loss: 0.18806 | val_0_rmse: 0.4155  | val_1_rmse: 0.41759 |  0:02:23s
epoch 88 | loss: 0.18926 | val_0_rmse: 0.41791 | val_1_rmse: 0.41907 |  0:02:24s
epoch 89 | loss: 0.18683 | val_0_rmse: 0.41058 | val_1_rmse: 0.41308 |  0:02:26s
epoch 90 | loss: 0.18779 | val_0_rmse: 0.40891 | val_1_rmse: 0.4123  |  0:02:28s
epoch 91 | loss: 0.1879  | val_0_rmse: 0.42266 | val_1_rmse: 0.42644 |  0:02:29s
epoch 92 | loss: 0.19158 | val_0_rmse: 0.42161 | val_1_rmse: 0.42229 |  0:02:31s
epoch 93 | loss: 0.19562 | val_0_rmse: 0.42075 | val_1_rmse: 0.42591 |  0:02:33s
epoch 94 | loss: 0.19046 | val_0_rmse: 0.41951 | val_1_rmse: 0.42054 |  0:02:34s
epoch 95 | loss: 0.1874  | val_0_rmse: 0.4125  | val_1_rmse: 0.4166  |  0:02:36s
epoch 96 | loss: 0.19216 | val_0_rmse: 0.42821 | val_1_rmse: 0.43088 |  0:02:37s
epoch 97 | loss: 0.188   | val_0_rmse: 0.41831 | val_1_rmse: 0.4214  |  0:02:39s
epoch 98 | loss: 0.18613 | val_0_rmse: 0.41209 | val_1_rmse: 0.41561 |  0:02:41s
epoch 99 | loss: 0.19033 | val_0_rmse: 0.41532 | val_1_rmse: 0.4205  |  0:02:42s
epoch 100| loss: 0.19585 | val_0_rmse: 0.42224 | val_1_rmse: 0.42319 |  0:02:44s
epoch 101| loss: 0.19542 | val_0_rmse: 0.42519 | val_1_rmse: 0.43084 |  0:02:46s
epoch 102| loss: 0.18815 | val_0_rmse: 0.41399 | val_1_rmse: 0.41781 |  0:02:47s
epoch 103| loss: 0.18821 | val_0_rmse: 0.43407 | val_1_rmse: 0.43826 |  0:02:49s
epoch 104| loss: 0.18589 | val_0_rmse: 0.4159  | val_1_rmse: 0.42024 |  0:02:50s
epoch 105| loss: 0.18781 | val_0_rmse: 0.40537 | val_1_rmse: 0.41087 |  0:02:52s
epoch 106| loss: 0.18593 | val_0_rmse: 0.4221  | val_1_rmse: 0.42688 |  0:02:54s
epoch 107| loss: 0.18639 | val_0_rmse: 0.41757 | val_1_rmse: 0.42507 |  0:02:55s
epoch 108| loss: 0.19068 | val_0_rmse: 0.41596 | val_1_rmse: 0.41786 |  0:02:57s
epoch 109| loss: 0.19111 | val_0_rmse: 0.41249 | val_1_rmse: 0.41751 |  0:02:59s
epoch 110| loss: 0.18615 | val_0_rmse: 0.41041 | val_1_rmse: 0.41647 |  0:03:00s
epoch 111| loss: 0.1846  | val_0_rmse: 0.41089 | val_1_rmse: 0.41471 |  0:03:02s
epoch 112| loss: 0.18653 | val_0_rmse: 0.40961 | val_1_rmse: 0.41408 |  0:03:04s
epoch 113| loss: 0.18192 | val_0_rmse: 0.42019 | val_1_rmse: 0.42478 |  0:03:05s
epoch 114| loss: 0.1917  | val_0_rmse: 0.41833 | val_1_rmse: 0.42199 |  0:03:07s
epoch 115| loss: 0.18699 | val_0_rmse: 0.41145 | val_1_rmse: 0.41601 |  0:03:08s
epoch 116| loss: 0.19168 | val_0_rmse: 0.44242 | val_1_rmse: 0.45228 |  0:03:10s
epoch 117| loss: 0.18738 | val_0_rmse: 0.41497 | val_1_rmse: 0.41999 |  0:03:12s
epoch 118| loss: 0.18442 | val_0_rmse: 0.40779 | val_1_rmse: 0.41362 |  0:03:13s
epoch 119| loss: 0.18355 | val_0_rmse: 0.41556 | val_1_rmse: 0.42512 |  0:03:15s
epoch 120| loss: 0.18526 | val_0_rmse: 0.40941 | val_1_rmse: 0.41324 |  0:03:17s
epoch 121| loss: 0.18371 | val_0_rmse: 0.41299 | val_1_rmse: 0.41643 |  0:03:18s
epoch 122| loss: 0.18874 | val_0_rmse: 0.42112 | val_1_rmse: 0.42535 |  0:03:20s
epoch 123| loss: 0.18884 | val_0_rmse: 0.41426 | val_1_rmse: 0.41837 |  0:03:21s
epoch 124| loss: 0.19043 | val_0_rmse: 0.42385 | val_1_rmse: 0.42791 |  0:03:23s
epoch 125| loss: 0.18351 | val_0_rmse: 0.40852 | val_1_rmse: 0.41481 |  0:03:25s
epoch 126| loss: 0.18736 | val_0_rmse: 0.41975 | val_1_rmse: 0.4247  |  0:03:26s
epoch 127| loss: 0.18444 | val_0_rmse: 0.40685 | val_1_rmse: 0.41407 |  0:03:28s
epoch 128| loss: 0.18253 | val_0_rmse: 0.40838 | val_1_rmse: 0.41638 |  0:03:30s
epoch 129| loss: 0.18642 | val_0_rmse: 0.40465 | val_1_rmse: 0.41198 |  0:03:31s
epoch 130| loss: 0.18378 | val_0_rmse: 0.4081  | val_1_rmse: 0.4169  |  0:03:33s
epoch 131| loss: 0.18681 | val_0_rmse: 0.40463 | val_1_rmse: 0.41233 |  0:03:34s
epoch 132| loss: 0.18249 | val_0_rmse: 0.40652 | val_1_rmse: 0.41437 |  0:03:36s
epoch 133| loss: 0.18607 | val_0_rmse: 0.41141 | val_1_rmse: 0.41533 |  0:03:38s
epoch 134| loss: 0.18778 | val_0_rmse: 0.43036 | val_1_rmse: 0.43685 |  0:03:39s
epoch 135| loss: 0.18419 | val_0_rmse: 0.4054  | val_1_rmse: 0.41202 |  0:03:41s

Early stopping occured at epoch 135 with best_epoch = 105 and best_val_1_rmse = 0.41087
Best weights from best epoch are automatically used!
ended training at: 13:48:49
Feature importance:
[('Area', 0.03359736986888469), ('Baths', 0.1392175422845615), ('Beds', 0.1562502447805359), ('Latitude', 0.08837545733821499), ('Longitude', 0.21718103225307855), ('Month', 0.028325791763555687), ('Year', 0.3370525617111687)]
Mean squared error is of 10522323259.95107
Mean absolute error:71608.50056359409
MAPE:0.31515603600363873
R2 score:0.8184879511659215
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:48:50
epoch 0  | loss: 0.51943 | val_0_rmse: 0.62522 | val_1_rmse: 0.6104  |  0:00:01s
epoch 1  | loss: 0.33051 | val_0_rmse: 0.55954 | val_1_rmse: 0.54486 |  0:00:03s
epoch 2  | loss: 0.2902  | val_0_rmse: 0.51788 | val_1_rmse: 0.50629 |  0:00:04s
epoch 3  | loss: 0.27603 | val_0_rmse: 0.50685 | val_1_rmse: 0.49409 |  0:00:06s
epoch 4  | loss: 0.26578 | val_0_rmse: 0.4956  | val_1_rmse: 0.48205 |  0:00:08s
epoch 5  | loss: 0.25419 | val_0_rmse: 0.49316 | val_1_rmse: 0.48519 |  0:00:09s
epoch 6  | loss: 0.2523  | val_0_rmse: 0.4817  | val_1_rmse: 0.4747  |  0:00:11s
epoch 7  | loss: 0.24026 | val_0_rmse: 0.46151 | val_1_rmse: 0.45223 |  0:00:13s
epoch 8  | loss: 0.23687 | val_0_rmse: 0.46718 | val_1_rmse: 0.46078 |  0:00:14s
epoch 9  | loss: 0.23715 | val_0_rmse: 0.46301 | val_1_rmse: 0.44861 |  0:00:16s
epoch 10 | loss: 0.22907 | val_0_rmse: 0.45601 | val_1_rmse: 0.44749 |  0:00:18s
epoch 11 | loss: 0.22884 | val_0_rmse: 0.48214 | val_1_rmse: 0.47072 |  0:00:19s
epoch 12 | loss: 0.23269 | val_0_rmse: 0.46656 | val_1_rmse: 0.46069 |  0:00:21s
epoch 13 | loss: 0.23622 | val_0_rmse: 0.45953 | val_1_rmse: 0.45154 |  0:00:22s
epoch 14 | loss: 0.21892 | val_0_rmse: 0.46424 | val_1_rmse: 0.45589 |  0:00:24s
epoch 15 | loss: 0.22133 | val_0_rmse: 0.44929 | val_1_rmse: 0.4389  |  0:00:26s
epoch 16 | loss: 0.21894 | val_0_rmse: 0.47645 | val_1_rmse: 0.46973 |  0:00:27s
epoch 17 | loss: 0.22548 | val_0_rmse: 0.47472 | val_1_rmse: 0.46846 |  0:00:29s
epoch 18 | loss: 0.21837 | val_0_rmse: 0.44893 | val_1_rmse: 0.44064 |  0:00:31s
epoch 19 | loss: 0.21462 | val_0_rmse: 0.44139 | val_1_rmse: 0.4327  |  0:00:32s
epoch 20 | loss: 0.21866 | val_0_rmse: 0.45208 | val_1_rmse: 0.44366 |  0:00:34s
epoch 21 | loss: 0.21993 | val_0_rmse: 0.44402 | val_1_rmse: 0.43732 |  0:00:35s
epoch 22 | loss: 0.21179 | val_0_rmse: 0.42912 | val_1_rmse: 0.42189 |  0:00:37s
epoch 23 | loss: 0.20731 | val_0_rmse: 0.46773 | val_1_rmse: 0.46277 |  0:00:39s
epoch 24 | loss: 0.21542 | val_0_rmse: 0.45036 | val_1_rmse: 0.44816 |  0:00:40s
epoch 25 | loss: 0.21403 | val_0_rmse: 0.43828 | val_1_rmse: 0.43602 |  0:00:42s
epoch 26 | loss: 0.20935 | val_0_rmse: 0.46297 | val_1_rmse: 0.46069 |  0:00:44s
epoch 27 | loss: 0.20954 | val_0_rmse: 0.43844 | val_1_rmse: 0.43448 |  0:00:45s
epoch 28 | loss: 0.20901 | val_0_rmse: 0.42939 | val_1_rmse: 0.42605 |  0:00:47s
epoch 29 | loss: 0.20485 | val_0_rmse: 0.44074 | val_1_rmse: 0.43453 |  0:00:49s
epoch 30 | loss: 0.21014 | val_0_rmse: 0.44153 | val_1_rmse: 0.43997 |  0:00:50s
epoch 31 | loss: 0.20866 | val_0_rmse: 0.43008 | val_1_rmse: 0.42387 |  0:00:52s
epoch 32 | loss: 0.21079 | val_0_rmse: 0.43194 | val_1_rmse: 0.42953 |  0:00:53s
epoch 33 | loss: 0.20514 | val_0_rmse: 0.43938 | val_1_rmse: 0.43857 |  0:00:55s
epoch 34 | loss: 0.20694 | val_0_rmse: 0.43683 | val_1_rmse: 0.43686 |  0:00:57s
epoch 35 | loss: 0.20567 | val_0_rmse: 0.48053 | val_1_rmse: 0.4772  |  0:00:58s
epoch 36 | loss: 0.22293 | val_0_rmse: 0.4425  | val_1_rmse: 0.44195 |  0:01:00s
epoch 37 | loss: 0.21688 | val_0_rmse: 0.42662 | val_1_rmse: 0.42536 |  0:01:02s
epoch 38 | loss: 0.21087 | val_0_rmse: 0.43342 | val_1_rmse: 0.434   |  0:01:03s
epoch 39 | loss: 0.20182 | val_0_rmse: 0.43417 | val_1_rmse: 0.43385 |  0:01:05s
epoch 40 | loss: 0.19998 | val_0_rmse: 0.41733 | val_1_rmse: 0.4172  |  0:01:06s
epoch 41 | loss: 0.19735 | val_0_rmse: 0.42507 | val_1_rmse: 0.4229  |  0:01:08s
epoch 42 | loss: 0.20388 | val_0_rmse: 0.44108 | val_1_rmse: 0.43942 |  0:01:10s
epoch 43 | loss: 0.20363 | val_0_rmse: 0.43242 | val_1_rmse: 0.43464 |  0:01:11s
epoch 44 | loss: 0.19816 | val_0_rmse: 0.46122 | val_1_rmse: 0.45921 |  0:01:13s
epoch 45 | loss: 0.20607 | val_0_rmse: 0.43804 | val_1_rmse: 0.43784 |  0:01:15s
epoch 46 | loss: 0.20516 | val_0_rmse: 0.44104 | val_1_rmse: 0.44198 |  0:01:16s
epoch 47 | loss: 0.20134 | val_0_rmse: 0.42311 | val_1_rmse: 0.42516 |  0:01:18s
epoch 48 | loss: 0.2052  | val_0_rmse: 0.43451 | val_1_rmse: 0.43469 |  0:01:20s
epoch 49 | loss: 0.20296 | val_0_rmse: 0.43786 | val_1_rmse: 0.43994 |  0:01:21s
epoch 50 | loss: 0.20283 | val_0_rmse: 0.44021 | val_1_rmse: 0.44137 |  0:01:23s
epoch 51 | loss: 0.19971 | val_0_rmse: 0.4285  | val_1_rmse: 0.43118 |  0:01:25s
epoch 52 | loss: 0.1983  | val_0_rmse: 0.42764 | val_1_rmse: 0.43396 |  0:01:26s
epoch 53 | loss: 0.19789 | val_0_rmse: 0.43918 | val_1_rmse: 0.43645 |  0:01:28s
epoch 54 | loss: 0.19977 | val_0_rmse: 0.42906 | val_1_rmse: 0.43234 |  0:01:29s
epoch 55 | loss: 0.20156 | val_0_rmse: 0.43322 | val_1_rmse: 0.43267 |  0:01:31s
epoch 56 | loss: 0.19861 | val_0_rmse: 0.4279  | val_1_rmse: 0.43125 |  0:01:33s
epoch 57 | loss: 0.19428 | val_0_rmse: 0.42481 | val_1_rmse: 0.4292  |  0:01:34s
epoch 58 | loss: 0.1961  | val_0_rmse: 0.42851 | val_1_rmse: 0.42758 |  0:01:36s
epoch 59 | loss: 0.19803 | val_0_rmse: 0.43404 | val_1_rmse: 0.43807 |  0:01:38s
epoch 60 | loss: 0.2023  | val_0_rmse: 0.4517  | val_1_rmse: 0.45651 |  0:01:39s
epoch 61 | loss: 0.19508 | val_0_rmse: 0.46699 | val_1_rmse: 0.46398 |  0:01:41s
epoch 62 | loss: 0.19807 | val_0_rmse: 0.41543 | val_1_rmse: 0.42171 |  0:01:42s
epoch 63 | loss: 0.19265 | val_0_rmse: 0.42904 | val_1_rmse: 0.43251 |  0:01:44s
epoch 64 | loss: 0.19789 | val_0_rmse: 0.41824 | val_1_rmse: 0.41929 |  0:01:46s
epoch 65 | loss: 0.192   | val_0_rmse: 0.4393  | val_1_rmse: 0.44479 |  0:01:47s
epoch 66 | loss: 0.19427 | val_0_rmse: 0.43804 | val_1_rmse: 0.44465 |  0:01:49s
epoch 67 | loss: 0.19471 | val_0_rmse: 0.42279 | val_1_rmse: 0.42569 |  0:01:51s
epoch 68 | loss: 0.19422 | val_0_rmse: 0.41798 | val_1_rmse: 0.42181 |  0:01:52s
epoch 69 | loss: 0.19504 | val_0_rmse: 0.42413 | val_1_rmse: 0.42825 |  0:01:54s
epoch 70 | loss: 0.19645 | val_0_rmse: 0.4214  | val_1_rmse: 0.42819 |  0:01:55s

Early stopping occured at epoch 70 with best_epoch = 40 and best_val_1_rmse = 0.4172
Best weights from best epoch are automatically used!
ended training at: 13:50:46
Feature importance:
[('Area', 0.04734396227887596), ('Baths', 0.21539688244373803), ('Beds', 0.09081894395414201), ('Latitude', 0.14133180203060003), ('Longitude', 0.15276499615810343), ('Month', 0.11337528306116085), ('Year', 0.2389681300733797)]
Mean squared error is of 10318188644.497509
Mean absolute error:72063.88875434146
MAPE:0.274276836885231
R2 score:0.8234806008976109
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:50:47
epoch 0  | loss: 0.56575 | val_0_rmse: 0.6849  | val_1_rmse: 0.69363 |  0:00:01s
epoch 1  | loss: 0.32201 | val_0_rmse: 0.54532 | val_1_rmse: 0.55318 |  0:00:03s
epoch 2  | loss: 0.27967 | val_0_rmse: 0.51179 | val_1_rmse: 0.52115 |  0:00:04s
epoch 3  | loss: 0.27065 | val_0_rmse: 0.49797 | val_1_rmse: 0.50609 |  0:00:06s
epoch 4  | loss: 0.25559 | val_0_rmse: 0.48393 | val_1_rmse: 0.48596 |  0:00:08s
epoch 5  | loss: 0.24939 | val_0_rmse: 0.52763 | val_1_rmse: 0.53191 |  0:00:09s
epoch 6  | loss: 0.24886 | val_0_rmse: 0.47113 | val_1_rmse: 0.47383 |  0:00:11s
epoch 7  | loss: 0.23745 | val_0_rmse: 0.46117 | val_1_rmse: 0.46489 |  0:00:13s
epoch 8  | loss: 0.23238 | val_0_rmse: 0.45587 | val_1_rmse: 0.4674  |  0:00:14s
epoch 9  | loss: 0.22445 | val_0_rmse: 0.45867 | val_1_rmse: 0.47247 |  0:00:16s
epoch 10 | loss: 0.21941 | val_0_rmse: 0.44429 | val_1_rmse: 0.45375 |  0:00:18s
epoch 11 | loss: 0.22214 | val_0_rmse: 0.45325 | val_1_rmse: 0.46578 |  0:00:19s
epoch 12 | loss: 0.21882 | val_0_rmse: 0.45635 | val_1_rmse: 0.45985 |  0:00:21s
epoch 13 | loss: 0.23174 | val_0_rmse: 0.47234 | val_1_rmse: 0.47476 |  0:00:23s
epoch 14 | loss: 0.22401 | val_0_rmse: 0.45876 | val_1_rmse: 0.46292 |  0:00:24s
epoch 15 | loss: 0.21751 | val_0_rmse: 0.44723 | val_1_rmse: 0.45855 |  0:00:26s
epoch 16 | loss: 0.21511 | val_0_rmse: 0.43307 | val_1_rmse: 0.44204 |  0:00:27s
epoch 17 | loss: 0.2158  | val_0_rmse: 0.46434 | val_1_rmse: 0.47553 |  0:00:29s
epoch 18 | loss: 0.22577 | val_0_rmse: 0.45559 | val_1_rmse: 0.46799 |  0:00:31s
epoch 19 | loss: 0.21277 | val_0_rmse: 0.45795 | val_1_rmse: 0.46036 |  0:00:32s
epoch 20 | loss: 0.21504 | val_0_rmse: 0.45109 | val_1_rmse: 0.45989 |  0:00:34s
epoch 21 | loss: 0.217   | val_0_rmse: 0.4412  | val_1_rmse: 0.45113 |  0:00:36s
epoch 22 | loss: 0.20624 | val_0_rmse: 0.44703 | val_1_rmse: 0.45787 |  0:00:37s
epoch 23 | loss: 0.21969 | val_0_rmse: 0.46061 | val_1_rmse: 0.47749 |  0:00:39s
epoch 24 | loss: 0.21061 | val_0_rmse: 0.44224 | val_1_rmse: 0.4564  |  0:00:41s
epoch 25 | loss: 0.20931 | val_0_rmse: 0.43297 | val_1_rmse: 0.44749 |  0:00:42s
epoch 26 | loss: 0.20866 | val_0_rmse: 0.43152 | val_1_rmse: 0.4456  |  0:00:44s
epoch 27 | loss: 0.20528 | val_0_rmse: 0.43484 | val_1_rmse: 0.44788 |  0:00:45s
epoch 28 | loss: 0.21198 | val_0_rmse: 0.45412 | val_1_rmse: 0.46458 |  0:00:47s
epoch 29 | loss: 0.20579 | val_0_rmse: 0.4377  | val_1_rmse: 0.45099 |  0:00:49s
epoch 30 | loss: 0.20837 | val_0_rmse: 0.43063 | val_1_rmse: 0.44256 |  0:00:50s
epoch 31 | loss: 0.20499 | val_0_rmse: 0.49529 | val_1_rmse: 0.50223 |  0:00:52s
epoch 32 | loss: 0.21117 | val_0_rmse: 0.43019 | val_1_rmse: 0.44111 |  0:00:54s
epoch 33 | loss: 0.20808 | val_0_rmse: 0.4464  | val_1_rmse: 0.45856 |  0:00:55s
epoch 34 | loss: 0.20366 | val_0_rmse: 0.43501 | val_1_rmse: 0.45028 |  0:00:57s
epoch 35 | loss: 0.20455 | val_0_rmse: 0.44133 | val_1_rmse: 0.44856 |  0:00:59s
epoch 36 | loss: 0.20034 | val_0_rmse: 0.42592 | val_1_rmse: 0.43976 |  0:01:00s
epoch 37 | loss: 0.19683 | val_0_rmse: 0.42415 | val_1_rmse: 0.43726 |  0:01:02s
epoch 38 | loss: 0.19889 | val_0_rmse: 0.443   | val_1_rmse: 0.45638 |  0:01:03s
epoch 39 | loss: 0.20271 | val_0_rmse: 0.41961 | val_1_rmse: 0.43302 |  0:01:05s
epoch 40 | loss: 0.2004  | val_0_rmse: 0.46033 | val_1_rmse: 0.47134 |  0:01:07s
epoch 41 | loss: 0.20152 | val_0_rmse: 0.43533 | val_1_rmse: 0.44982 |  0:01:08s
epoch 42 | loss: 0.19808 | val_0_rmse: 0.42718 | val_1_rmse: 0.44665 |  0:01:10s
epoch 43 | loss: 0.19706 | val_0_rmse: 0.41971 | val_1_rmse: 0.44137 |  0:01:12s
epoch 44 | loss: 0.19898 | val_0_rmse: 0.44661 | val_1_rmse: 0.47435 |  0:01:13s
epoch 45 | loss: 0.19647 | val_0_rmse: 0.44791 | val_1_rmse: 0.45533 |  0:01:15s
epoch 46 | loss: 0.1965  | val_0_rmse: 0.42588 | val_1_rmse: 0.44583 |  0:01:16s
epoch 47 | loss: 0.19658 | val_0_rmse: 0.42155 | val_1_rmse: 0.43631 |  0:01:18s
epoch 48 | loss: 0.19302 | val_0_rmse: 0.41815 | val_1_rmse: 0.43424 |  0:01:20s
epoch 49 | loss: 0.1926  | val_0_rmse: 0.42546 | val_1_rmse: 0.44229 |  0:01:21s
epoch 50 | loss: 0.20226 | val_0_rmse: 0.46356 | val_1_rmse: 0.46918 |  0:01:23s
epoch 51 | loss: 0.20295 | val_0_rmse: 0.42834 | val_1_rmse: 0.44729 |  0:01:25s
epoch 52 | loss: 0.19126 | val_0_rmse: 0.41656 | val_1_rmse: 0.43151 |  0:01:26s
epoch 53 | loss: 0.19244 | val_0_rmse: 0.4367  | val_1_rmse: 0.44948 |  0:01:28s
epoch 54 | loss: 0.20057 | val_0_rmse: 0.49942 | val_1_rmse: 0.5113  |  0:01:30s
epoch 55 | loss: 0.20064 | val_0_rmse: 0.43388 | val_1_rmse: 0.45374 |  0:01:31s
epoch 56 | loss: 0.1955  | val_0_rmse: 0.42521 | val_1_rmse: 0.43952 |  0:01:33s
epoch 57 | loss: 0.20005 | val_0_rmse: 0.42131 | val_1_rmse: 0.43963 |  0:01:34s
epoch 58 | loss: 0.19319 | val_0_rmse: 0.4218  | val_1_rmse: 0.44011 |  0:01:36s
epoch 59 | loss: 0.19996 | val_0_rmse: 0.45253 | val_1_rmse: 0.4746  |  0:01:38s
epoch 60 | loss: 0.19757 | val_0_rmse: 0.42355 | val_1_rmse: 0.44388 |  0:01:39s
epoch 61 | loss: 0.19224 | val_0_rmse: 0.43284 | val_1_rmse: 0.44826 |  0:01:41s
epoch 62 | loss: 0.1898  | val_0_rmse: 0.43983 | val_1_rmse: 0.45282 |  0:01:43s
epoch 63 | loss: 0.19427 | val_0_rmse: 0.43368 | val_1_rmse: 0.44927 |  0:01:44s
epoch 64 | loss: 0.18804 | val_0_rmse: 0.43563 | val_1_rmse: 0.449   |  0:01:46s
epoch 65 | loss: 0.1884  | val_0_rmse: 0.41653 | val_1_rmse: 0.42633 |  0:01:47s
epoch 66 | loss: 0.19266 | val_0_rmse: 0.41555 | val_1_rmse: 0.43351 |  0:01:49s
epoch 67 | loss: 0.19297 | val_0_rmse: 0.41467 | val_1_rmse: 0.4369  |  0:01:51s
epoch 68 | loss: 0.1933  | val_0_rmse: 0.45162 | val_1_rmse: 0.47141 |  0:01:52s
epoch 69 | loss: 0.18914 | val_0_rmse: 0.42487 | val_1_rmse: 0.4449  |  0:01:54s
epoch 70 | loss: 0.19672 | val_0_rmse: 0.41911 | val_1_rmse: 0.43712 |  0:01:56s
epoch 71 | loss: 0.19071 | val_0_rmse: 0.43344 | val_1_rmse: 0.44072 |  0:01:57s
epoch 72 | loss: 0.18883 | val_0_rmse: 0.43152 | val_1_rmse: 0.45148 |  0:01:59s
epoch 73 | loss: 0.19023 | val_0_rmse: 0.41534 | val_1_rmse: 0.43643 |  0:02:01s
epoch 74 | loss: 0.19477 | val_0_rmse: 0.41324 | val_1_rmse: 0.42971 |  0:02:02s
epoch 75 | loss: 0.19018 | val_0_rmse: 0.41599 | val_1_rmse: 0.43451 |  0:02:04s
epoch 76 | loss: 0.1875  | val_0_rmse: 0.42381 | val_1_rmse: 0.43666 |  0:02:06s
epoch 77 | loss: 0.195   | val_0_rmse: 0.41459 | val_1_rmse: 0.43702 |  0:02:07s
epoch 78 | loss: 0.19927 | val_0_rmse: 0.46138 | val_1_rmse: 0.48017 |  0:02:09s
epoch 79 | loss: 0.19797 | val_0_rmse: 0.43021 | val_1_rmse: 0.43905 |  0:02:10s
epoch 80 | loss: 0.18771 | val_0_rmse: 0.41848 | val_1_rmse: 0.44197 |  0:02:12s
epoch 81 | loss: 0.18872 | val_0_rmse: 0.41156 | val_1_rmse: 0.43032 |  0:02:14s
epoch 82 | loss: 0.19435 | val_0_rmse: 0.42318 | val_1_rmse: 0.44045 |  0:02:15s
epoch 83 | loss: 0.18855 | val_0_rmse: 0.41211 | val_1_rmse: 0.42979 |  0:02:17s
epoch 84 | loss: 0.18706 | val_0_rmse: 0.42911 | val_1_rmse: 0.44422 |  0:02:19s
epoch 85 | loss: 0.18712 | val_0_rmse: 0.41952 | val_1_rmse: 0.43836 |  0:02:20s
epoch 86 | loss: 0.18323 | val_0_rmse: 0.41693 | val_1_rmse: 0.43123 |  0:02:22s
epoch 87 | loss: 0.18381 | val_0_rmse: 0.42529 | val_1_rmse: 0.45377 |  0:02:23s
epoch 88 | loss: 0.1933  | val_0_rmse: 0.44264 | val_1_rmse: 0.45853 |  0:02:25s
epoch 89 | loss: 0.18822 | val_0_rmse: 0.44997 | val_1_rmse: 0.47377 |  0:02:27s
epoch 90 | loss: 0.18494 | val_0_rmse: 0.41814 | val_1_rmse: 0.44163 |  0:02:28s
epoch 91 | loss: 0.18941 | val_0_rmse: 0.42338 | val_1_rmse: 0.44245 |  0:02:30s
epoch 92 | loss: 0.18956 | val_0_rmse: 0.45452 | val_1_rmse: 0.46939 |  0:02:32s
epoch 93 | loss: 0.19241 | val_0_rmse: 0.42476 | val_1_rmse: 0.44207 |  0:02:33s
epoch 94 | loss: 0.18738 | val_0_rmse: 0.4056  | val_1_rmse: 0.42584 |  0:02:35s
epoch 95 | loss: 0.18337 | val_0_rmse: 0.42291 | val_1_rmse: 0.44185 |  0:02:37s
epoch 96 | loss: 0.18495 | val_0_rmse: 0.41756 | val_1_rmse: 0.44475 |  0:02:38s
epoch 97 | loss: 0.18851 | val_0_rmse: 0.41365 | val_1_rmse: 0.43556 |  0:02:40s
epoch 98 | loss: 0.1858  | val_0_rmse: 0.41196 | val_1_rmse: 0.43479 |  0:02:41s
epoch 99 | loss: 0.18342 | val_0_rmse: 0.41587 | val_1_rmse: 0.43437 |  0:02:43s
epoch 100| loss: 0.18182 | val_0_rmse: 0.42898 | val_1_rmse: 0.44531 |  0:02:45s
epoch 101| loss: 0.18327 | val_0_rmse: 0.42812 | val_1_rmse: 0.4501  |  0:02:46s
epoch 102| loss: 0.18403 | val_0_rmse: 0.42018 | val_1_rmse: 0.44291 |  0:02:48s
epoch 103| loss: 0.18583 | val_0_rmse: 0.41022 | val_1_rmse: 0.43787 |  0:02:50s
epoch 104| loss: 0.17873 | val_0_rmse: 0.40427 | val_1_rmse: 0.42663 |  0:02:51s
epoch 105| loss: 0.1788  | val_0_rmse: 0.4153  | val_1_rmse: 0.44386 |  0:02:53s
epoch 106| loss: 0.18445 | val_0_rmse: 0.42509 | val_1_rmse: 0.44604 |  0:02:55s
epoch 107| loss: 0.19172 | val_0_rmse: 0.41594 | val_1_rmse: 0.43375 |  0:02:56s
epoch 108| loss: 0.18639 | val_0_rmse: 0.42255 | val_1_rmse: 0.43851 |  0:02:58s
epoch 109| loss: 0.18346 | val_0_rmse: 0.41544 | val_1_rmse: 0.434   |  0:02:59s
epoch 110| loss: 0.18912 | val_0_rmse: 0.43234 | val_1_rmse: 0.44879 |  0:03:01s
epoch 111| loss: 0.18355 | val_0_rmse: 0.42361 | val_1_rmse: 0.44276 |  0:03:03s
epoch 112| loss: 0.18606 | val_0_rmse: 0.42075 | val_1_rmse: 0.43691 |  0:03:04s
epoch 113| loss: 0.18192 | val_0_rmse: 0.40358 | val_1_rmse: 0.42568 |  0:03:06s
epoch 114| loss: 0.18198 | val_0_rmse: 0.42836 | val_1_rmse: 0.4497  |  0:03:08s
epoch 115| loss: 0.18527 | val_0_rmse: 0.41965 | val_1_rmse: 0.43403 |  0:03:09s
epoch 116| loss: 0.18853 | val_0_rmse: 0.4249  | val_1_rmse: 0.44905 |  0:03:11s
epoch 117| loss: 0.19043 | val_0_rmse: 0.43061 | val_1_rmse: 0.45518 |  0:03:12s
epoch 118| loss: 0.18249 | val_0_rmse: 0.40575 | val_1_rmse: 0.42729 |  0:03:14s
epoch 119| loss: 0.18418 | val_0_rmse: 0.42707 | val_1_rmse: 0.45348 |  0:03:16s
epoch 120| loss: 0.18749 | val_0_rmse: 0.43694 | val_1_rmse: 0.46216 |  0:03:17s
epoch 121| loss: 0.17918 | val_0_rmse: 0.42    | val_1_rmse: 0.44774 |  0:03:19s
epoch 122| loss: 0.18038 | val_0_rmse: 0.41443 | val_1_rmse: 0.43867 |  0:03:21s
epoch 123| loss: 0.18654 | val_0_rmse: 0.43318 | val_1_rmse: 0.45137 |  0:03:22s
epoch 124| loss: 0.18936 | val_0_rmse: 0.41205 | val_1_rmse: 0.43146 |  0:03:24s
epoch 125| loss: 0.19547 | val_0_rmse: 0.41932 | val_1_rmse: 0.44065 |  0:03:25s
epoch 126| loss: 0.18913 | val_0_rmse: 0.41282 | val_1_rmse: 0.43482 |  0:03:27s
epoch 127| loss: 0.18964 | val_0_rmse: 0.41476 | val_1_rmse: 0.43463 |  0:03:29s
epoch 128| loss: 0.18404 | val_0_rmse: 0.4237  | val_1_rmse: 0.44454 |  0:03:30s
epoch 129| loss: 0.18314 | val_0_rmse: 0.41687 | val_1_rmse: 0.43194 |  0:03:32s
epoch 130| loss: 0.18321 | val_0_rmse: 0.41431 | val_1_rmse: 0.44065 |  0:03:34s
epoch 131| loss: 0.19001 | val_0_rmse: 0.44253 | val_1_rmse: 0.45816 |  0:03:35s
epoch 132| loss: 0.18811 | val_0_rmse: 0.42047 | val_1_rmse: 0.44255 |  0:03:37s
epoch 133| loss: 0.18227 | val_0_rmse: 0.40501 | val_1_rmse: 0.42929 |  0:03:38s
epoch 134| loss: 0.19944 | val_0_rmse: 0.43268 | val_1_rmse: 0.44505 |  0:03:40s
epoch 135| loss: 0.19144 | val_0_rmse: 0.43216 | val_1_rmse: 0.4509  |  0:03:42s
epoch 136| loss: 0.18871 | val_0_rmse: 0.40973 | val_1_rmse: 0.43067 |  0:03:43s
epoch 137| loss: 0.1823  | val_0_rmse: 0.41116 | val_1_rmse: 0.43211 |  0:03:45s
epoch 138| loss: 0.1814  | val_0_rmse: 0.42605 | val_1_rmse: 0.44394 |  0:03:47s
epoch 139| loss: 0.18457 | val_0_rmse: 0.41812 | val_1_rmse: 0.43709 |  0:03:48s
epoch 140| loss: 0.18453 | val_0_rmse: 0.4226  | val_1_rmse: 0.4393  |  0:03:50s
epoch 141| loss: 0.18105 | val_0_rmse: 0.4135  | val_1_rmse: 0.43403 |  0:03:51s
epoch 142| loss: 0.18379 | val_0_rmse: 0.4145  | val_1_rmse: 0.44015 |  0:03:53s
epoch 143| loss: 0.18096 | val_0_rmse: 0.40405 | val_1_rmse: 0.428   |  0:03:55s

Early stopping occured at epoch 143 with best_epoch = 113 and best_val_1_rmse = 0.42568
Best weights from best epoch are automatically used!
ended training at: 13:54:42
Feature importance:
[('Area', 0.0), ('Baths', 0.13960035019882752), ('Beds', 0.10929650262918855), ('Latitude', 0.2558609781967206), ('Longitude', 0.304844321317877), ('Month', 0.0), ('Year', 0.19039784765738638)]
Mean squared error is of 10554531305.634796
Mean absolute error:71381.50921108309
MAPE:0.3012250098827794
R2 score:0.8157589099271034
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:54:42
epoch 0  | loss: 0.54329 | val_0_rmse: 0.63845 | val_1_rmse: 0.65315 |  0:00:01s
epoch 1  | loss: 0.34075 | val_0_rmse: 0.55568 | val_1_rmse: 0.56874 |  0:00:03s
epoch 2  | loss: 0.3039  | val_0_rmse: 0.53038 | val_1_rmse: 0.5424  |  0:00:04s
epoch 3  | loss: 0.2869  | val_0_rmse: 0.52601 | val_1_rmse: 0.53525 |  0:00:06s
epoch 4  | loss: 0.27525 | val_0_rmse: 0.52505 | val_1_rmse: 0.52665 |  0:00:08s
epoch 5  | loss: 0.27088 | val_0_rmse: 0.50774 | val_1_rmse: 0.51408 |  0:00:09s
epoch 6  | loss: 0.27923 | val_0_rmse: 0.50903 | val_1_rmse: 0.51651 |  0:00:11s
epoch 7  | loss: 0.26177 | val_0_rmse: 0.49871 | val_1_rmse: 0.50533 |  0:00:13s
epoch 8  | loss: 0.25472 | val_0_rmse: 0.47318 | val_1_rmse: 0.47747 |  0:00:14s
epoch 9  | loss: 0.24786 | val_0_rmse: 0.56257 | val_1_rmse: 0.56524 |  0:00:16s
epoch 10 | loss: 0.24758 | val_0_rmse: 0.50142 | val_1_rmse: 0.50593 |  0:00:17s
epoch 11 | loss: 0.24124 | val_0_rmse: 0.46318 | val_1_rmse: 0.47095 |  0:00:19s
epoch 12 | loss: 0.23358 | val_0_rmse: 0.46066 | val_1_rmse: 0.46987 |  0:00:21s
epoch 13 | loss: 0.23131 | val_0_rmse: 0.49397 | val_1_rmse: 0.50187 |  0:00:22s
epoch 14 | loss: 0.23749 | val_0_rmse: 0.45858 | val_1_rmse: 0.46536 |  0:00:24s
epoch 15 | loss: 0.22623 | val_0_rmse: 0.45647 | val_1_rmse: 0.46354 |  0:00:26s
epoch 16 | loss: 0.22669 | val_0_rmse: 0.4569  | val_1_rmse: 0.4667  |  0:00:27s
epoch 17 | loss: 0.22755 | val_0_rmse: 0.45922 | val_1_rmse: 0.46736 |  0:00:29s
epoch 18 | loss: 0.22757 | val_0_rmse: 0.45514 | val_1_rmse: 0.46219 |  0:00:31s
epoch 19 | loss: 0.22063 | val_0_rmse: 0.44832 | val_1_rmse: 0.45564 |  0:00:32s
epoch 20 | loss: 0.2165  | val_0_rmse: 0.43718 | val_1_rmse: 0.44456 |  0:00:34s
epoch 21 | loss: 0.21577 | val_0_rmse: 0.46175 | val_1_rmse: 0.46941 |  0:00:35s
epoch 22 | loss: 0.21821 | val_0_rmse: 0.439   | val_1_rmse: 0.44519 |  0:00:37s
epoch 23 | loss: 0.21514 | val_0_rmse: 0.45267 | val_1_rmse: 0.46069 |  0:00:39s
epoch 24 | loss: 0.20993 | val_0_rmse: 0.45299 | val_1_rmse: 0.45947 |  0:00:40s
epoch 25 | loss: 0.21623 | val_0_rmse: 0.44851 | val_1_rmse: 0.45376 |  0:00:42s
epoch 26 | loss: 0.20838 | val_0_rmse: 0.448   | val_1_rmse: 0.45929 |  0:00:44s
epoch 27 | loss: 0.2052  | val_0_rmse: 0.44037 | val_1_rmse: 0.44765 |  0:00:45s
epoch 28 | loss: 0.20833 | val_0_rmse: 0.43552 | val_1_rmse: 0.44247 |  0:00:47s
epoch 29 | loss: 0.20454 | val_0_rmse: 0.43004 | val_1_rmse: 0.43584 |  0:00:49s
epoch 30 | loss: 0.20612 | val_0_rmse: 0.43911 | val_1_rmse: 0.44707 |  0:00:50s
epoch 31 | loss: 0.21118 | val_0_rmse: 0.44819 | val_1_rmse: 0.45708 |  0:00:52s
epoch 32 | loss: 0.20834 | val_0_rmse: 0.45315 | val_1_rmse: 0.46144 |  0:00:54s
epoch 33 | loss: 0.20603 | val_0_rmse: 0.44627 | val_1_rmse: 0.45565 |  0:00:55s
epoch 34 | loss: 0.20374 | val_0_rmse: 0.43194 | val_1_rmse: 0.43872 |  0:00:57s
epoch 35 | loss: 0.20022 | val_0_rmse: 0.43885 | val_1_rmse: 0.44833 |  0:00:58s
epoch 36 | loss: 0.20638 | val_0_rmse: 0.43834 | val_1_rmse: 0.44813 |  0:01:00s
epoch 37 | loss: 0.2026  | val_0_rmse: 0.43303 | val_1_rmse: 0.44177 |  0:01:02s
epoch 38 | loss: 0.19865 | val_0_rmse: 0.43967 | val_1_rmse: 0.44716 |  0:01:03s
epoch 39 | loss: 0.20595 | val_0_rmse: 0.44077 | val_1_rmse: 0.4489  |  0:01:05s
epoch 40 | loss: 0.20691 | val_0_rmse: 0.43281 | val_1_rmse: 0.44101 |  0:01:07s
epoch 41 | loss: 0.19973 | val_0_rmse: 0.43703 | val_1_rmse: 0.44761 |  0:01:08s
epoch 42 | loss: 0.20044 | val_0_rmse: 0.43295 | val_1_rmse: 0.44473 |  0:01:10s
epoch 43 | loss: 0.19824 | val_0_rmse: 0.42348 | val_1_rmse: 0.43595 |  0:01:12s
epoch 44 | loss: 0.19989 | val_0_rmse: 0.4506  | val_1_rmse: 0.45804 |  0:01:13s
epoch 45 | loss: 0.20252 | val_0_rmse: 0.43416 | val_1_rmse: 0.44409 |  0:01:15s
epoch 46 | loss: 0.1979  | val_0_rmse: 0.42113 | val_1_rmse: 0.43058 |  0:01:16s
epoch 47 | loss: 0.19551 | val_0_rmse: 0.42443 | val_1_rmse: 0.43404 |  0:01:18s
epoch 48 | loss: 0.19518 | val_0_rmse: 0.43389 | val_1_rmse: 0.44341 |  0:01:20s
epoch 49 | loss: 0.19692 | val_0_rmse: 0.43088 | val_1_rmse: 0.4409  |  0:01:21s
epoch 50 | loss: 0.19417 | val_0_rmse: 0.42777 | val_1_rmse: 0.43612 |  0:01:23s
epoch 51 | loss: 0.19289 | val_0_rmse: 0.41983 | val_1_rmse: 0.42814 |  0:01:25s
epoch 52 | loss: 0.21037 | val_0_rmse: 0.43353 | val_1_rmse: 0.44361 |  0:01:26s
epoch 53 | loss: 0.20502 | val_0_rmse: 0.42835 | val_1_rmse: 0.43672 |  0:01:28s
epoch 54 | loss: 0.19984 | val_0_rmse: 0.43085 | val_1_rmse: 0.43919 |  0:01:29s
epoch 55 | loss: 0.19776 | val_0_rmse: 0.42826 | val_1_rmse: 0.43725 |  0:01:31s
epoch 56 | loss: 0.19603 | val_0_rmse: 0.42134 | val_1_rmse: 0.43135 |  0:01:33s
epoch 57 | loss: 0.19257 | val_0_rmse: 0.41693 | val_1_rmse: 0.42412 |  0:01:34s
epoch 58 | loss: 0.19276 | val_0_rmse: 0.42744 | val_1_rmse: 0.43639 |  0:01:36s
epoch 59 | loss: 0.19529 | val_0_rmse: 0.41498 | val_1_rmse: 0.42484 |  0:01:38s
epoch 60 | loss: 0.18801 | val_0_rmse: 0.42479 | val_1_rmse: 0.43297 |  0:01:39s
epoch 61 | loss: 0.19132 | val_0_rmse: 0.44552 | val_1_rmse: 0.45348 |  0:01:41s
epoch 62 | loss: 0.19167 | val_0_rmse: 0.42742 | val_1_rmse: 0.43977 |  0:01:43s
epoch 63 | loss: 0.19256 | val_0_rmse: 0.42236 | val_1_rmse: 0.43275 |  0:01:44s
epoch 64 | loss: 0.19769 | val_0_rmse: 0.42869 | val_1_rmse: 0.43736 |  0:01:46s
epoch 65 | loss: 0.19259 | val_0_rmse: 0.42427 | val_1_rmse: 0.43365 |  0:01:47s
epoch 66 | loss: 0.19417 | val_0_rmse: 0.43005 | val_1_rmse: 0.44289 |  0:01:49s
epoch 67 | loss: 0.19759 | val_0_rmse: 0.4449  | val_1_rmse: 0.4573  |  0:01:51s
epoch 68 | loss: 0.1992  | val_0_rmse: 0.44604 | val_1_rmse: 0.45558 |  0:01:52s
epoch 69 | loss: 0.19215 | val_0_rmse: 0.42809 | val_1_rmse: 0.43705 |  0:01:54s
epoch 70 | loss: 0.198   | val_0_rmse: 0.42153 | val_1_rmse: 0.43356 |  0:01:56s
epoch 71 | loss: 0.19337 | val_0_rmse: 0.43928 | val_1_rmse: 0.45097 |  0:01:57s
epoch 72 | loss: 0.19402 | val_0_rmse: 0.4327  | val_1_rmse: 0.44055 |  0:01:59s
epoch 73 | loss: 0.19061 | val_0_rmse: 0.44423 | val_1_rmse: 0.45331 |  0:02:01s
epoch 74 | loss: 0.19384 | val_0_rmse: 0.42344 | val_1_rmse: 0.43331 |  0:02:02s
epoch 75 | loss: 0.18841 | val_0_rmse: 0.40977 | val_1_rmse: 0.42248 |  0:02:04s
epoch 76 | loss: 0.1899  | val_0_rmse: 0.41412 | val_1_rmse: 0.42467 |  0:02:05s
epoch 77 | loss: 0.19074 | val_0_rmse: 0.41905 | val_1_rmse: 0.43195 |  0:02:07s
epoch 78 | loss: 0.18866 | val_0_rmse: 0.41013 | val_1_rmse: 0.42331 |  0:02:09s
epoch 79 | loss: 0.18861 | val_0_rmse: 0.42389 | val_1_rmse: 0.43661 |  0:02:10s
epoch 80 | loss: 0.18731 | val_0_rmse: 0.41263 | val_1_rmse: 0.42618 |  0:02:12s
epoch 81 | loss: 0.18912 | val_0_rmse: 0.43577 | val_1_rmse: 0.45041 |  0:02:14s
epoch 82 | loss: 0.19081 | val_0_rmse: 0.41867 | val_1_rmse: 0.43067 |  0:02:15s
epoch 83 | loss: 0.18708 | val_0_rmse: 0.4208  | val_1_rmse: 0.43368 |  0:02:17s
epoch 84 | loss: 0.19075 | val_0_rmse: 0.41695 | val_1_rmse: 0.434   |  0:02:18s
epoch 85 | loss: 0.18685 | val_0_rmse: 0.41084 | val_1_rmse: 0.42457 |  0:02:20s
epoch 86 | loss: 0.18587 | val_0_rmse: 0.41905 | val_1_rmse: 0.43159 |  0:02:22s
epoch 87 | loss: 0.18221 | val_0_rmse: 0.41212 | val_1_rmse: 0.4256  |  0:02:23s
epoch 88 | loss: 0.18503 | val_0_rmse: 0.41779 | val_1_rmse: 0.43161 |  0:02:25s
epoch 89 | loss: 0.18935 | val_0_rmse: 0.43587 | val_1_rmse: 0.44508 |  0:02:27s
epoch 90 | loss: 0.18541 | val_0_rmse: 0.41725 | val_1_rmse: 0.42991 |  0:02:28s
epoch 91 | loss: 0.18409 | val_0_rmse: 0.42391 | val_1_rmse: 0.43884 |  0:02:30s
epoch 92 | loss: 0.1871  | val_0_rmse: 0.41596 | val_1_rmse: 0.42657 |  0:02:32s
epoch 93 | loss: 0.1853  | val_0_rmse: 0.42998 | val_1_rmse: 0.44644 |  0:02:33s
epoch 94 | loss: 0.17957 | val_0_rmse: 0.40473 | val_1_rmse: 0.41953 |  0:02:35s
epoch 95 | loss: 0.18462 | val_0_rmse: 0.41808 | val_1_rmse: 0.43476 |  0:02:36s
epoch 96 | loss: 0.19024 | val_0_rmse: 0.41674 | val_1_rmse: 0.42954 |  0:02:38s
epoch 97 | loss: 0.18163 | val_0_rmse: 0.43157 | val_1_rmse: 0.44845 |  0:02:40s
epoch 98 | loss: 0.19079 | val_0_rmse: 0.42199 | val_1_rmse: 0.43422 |  0:02:41s
epoch 99 | loss: 0.18631 | val_0_rmse: 0.40997 | val_1_rmse: 0.42326 |  0:02:43s
epoch 100| loss: 0.1835  | val_0_rmse: 0.4156  | val_1_rmse: 0.43214 |  0:02:45s
epoch 101| loss: 0.1822  | val_0_rmse: 0.40147 | val_1_rmse: 0.41924 |  0:02:46s
epoch 102| loss: 0.17957 | val_0_rmse: 0.41899 | val_1_rmse: 0.43624 |  0:02:48s
epoch 103| loss: 0.1842  | val_0_rmse: 0.47948 | val_1_rmse: 0.4902  |  0:02:49s
epoch 104| loss: 0.18694 | val_0_rmse: 0.40624 | val_1_rmse: 0.42181 |  0:02:51s
epoch 105| loss: 0.18394 | val_0_rmse: 0.4177  | val_1_rmse: 0.43555 |  0:02:53s
epoch 106| loss: 0.18069 | val_0_rmse: 0.42712 | val_1_rmse: 0.4393  |  0:02:54s
epoch 107| loss: 0.19238 | val_0_rmse: 0.42447 | val_1_rmse: 0.4378  |  0:02:56s
epoch 108| loss: 0.18314 | val_0_rmse: 0.40747 | val_1_rmse: 0.42025 |  0:02:58s
epoch 109| loss: 0.18487 | val_0_rmse: 0.42176 | val_1_rmse: 0.43227 |  0:02:59s
epoch 110| loss: 0.18461 | val_0_rmse: 0.41402 | val_1_rmse: 0.43122 |  0:03:01s
epoch 111| loss: 0.18236 | val_0_rmse: 0.42814 | val_1_rmse: 0.44638 |  0:03:03s
epoch 112| loss: 0.18537 | val_0_rmse: 0.42208 | val_1_rmse: 0.43735 |  0:03:04s
epoch 113| loss: 0.18317 | val_0_rmse: 0.42249 | val_1_rmse: 0.43669 |  0:03:06s
epoch 114| loss: 0.1811  | val_0_rmse: 0.41468 | val_1_rmse: 0.43323 |  0:03:07s
epoch 115| loss: 0.18363 | val_0_rmse: 0.40848 | val_1_rmse: 0.42429 |  0:03:09s
epoch 116| loss: 0.18113 | val_0_rmse: 0.42278 | val_1_rmse: 0.43797 |  0:03:11s
epoch 117| loss: 0.17815 | val_0_rmse: 0.42055 | val_1_rmse: 0.44046 |  0:03:12s
epoch 118| loss: 0.18411 | val_0_rmse: 0.40784 | val_1_rmse: 0.42399 |  0:03:14s
epoch 119| loss: 0.18218 | val_0_rmse: 0.4381  | val_1_rmse: 0.45677 |  0:03:16s
epoch 120| loss: 0.18177 | val_0_rmse: 0.40714 | val_1_rmse: 0.42702 |  0:03:17s
epoch 121| loss: 0.18304 | val_0_rmse: 0.40095 | val_1_rmse: 0.42057 |  0:03:19s
epoch 122| loss: 0.17746 | val_0_rmse: 0.41837 | val_1_rmse: 0.43494 |  0:03:21s
epoch 123| loss: 0.18075 | val_0_rmse: 0.4039  | val_1_rmse: 0.4205  |  0:03:22s
epoch 124| loss: 0.18009 | val_0_rmse: 0.40615 | val_1_rmse: 0.42446 |  0:03:24s
epoch 125| loss: 0.18353 | val_0_rmse: 0.41596 | val_1_rmse: 0.43754 |  0:03:25s
epoch 126| loss: 0.18403 | val_0_rmse: 0.40897 | val_1_rmse: 0.42625 |  0:03:27s
epoch 127| loss: 0.17974 | val_0_rmse: 0.4224  | val_1_rmse: 0.4389  |  0:03:29s
epoch 128| loss: 0.17889 | val_0_rmse: 0.42853 | val_1_rmse: 0.44914 |  0:03:30s
epoch 129| loss: 0.18197 | val_0_rmse: 0.40033 | val_1_rmse: 0.41764 |  0:03:32s
epoch 130| loss: 0.17347 | val_0_rmse: 0.39712 | val_1_rmse: 0.41336 |  0:03:34s
epoch 131| loss: 0.17549 | val_0_rmse: 0.41364 | val_1_rmse: 0.4306  |  0:03:35s
epoch 132| loss: 0.18298 | val_0_rmse: 0.44228 | val_1_rmse: 0.45627 |  0:03:37s
epoch 133| loss: 0.18534 | val_0_rmse: 0.42645 | val_1_rmse: 0.44389 |  0:03:39s
epoch 134| loss: 0.18402 | val_0_rmse: 0.4077  | val_1_rmse: 0.42579 |  0:03:40s
epoch 135| loss: 0.17787 | val_0_rmse: 0.40534 | val_1_rmse: 0.4231  |  0:03:42s
epoch 136| loss: 0.17567 | val_0_rmse: 0.40778 | val_1_rmse: 0.4291  |  0:03:43s
epoch 137| loss: 0.18149 | val_0_rmse: 0.42275 | val_1_rmse: 0.44076 |  0:03:45s
epoch 138| loss: 0.17766 | val_0_rmse: 0.42026 | val_1_rmse: 0.43875 |  0:03:47s
epoch 139| loss: 0.18329 | val_0_rmse: 0.4153  | val_1_rmse: 0.43422 |  0:03:48s
epoch 140| loss: 0.18057 | val_0_rmse: 0.41094 | val_1_rmse: 0.42898 |  0:03:50s
epoch 141| loss: 0.17584 | val_0_rmse: 0.41085 | val_1_rmse: 0.42835 |  0:03:52s
epoch 142| loss: 0.17621 | val_0_rmse: 0.40985 | val_1_rmse: 0.43061 |  0:03:53s
epoch 143| loss: 0.17837 | val_0_rmse: 0.42767 | val_1_rmse: 0.44387 |  0:03:55s
epoch 144| loss: 0.17313 | val_0_rmse: 0.39838 | val_1_rmse: 0.4219  |  0:03:56s
epoch 145| loss: 0.17612 | val_0_rmse: 0.42087 | val_1_rmse: 0.4348  |  0:03:58s
epoch 146| loss: 0.17253 | val_0_rmse: 0.40437 | val_1_rmse: 0.42634 |  0:04:00s
epoch 147| loss: 0.17588 | val_0_rmse: 0.39784 | val_1_rmse: 0.4202  |  0:04:01s
epoch 148| loss: 0.17493 | val_0_rmse: 0.40494 | val_1_rmse: 0.42479 |  0:04:03s
epoch 149| loss: 0.17808 | val_0_rmse: 0.40598 | val_1_rmse: 0.43208 |  0:04:05s
Stop training because you reached max_epochs = 150 with best_epoch = 130 and best_val_1_rmse = 0.41336
Best weights from best epoch are automatically used!
ended training at: 13:58:48
Feature importance:
[('Area', 0.02810599479720856), ('Baths', 0.09715831716182374), ('Beds', 0.1561093855025974), ('Latitude', 0.23640469821099863), ('Longitude', 0.2354352695597655), ('Month', 0.026545504129478086), ('Year', 0.22024083063812808)]
Mean squared error is of 10248691517.169527
Mean absolute error:69674.4784349152
MAPE:0.3007825723322671
R2 score:0.8183244786584212
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:58:48
epoch 0  | loss: 0.54894 | val_0_rmse: 0.70329 | val_1_rmse: 0.70127 |  0:00:01s
epoch 1  | loss: 0.35643 | val_0_rmse: 0.58115 | val_1_rmse: 0.57643 |  0:00:03s
epoch 2  | loss: 0.31393 | val_0_rmse: 0.53836 | val_1_rmse: 0.53219 |  0:00:04s
epoch 3  | loss: 0.29608 | val_0_rmse: 0.52402 | val_1_rmse: 0.52009 |  0:00:06s
epoch 4  | loss: 0.28926 | val_0_rmse: 0.50823 | val_1_rmse: 0.50173 |  0:00:08s
epoch 5  | loss: 0.26818 | val_0_rmse: 0.52186 | val_1_rmse: 0.51306 |  0:00:09s
epoch 6  | loss: 0.26073 | val_0_rmse: 0.50819 | val_1_rmse: 0.5003  |  0:00:11s
epoch 7  | loss: 0.24808 | val_0_rmse: 0.47589 | val_1_rmse: 0.46863 |  0:00:13s
epoch 8  | loss: 0.24365 | val_0_rmse: 0.48244 | val_1_rmse: 0.47402 |  0:00:14s
epoch 9  | loss: 0.23886 | val_0_rmse: 0.47788 | val_1_rmse: 0.46348 |  0:00:16s
epoch 10 | loss: 0.24095 | val_0_rmse: 0.47674 | val_1_rmse: 0.47284 |  0:00:18s
epoch 11 | loss: 0.23718 | val_0_rmse: 0.45885 | val_1_rmse: 0.45306 |  0:00:19s
epoch 12 | loss: 0.23146 | val_0_rmse: 0.5027  | val_1_rmse: 0.49044 |  0:00:21s
epoch 13 | loss: 0.22931 | val_0_rmse: 0.46067 | val_1_rmse: 0.45256 |  0:00:22s
epoch 14 | loss: 0.22889 | val_0_rmse: 0.46006 | val_1_rmse: 0.45507 |  0:00:24s
epoch 15 | loss: 0.22352 | val_0_rmse: 0.46046 | val_1_rmse: 0.45192 |  0:00:26s
epoch 16 | loss: 0.2198  | val_0_rmse: 0.43908 | val_1_rmse: 0.43113 |  0:00:27s
epoch 17 | loss: 0.21619 | val_0_rmse: 0.43712 | val_1_rmse: 0.4307  |  0:00:29s
epoch 18 | loss: 0.21316 | val_0_rmse: 0.47763 | val_1_rmse: 0.4621  |  0:00:31s
epoch 19 | loss: 0.21515 | val_0_rmse: 0.44806 | val_1_rmse: 0.43993 |  0:00:32s
epoch 20 | loss: 0.21536 | val_0_rmse: 0.44893 | val_1_rmse: 0.44192 |  0:00:34s
epoch 21 | loss: 0.21669 | val_0_rmse: 0.43207 | val_1_rmse: 0.42315 |  0:00:36s
epoch 22 | loss: 0.2143  | val_0_rmse: 0.46229 | val_1_rmse: 0.46217 |  0:00:37s
epoch 23 | loss: 0.20841 | val_0_rmse: 0.43692 | val_1_rmse: 0.43124 |  0:00:39s
epoch 24 | loss: 0.20378 | val_0_rmse: 0.44565 | val_1_rmse: 0.44077 |  0:00:40s
epoch 25 | loss: 0.2049  | val_0_rmse: 0.44069 | val_1_rmse: 0.43151 |  0:00:42s
epoch 26 | loss: 0.20921 | val_0_rmse: 0.43079 | val_1_rmse: 0.42744 |  0:00:44s
epoch 27 | loss: 0.20187 | val_0_rmse: 0.43012 | val_1_rmse: 0.42338 |  0:00:45s
epoch 28 | loss: 0.20311 | val_0_rmse: 0.42957 | val_1_rmse: 0.42879 |  0:00:47s
epoch 29 | loss: 0.20569 | val_0_rmse: 0.45253 | val_1_rmse: 0.45403 |  0:00:49s
epoch 30 | loss: 0.20573 | val_0_rmse: 0.42861 | val_1_rmse: 0.42145 |  0:00:50s
epoch 31 | loss: 0.20327 | val_0_rmse: 0.43806 | val_1_rmse: 0.4355  |  0:00:52s
epoch 32 | loss: 0.20416 | val_0_rmse: 0.43658 | val_1_rmse: 0.43138 |  0:00:53s
epoch 33 | loss: 0.20844 | val_0_rmse: 0.44851 | val_1_rmse: 0.45019 |  0:00:55s
epoch 34 | loss: 0.20138 | val_0_rmse: 0.456   | val_1_rmse: 0.45701 |  0:00:57s
epoch 35 | loss: 0.21012 | val_0_rmse: 0.44317 | val_1_rmse: 0.43806 |  0:00:58s
epoch 36 | loss: 0.20499 | val_0_rmse: 0.43128 | val_1_rmse: 0.42546 |  0:01:00s
epoch 37 | loss: 0.20561 | val_0_rmse: 0.43324 | val_1_rmse: 0.42713 |  0:01:02s
epoch 38 | loss: 0.20395 | val_0_rmse: 0.45009 | val_1_rmse: 0.44386 |  0:01:03s
epoch 39 | loss: 0.20028 | val_0_rmse: 0.43747 | val_1_rmse: 0.43388 |  0:01:05s
epoch 40 | loss: 0.19853 | val_0_rmse: 0.43068 | val_1_rmse: 0.43002 |  0:01:07s
epoch 41 | loss: 0.20639 | val_0_rmse: 0.4476  | val_1_rmse: 0.44051 |  0:01:08s
epoch 42 | loss: 0.20133 | val_0_rmse: 0.45123 | val_1_rmse: 0.44779 |  0:01:10s
epoch 43 | loss: 0.20511 | val_0_rmse: 0.43827 | val_1_rmse: 0.43834 |  0:01:11s
epoch 44 | loss: 0.20176 | val_0_rmse: 0.43266 | val_1_rmse: 0.42779 |  0:01:13s
epoch 45 | loss: 0.20059 | val_0_rmse: 0.42519 | val_1_rmse: 0.4269  |  0:01:15s
epoch 46 | loss: 0.19589 | val_0_rmse: 0.42332 | val_1_rmse: 0.42152 |  0:01:16s
epoch 47 | loss: 0.1967  | val_0_rmse: 0.42312 | val_1_rmse: 0.42548 |  0:01:18s
epoch 48 | loss: 0.20091 | val_0_rmse: 0.43662 | val_1_rmse: 0.43643 |  0:01:20s
epoch 49 | loss: 0.20175 | val_0_rmse: 0.42353 | val_1_rmse: 0.42708 |  0:01:21s
epoch 50 | loss: 0.20032 | val_0_rmse: 0.42398 | val_1_rmse: 0.42205 |  0:01:23s
epoch 51 | loss: 0.19882 | val_0_rmse: 0.42698 | val_1_rmse: 0.42661 |  0:01:24s
epoch 52 | loss: 0.19944 | val_0_rmse: 0.43369 | val_1_rmse: 0.43415 |  0:01:26s
epoch 53 | loss: 0.19844 | val_0_rmse: 0.43074 | val_1_rmse: 0.42645 |  0:01:28s
epoch 54 | loss: 0.19303 | val_0_rmse: 0.42253 | val_1_rmse: 0.42473 |  0:01:29s
epoch 55 | loss: 0.1965  | val_0_rmse: 0.41948 | val_1_rmse: 0.421   |  0:01:31s
epoch 56 | loss: 0.19655 | val_0_rmse: 0.42506 | val_1_rmse: 0.42602 |  0:01:33s
epoch 57 | loss: 0.19874 | val_0_rmse: 0.42274 | val_1_rmse: 0.42338 |  0:01:34s
epoch 58 | loss: 0.20021 | val_0_rmse: 0.42842 | val_1_rmse: 0.42663 |  0:01:36s
epoch 59 | loss: 0.19407 | val_0_rmse: 0.43494 | val_1_rmse: 0.43077 |  0:01:38s
epoch 60 | loss: 0.19541 | val_0_rmse: 0.44531 | val_1_rmse: 0.44711 |  0:01:39s
epoch 61 | loss: 0.19389 | val_0_rmse: 0.42225 | val_1_rmse: 0.42507 |  0:01:41s
epoch 62 | loss: 0.19633 | val_0_rmse: 0.44704 | val_1_rmse: 0.44611 |  0:01:43s
epoch 63 | loss: 0.19497 | val_0_rmse: 0.45979 | val_1_rmse: 0.46179 |  0:01:44s
epoch 64 | loss: 0.19603 | val_0_rmse: 0.43039 | val_1_rmse: 0.42997 |  0:01:46s
epoch 65 | loss: 0.1967  | val_0_rmse: 0.42561 | val_1_rmse: 0.42473 |  0:01:47s
epoch 66 | loss: 0.19642 | val_0_rmse: 0.42515 | val_1_rmse: 0.42363 |  0:01:49s
epoch 67 | loss: 0.19803 | val_0_rmse: 0.44654 | val_1_rmse: 0.45437 |  0:01:51s
epoch 68 | loss: 0.19602 | val_0_rmse: 0.41939 | val_1_rmse: 0.4241  |  0:01:52s
epoch 69 | loss: 0.18758 | val_0_rmse: 0.41834 | val_1_rmse: 0.42572 |  0:01:54s
epoch 70 | loss: 0.19272 | val_0_rmse: 0.42117 | val_1_rmse: 0.42686 |  0:01:56s
epoch 71 | loss: 0.1923  | val_0_rmse: 0.44089 | val_1_rmse: 0.45065 |  0:01:57s
epoch 72 | loss: 0.19356 | val_0_rmse: 0.42221 | val_1_rmse: 0.4208  |  0:01:59s
epoch 73 | loss: 0.1925  | val_0_rmse: 0.41993 | val_1_rmse: 0.41662 |  0:02:00s
epoch 74 | loss: 0.18925 | val_0_rmse: 0.42542 | val_1_rmse: 0.42309 |  0:02:02s
epoch 75 | loss: 0.19248 | val_0_rmse: 0.44007 | val_1_rmse: 0.43944 |  0:02:04s
epoch 76 | loss: 0.20265 | val_0_rmse: 0.46597 | val_1_rmse: 0.46571 |  0:02:05s
epoch 77 | loss: 0.19785 | val_0_rmse: 0.45681 | val_1_rmse: 0.45078 |  0:02:07s
epoch 78 | loss: 0.19131 | val_0_rmse: 0.42167 | val_1_rmse: 0.42262 |  0:02:09s
epoch 79 | loss: 0.18873 | val_0_rmse: 0.42456 | val_1_rmse: 0.42956 |  0:02:10s
epoch 80 | loss: 0.2007  | val_0_rmse: 0.42307 | val_1_rmse: 0.42112 |  0:02:12s
epoch 81 | loss: 0.19083 | val_0_rmse: 0.42755 | val_1_rmse: 0.43173 |  0:02:13s
epoch 82 | loss: 0.19141 | val_0_rmse: 0.44317 | val_1_rmse: 0.45255 |  0:02:15s
epoch 83 | loss: 0.1903  | val_0_rmse: 0.41803 | val_1_rmse: 0.41986 |  0:02:17s
epoch 84 | loss: 0.1909  | val_0_rmse: 0.41822 | val_1_rmse: 0.42418 |  0:02:18s
epoch 85 | loss: 0.18816 | val_0_rmse: 0.41964 | val_1_rmse: 0.42482 |  0:02:20s
epoch 86 | loss: 0.19015 | val_0_rmse: 0.42852 | val_1_rmse: 0.43065 |  0:02:22s
epoch 87 | loss: 0.19338 | val_0_rmse: 0.43053 | val_1_rmse: 0.43352 |  0:02:23s
epoch 88 | loss: 0.19635 | val_0_rmse: 0.4281  | val_1_rmse: 0.42857 |  0:02:25s
epoch 89 | loss: 0.19178 | val_0_rmse: 0.42951 | val_1_rmse: 0.43415 |  0:02:26s
epoch 90 | loss: 0.1917  | val_0_rmse: 0.42489 | val_1_rmse: 0.42763 |  0:02:28s
epoch 91 | loss: 0.18723 | val_0_rmse: 0.41811 | val_1_rmse: 0.42383 |  0:02:30s
epoch 92 | loss: 0.19195 | val_0_rmse: 0.42168 | val_1_rmse: 0.42244 |  0:02:31s
epoch 93 | loss: 0.19713 | val_0_rmse: 0.44374 | val_1_rmse: 0.44164 |  0:02:33s
epoch 94 | loss: 0.20385 | val_0_rmse: 0.43801 | val_1_rmse: 0.44337 |  0:02:35s
epoch 95 | loss: 0.18802 | val_0_rmse: 0.41699 | val_1_rmse: 0.42251 |  0:02:36s
epoch 96 | loss: 0.19209 | val_0_rmse: 0.43217 | val_1_rmse: 0.42927 |  0:02:38s
epoch 97 | loss: 0.19249 | val_0_rmse: 0.41771 | val_1_rmse: 0.42178 |  0:02:40s
epoch 98 | loss: 0.18696 | val_0_rmse: 0.41847 | val_1_rmse: 0.42479 |  0:02:41s
epoch 99 | loss: 0.18579 | val_0_rmse: 0.4153  | val_1_rmse: 0.42067 |  0:02:43s
epoch 100| loss: 0.18955 | val_0_rmse: 0.4274  | val_1_rmse: 0.43315 |  0:02:44s
epoch 101| loss: 0.19024 | val_0_rmse: 0.41264 | val_1_rmse: 0.41601 |  0:02:46s
epoch 102| loss: 0.19069 | val_0_rmse: 0.41753 | val_1_rmse: 0.42387 |  0:02:48s
epoch 103| loss: 0.197   | val_0_rmse: 0.47916 | val_1_rmse: 0.47599 |  0:02:49s
epoch 104| loss: 0.23929 | val_0_rmse: 0.56505 | val_1_rmse: 0.56377 |  0:02:51s
epoch 105| loss: 0.22458 | val_0_rmse: 0.45958 | val_1_rmse: 0.46255 |  0:02:53s
epoch 106| loss: 0.20999 | val_0_rmse: 0.45939 | val_1_rmse: 0.46126 |  0:02:54s
epoch 107| loss: 0.21347 | val_0_rmse: 0.46457 | val_1_rmse: 0.46335 |  0:02:56s
epoch 108| loss: 0.20571 | val_0_rmse: 0.44031 | val_1_rmse: 0.4366  |  0:02:58s
epoch 109| loss: 0.20374 | val_0_rmse: 0.4258  | val_1_rmse: 0.4252  |  0:02:59s
epoch 110| loss: 0.20231 | val_0_rmse: 0.43673 | val_1_rmse: 0.4371  |  0:03:01s
epoch 111| loss: 0.20007 | val_0_rmse: 0.45951 | val_1_rmse: 0.46511 |  0:03:02s
epoch 112| loss: 0.19656 | val_0_rmse: 0.42103 | val_1_rmse: 0.41653 |  0:03:04s
epoch 113| loss: 0.19398 | val_0_rmse: 0.42214 | val_1_rmse: 0.42413 |  0:03:06s
epoch 114| loss: 0.19835 | val_0_rmse: 0.43007 | val_1_rmse: 0.43289 |  0:03:07s
epoch 115| loss: 0.19481 | val_0_rmse: 0.41992 | val_1_rmse: 0.41858 |  0:03:09s
epoch 116| loss: 0.20279 | val_0_rmse: 0.45993 | val_1_rmse: 0.46319 |  0:03:11s
epoch 117| loss: 0.19596 | val_0_rmse: 0.4202  | val_1_rmse: 0.41973 |  0:03:12s
epoch 118| loss: 0.19136 | val_0_rmse: 0.4172  | val_1_rmse: 0.42017 |  0:03:14s
epoch 119| loss: 0.20375 | val_0_rmse: 0.4604  | val_1_rmse: 0.45401 |  0:03:16s
epoch 120| loss: 0.22273 | val_0_rmse: 0.43926 | val_1_rmse: 0.43775 |  0:03:17s
epoch 121| loss: 0.20127 | val_0_rmse: 0.42675 | val_1_rmse: 0.42438 |  0:03:19s
epoch 122| loss: 0.19499 | val_0_rmse: 0.44444 | val_1_rmse: 0.43932 |  0:03:20s
epoch 123| loss: 0.19359 | val_0_rmse: 0.46566 | val_1_rmse: 0.47647 |  0:03:22s
epoch 124| loss: 0.18875 | val_0_rmse: 0.424   | val_1_rmse: 0.4203  |  0:03:24s
epoch 125| loss: 0.19649 | val_0_rmse: 0.42899 | val_1_rmse: 0.43495 |  0:03:25s
epoch 126| loss: 0.19299 | val_0_rmse: 0.41674 | val_1_rmse: 0.41766 |  0:03:27s
epoch 127| loss: 0.19093 | val_0_rmse: 0.42406 | val_1_rmse: 0.42367 |  0:03:29s
epoch 128| loss: 0.19221 | val_0_rmse: 0.42576 | val_1_rmse: 0.43264 |  0:03:30s
epoch 129| loss: 0.18778 | val_0_rmse: 0.41673 | val_1_rmse: 0.42547 |  0:03:32s
epoch 130| loss: 0.18786 | val_0_rmse: 0.41411 | val_1_rmse: 0.42327 |  0:03:33s
epoch 131| loss: 0.19013 | val_0_rmse: 0.42797 | val_1_rmse: 0.43267 |  0:03:35s

Early stopping occured at epoch 131 with best_epoch = 101 and best_val_1_rmse = 0.41601
Best weights from best epoch are automatically used!
ended training at: 14:02:24
Feature importance:
[('Area', 0.03848121413536709), ('Baths', 0.12256973840902717), ('Beds', 0.18187245431817534), ('Latitude', 0.12112740018033272), ('Longitude', 0.2566063514242661), ('Month', 0.0032409454485602144), ('Year', 0.2761018960842714)]
Mean squared error is of 10185151174.089155
Mean absolute error:70396.49291679432
MAPE:0.2986283698046847
R2 score:0.8251117426668004
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:02:25
epoch 0  | loss: 0.87682 | val_0_rmse: 0.85242 | val_1_rmse: 0.83429 |  0:00:00s
epoch 1  | loss: 0.43313 | val_0_rmse: 0.71365 | val_1_rmse: 0.72162 |  0:00:00s
epoch 2  | loss: 0.3601  | val_0_rmse: 0.677   | val_1_rmse: 0.68824 |  0:00:01s
epoch 3  | loss: 0.34588 | val_0_rmse: 0.67214 | val_1_rmse: 0.70644 |  0:00:02s
epoch 4  | loss: 0.33937 | val_0_rmse: 0.57495 | val_1_rmse: 0.57761 |  0:00:02s
epoch 5  | loss: 0.31163 | val_0_rmse: 0.55108 | val_1_rmse: 0.5544  |  0:00:03s
epoch 6  | loss: 0.29828 | val_0_rmse: 0.56987 | val_1_rmse: 0.56492 |  0:00:03s
epoch 7  | loss: 0.3052  | val_0_rmse: 0.55364 | val_1_rmse: 0.5515  |  0:00:04s
epoch 8  | loss: 0.30524 | val_0_rmse: 0.53491 | val_1_rmse: 0.52803 |  0:00:04s
epoch 9  | loss: 0.2944  | val_0_rmse: 0.55496 | val_1_rmse: 0.5552  |  0:00:05s
epoch 10 | loss: 0.28713 | val_0_rmse: 0.55789 | val_1_rmse: 0.54874 |  0:00:05s
epoch 11 | loss: 0.29172 | val_0_rmse: 0.52791 | val_1_rmse: 0.52614 |  0:00:06s
epoch 12 | loss: 0.27785 | val_0_rmse: 0.51204 | val_1_rmse: 0.50918 |  0:00:06s
epoch 13 | loss: 0.27362 | val_0_rmse: 0.5074  | val_1_rmse: 0.50598 |  0:00:07s
epoch 14 | loss: 0.2734  | val_0_rmse: 0.5107  | val_1_rmse: 0.51661 |  0:00:07s
epoch 15 | loss: 0.26706 | val_0_rmse: 0.50815 | val_1_rmse: 0.51038 |  0:00:08s
epoch 16 | loss: 0.27813 | val_0_rmse: 0.50074 | val_1_rmse: 0.49992 |  0:00:08s
epoch 17 | loss: 0.26202 | val_0_rmse: 0.52276 | val_1_rmse: 0.51396 |  0:00:09s
epoch 18 | loss: 0.27601 | val_0_rmse: 0.50626 | val_1_rmse: 0.50233 |  0:00:09s
epoch 19 | loss: 0.27601 | val_0_rmse: 0.50338 | val_1_rmse: 0.49471 |  0:00:10s
epoch 20 | loss: 0.26814 | val_0_rmse: 0.49529 | val_1_rmse: 0.48823 |  0:00:10s
epoch 21 | loss: 0.27355 | val_0_rmse: 0.5184  | val_1_rmse: 0.52123 |  0:00:11s
epoch 22 | loss: 0.26072 | val_0_rmse: 0.49555 | val_1_rmse: 0.49077 |  0:00:11s
epoch 23 | loss: 0.2637  | val_0_rmse: 0.49548 | val_1_rmse: 0.49271 |  0:00:12s
epoch 24 | loss: 0.2532  | val_0_rmse: 0.50139 | val_1_rmse: 0.4952  |  0:00:12s
epoch 25 | loss: 0.26491 | val_0_rmse: 0.49502 | val_1_rmse: 0.49271 |  0:00:13s
epoch 26 | loss: 0.25753 | val_0_rmse: 0.50094 | val_1_rmse: 0.49995 |  0:00:13s
epoch 27 | loss: 0.25766 | val_0_rmse: 0.49423 | val_1_rmse: 0.49858 |  0:00:14s
epoch 28 | loss: 0.26186 | val_0_rmse: 0.49956 | val_1_rmse: 0.49502 |  0:00:14s
epoch 29 | loss: 0.25511 | val_0_rmse: 0.51056 | val_1_rmse: 0.50206 |  0:00:15s
epoch 30 | loss: 0.26695 | val_0_rmse: 0.50672 | val_1_rmse: 0.49767 |  0:00:15s
epoch 31 | loss: 0.25936 | val_0_rmse: 0.492   | val_1_rmse: 0.48815 |  0:00:16s
epoch 32 | loss: 0.25938 | val_0_rmse: 0.48751 | val_1_rmse: 0.48071 |  0:00:16s
epoch 33 | loss: 0.25309 | val_0_rmse: 0.51087 | val_1_rmse: 0.49557 |  0:00:17s
epoch 34 | loss: 0.25771 | val_0_rmse: 0.49676 | val_1_rmse: 0.48999 |  0:00:17s
epoch 35 | loss: 0.26065 | val_0_rmse: 0.49915 | val_1_rmse: 0.49241 |  0:00:18s
epoch 36 | loss: 0.25476 | val_0_rmse: 0.49327 | val_1_rmse: 0.48645 |  0:00:18s
epoch 37 | loss: 0.25343 | val_0_rmse: 0.48186 | val_1_rmse: 0.47527 |  0:00:19s
epoch 38 | loss: 0.24802 | val_0_rmse: 0.49319 | val_1_rmse: 0.49182 |  0:00:19s
epoch 39 | loss: 0.24953 | val_0_rmse: 0.48627 | val_1_rmse: 0.48061 |  0:00:20s
epoch 40 | loss: 0.24753 | val_0_rmse: 0.48508 | val_1_rmse: 0.4774  |  0:00:20s
epoch 41 | loss: 0.24625 | val_0_rmse: 0.48193 | val_1_rmse: 0.48091 |  0:00:21s
epoch 42 | loss: 0.24659 | val_0_rmse: 0.47747 | val_1_rmse: 0.47419 |  0:00:21s
epoch 43 | loss: 0.24059 | val_0_rmse: 0.47585 | val_1_rmse: 0.47616 |  0:00:22s
epoch 44 | loss: 0.24819 | val_0_rmse: 0.4838  | val_1_rmse: 0.48224 |  0:00:22s
epoch 45 | loss: 0.24699 | val_0_rmse: 0.48167 | val_1_rmse: 0.48924 |  0:00:23s
epoch 46 | loss: 0.24384 | val_0_rmse: 0.481   | val_1_rmse: 0.47892 |  0:00:23s
epoch 47 | loss: 0.24608 | val_0_rmse: 0.49186 | val_1_rmse: 0.48959 |  0:00:24s
epoch 48 | loss: 0.24348 | val_0_rmse: 0.48729 | val_1_rmse: 0.49252 |  0:00:24s
epoch 49 | loss: 0.24234 | val_0_rmse: 0.49544 | val_1_rmse: 0.48972 |  0:00:25s
epoch 50 | loss: 0.24109 | val_0_rmse: 0.48809 | val_1_rmse: 0.49413 |  0:00:25s
epoch 51 | loss: 0.2411  | val_0_rmse: 0.47703 | val_1_rmse: 0.48563 |  0:00:25s
epoch 52 | loss: 0.23655 | val_0_rmse: 0.47823 | val_1_rmse: 0.47324 |  0:00:26s
epoch 53 | loss: 0.23853 | val_0_rmse: 0.48413 | val_1_rmse: 0.49326 |  0:00:26s
epoch 54 | loss: 0.23949 | val_0_rmse: 0.47775 | val_1_rmse: 0.47411 |  0:00:27s
epoch 55 | loss: 0.24104 | val_0_rmse: 0.50415 | val_1_rmse: 0.5151  |  0:00:27s
epoch 56 | loss: 0.24746 | val_0_rmse: 0.50097 | val_1_rmse: 0.50106 |  0:00:28s
epoch 57 | loss: 0.2615  | val_0_rmse: 0.49764 | val_1_rmse: 0.50006 |  0:00:28s
epoch 58 | loss: 0.24469 | val_0_rmse: 0.50277 | val_1_rmse: 0.50918 |  0:00:29s
epoch 59 | loss: 0.2391  | val_0_rmse: 0.47262 | val_1_rmse: 0.47191 |  0:00:29s
epoch 60 | loss: 0.24467 | val_0_rmse: 0.47743 | val_1_rmse: 0.4727  |  0:00:30s
epoch 61 | loss: 0.2452  | val_0_rmse: 0.50703 | val_1_rmse: 0.50295 |  0:00:30s
epoch 62 | loss: 0.24216 | val_0_rmse: 0.4761  | val_1_rmse: 0.47599 |  0:00:31s
epoch 63 | loss: 0.24301 | val_0_rmse: 0.46708 | val_1_rmse: 0.46059 |  0:00:31s
epoch 64 | loss: 0.23312 | val_0_rmse: 0.47354 | val_1_rmse: 0.46559 |  0:00:32s
epoch 65 | loss: 0.23765 | val_0_rmse: 0.47159 | val_1_rmse: 0.46947 |  0:00:32s
epoch 66 | loss: 0.24285 | val_0_rmse: 0.47069 | val_1_rmse: 0.46813 |  0:00:33s
epoch 67 | loss: 0.235   | val_0_rmse: 0.46714 | val_1_rmse: 0.4668  |  0:00:33s
epoch 68 | loss: 0.23299 | val_0_rmse: 0.46482 | val_1_rmse: 0.46467 |  0:00:34s
epoch 69 | loss: 0.22965 | val_0_rmse: 0.46578 | val_1_rmse: 0.46538 |  0:00:34s
epoch 70 | loss: 0.23061 | val_0_rmse: 0.46979 | val_1_rmse: 0.47171 |  0:00:35s
epoch 71 | loss: 0.23177 | val_0_rmse: 0.47567 | val_1_rmse: 0.47766 |  0:00:35s
epoch 72 | loss: 0.23132 | val_0_rmse: 0.46603 | val_1_rmse: 0.46955 |  0:00:36s
epoch 73 | loss: 0.22757 | val_0_rmse: 0.51167 | val_1_rmse: 0.52485 |  0:00:36s
epoch 74 | loss: 0.23468 | val_0_rmse: 0.47256 | val_1_rmse: 0.47685 |  0:00:37s
epoch 75 | loss: 0.23887 | val_0_rmse: 0.49085 | val_1_rmse: 0.48971 |  0:00:37s
epoch 76 | loss: 0.24301 | val_0_rmse: 0.48872 | val_1_rmse: 0.49192 |  0:00:38s
epoch 77 | loss: 0.24026 | val_0_rmse: 0.47169 | val_1_rmse: 0.47761 |  0:00:38s
epoch 78 | loss: 0.23569 | val_0_rmse: 0.46536 | val_1_rmse: 0.47547 |  0:00:39s
epoch 79 | loss: 0.2321  | val_0_rmse: 0.47346 | val_1_rmse: 0.47523 |  0:00:39s
epoch 80 | loss: 0.22807 | val_0_rmse: 0.46776 | val_1_rmse: 0.47258 |  0:00:40s
epoch 81 | loss: 0.22533 | val_0_rmse: 0.45353 | val_1_rmse: 0.45806 |  0:00:40s
epoch 82 | loss: 0.2228  | val_0_rmse: 0.47394 | val_1_rmse: 0.47028 |  0:00:41s
epoch 83 | loss: 0.23116 | val_0_rmse: 0.46684 | val_1_rmse: 0.47295 |  0:00:41s
epoch 84 | loss: 0.22611 | val_0_rmse: 0.47315 | val_1_rmse: 0.48381 |  0:00:42s
epoch 85 | loss: 0.22888 | val_0_rmse: 0.50436 | val_1_rmse: 0.50741 |  0:00:42s
epoch 86 | loss: 0.2326  | val_0_rmse: 0.46247 | val_1_rmse: 0.45915 |  0:00:43s
epoch 87 | loss: 0.23262 | val_0_rmse: 0.45877 | val_1_rmse: 0.46906 |  0:00:43s
epoch 88 | loss: 0.2313  | val_0_rmse: 0.45952 | val_1_rmse: 0.47197 |  0:00:44s
epoch 89 | loss: 0.23201 | val_0_rmse: 0.47072 | val_1_rmse: 0.46965 |  0:00:44s
epoch 90 | loss: 0.23026 | val_0_rmse: 0.4602  | val_1_rmse: 0.45973 |  0:00:45s
epoch 91 | loss: 0.23238 | val_0_rmse: 0.47882 | val_1_rmse: 0.48047 |  0:00:45s
epoch 92 | loss: 0.2342  | val_0_rmse: 0.46598 | val_1_rmse: 0.46937 |  0:00:46s
epoch 93 | loss: 0.22776 | val_0_rmse: 0.45684 | val_1_rmse: 0.45483 |  0:00:46s
epoch 94 | loss: 0.23202 | val_0_rmse: 0.46617 | val_1_rmse: 0.4637  |  0:00:47s
epoch 95 | loss: 0.23426 | val_0_rmse: 0.46099 | val_1_rmse: 0.46635 |  0:00:47s
epoch 96 | loss: 0.22473 | val_0_rmse: 0.45962 | val_1_rmse: 0.46255 |  0:00:48s
epoch 97 | loss: 0.22267 | val_0_rmse: 0.46744 | val_1_rmse: 0.4757  |  0:00:48s
epoch 98 | loss: 0.2296  | val_0_rmse: 0.47036 | val_1_rmse: 0.47408 |  0:00:49s
epoch 99 | loss: 0.2351  | val_0_rmse: 0.49041 | val_1_rmse: 0.49878 |  0:00:49s
epoch 100| loss: 0.24482 | val_0_rmse: 0.47145 | val_1_rmse: 0.48123 |  0:00:50s
epoch 101| loss: 0.23903 | val_0_rmse: 0.47541 | val_1_rmse: 0.48262 |  0:00:50s
epoch 102| loss: 0.25148 | val_0_rmse: 0.51146 | val_1_rmse: 0.51924 |  0:00:51s
epoch 103| loss: 0.24736 | val_0_rmse: 0.49554 | val_1_rmse: 0.51654 |  0:00:51s
epoch 104| loss: 0.24338 | val_0_rmse: 0.48957 | val_1_rmse: 0.49438 |  0:00:52s
epoch 105| loss: 0.23971 | val_0_rmse: 0.47003 | val_1_rmse: 0.46978 |  0:00:52s
epoch 106| loss: 0.23992 | val_0_rmse: 0.46852 | val_1_rmse: 0.46331 |  0:00:53s
epoch 107| loss: 0.23292 | val_0_rmse: 0.50266 | val_1_rmse: 0.50966 |  0:00:53s
epoch 108| loss: 0.24335 | val_0_rmse: 0.48248 | val_1_rmse: 0.48964 |  0:00:54s
epoch 109| loss: 0.23386 | val_0_rmse: 0.47642 | val_1_rmse: 0.47501 |  0:00:54s
epoch 110| loss: 0.23241 | val_0_rmse: 0.47111 | val_1_rmse: 0.4773  |  0:00:55s
epoch 111| loss: 0.23367 | val_0_rmse: 0.47077 | val_1_rmse: 0.47491 |  0:00:55s
epoch 112| loss: 0.23258 | val_0_rmse: 0.47232 | val_1_rmse: 0.4765  |  0:00:56s
epoch 113| loss: 0.22275 | val_0_rmse: 0.45443 | val_1_rmse: 0.46604 |  0:00:56s
epoch 114| loss: 0.22511 | val_0_rmse: 0.47156 | val_1_rmse: 0.47424 |  0:00:57s
epoch 115| loss: 0.23215 | val_0_rmse: 0.47474 | val_1_rmse: 0.47362 |  0:00:57s
epoch 116| loss: 0.23083 | val_0_rmse: 0.49968 | val_1_rmse: 0.51055 |  0:00:58s
epoch 117| loss: 0.23051 | val_0_rmse: 0.46988 | val_1_rmse: 0.47424 |  0:00:58s
epoch 118| loss: 0.2381  | val_0_rmse: 0.46902 | val_1_rmse: 0.47707 |  0:00:59s
epoch 119| loss: 0.22397 | val_0_rmse: 0.46556 | val_1_rmse: 0.46367 |  0:00:59s
epoch 120| loss: 0.23103 | val_0_rmse: 0.47199 | val_1_rmse: 0.47443 |  0:01:00s
epoch 121| loss: 0.24754 | val_0_rmse: 0.4795  | val_1_rmse: 0.48459 |  0:01:00s
epoch 122| loss: 0.23931 | val_0_rmse: 0.49772 | val_1_rmse: 0.50352 |  0:01:01s
epoch 123| loss: 0.24082 | val_0_rmse: 0.47645 | val_1_rmse: 0.48105 |  0:01:01s

Early stopping occured at epoch 123 with best_epoch = 93 and best_val_1_rmse = 0.45483
Best weights from best epoch are automatically used!
ended training at: 14:03:27
Feature importance:
[('Area', 0.4301442805109636), ('Baths', 0.0), ('Beds', 0.015010971589718123), ('Latitude', 0.29455392490321647), ('Longitude', 0.10723814381304585), ('Month', 0.0), ('Year', 0.15305267918305598)]
Mean squared error is of 7348925484.311654
Mean absolute error:61488.40662820835
MAPE:0.1600145001406894
R2 score:0.7687366102367781
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:03:27
epoch 0  | loss: 0.90769 | val_0_rmse: 0.98266 | val_1_rmse: 0.98533 |  0:00:00s
epoch 1  | loss: 0.4261  | val_0_rmse: 0.66035 | val_1_rmse: 0.66193 |  0:00:01s
epoch 2  | loss: 0.37339 | val_0_rmse: 0.64411 | val_1_rmse: 0.64416 |  0:00:01s
epoch 3  | loss: 0.34175 | val_0_rmse: 0.58447 | val_1_rmse: 0.574   |  0:00:02s
epoch 4  | loss: 0.31816 | val_0_rmse: 0.56181 | val_1_rmse: 0.55948 |  0:00:02s
epoch 5  | loss: 0.30398 | val_0_rmse: 0.56129 | val_1_rmse: 0.56442 |  0:00:03s
epoch 6  | loss: 0.30717 | val_0_rmse: 0.52425 | val_1_rmse: 0.5277  |  0:00:03s
epoch 7  | loss: 0.2994  | val_0_rmse: 0.53231 | val_1_rmse: 0.53705 |  0:00:04s
epoch 8  | loss: 0.29261 | val_0_rmse: 0.54116 | val_1_rmse: 0.54848 |  0:00:04s
epoch 9  | loss: 0.29176 | val_0_rmse: 0.56698 | val_1_rmse: 0.56632 |  0:00:04s
epoch 10 | loss: 0.30348 | val_0_rmse: 0.51696 | val_1_rmse: 0.51885 |  0:00:05s
epoch 11 | loss: 0.3031  | val_0_rmse: 0.55431 | val_1_rmse: 0.55333 |  0:00:06s
epoch 12 | loss: 0.30762 | val_0_rmse: 0.53559 | val_1_rmse: 0.53952 |  0:00:06s
epoch 13 | loss: 0.29212 | val_0_rmse: 0.51106 | val_1_rmse: 0.50868 |  0:00:07s
epoch 14 | loss: 0.277   | val_0_rmse: 0.5051  | val_1_rmse: 0.50489 |  0:00:07s
epoch 15 | loss: 0.26913 | val_0_rmse: 0.51454 | val_1_rmse: 0.51216 |  0:00:08s
epoch 16 | loss: 0.27922 | val_0_rmse: 0.51459 | val_1_rmse: 0.51449 |  0:00:08s
epoch 17 | loss: 0.27134 | val_0_rmse: 0.50774 | val_1_rmse: 0.50461 |  0:00:08s
epoch 18 | loss: 0.27324 | val_0_rmse: 0.50899 | val_1_rmse: 0.50741 |  0:00:09s
epoch 19 | loss: 0.27089 | val_0_rmse: 0.50695 | val_1_rmse: 0.50943 |  0:00:09s
epoch 20 | loss: 0.27636 | val_0_rmse: 0.56732 | val_1_rmse: 0.56696 |  0:00:10s
epoch 21 | loss: 0.28542 | val_0_rmse: 0.54733 | val_1_rmse: 0.53782 |  0:00:10s
epoch 22 | loss: 0.27831 | val_0_rmse: 0.55639 | val_1_rmse: 0.56311 |  0:00:11s
epoch 23 | loss: 0.28295 | val_0_rmse: 0.51173 | val_1_rmse: 0.50532 |  0:00:11s
epoch 24 | loss: 0.27752 | val_0_rmse: 0.50925 | val_1_rmse: 0.50773 |  0:00:12s
epoch 25 | loss: 0.27457 | val_0_rmse: 0.51285 | val_1_rmse: 0.51381 |  0:00:12s
epoch 26 | loss: 0.27562 | val_0_rmse: 0.50929 | val_1_rmse: 0.50527 |  0:00:13s
epoch 27 | loss: 0.27008 | val_0_rmse: 0.50524 | val_1_rmse: 0.50549 |  0:00:13s
epoch 28 | loss: 0.26609 | val_0_rmse: 0.50804 | val_1_rmse: 0.50673 |  0:00:14s
epoch 29 | loss: 0.26968 | val_0_rmse: 0.50687 | val_1_rmse: 0.50687 |  0:00:14s
epoch 30 | loss: 0.26166 | val_0_rmse: 0.50731 | val_1_rmse: 0.50983 |  0:00:15s
epoch 31 | loss: 0.26536 | val_0_rmse: 0.50353 | val_1_rmse: 0.51104 |  0:00:15s
epoch 32 | loss: 0.26483 | val_0_rmse: 0.50401 | val_1_rmse: 0.51053 |  0:00:16s
epoch 33 | loss: 0.26414 | val_0_rmse: 0.50624 | val_1_rmse: 0.51113 |  0:00:16s
epoch 34 | loss: 0.26824 | val_0_rmse: 0.49671 | val_1_rmse: 0.50121 |  0:00:17s
epoch 35 | loss: 0.26088 | val_0_rmse: 0.499   | val_1_rmse: 0.49823 |  0:00:17s
epoch 36 | loss: 0.26446 | val_0_rmse: 0.50909 | val_1_rmse: 0.50922 |  0:00:18s
epoch 37 | loss: 0.27817 | val_0_rmse: 0.52287 | val_1_rmse: 0.52674 |  0:00:18s
epoch 38 | loss: 0.27842 | val_0_rmse: 0.51117 | val_1_rmse: 0.51081 |  0:00:19s
epoch 39 | loss: 0.27342 | val_0_rmse: 0.5241  | val_1_rmse: 0.51273 |  0:00:19s
epoch 40 | loss: 0.27293 | val_0_rmse: 0.50271 | val_1_rmse: 0.50204 |  0:00:20s
epoch 41 | loss: 0.2704  | val_0_rmse: 0.50854 | val_1_rmse: 0.50004 |  0:00:20s
epoch 42 | loss: 0.2683  | val_0_rmse: 0.50768 | val_1_rmse: 0.49567 |  0:00:21s
epoch 43 | loss: 0.26488 | val_0_rmse: 0.51121 | val_1_rmse: 0.49744 |  0:00:21s
epoch 44 | loss: 0.26206 | val_0_rmse: 0.5225  | val_1_rmse: 0.51256 |  0:00:22s
epoch 45 | loss: 0.26857 | val_0_rmse: 0.50656 | val_1_rmse: 0.50361 |  0:00:22s
epoch 46 | loss: 0.27135 | val_0_rmse: 0.51926 | val_1_rmse: 0.52008 |  0:00:23s
epoch 47 | loss: 0.26987 | val_0_rmse: 0.52352 | val_1_rmse: 0.52588 |  0:00:23s
epoch 48 | loss: 0.26485 | val_0_rmse: 0.49609 | val_1_rmse: 0.50166 |  0:00:24s
epoch 49 | loss: 0.25749 | val_0_rmse: 0.50395 | val_1_rmse: 0.5004  |  0:00:24s
epoch 50 | loss: 0.26067 | val_0_rmse: 0.50195 | val_1_rmse: 0.4994  |  0:00:25s
epoch 51 | loss: 0.26587 | val_0_rmse: 0.50037 | val_1_rmse: 0.49681 |  0:00:25s
epoch 52 | loss: 0.26437 | val_0_rmse: 0.49501 | val_1_rmse: 0.48846 |  0:00:26s
epoch 53 | loss: 0.25848 | val_0_rmse: 0.50127 | val_1_rmse: 0.50228 |  0:00:26s
epoch 54 | loss: 0.26176 | val_0_rmse: 0.50237 | val_1_rmse: 0.49919 |  0:00:27s
epoch 55 | loss: 0.2565  | val_0_rmse: 0.49056 | val_1_rmse: 0.49357 |  0:00:27s
epoch 56 | loss: 0.24906 | val_0_rmse: 0.49563 | val_1_rmse: 0.49523 |  0:00:28s
epoch 57 | loss: 0.25297 | val_0_rmse: 0.49082 | val_1_rmse: 0.49317 |  0:00:28s
epoch 58 | loss: 0.25408 | val_0_rmse: 0.49044 | val_1_rmse: 0.49031 |  0:00:29s
epoch 59 | loss: 0.25565 | val_0_rmse: 0.49128 | val_1_rmse: 0.49314 |  0:00:29s
epoch 60 | loss: 0.25138 | val_0_rmse: 0.48833 | val_1_rmse: 0.48785 |  0:00:30s
epoch 61 | loss: 0.25798 | val_0_rmse: 0.48946 | val_1_rmse: 0.48818 |  0:00:30s
epoch 62 | loss: 0.25604 | val_0_rmse: 0.50656 | val_1_rmse: 0.50077 |  0:00:31s
epoch 63 | loss: 0.25643 | val_0_rmse: 0.48816 | val_1_rmse: 0.48599 |  0:00:31s
epoch 64 | loss: 0.24928 | val_0_rmse: 0.49936 | val_1_rmse: 0.50071 |  0:00:32s
epoch 65 | loss: 0.25313 | val_0_rmse: 0.48317 | val_1_rmse: 0.4832  |  0:00:32s
epoch 66 | loss: 0.2502  | val_0_rmse: 0.4887  | val_1_rmse: 0.48731 |  0:00:33s
epoch 67 | loss: 0.2507  | val_0_rmse: 0.48467 | val_1_rmse: 0.48582 |  0:00:33s
epoch 68 | loss: 0.24867 | val_0_rmse: 0.48573 | val_1_rmse: 0.48724 |  0:00:34s
epoch 69 | loss: 0.25109 | val_0_rmse: 0.48397 | val_1_rmse: 0.48622 |  0:00:34s
epoch 70 | loss: 0.25139 | val_0_rmse: 0.48624 | val_1_rmse: 0.48704 |  0:00:35s
epoch 71 | loss: 0.24553 | val_0_rmse: 0.47688 | val_1_rmse: 0.48104 |  0:00:35s
epoch 72 | loss: 0.24698 | val_0_rmse: 0.49618 | val_1_rmse: 0.49665 |  0:00:36s
epoch 73 | loss: 0.24539 | val_0_rmse: 0.47921 | val_1_rmse: 0.47859 |  0:00:36s
epoch 74 | loss: 0.2434  | val_0_rmse: 0.477   | val_1_rmse: 0.48025 |  0:00:37s
epoch 75 | loss: 0.23776 | val_0_rmse: 0.47758 | val_1_rmse: 0.47872 |  0:00:37s
epoch 76 | loss: 0.23716 | val_0_rmse: 0.47265 | val_1_rmse: 0.47239 |  0:00:38s
epoch 77 | loss: 0.23752 | val_0_rmse: 0.48836 | val_1_rmse: 0.48942 |  0:00:38s
epoch 78 | loss: 0.24259 | val_0_rmse: 0.47495 | val_1_rmse: 0.47845 |  0:00:39s
epoch 79 | loss: 0.24735 | val_0_rmse: 0.48094 | val_1_rmse: 0.48705 |  0:00:39s
epoch 80 | loss: 0.23987 | val_0_rmse: 0.47388 | val_1_rmse: 0.47845 |  0:00:40s
epoch 81 | loss: 0.23521 | val_0_rmse: 0.47114 | val_1_rmse: 0.4796  |  0:00:40s
epoch 82 | loss: 0.24089 | val_0_rmse: 0.47276 | val_1_rmse: 0.47765 |  0:00:41s
epoch 83 | loss: 0.23353 | val_0_rmse: 0.47442 | val_1_rmse: 0.48355 |  0:00:41s
epoch 84 | loss: 0.2343  | val_0_rmse: 0.46701 | val_1_rmse: 0.47439 |  0:00:42s
epoch 85 | loss: 0.24059 | val_0_rmse: 0.4691  | val_1_rmse: 0.47672 |  0:00:42s
epoch 86 | loss: 0.23506 | val_0_rmse: 0.46871 | val_1_rmse: 0.4778  |  0:00:43s
epoch 87 | loss: 0.23384 | val_0_rmse: 0.46653 | val_1_rmse: 0.47437 |  0:00:43s
epoch 88 | loss: 0.23659 | val_0_rmse: 0.4776  | val_1_rmse: 0.47927 |  0:00:44s
epoch 89 | loss: 0.24003 | val_0_rmse: 0.47416 | val_1_rmse: 0.48188 |  0:00:44s
epoch 90 | loss: 0.23916 | val_0_rmse: 0.46474 | val_1_rmse: 0.47163 |  0:00:45s
epoch 91 | loss: 0.23027 | val_0_rmse: 0.46271 | val_1_rmse: 0.47156 |  0:00:45s
epoch 92 | loss: 0.22819 | val_0_rmse: 0.47157 | val_1_rmse: 0.48227 |  0:00:46s
epoch 93 | loss: 0.23237 | val_0_rmse: 0.46515 | val_1_rmse: 0.47142 |  0:00:46s
epoch 94 | loss: 0.23039 | val_0_rmse: 0.46014 | val_1_rmse: 0.47049 |  0:00:47s
epoch 95 | loss: 0.23222 | val_0_rmse: 0.46959 | val_1_rmse: 0.47754 |  0:00:47s
epoch 96 | loss: 0.23064 | val_0_rmse: 0.483   | val_1_rmse: 0.49289 |  0:00:48s
epoch 97 | loss: 0.24029 | val_0_rmse: 0.48669 | val_1_rmse: 0.49137 |  0:00:48s
epoch 98 | loss: 0.23213 | val_0_rmse: 0.4605  | val_1_rmse: 0.46841 |  0:00:49s
epoch 99 | loss: 0.23063 | val_0_rmse: 0.45908 | val_1_rmse: 0.47129 |  0:00:49s
epoch 100| loss: 0.23314 | val_0_rmse: 0.48253 | val_1_rmse: 0.49401 |  0:00:50s
epoch 101| loss: 0.23716 | val_0_rmse: 0.4587  | val_1_rmse: 0.47136 |  0:00:50s
epoch 102| loss: 0.23487 | val_0_rmse: 0.46035 | val_1_rmse: 0.47281 |  0:00:51s
epoch 103| loss: 0.22854 | val_0_rmse: 0.46825 | val_1_rmse: 0.47737 |  0:00:51s
epoch 104| loss: 0.227   | val_0_rmse: 0.45316 | val_1_rmse: 0.46606 |  0:00:52s
epoch 105| loss: 0.23012 | val_0_rmse: 0.45758 | val_1_rmse: 0.46838 |  0:00:52s
epoch 106| loss: 0.22569 | val_0_rmse: 0.45657 | val_1_rmse: 0.46221 |  0:00:53s
epoch 107| loss: 0.22471 | val_0_rmse: 0.45964 | val_1_rmse: 0.47073 |  0:00:53s
epoch 108| loss: 0.22485 | val_0_rmse: 0.46517 | val_1_rmse: 0.48013 |  0:00:54s
epoch 109| loss: 0.2259  | val_0_rmse: 0.46093 | val_1_rmse: 0.472   |  0:00:54s
epoch 110| loss: 0.22257 | val_0_rmse: 0.45157 | val_1_rmse: 0.45827 |  0:00:55s
epoch 111| loss: 0.22117 | val_0_rmse: 0.4575  | val_1_rmse: 0.4733  |  0:00:55s
epoch 112| loss: 0.21542 | val_0_rmse: 0.45113 | val_1_rmse: 0.46009 |  0:00:56s
epoch 113| loss: 0.22382 | val_0_rmse: 0.4491  | val_1_rmse: 0.46002 |  0:00:56s
epoch 114| loss: 0.21691 | val_0_rmse: 0.44705 | val_1_rmse: 0.46357 |  0:00:57s
epoch 115| loss: 0.22133 | val_0_rmse: 0.44783 | val_1_rmse: 0.45776 |  0:00:57s
epoch 116| loss: 0.21606 | val_0_rmse: 0.45432 | val_1_rmse: 0.46464 |  0:00:58s
epoch 117| loss: 0.22565 | val_0_rmse: 0.46021 | val_1_rmse: 0.46987 |  0:00:58s
epoch 118| loss: 0.21933 | val_0_rmse: 0.45169 | val_1_rmse: 0.46222 |  0:00:59s
epoch 119| loss: 0.22333 | val_0_rmse: 0.45269 | val_1_rmse: 0.46616 |  0:00:59s
epoch 120| loss: 0.22721 | val_0_rmse: 0.46465 | val_1_rmse: 0.47174 |  0:01:00s
epoch 121| loss: 0.22865 | val_0_rmse: 0.45833 | val_1_rmse: 0.47022 |  0:01:00s
epoch 122| loss: 0.22466 | val_0_rmse: 0.469   | val_1_rmse: 0.47909 |  0:01:01s
epoch 123| loss: 0.22443 | val_0_rmse: 0.47714 | val_1_rmse: 0.4897  |  0:01:01s
epoch 124| loss: 0.2314  | val_0_rmse: 0.4739  | val_1_rmse: 0.4872  |  0:01:02s
epoch 125| loss: 0.2187  | val_0_rmse: 0.46844 | val_1_rmse: 0.48024 |  0:01:02s
epoch 126| loss: 0.22983 | val_0_rmse: 0.45109 | val_1_rmse: 0.46472 |  0:01:02s
epoch 127| loss: 0.22287 | val_0_rmse: 0.45544 | val_1_rmse: 0.4691  |  0:01:03s
epoch 128| loss: 0.22629 | val_0_rmse: 0.45105 | val_1_rmse: 0.45697 |  0:01:04s
epoch 129| loss: 0.22073 | val_0_rmse: 0.46048 | val_1_rmse: 0.46636 |  0:01:04s
epoch 130| loss: 0.22339 | val_0_rmse: 0.44576 | val_1_rmse: 0.45481 |  0:01:04s
epoch 131| loss: 0.21612 | val_0_rmse: 0.44763 | val_1_rmse: 0.4646  |  0:01:05s
epoch 132| loss: 0.21888 | val_0_rmse: 0.44894 | val_1_rmse: 0.45816 |  0:01:05s
epoch 133| loss: 0.22592 | val_0_rmse: 0.45653 | val_1_rmse: 0.46953 |  0:01:06s
epoch 134| loss: 0.22774 | val_0_rmse: 0.44906 | val_1_rmse: 0.46042 |  0:01:06s
epoch 135| loss: 0.22183 | val_0_rmse: 0.45546 | val_1_rmse: 0.4668  |  0:01:07s
epoch 136| loss: 0.22308 | val_0_rmse: 0.45001 | val_1_rmse: 0.46472 |  0:01:07s
epoch 137| loss: 0.2143  | val_0_rmse: 0.45609 | val_1_rmse: 0.4771  |  0:01:08s
epoch 138| loss: 0.22009 | val_0_rmse: 0.44395 | val_1_rmse: 0.4569  |  0:01:08s
epoch 139| loss: 0.2206  | val_0_rmse: 0.43997 | val_1_rmse: 0.45693 |  0:01:09s
epoch 140| loss: 0.2185  | val_0_rmse: 0.44757 | val_1_rmse: 0.46057 |  0:01:09s
epoch 141| loss: 0.21369 | val_0_rmse: 0.4476  | val_1_rmse: 0.46047 |  0:01:10s
epoch 142| loss: 0.21317 | val_0_rmse: 0.44908 | val_1_rmse: 0.46795 |  0:01:10s
epoch 143| loss: 0.21446 | val_0_rmse: 0.44307 | val_1_rmse: 0.45979 |  0:01:11s
epoch 144| loss: 0.21169 | val_0_rmse: 0.44421 | val_1_rmse: 0.45983 |  0:01:11s
epoch 145| loss: 0.21781 | val_0_rmse: 0.45036 | val_1_rmse: 0.47264 |  0:01:12s
epoch 146| loss: 0.2206  | val_0_rmse: 0.44285 | val_1_rmse: 0.46037 |  0:01:12s
epoch 147| loss: 0.21751 | val_0_rmse: 0.45073 | val_1_rmse: 0.46326 |  0:01:13s
epoch 148| loss: 0.21438 | val_0_rmse: 0.45353 | val_1_rmse: 0.47008 |  0:01:13s
epoch 149| loss: 0.22667 | val_0_rmse: 0.43543 | val_1_rmse: 0.45549 |  0:01:14s
Stop training because you reached max_epochs = 150 with best_epoch = 130 and best_val_1_rmse = 0.45481
Best weights from best epoch are automatically used!
ended training at: 14:04:42
Feature importance:
[('Area', 0.3934193845304793), ('Baths', 0.0), ('Beds', 0.0), ('Latitude', 0.36405628836167864), ('Longitude', 0.12854670955212685), ('Month', 0.0), ('Year', 0.1139776175557152)]
Mean squared error is of 7030029723.951119
Mean absolute error:59375.16685914696
MAPE:0.1572330146926391
R2 score:0.7840978712975883
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:04:42
epoch 0  | loss: 0.85679 | val_0_rmse: 0.8525  | val_1_rmse: 0.85608 |  0:00:00s
epoch 1  | loss: 0.40975 | val_0_rmse: 0.68172 | val_1_rmse: 0.69306 |  0:00:00s
epoch 2  | loss: 0.38415 | val_0_rmse: 0.61633 | val_1_rmse: 0.62348 |  0:00:01s
epoch 3  | loss: 0.34428 | val_0_rmse: 0.59044 | val_1_rmse: 0.60834 |  0:00:01s
epoch 4  | loss: 0.31736 | val_0_rmse: 0.56011 | val_1_rmse: 0.56944 |  0:00:02s
epoch 5  | loss: 0.30614 | val_0_rmse: 0.55557 | val_1_rmse: 0.55757 |  0:00:02s
epoch 6  | loss: 0.30003 | val_0_rmse: 0.52693 | val_1_rmse: 0.53006 |  0:00:03s
epoch 7  | loss: 0.2923  | val_0_rmse: 0.51966 | val_1_rmse: 0.52507 |  0:00:04s
epoch 8  | loss: 0.28322 | val_0_rmse: 0.52143 | val_1_rmse: 0.52111 |  0:00:04s
epoch 9  | loss: 0.27935 | val_0_rmse: 0.52527 | val_1_rmse: 0.52853 |  0:00:05s
epoch 10 | loss: 0.2868  | val_0_rmse: 0.53886 | val_1_rmse: 0.54211 |  0:00:05s
epoch 11 | loss: 0.29403 | val_0_rmse: 0.53925 | val_1_rmse: 0.53792 |  0:00:05s
epoch 12 | loss: 0.27865 | val_0_rmse: 0.52118 | val_1_rmse: 0.53507 |  0:00:06s
epoch 13 | loss: 0.28406 | val_0_rmse: 0.52312 | val_1_rmse: 0.53306 |  0:00:06s
epoch 14 | loss: 0.27655 | val_0_rmse: 0.51303 | val_1_rmse: 0.52158 |  0:00:07s
epoch 15 | loss: 0.27397 | val_0_rmse: 0.51161 | val_1_rmse: 0.51575 |  0:00:07s
epoch 16 | loss: 0.27728 | val_0_rmse: 0.51977 | val_1_rmse: 0.52555 |  0:00:08s
epoch 17 | loss: 0.27167 | val_0_rmse: 0.53494 | val_1_rmse: 0.54433 |  0:00:09s
epoch 18 | loss: 0.26952 | val_0_rmse: 0.50589 | val_1_rmse: 0.51929 |  0:00:09s
epoch 19 | loss: 0.27742 | val_0_rmse: 0.52086 | val_1_rmse: 0.53173 |  0:00:09s
epoch 20 | loss: 0.28237 | val_0_rmse: 0.50374 | val_1_rmse: 0.51194 |  0:00:10s
epoch 21 | loss: 0.26647 | val_0_rmse: 0.50859 | val_1_rmse: 0.5138  |  0:00:10s
epoch 22 | loss: 0.26144 | val_0_rmse: 0.5153  | val_1_rmse: 0.51719 |  0:00:11s
epoch 23 | loss: 0.26774 | val_0_rmse: 0.49743 | val_1_rmse: 0.50693 |  0:00:11s
epoch 24 | loss: 0.26444 | val_0_rmse: 0.49896 | val_1_rmse: 0.50985 |  0:00:12s
epoch 25 | loss: 0.26586 | val_0_rmse: 0.51373 | val_1_rmse: 0.512   |  0:00:12s
epoch 26 | loss: 0.26275 | val_0_rmse: 0.49518 | val_1_rmse: 0.50196 |  0:00:13s
epoch 27 | loss: 0.26178 | val_0_rmse: 0.49801 | val_1_rmse: 0.50554 |  0:00:13s
epoch 28 | loss: 0.25679 | val_0_rmse: 0.49812 | val_1_rmse: 0.51206 |  0:00:14s
epoch 29 | loss: 0.25297 | val_0_rmse: 0.50933 | val_1_rmse: 0.51165 |  0:00:14s
epoch 30 | loss: 0.26514 | val_0_rmse: 0.50375 | val_1_rmse: 0.51865 |  0:00:15s
epoch 31 | loss: 0.25473 | val_0_rmse: 0.51805 | val_1_rmse: 0.52552 |  0:00:15s
epoch 32 | loss: 0.25836 | val_0_rmse: 0.49069 | val_1_rmse: 0.49739 |  0:00:16s
epoch 33 | loss: 0.25655 | val_0_rmse: 0.49301 | val_1_rmse: 0.5046  |  0:00:16s
epoch 34 | loss: 0.25336 | val_0_rmse: 0.50089 | val_1_rmse: 0.50717 |  0:00:17s
epoch 35 | loss: 0.25068 | val_0_rmse: 0.49002 | val_1_rmse: 0.50013 |  0:00:17s
epoch 36 | loss: 0.24913 | val_0_rmse: 0.48917 | val_1_rmse: 0.49654 |  0:00:18s
epoch 37 | loss: 0.25904 | val_0_rmse: 0.49435 | val_1_rmse: 0.49753 |  0:00:18s
epoch 38 | loss: 0.26122 | val_0_rmse: 0.49758 | val_1_rmse: 0.50534 |  0:00:19s
epoch 39 | loss: 0.25961 | val_0_rmse: 0.51957 | val_1_rmse: 0.52168 |  0:00:19s
epoch 40 | loss: 0.26623 | val_0_rmse: 0.4872  | val_1_rmse: 0.50271 |  0:00:20s
epoch 41 | loss: 0.26626 | val_0_rmse: 0.51108 | val_1_rmse: 0.52634 |  0:00:20s
epoch 42 | loss: 0.26908 | val_0_rmse: 0.4907  | val_1_rmse: 0.50322 |  0:00:21s
epoch 43 | loss: 0.25528 | val_0_rmse: 0.48843 | val_1_rmse: 0.49905 |  0:00:21s
epoch 44 | loss: 0.24133 | val_0_rmse: 0.48602 | val_1_rmse: 0.49248 |  0:00:22s
epoch 45 | loss: 0.24559 | val_0_rmse: 0.51327 | val_1_rmse: 0.51557 |  0:00:22s
epoch 46 | loss: 0.2534  | val_0_rmse: 0.50709 | val_1_rmse: 0.51649 |  0:00:23s
epoch 47 | loss: 0.24786 | val_0_rmse: 0.47879 | val_1_rmse: 0.49011 |  0:00:23s
epoch 48 | loss: 0.25401 | val_0_rmse: 0.50005 | val_1_rmse: 0.50755 |  0:00:24s
epoch 49 | loss: 0.2461  | val_0_rmse: 0.48747 | val_1_rmse: 0.49544 |  0:00:24s
epoch 50 | loss: 0.24772 | val_0_rmse: 0.48408 | val_1_rmse: 0.50113 |  0:00:25s
epoch 51 | loss: 0.24182 | val_0_rmse: 0.47205 | val_1_rmse: 0.48352 |  0:00:25s
epoch 52 | loss: 0.23382 | val_0_rmse: 0.486   | val_1_rmse: 0.49788 |  0:00:26s
epoch 53 | loss: 0.24254 | val_0_rmse: 0.47028 | val_1_rmse: 0.47411 |  0:00:26s
epoch 54 | loss: 0.23397 | val_0_rmse: 0.47061 | val_1_rmse: 0.48465 |  0:00:27s
epoch 55 | loss: 0.23356 | val_0_rmse: 0.47592 | val_1_rmse: 0.48387 |  0:00:27s
epoch 56 | loss: 0.2317  | val_0_rmse: 0.46545 | val_1_rmse: 0.47065 |  0:00:28s
epoch 57 | loss: 0.23504 | val_0_rmse: 0.47169 | val_1_rmse: 0.477   |  0:00:28s
epoch 58 | loss: 0.23106 | val_0_rmse: 0.47015 | val_1_rmse: 0.48468 |  0:00:29s
epoch 59 | loss: 0.23289 | val_0_rmse: 0.46635 | val_1_rmse: 0.47589 |  0:00:29s
epoch 60 | loss: 0.22585 | val_0_rmse: 0.47001 | val_1_rmse: 0.48331 |  0:00:30s
epoch 61 | loss: 0.24341 | val_0_rmse: 0.50742 | val_1_rmse: 0.51587 |  0:00:30s
epoch 62 | loss: 0.24904 | val_0_rmse: 0.48917 | val_1_rmse: 0.50291 |  0:00:31s
epoch 63 | loss: 0.2441  | val_0_rmse: 0.48831 | val_1_rmse: 0.49786 |  0:00:31s
epoch 64 | loss: 0.23851 | val_0_rmse: 0.48539 | val_1_rmse: 0.50542 |  0:00:32s
epoch 65 | loss: 0.23617 | val_0_rmse: 0.49817 | val_1_rmse: 0.50841 |  0:00:32s
epoch 66 | loss: 0.24128 | val_0_rmse: 0.47112 | val_1_rmse: 0.48759 |  0:00:33s
epoch 67 | loss: 0.23694 | val_0_rmse: 0.47916 | val_1_rmse: 0.49035 |  0:00:33s
epoch 68 | loss: 0.23026 | val_0_rmse: 0.48558 | val_1_rmse: 0.49935 |  0:00:34s
epoch 69 | loss: 0.23711 | val_0_rmse: 0.47    | val_1_rmse: 0.48228 |  0:00:34s
epoch 70 | loss: 0.23706 | val_0_rmse: 0.4647  | val_1_rmse: 0.47616 |  0:00:35s
epoch 71 | loss: 0.22978 | val_0_rmse: 0.4784  | val_1_rmse: 0.4984  |  0:00:35s
epoch 72 | loss: 0.23089 | val_0_rmse: 0.47668 | val_1_rmse: 0.48625 |  0:00:36s
epoch 73 | loss: 0.22827 | val_0_rmse: 0.46137 | val_1_rmse: 0.46893 |  0:00:36s
epoch 74 | loss: 0.22414 | val_0_rmse: 0.47366 | val_1_rmse: 0.48336 |  0:00:37s
epoch 75 | loss: 0.22599 | val_0_rmse: 0.46376 | val_1_rmse: 0.4728  |  0:00:37s
epoch 76 | loss: 0.22962 | val_0_rmse: 0.46783 | val_1_rmse: 0.48241 |  0:00:38s
epoch 77 | loss: 0.22748 | val_0_rmse: 0.47953 | val_1_rmse: 0.49104 |  0:00:38s
epoch 78 | loss: 0.24778 | val_0_rmse: 0.48046 | val_1_rmse: 0.49329 |  0:00:39s
epoch 79 | loss: 0.24131 | val_0_rmse: 0.4884  | val_1_rmse: 0.4973  |  0:00:39s
epoch 80 | loss: 0.24472 | val_0_rmse: 0.46565 | val_1_rmse: 0.47782 |  0:00:40s
epoch 81 | loss: 0.23887 | val_0_rmse: 0.46936 | val_1_rmse: 0.48711 |  0:00:40s
epoch 82 | loss: 0.22571 | val_0_rmse: 0.47367 | val_1_rmse: 0.48418 |  0:00:41s
epoch 83 | loss: 0.22469 | val_0_rmse: 0.46442 | val_1_rmse: 0.48446 |  0:00:41s
epoch 84 | loss: 0.22702 | val_0_rmse: 0.46786 | val_1_rmse: 0.4863  |  0:00:42s
epoch 85 | loss: 0.22595 | val_0_rmse: 0.46064 | val_1_rmse: 0.47299 |  0:00:42s
epoch 86 | loss: 0.21605 | val_0_rmse: 0.45905 | val_1_rmse: 0.46744 |  0:00:43s
epoch 87 | loss: 0.22906 | val_0_rmse: 0.46348 | val_1_rmse: 0.47957 |  0:00:43s
epoch 88 | loss: 0.22548 | val_0_rmse: 0.46098 | val_1_rmse: 0.47478 |  0:00:44s
epoch 89 | loss: 0.22299 | val_0_rmse: 0.47658 | val_1_rmse: 0.48182 |  0:00:44s
epoch 90 | loss: 0.22335 | val_0_rmse: 0.46791 | val_1_rmse: 0.48816 |  0:00:45s
epoch 91 | loss: 0.22902 | val_0_rmse: 0.46545 | val_1_rmse: 0.47904 |  0:00:45s
epoch 92 | loss: 0.22642 | val_0_rmse: 0.4641  | val_1_rmse: 0.48123 |  0:00:46s
epoch 93 | loss: 0.22968 | val_0_rmse: 0.46702 | val_1_rmse: 0.48328 |  0:00:46s
epoch 94 | loss: 0.22903 | val_0_rmse: 0.46866 | val_1_rmse: 0.48278 |  0:00:47s
epoch 95 | loss: 0.22788 | val_0_rmse: 0.47881 | val_1_rmse: 0.49298 |  0:00:47s
epoch 96 | loss: 0.22731 | val_0_rmse: 0.46544 | val_1_rmse: 0.47903 |  0:00:48s
epoch 97 | loss: 0.22901 | val_0_rmse: 0.46881 | val_1_rmse: 0.48615 |  0:00:48s
epoch 98 | loss: 0.23363 | val_0_rmse: 0.4737  | val_1_rmse: 0.49245 |  0:00:49s
epoch 99 | loss: 0.23133 | val_0_rmse: 0.46948 | val_1_rmse: 0.49178 |  0:00:49s
epoch 100| loss: 0.23424 | val_0_rmse: 0.46425 | val_1_rmse: 0.47868 |  0:00:50s
epoch 101| loss: 0.2264  | val_0_rmse: 0.4663  | val_1_rmse: 0.48253 |  0:00:50s
epoch 102| loss: 0.22218 | val_0_rmse: 0.4619  | val_1_rmse: 0.47397 |  0:00:51s
epoch 103| loss: 0.2286  | val_0_rmse: 0.46372 | val_1_rmse: 0.47352 |  0:00:51s
epoch 104| loss: 0.23079 | val_0_rmse: 0.48935 | val_1_rmse: 0.50671 |  0:00:52s
epoch 105| loss: 0.23837 | val_0_rmse: 0.46942 | val_1_rmse: 0.48174 |  0:00:52s
epoch 106| loss: 0.23153 | val_0_rmse: 0.46464 | val_1_rmse: 0.47908 |  0:00:53s
epoch 107| loss: 0.22938 | val_0_rmse: 0.46483 | val_1_rmse: 0.47578 |  0:00:53s
epoch 108| loss: 0.22503 | val_0_rmse: 0.46767 | val_1_rmse: 0.48437 |  0:00:54s
epoch 109| loss: 0.22872 | val_0_rmse: 0.47689 | val_1_rmse: 0.48664 |  0:00:54s
epoch 110| loss: 0.23441 | val_0_rmse: 0.46744 | val_1_rmse: 0.48372 |  0:00:55s
epoch 111| loss: 0.22724 | val_0_rmse: 0.4688  | val_1_rmse: 0.48213 |  0:00:55s
epoch 112| loss: 0.22445 | val_0_rmse: 0.46377 | val_1_rmse: 0.47911 |  0:00:56s
epoch 113| loss: 0.22904 | val_0_rmse: 0.464   | val_1_rmse: 0.48063 |  0:00:56s
epoch 114| loss: 0.22485 | val_0_rmse: 0.45658 | val_1_rmse: 0.47646 |  0:00:57s
epoch 115| loss: 0.2293  | val_0_rmse: 0.5062  | val_1_rmse: 0.52305 |  0:00:57s
epoch 116| loss: 0.23062 | val_0_rmse: 0.48676 | val_1_rmse: 0.50595 |  0:00:57s

Early stopping occured at epoch 116 with best_epoch = 86 and best_val_1_rmse = 0.46744
Best weights from best epoch are automatically used!
ended training at: 14:05:40
Feature importance:
[('Area', 0.4146439454233931), ('Baths', 0.0), ('Beds', 0.016828047567515474), ('Latitude', 0.37406109215698713), ('Longitude', 0.0744199058580153), ('Month', 0.015555893629400661), ('Year', 0.10449111536468834)]
Mean squared error is of 6625491946.227506
Mean absolute error:58831.37354158279
MAPE:0.15884280767364958
R2 score:0.7887005574322361
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:05:40
epoch 0  | loss: 0.83976 | val_0_rmse: 1.00983 | val_1_rmse: 1.02099 |  0:00:00s
epoch 1  | loss: 0.42313 | val_0_rmse: 0.66139 | val_1_rmse: 0.68079 |  0:00:00s
epoch 2  | loss: 0.35768 | val_0_rmse: 0.67712 | val_1_rmse: 0.7036  |  0:00:01s
epoch 3  | loss: 0.32771 | val_0_rmse: 0.67395 | val_1_rmse: 0.69009 |  0:00:01s
epoch 4  | loss: 0.30768 | val_0_rmse: 0.59948 | val_1_rmse: 0.62064 |  0:00:02s
epoch 5  | loss: 0.30202 | val_0_rmse: 0.55915 | val_1_rmse: 0.57673 |  0:00:03s
epoch 6  | loss: 0.29954 | val_0_rmse: 0.54457 | val_1_rmse: 0.54122 |  0:00:03s
epoch 7  | loss: 0.30726 | val_0_rmse: 0.54336 | val_1_rmse: 0.55023 |  0:00:03s
epoch 8  | loss: 0.30229 | val_0_rmse: 0.53924 | val_1_rmse: 0.53862 |  0:00:04s
epoch 9  | loss: 0.29643 | val_0_rmse: 0.53929 | val_1_rmse: 0.54479 |  0:00:04s
epoch 10 | loss: 0.29986 | val_0_rmse: 0.5346  | val_1_rmse: 0.54354 |  0:00:05s
epoch 11 | loss: 0.2896  | val_0_rmse: 0.52262 | val_1_rmse: 0.53361 |  0:00:05s
epoch 12 | loss: 0.28696 | val_0_rmse: 0.52522 | val_1_rmse: 0.54221 |  0:00:06s
epoch 13 | loss: 0.28899 | val_0_rmse: 0.51355 | val_1_rmse: 0.52463 |  0:00:06s
epoch 14 | loss: 0.28605 | val_0_rmse: 0.51465 | val_1_rmse: 0.52832 |  0:00:07s
epoch 15 | loss: 0.27662 | val_0_rmse: 0.5228  | val_1_rmse: 0.54104 |  0:00:07s
epoch 16 | loss: 0.27439 | val_0_rmse: 0.51461 | val_1_rmse: 0.52624 |  0:00:08s
epoch 17 | loss: 0.27842 | val_0_rmse: 0.52768 | val_1_rmse: 0.54844 |  0:00:08s
epoch 18 | loss: 0.2819  | val_0_rmse: 0.51524 | val_1_rmse: 0.53267 |  0:00:09s
epoch 19 | loss: 0.27454 | val_0_rmse: 0.51059 | val_1_rmse: 0.52244 |  0:00:09s
epoch 20 | loss: 0.27459 | val_0_rmse: 0.52127 | val_1_rmse: 0.53476 |  0:00:10s
epoch 21 | loss: 0.26884 | val_0_rmse: 0.50402 | val_1_rmse: 0.51536 |  0:00:10s
epoch 22 | loss: 0.26591 | val_0_rmse: 0.50824 | val_1_rmse: 0.52237 |  0:00:11s
epoch 23 | loss: 0.26594 | val_0_rmse: 0.51262 | val_1_rmse: 0.5251  |  0:00:11s
epoch 24 | loss: 0.26643 | val_0_rmse: 0.5164  | val_1_rmse: 0.53376 |  0:00:12s
epoch 25 | loss: 0.26603 | val_0_rmse: 0.50579 | val_1_rmse: 0.52387 |  0:00:12s
epoch 26 | loss: 0.26234 | val_0_rmse: 0.51047 | val_1_rmse: 0.52019 |  0:00:13s
epoch 27 | loss: 0.26924 | val_0_rmse: 0.50125 | val_1_rmse: 0.51567 |  0:00:13s
epoch 28 | loss: 0.26814 | val_0_rmse: 0.50944 | val_1_rmse: 0.52513 |  0:00:14s
epoch 29 | loss: 0.26477 | val_0_rmse: 0.51418 | val_1_rmse: 0.52988 |  0:00:14s
epoch 30 | loss: 0.26407 | val_0_rmse: 0.50804 | val_1_rmse: 0.52434 |  0:00:15s
epoch 31 | loss: 0.26445 | val_0_rmse: 0.50439 | val_1_rmse: 0.51464 |  0:00:15s
epoch 32 | loss: 0.27021 | val_0_rmse: 0.50473 | val_1_rmse: 0.5174  |  0:00:16s
epoch 33 | loss: 0.26629 | val_0_rmse: 0.53852 | val_1_rmse: 0.54314 |  0:00:16s
epoch 34 | loss: 0.28665 | val_0_rmse: 0.54587 | val_1_rmse: 0.54756 |  0:00:17s
epoch 35 | loss: 0.27816 | val_0_rmse: 0.50554 | val_1_rmse: 0.51676 |  0:00:17s
epoch 36 | loss: 0.26891 | val_0_rmse: 0.50272 | val_1_rmse: 0.51162 |  0:00:18s
epoch 37 | loss: 0.26151 | val_0_rmse: 0.49709 | val_1_rmse: 0.5107  |  0:00:18s
epoch 38 | loss: 0.26764 | val_0_rmse: 0.50412 | val_1_rmse: 0.51673 |  0:00:19s
epoch 39 | loss: 0.26778 | val_0_rmse: 0.50159 | val_1_rmse: 0.51367 |  0:00:19s
epoch 40 | loss: 0.26276 | val_0_rmse: 0.50426 | val_1_rmse: 0.52091 |  0:00:20s
epoch 41 | loss: 0.26159 | val_0_rmse: 0.50203 | val_1_rmse: 0.51363 |  0:00:20s
epoch 42 | loss: 0.27028 | val_0_rmse: 0.50138 | val_1_rmse: 0.51939 |  0:00:21s
epoch 43 | loss: 0.25935 | val_0_rmse: 0.49736 | val_1_rmse: 0.50714 |  0:00:21s
epoch 44 | loss: 0.26059 | val_0_rmse: 0.49918 | val_1_rmse: 0.51428 |  0:00:22s
epoch 45 | loss: 0.26213 | val_0_rmse: 0.51122 | val_1_rmse: 0.52214 |  0:00:22s
epoch 46 | loss: 0.25904 | val_0_rmse: 0.49627 | val_1_rmse: 0.50632 |  0:00:23s
epoch 47 | loss: 0.25572 | val_0_rmse: 0.49239 | val_1_rmse: 0.49847 |  0:00:23s
epoch 48 | loss: 0.25577 | val_0_rmse: 0.49017 | val_1_rmse: 0.49902 |  0:00:24s
epoch 49 | loss: 0.25372 | val_0_rmse: 0.48846 | val_1_rmse: 0.50325 |  0:00:24s
epoch 50 | loss: 0.25077 | val_0_rmse: 0.50076 | val_1_rmse: 0.50345 |  0:00:25s
epoch 51 | loss: 0.25685 | val_0_rmse: 0.49777 | val_1_rmse: 0.51312 |  0:00:25s
epoch 52 | loss: 0.26449 | val_0_rmse: 0.50695 | val_1_rmse: 0.50988 |  0:00:26s
epoch 53 | loss: 0.27412 | val_0_rmse: 0.49079 | val_1_rmse: 0.49474 |  0:00:26s
epoch 54 | loss: 0.27172 | val_0_rmse: 0.5081  | val_1_rmse: 0.51929 |  0:00:27s
epoch 55 | loss: 0.25829 | val_0_rmse: 0.51585 | val_1_rmse: 0.52532 |  0:00:27s
epoch 56 | loss: 0.25349 | val_0_rmse: 0.49312 | val_1_rmse: 0.50183 |  0:00:28s
epoch 57 | loss: 0.25646 | val_0_rmse: 0.50421 | val_1_rmse: 0.50683 |  0:00:28s
epoch 58 | loss: 0.25727 | val_0_rmse: 0.5108  | val_1_rmse: 0.52963 |  0:00:29s
epoch 59 | loss: 0.25254 | val_0_rmse: 0.5036  | val_1_rmse: 0.51011 |  0:00:29s
epoch 60 | loss: 0.24862 | val_0_rmse: 0.48183 | val_1_rmse: 0.49226 |  0:00:30s
epoch 61 | loss: 0.24433 | val_0_rmse: 0.48667 | val_1_rmse: 0.4977  |  0:00:30s
epoch 62 | loss: 0.24282 | val_0_rmse: 0.48105 | val_1_rmse: 0.48887 |  0:00:31s
epoch 63 | loss: 0.24339 | val_0_rmse: 0.48529 | val_1_rmse: 0.4965  |  0:00:31s
epoch 64 | loss: 0.24872 | val_0_rmse: 0.48615 | val_1_rmse: 0.50245 |  0:00:32s
epoch 65 | loss: 0.25529 | val_0_rmse: 0.58044 | val_1_rmse: 0.57838 |  0:00:32s
epoch 66 | loss: 0.26071 | val_0_rmse: 0.50881 | val_1_rmse: 0.5064  |  0:00:33s
epoch 67 | loss: 0.25189 | val_0_rmse: 0.49879 | val_1_rmse: 0.50418 |  0:00:33s
epoch 68 | loss: 0.25066 | val_0_rmse: 0.49185 | val_1_rmse: 0.50136 |  0:00:34s
epoch 69 | loss: 0.24469 | val_0_rmse: 0.47672 | val_1_rmse: 0.48677 |  0:00:34s
epoch 70 | loss: 0.23911 | val_0_rmse: 0.46932 | val_1_rmse: 0.47873 |  0:00:35s
epoch 71 | loss: 0.2372  | val_0_rmse: 0.47803 | val_1_rmse: 0.494   |  0:00:35s
epoch 72 | loss: 0.23802 | val_0_rmse: 0.48682 | val_1_rmse: 0.5004  |  0:00:36s
epoch 73 | loss: 0.23742 | val_0_rmse: 0.482   | val_1_rmse: 0.49201 |  0:00:36s
epoch 74 | loss: 0.24872 | val_0_rmse: 0.49295 | val_1_rmse: 0.50382 |  0:00:37s
epoch 75 | loss: 0.26398 | val_0_rmse: 0.47008 | val_1_rmse: 0.48092 |  0:00:37s
epoch 76 | loss: 0.24341 | val_0_rmse: 0.47668 | val_1_rmse: 0.48635 |  0:00:38s
epoch 77 | loss: 0.23706 | val_0_rmse: 0.47725 | val_1_rmse: 0.48536 |  0:00:38s
epoch 78 | loss: 0.24359 | val_0_rmse: 0.47897 | val_1_rmse: 0.48627 |  0:00:39s
epoch 79 | loss: 0.24829 | val_0_rmse: 0.47222 | val_1_rmse: 0.48425 |  0:00:39s
epoch 80 | loss: 0.23389 | val_0_rmse: 0.46493 | val_1_rmse: 0.47981 |  0:00:40s
epoch 81 | loss: 0.23798 | val_0_rmse: 0.47005 | val_1_rmse: 0.48334 |  0:00:40s
epoch 82 | loss: 0.23146 | val_0_rmse: 0.47244 | val_1_rmse: 0.48017 |  0:00:41s
epoch 83 | loss: 0.23229 | val_0_rmse: 0.4763  | val_1_rmse: 0.48822 |  0:00:41s
epoch 84 | loss: 0.22904 | val_0_rmse: 0.46766 | val_1_rmse: 0.48425 |  0:00:42s
epoch 85 | loss: 0.23071 | val_0_rmse: 0.4645  | val_1_rmse: 0.47331 |  0:00:42s
epoch 86 | loss: 0.2246  | val_0_rmse: 0.46549 | val_1_rmse: 0.48535 |  0:00:43s
epoch 87 | loss: 0.22961 | val_0_rmse: 0.45497 | val_1_rmse: 0.46675 |  0:00:43s
epoch 88 | loss: 0.22324 | val_0_rmse: 0.45027 | val_1_rmse: 0.46443 |  0:00:44s
epoch 89 | loss: 0.22442 | val_0_rmse: 0.4588  | val_1_rmse: 0.47071 |  0:00:44s
epoch 90 | loss: 0.22803 | val_0_rmse: 0.46089 | val_1_rmse: 0.47467 |  0:00:45s
epoch 91 | loss: 0.23053 | val_0_rmse: 0.44934 | val_1_rmse: 0.46342 |  0:00:45s
epoch 92 | loss: 0.22003 | val_0_rmse: 0.45141 | val_1_rmse: 0.46238 |  0:00:46s
epoch 93 | loss: 0.22097 | val_0_rmse: 0.45981 | val_1_rmse: 0.47271 |  0:00:46s
epoch 94 | loss: 0.23467 | val_0_rmse: 0.46335 | val_1_rmse: 0.47571 |  0:00:47s
epoch 95 | loss: 0.22479 | val_0_rmse: 0.47241 | val_1_rmse: 0.48783 |  0:00:47s
epoch 96 | loss: 0.24226 | val_0_rmse: 0.45977 | val_1_rmse: 0.47254 |  0:00:48s
epoch 97 | loss: 0.23708 | val_0_rmse: 0.46475 | val_1_rmse: 0.47797 |  0:00:48s
epoch 98 | loss: 0.22357 | val_0_rmse: 0.45516 | val_1_rmse: 0.46905 |  0:00:48s
epoch 99 | loss: 0.2202  | val_0_rmse: 0.4545  | val_1_rmse: 0.46928 |  0:00:49s
epoch 100| loss: 0.22554 | val_0_rmse: 0.45331 | val_1_rmse: 0.46562 |  0:00:50s
epoch 101| loss: 0.22508 | val_0_rmse: 0.45425 | val_1_rmse: 0.46787 |  0:00:50s
epoch 102| loss: 0.21812 | val_0_rmse: 0.45316 | val_1_rmse: 0.46954 |  0:00:50s
epoch 103| loss: 0.21963 | val_0_rmse: 0.46191 | val_1_rmse: 0.48034 |  0:00:51s
epoch 104| loss: 0.22028 | val_0_rmse: 0.4642  | val_1_rmse: 0.47876 |  0:00:51s
epoch 105| loss: 0.22268 | val_0_rmse: 0.45375 | val_1_rmse: 0.47078 |  0:00:52s
epoch 106| loss: 0.21768 | val_0_rmse: 0.44659 | val_1_rmse: 0.45779 |  0:00:52s
epoch 107| loss: 0.21211 | val_0_rmse: 0.4451  | val_1_rmse: 0.46479 |  0:00:53s
epoch 108| loss: 0.22083 | val_0_rmse: 0.44748 | val_1_rmse: 0.45889 |  0:00:53s
epoch 109| loss: 0.22282 | val_0_rmse: 0.46348 | val_1_rmse: 0.47514 |  0:00:54s
epoch 110| loss: 0.23208 | val_0_rmse: 0.46399 | val_1_rmse: 0.48092 |  0:00:54s
epoch 111| loss: 0.22415 | val_0_rmse: 0.46281 | val_1_rmse: 0.47363 |  0:00:55s
epoch 112| loss: 0.22825 | val_0_rmse: 0.4643  | val_1_rmse: 0.47585 |  0:00:55s
epoch 113| loss: 0.21945 | val_0_rmse: 0.45885 | val_1_rmse: 0.47567 |  0:00:56s
epoch 114| loss: 0.22421 | val_0_rmse: 0.46298 | val_1_rmse: 0.47975 |  0:00:56s
epoch 115| loss: 0.23287 | val_0_rmse: 0.45914 | val_1_rmse: 0.47719 |  0:00:57s
epoch 116| loss: 0.2236  | val_0_rmse: 0.45276 | val_1_rmse: 0.46887 |  0:00:57s
epoch 117| loss: 0.21884 | val_0_rmse: 0.45991 | val_1_rmse: 0.47702 |  0:00:58s
epoch 118| loss: 0.22968 | val_0_rmse: 0.46411 | val_1_rmse: 0.47866 |  0:00:58s
epoch 119| loss: 0.22295 | val_0_rmse: 0.47589 | val_1_rmse: 0.49647 |  0:00:59s
epoch 120| loss: 0.23915 | val_0_rmse: 0.4704  | val_1_rmse: 0.48285 |  0:00:59s
epoch 121| loss: 0.2301  | val_0_rmse: 0.47377 | val_1_rmse: 0.49653 |  0:01:00s
epoch 122| loss: 0.22888 | val_0_rmse: 0.45946 | val_1_rmse: 0.48066 |  0:01:00s
epoch 123| loss: 0.22278 | val_0_rmse: 0.46652 | val_1_rmse: 0.48185 |  0:01:01s
epoch 124| loss: 0.2232  | val_0_rmse: 0.45589 | val_1_rmse: 0.47436 |  0:01:01s
epoch 125| loss: 0.22206 | val_0_rmse: 0.44563 | val_1_rmse: 0.46015 |  0:01:02s
epoch 126| loss: 0.21678 | val_0_rmse: 0.44884 | val_1_rmse: 0.46519 |  0:01:02s
epoch 127| loss: 0.22121 | val_0_rmse: 0.44442 | val_1_rmse: 0.45938 |  0:01:03s
epoch 128| loss: 0.21952 | val_0_rmse: 0.45133 | val_1_rmse: 0.46769 |  0:01:03s
epoch 129| loss: 0.21874 | val_0_rmse: 0.44777 | val_1_rmse: 0.46241 |  0:01:04s
epoch 130| loss: 0.21858 | val_0_rmse: 0.45776 | val_1_rmse: 0.4687  |  0:01:04s
epoch 131| loss: 0.2223  | val_0_rmse: 0.4521  | val_1_rmse: 0.46748 |  0:01:05s
epoch 132| loss: 0.23222 | val_0_rmse: 0.44773 | val_1_rmse: 0.46425 |  0:01:05s
epoch 133| loss: 0.21629 | val_0_rmse: 0.44632 | val_1_rmse: 0.46667 |  0:01:06s
epoch 134| loss: 0.21267 | val_0_rmse: 0.44608 | val_1_rmse: 0.46687 |  0:01:06s
epoch 135| loss: 0.21786 | val_0_rmse: 0.44681 | val_1_rmse: 0.46268 |  0:01:07s
epoch 136| loss: 0.21062 | val_0_rmse: 0.44414 | val_1_rmse: 0.46522 |  0:01:07s

Early stopping occured at epoch 136 with best_epoch = 106 and best_val_1_rmse = 0.45779
Best weights from best epoch are automatically used!
ended training at: 14:06:48
Feature importance:
[('Area', 0.39400680542728916), ('Baths', 0.037439969103970364), ('Beds', 0.0), ('Latitude', 0.3684145364160603), ('Longitude', 0.17929392757729953), ('Month', 0.0), ('Year', 0.020844761475380613)]
Mean squared error is of 6252575033.677188
Mean absolute error:55649.36932014343
MAPE:0.15056655918955564
R2 score:0.8003880399129766
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:06:48
epoch 0  | loss: 0.80554 | val_0_rmse: 0.99346 | val_1_rmse: 0.98215 |  0:00:00s
epoch 1  | loss: 0.42887 | val_0_rmse: 0.77927 | val_1_rmse: 0.77559 |  0:00:00s
epoch 2  | loss: 0.37545 | val_0_rmse: 0.6341  | val_1_rmse: 0.63643 |  0:00:01s
epoch 3  | loss: 0.32978 | val_0_rmse: 0.58879 | val_1_rmse: 0.59447 |  0:00:01s
epoch 4  | loss: 0.31708 | val_0_rmse: 0.54262 | val_1_rmse: 0.54102 |  0:00:02s
epoch 5  | loss: 0.28672 | val_0_rmse: 0.52927 | val_1_rmse: 0.52114 |  0:00:03s
epoch 6  | loss: 0.28065 | val_0_rmse: 0.51449 | val_1_rmse: 0.51092 |  0:00:03s
epoch 7  | loss: 0.27301 | val_0_rmse: 0.53893 | val_1_rmse: 0.53698 |  0:00:04s
epoch 8  | loss: 0.2841  | val_0_rmse: 0.56002 | val_1_rmse: 0.56749 |  0:00:04s
epoch 9  | loss: 0.28371 | val_0_rmse: 0.50886 | val_1_rmse: 0.50943 |  0:00:05s
epoch 10 | loss: 0.29397 | val_0_rmse: 0.50857 | val_1_rmse: 0.50911 |  0:00:05s
epoch 11 | loss: 0.28083 | val_0_rmse: 0.5332  | val_1_rmse: 0.5298  |  0:00:06s
epoch 12 | loss: 0.28502 | val_0_rmse: 0.50143 | val_1_rmse: 0.50503 |  0:00:06s
epoch 13 | loss: 0.27404 | val_0_rmse: 0.50819 | val_1_rmse: 0.51417 |  0:00:06s
epoch 14 | loss: 0.28447 | val_0_rmse: 0.52644 | val_1_rmse: 0.52733 |  0:00:07s
epoch 15 | loss: 0.27353 | val_0_rmse: 0.50751 | val_1_rmse: 0.51439 |  0:00:08s
epoch 16 | loss: 0.2712  | val_0_rmse: 0.49881 | val_1_rmse: 0.50254 |  0:00:08s
epoch 17 | loss: 0.26434 | val_0_rmse: 0.49609 | val_1_rmse: 0.50017 |  0:00:09s
epoch 18 | loss: 0.26239 | val_0_rmse: 0.50393 | val_1_rmse: 0.5095  |  0:00:09s
epoch 19 | loss: 0.26319 | val_0_rmse: 0.4992  | val_1_rmse: 0.49495 |  0:00:10s
epoch 20 | loss: 0.26429 | val_0_rmse: 0.49821 | val_1_rmse: 0.50075 |  0:00:10s
epoch 21 | loss: 0.26652 | val_0_rmse: 0.49697 | val_1_rmse: 0.49743 |  0:00:10s
epoch 22 | loss: 0.25781 | val_0_rmse: 0.49524 | val_1_rmse: 0.4961  |  0:00:11s
epoch 23 | loss: 0.25809 | val_0_rmse: 0.49127 | val_1_rmse: 0.49195 |  0:00:11s
epoch 24 | loss: 0.2605  | val_0_rmse: 0.49243 | val_1_rmse: 0.49717 |  0:00:12s
epoch 25 | loss: 0.2628  | val_0_rmse: 0.49941 | val_1_rmse: 0.49958 |  0:00:13s
epoch 26 | loss: 0.25753 | val_0_rmse: 0.50624 | val_1_rmse: 0.50943 |  0:00:13s
epoch 27 | loss: 0.25935 | val_0_rmse: 0.49467 | val_1_rmse: 0.49323 |  0:00:14s
epoch 28 | loss: 0.26444 | val_0_rmse: 0.49781 | val_1_rmse: 0.50139 |  0:00:14s
epoch 29 | loss: 0.2598  | val_0_rmse: 0.49556 | val_1_rmse: 0.49329 |  0:00:14s
epoch 30 | loss: 0.2605  | val_0_rmse: 0.49458 | val_1_rmse: 0.49054 |  0:00:15s
epoch 31 | loss: 0.25415 | val_0_rmse: 0.48758 | val_1_rmse: 0.48801 |  0:00:15s
epoch 32 | loss: 0.24992 | val_0_rmse: 0.49782 | val_1_rmse: 0.50223 |  0:00:16s
epoch 33 | loss: 0.25863 | val_0_rmse: 0.51013 | val_1_rmse: 0.52082 |  0:00:16s
epoch 34 | loss: 0.26004 | val_0_rmse: 0.49192 | val_1_rmse: 0.4928  |  0:00:17s
epoch 35 | loss: 0.26159 | val_0_rmse: 0.49704 | val_1_rmse: 0.49946 |  0:00:18s
epoch 36 | loss: 0.25832 | val_0_rmse: 0.49814 | val_1_rmse: 0.49174 |  0:00:18s
epoch 37 | loss: 0.25304 | val_0_rmse: 0.50135 | val_1_rmse: 0.5052  |  0:00:18s
epoch 38 | loss: 0.25132 | val_0_rmse: 0.48251 | val_1_rmse: 0.48462 |  0:00:19s
epoch 39 | loss: 0.25636 | val_0_rmse: 0.48946 | val_1_rmse: 0.48909 |  0:00:19s
epoch 40 | loss: 0.2485  | val_0_rmse: 0.48373 | val_1_rmse: 0.48528 |  0:00:20s
epoch 41 | loss: 0.25507 | val_0_rmse: 0.49371 | val_1_rmse: 0.492   |  0:00:20s
epoch 42 | loss: 0.25391 | val_0_rmse: 0.48362 | val_1_rmse: 0.48792 |  0:00:21s
epoch 43 | loss: 0.25909 | val_0_rmse: 0.52242 | val_1_rmse: 0.53265 |  0:00:21s
epoch 44 | loss: 0.25639 | val_0_rmse: 0.48308 | val_1_rmse: 0.48121 |  0:00:22s
epoch 45 | loss: 0.25276 | val_0_rmse: 0.49463 | val_1_rmse: 0.49059 |  0:00:22s
epoch 46 | loss: 0.25416 | val_0_rmse: 0.48499 | val_1_rmse: 0.4839  |  0:00:23s
epoch 47 | loss: 0.24775 | val_0_rmse: 0.48602 | val_1_rmse: 0.48983 |  0:00:23s
epoch 48 | loss: 0.25212 | val_0_rmse: 0.48597 | val_1_rmse: 0.49042 |  0:00:24s
epoch 49 | loss: 0.24807 | val_0_rmse: 0.4819  | val_1_rmse: 0.484   |  0:00:24s
epoch 50 | loss: 0.24257 | val_0_rmse: 0.47985 | val_1_rmse: 0.48499 |  0:00:25s
epoch 51 | loss: 0.24843 | val_0_rmse: 0.48415 | val_1_rmse: 0.49213 |  0:00:25s
epoch 52 | loss: 0.2456  | val_0_rmse: 0.48037 | val_1_rmse: 0.4817  |  0:00:26s
epoch 53 | loss: 0.24604 | val_0_rmse: 0.48233 | val_1_rmse: 0.4836  |  0:00:26s
epoch 54 | loss: 0.24652 | val_0_rmse: 0.48888 | val_1_rmse: 0.49066 |  0:00:27s
epoch 55 | loss: 0.24516 | val_0_rmse: 0.48187 | val_1_rmse: 0.48231 |  0:00:27s
epoch 56 | loss: 0.24826 | val_0_rmse: 0.50931 | val_1_rmse: 0.51811 |  0:00:28s
epoch 57 | loss: 0.24743 | val_0_rmse: 0.48182 | val_1_rmse: 0.48327 |  0:00:28s
epoch 58 | loss: 0.2476  | val_0_rmse: 0.48967 | val_1_rmse: 0.48987 |  0:00:29s
epoch 59 | loss: 0.25744 | val_0_rmse: 0.47442 | val_1_rmse: 0.47974 |  0:00:29s
epoch 60 | loss: 0.24357 | val_0_rmse: 0.48023 | val_1_rmse: 0.48322 |  0:00:30s
epoch 61 | loss: 0.24074 | val_0_rmse: 0.49044 | val_1_rmse: 0.49612 |  0:00:30s
epoch 62 | loss: 0.24839 | val_0_rmse: 0.48494 | val_1_rmse: 0.48339 |  0:00:31s
epoch 63 | loss: 0.24556 | val_0_rmse: 0.48506 | val_1_rmse: 0.48703 |  0:00:31s
epoch 64 | loss: 0.24497 | val_0_rmse: 0.48096 | val_1_rmse: 0.48714 |  0:00:32s
epoch 65 | loss: 0.23378 | val_0_rmse: 0.47111 | val_1_rmse: 0.4854  |  0:00:32s
epoch 66 | loss: 0.23921 | val_0_rmse: 0.47574 | val_1_rmse: 0.48687 |  0:00:33s
epoch 67 | loss: 0.23541 | val_0_rmse: 0.48583 | val_1_rmse: 0.49611 |  0:00:33s
epoch 68 | loss: 0.23971 | val_0_rmse: 0.47096 | val_1_rmse: 0.47763 |  0:00:34s
epoch 69 | loss: 0.23916 | val_0_rmse: 0.46771 | val_1_rmse: 0.47323 |  0:00:34s
epoch 70 | loss: 0.23217 | val_0_rmse: 0.46404 | val_1_rmse: 0.47157 |  0:00:35s
epoch 71 | loss: 0.23031 | val_0_rmse: 0.46375 | val_1_rmse: 0.47813 |  0:00:35s
epoch 72 | loss: 0.23299 | val_0_rmse: 0.47311 | val_1_rmse: 0.47671 |  0:00:36s
epoch 73 | loss: 0.23162 | val_0_rmse: 0.46034 | val_1_rmse: 0.47498 |  0:00:36s
epoch 74 | loss: 0.23187 | val_0_rmse: 0.48015 | val_1_rmse: 0.49067 |  0:00:37s
epoch 75 | loss: 0.23633 | val_0_rmse: 0.4807  | val_1_rmse: 0.49512 |  0:00:37s
epoch 76 | loss: 0.23322 | val_0_rmse: 0.4671  | val_1_rmse: 0.47104 |  0:00:38s
epoch 77 | loss: 0.22562 | val_0_rmse: 0.46955 | val_1_rmse: 0.48095 |  0:00:38s
epoch 78 | loss: 0.2314  | val_0_rmse: 0.4627  | val_1_rmse: 0.47017 |  0:00:39s
epoch 79 | loss: 0.23089 | val_0_rmse: 0.45881 | val_1_rmse: 0.46905 |  0:00:39s
epoch 80 | loss: 0.23198 | val_0_rmse: 0.47604 | val_1_rmse: 0.48688 |  0:00:40s
epoch 81 | loss: 0.2306  | val_0_rmse: 0.46731 | val_1_rmse: 0.48002 |  0:00:40s
epoch 82 | loss: 0.23144 | val_0_rmse: 0.45876 | val_1_rmse: 0.47158 |  0:00:41s
epoch 83 | loss: 0.22872 | val_0_rmse: 0.46048 | val_1_rmse: 0.4715  |  0:00:41s
epoch 84 | loss: 0.22891 | val_0_rmse: 0.46076 | val_1_rmse: 0.46898 |  0:00:42s
epoch 85 | loss: 0.22309 | val_0_rmse: 0.45739 | val_1_rmse: 0.46701 |  0:00:42s
epoch 86 | loss: 0.23207 | val_0_rmse: 0.4541  | val_1_rmse: 0.46582 |  0:00:43s
epoch 87 | loss: 0.2323  | val_0_rmse: 0.4841  | val_1_rmse: 0.49077 |  0:00:43s
epoch 88 | loss: 0.23585 | val_0_rmse: 0.46437 | val_1_rmse: 0.47537 |  0:00:44s
epoch 89 | loss: 0.23438 | val_0_rmse: 0.46175 | val_1_rmse: 0.47433 |  0:00:44s
epoch 90 | loss: 0.22724 | val_0_rmse: 0.45998 | val_1_rmse: 0.46261 |  0:00:45s
epoch 91 | loss: 0.22616 | val_0_rmse: 0.46031 | val_1_rmse: 0.46567 |  0:00:45s
epoch 92 | loss: 0.23233 | val_0_rmse: 0.45773 | val_1_rmse: 0.47073 |  0:00:46s
epoch 93 | loss: 0.22336 | val_0_rmse: 0.45204 | val_1_rmse: 0.46088 |  0:00:46s
epoch 94 | loss: 0.2246  | val_0_rmse: 0.4509  | val_1_rmse: 0.45964 |  0:00:47s
epoch 95 | loss: 0.21903 | val_0_rmse: 0.47401 | val_1_rmse: 0.49036 |  0:00:47s
epoch 96 | loss: 0.22473 | val_0_rmse: 0.45688 | val_1_rmse: 0.46974 |  0:00:48s
epoch 97 | loss: 0.21631 | val_0_rmse: 0.46287 | val_1_rmse: 0.47422 |  0:00:48s
epoch 98 | loss: 0.21809 | val_0_rmse: 0.45055 | val_1_rmse: 0.46147 |  0:00:49s
epoch 99 | loss: 0.21625 | val_0_rmse: 0.45094 | val_1_rmse: 0.46096 |  0:00:49s
epoch 100| loss: 0.22099 | val_0_rmse: 0.46064 | val_1_rmse: 0.47777 |  0:00:50s
epoch 101| loss: 0.2165  | val_0_rmse: 0.44542 | val_1_rmse: 0.46099 |  0:00:50s
epoch 102| loss: 0.22122 | val_0_rmse: 0.45008 | val_1_rmse: 0.45799 |  0:00:51s
epoch 103| loss: 0.21512 | val_0_rmse: 0.44489 | val_1_rmse: 0.45264 |  0:00:51s
epoch 104| loss: 0.21965 | val_0_rmse: 0.4515  | val_1_rmse: 0.46619 |  0:00:52s
epoch 105| loss: 0.2155  | val_0_rmse: 0.45443 | val_1_rmse: 0.47158 |  0:00:52s
epoch 106| loss: 0.21821 | val_0_rmse: 0.4393  | val_1_rmse: 0.45089 |  0:00:53s
epoch 107| loss: 0.21336 | val_0_rmse: 0.4461  | val_1_rmse: 0.46293 |  0:00:53s
epoch 108| loss: 0.20993 | val_0_rmse: 0.44349 | val_1_rmse: 0.46117 |  0:00:54s
epoch 109| loss: 0.21614 | val_0_rmse: 0.44639 | val_1_rmse: 0.45431 |  0:00:54s
epoch 110| loss: 0.20718 | val_0_rmse: 0.45062 | val_1_rmse: 0.46325 |  0:00:55s
epoch 111| loss: 0.20558 | val_0_rmse: 0.4358  | val_1_rmse: 0.45587 |  0:00:55s
epoch 112| loss: 0.20598 | val_0_rmse: 0.45206 | val_1_rmse: 0.47024 |  0:00:56s
epoch 113| loss: 0.22145 | val_0_rmse: 0.44515 | val_1_rmse: 0.45963 |  0:00:56s
epoch 114| loss: 0.21551 | val_0_rmse: 0.44766 | val_1_rmse: 0.46215 |  0:00:57s
epoch 115| loss: 0.22449 | val_0_rmse: 0.47437 | val_1_rmse: 0.47839 |  0:00:57s
epoch 116| loss: 0.22615 | val_0_rmse: 0.45194 | val_1_rmse: 0.4642  |  0:00:58s
epoch 117| loss: 0.22401 | val_0_rmse: 0.45825 | val_1_rmse: 0.47982 |  0:00:58s
epoch 118| loss: 0.2168  | val_0_rmse: 0.43786 | val_1_rmse: 0.4535  |  0:00:59s
epoch 119| loss: 0.21312 | val_0_rmse: 0.44318 | val_1_rmse: 0.46146 |  0:00:59s
epoch 120| loss: 0.21269 | val_0_rmse: 0.4332  | val_1_rmse: 0.44717 |  0:01:00s
epoch 121| loss: 0.20987 | val_0_rmse: 0.44802 | val_1_rmse: 0.46505 |  0:01:00s
epoch 122| loss: 0.20664 | val_0_rmse: 0.43751 | val_1_rmse: 0.45863 |  0:01:01s
epoch 123| loss: 0.21674 | val_0_rmse: 0.44682 | val_1_rmse: 0.46161 |  0:01:01s
epoch 124| loss: 0.21195 | val_0_rmse: 0.44445 | val_1_rmse: 0.46012 |  0:01:02s
epoch 125| loss: 0.21076 | val_0_rmse: 0.43961 | val_1_rmse: 0.45931 |  0:01:02s
epoch 126| loss: 0.20548 | val_0_rmse: 0.43263 | val_1_rmse: 0.45105 |  0:01:03s
epoch 127| loss: 0.20317 | val_0_rmse: 0.43422 | val_1_rmse: 0.45892 |  0:01:03s
epoch 128| loss: 0.2057  | val_0_rmse: 0.42603 | val_1_rmse: 0.44624 |  0:01:04s
epoch 129| loss: 0.21334 | val_0_rmse: 0.4435  | val_1_rmse: 0.46534 |  0:01:04s
epoch 130| loss: 0.20133 | val_0_rmse: 0.43158 | val_1_rmse: 0.4523  |  0:01:05s
epoch 131| loss: 0.20698 | val_0_rmse: 0.43322 | val_1_rmse: 0.4546  |  0:01:05s
epoch 132| loss: 0.20633 | val_0_rmse: 0.42814 | val_1_rmse: 0.44866 |  0:01:06s
epoch 133| loss: 0.20485 | val_0_rmse: 0.45855 | val_1_rmse: 0.48395 |  0:01:06s
epoch 134| loss: 0.21941 | val_0_rmse: 0.43829 | val_1_rmse: 0.45451 |  0:01:07s
epoch 135| loss: 0.21139 | val_0_rmse: 0.43882 | val_1_rmse: 0.46554 |  0:01:07s
epoch 136| loss: 0.2068  | val_0_rmse: 0.45582 | val_1_rmse: 0.46792 |  0:01:07s
epoch 137| loss: 0.23603 | val_0_rmse: 0.46412 | val_1_rmse: 0.48068 |  0:01:08s
epoch 138| loss: 0.22659 | val_0_rmse: 0.44268 | val_1_rmse: 0.46243 |  0:01:08s
epoch 139| loss: 0.20633 | val_0_rmse: 0.42985 | val_1_rmse: 0.45572 |  0:01:09s
epoch 140| loss: 0.20133 | val_0_rmse: 0.43427 | val_1_rmse: 0.46332 |  0:01:09s
epoch 141| loss: 0.20636 | val_0_rmse: 0.43509 | val_1_rmse: 0.46148 |  0:01:10s
epoch 142| loss: 0.21433 | val_0_rmse: 0.44779 | val_1_rmse: 0.46762 |  0:01:10s
epoch 143| loss: 0.20797 | val_0_rmse: 0.43722 | val_1_rmse: 0.45472 |  0:01:11s
epoch 144| loss: 0.20438 | val_0_rmse: 0.44582 | val_1_rmse: 0.47842 |  0:01:11s
epoch 145| loss: 0.20137 | val_0_rmse: 0.43853 | val_1_rmse: 0.46333 |  0:01:12s
epoch 146| loss: 0.20478 | val_0_rmse: 0.42943 | val_1_rmse: 0.45406 |  0:01:12s
epoch 147| loss: 0.19625 | val_0_rmse: 0.42782 | val_1_rmse: 0.45748 |  0:01:13s
epoch 148| loss: 0.20037 | val_0_rmse: 0.42161 | val_1_rmse: 0.45169 |  0:01:13s
epoch 149| loss: 0.2112  | val_0_rmse: 0.43266 | val_1_rmse: 0.44902 |  0:01:14s
Stop training because you reached max_epochs = 150 with best_epoch = 128 and best_val_1_rmse = 0.44624
Best weights from best epoch are automatically used!
ended training at: 14:08:02
Feature importance:
[('Area', 0.3761889408619719), ('Baths', 0.08920407438538086), ('Beds', 0.06330008348256679), ('Latitude', 0.33850664652716067), ('Longitude', 0.06366113695222167), ('Month', 0.009154850209122106), ('Year', 0.059984267581576003)]
Mean squared error is of 6700634973.209876
Mean absolute error:57813.34829151988
MAPE:0.15327328120577788
R2 score:0.7912224069604075
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:08:03
epoch 0  | loss: 1.30879 | val_0_rmse: 0.96615 | val_1_rmse: 0.99707 |  0:00:00s
epoch 1  | loss: 0.75731 | val_0_rmse: 1.20151 | val_1_rmse: 1.25384 |  0:00:00s
epoch 2  | loss: 0.67758 | val_0_rmse: 0.99279 | val_1_rmse: 1.03868 |  0:00:00s
epoch 3  | loss: 0.59272 | val_0_rmse: 0.88878 | val_1_rmse: 0.94085 |  0:00:00s
epoch 4  | loss: 0.54123 | val_0_rmse: 0.78439 | val_1_rmse: 0.84137 |  0:00:01s
epoch 5  | loss: 0.48046 | val_0_rmse: 0.74449 | val_1_rmse: 0.81497 |  0:00:01s
epoch 6  | loss: 0.44389 | val_0_rmse: 0.70312 | val_1_rmse: 0.77223 |  0:00:01s
epoch 7  | loss: 0.41509 | val_0_rmse: 0.69222 | val_1_rmse: 0.75104 |  0:00:01s
epoch 8  | loss: 0.39724 | val_0_rmse: 0.67491 | val_1_rmse: 0.71808 |  0:00:02s
epoch 9  | loss: 0.38297 | val_0_rmse: 0.61308 | val_1_rmse: 0.66172 |  0:00:02s
epoch 10 | loss: 0.37254 | val_0_rmse: 0.61086 | val_1_rmse: 0.64641 |  0:00:02s
epoch 11 | loss: 0.37389 | val_0_rmse: 0.61941 | val_1_rmse: 0.66049 |  0:00:02s
epoch 12 | loss: 0.35213 | val_0_rmse: 0.57616 | val_1_rmse: 0.62126 |  0:00:03s
epoch 13 | loss: 0.3402  | val_0_rmse: 0.57903 | val_1_rmse: 0.62087 |  0:00:03s
epoch 14 | loss: 0.34866 | val_0_rmse: 0.58582 | val_1_rmse: 0.6164  |  0:00:03s
epoch 15 | loss: 0.34111 | val_0_rmse: 0.587   | val_1_rmse: 0.62169 |  0:00:03s
epoch 16 | loss: 0.33458 | val_0_rmse: 0.57894 | val_1_rmse: 0.5946  |  0:00:04s
epoch 17 | loss: 0.34151 | val_0_rmse: 0.56254 | val_1_rmse: 0.6041  |  0:00:04s
epoch 18 | loss: 0.32329 | val_0_rmse: 0.55691 | val_1_rmse: 0.59529 |  0:00:04s
epoch 19 | loss: 0.32752 | val_0_rmse: 0.56249 | val_1_rmse: 0.6113  |  0:00:04s
epoch 20 | loss: 0.32552 | val_0_rmse: 0.54663 | val_1_rmse: 0.5918  |  0:00:04s
epoch 21 | loss: 0.31305 | val_0_rmse: 0.54409 | val_1_rmse: 0.60097 |  0:00:05s
epoch 22 | loss: 0.31202 | val_0_rmse: 0.53751 | val_1_rmse: 0.58824 |  0:00:05s
epoch 23 | loss: 0.29116 | val_0_rmse: 0.52958 | val_1_rmse: 0.58125 |  0:00:05s
epoch 24 | loss: 0.29214 | val_0_rmse: 0.52761 | val_1_rmse: 0.57986 |  0:00:05s
epoch 25 | loss: 0.31326 | val_0_rmse: 0.56217 | val_1_rmse: 0.61829 |  0:00:06s
epoch 26 | loss: 0.30206 | val_0_rmse: 0.5245  | val_1_rmse: 0.57787 |  0:00:06s
epoch 27 | loss: 0.29571 | val_0_rmse: 0.53179 | val_1_rmse: 0.58887 |  0:00:06s
epoch 28 | loss: 0.29927 | val_0_rmse: 0.51564 | val_1_rmse: 0.57585 |  0:00:06s
epoch 29 | loss: 0.28352 | val_0_rmse: 0.53351 | val_1_rmse: 0.59703 |  0:00:07s
epoch 30 | loss: 0.27971 | val_0_rmse: 0.51269 | val_1_rmse: 0.57465 |  0:00:07s
epoch 31 | loss: 0.28163 | val_0_rmse: 0.51523 | val_1_rmse: 0.5806  |  0:00:07s
epoch 32 | loss: 0.2804  | val_0_rmse: 0.50484 | val_1_rmse: 0.57063 |  0:00:07s
epoch 33 | loss: 0.27441 | val_0_rmse: 0.51289 | val_1_rmse: 0.5738  |  0:00:07s
epoch 34 | loss: 0.2765  | val_0_rmse: 0.49762 | val_1_rmse: 0.56459 |  0:00:08s
epoch 35 | loss: 0.26434 | val_0_rmse: 0.50136 | val_1_rmse: 0.56911 |  0:00:08s
epoch 36 | loss: 0.27081 | val_0_rmse: 0.50452 | val_1_rmse: 0.57518 |  0:00:08s
epoch 37 | loss: 0.26668 | val_0_rmse: 0.50497 | val_1_rmse: 0.56289 |  0:00:08s
epoch 38 | loss: 0.27525 | val_0_rmse: 0.5033  | val_1_rmse: 0.56594 |  0:00:09s
epoch 39 | loss: 0.26302 | val_0_rmse: 0.50375 | val_1_rmse: 0.56736 |  0:00:09s
epoch 40 | loss: 0.27974 | val_0_rmse: 0.50186 | val_1_rmse: 0.56877 |  0:00:09s
epoch 41 | loss: 0.27199 | val_0_rmse: 0.50177 | val_1_rmse: 0.56562 |  0:00:09s
epoch 42 | loss: 0.26295 | val_0_rmse: 0.51341 | val_1_rmse: 0.57969 |  0:00:10s
epoch 43 | loss: 0.26563 | val_0_rmse: 0.5028  | val_1_rmse: 0.56386 |  0:00:10s
epoch 44 | loss: 0.26924 | val_0_rmse: 0.4989  | val_1_rmse: 0.55977 |  0:00:10s
epoch 45 | loss: 0.26823 | val_0_rmse: 0.49476 | val_1_rmse: 0.55869 |  0:00:10s
epoch 46 | loss: 0.26734 | val_0_rmse: 0.49256 | val_1_rmse: 0.56372 |  0:00:11s
epoch 47 | loss: 0.26658 | val_0_rmse: 0.49366 | val_1_rmse: 0.55654 |  0:00:11s
epoch 48 | loss: 0.25568 | val_0_rmse: 0.49481 | val_1_rmse: 0.56306 |  0:00:11s
epoch 49 | loss: 0.26039 | val_0_rmse: 0.49168 | val_1_rmse: 0.56041 |  0:00:11s
epoch 50 | loss: 0.26361 | val_0_rmse: 0.48912 | val_1_rmse: 0.5707  |  0:00:11s
epoch 51 | loss: 0.25576 | val_0_rmse: 0.48741 | val_1_rmse: 0.56498 |  0:00:12s
epoch 52 | loss: 0.26502 | val_0_rmse: 0.49722 | val_1_rmse: 0.57375 |  0:00:12s
epoch 53 | loss: 0.25792 | val_0_rmse: 0.49926 | val_1_rmse: 0.5653  |  0:00:12s
epoch 54 | loss: 0.25397 | val_0_rmse: 0.48647 | val_1_rmse: 0.56151 |  0:00:12s
epoch 55 | loss: 0.2611  | val_0_rmse: 0.49177 | val_1_rmse: 0.56087 |  0:00:13s
epoch 56 | loss: 0.24845 | val_0_rmse: 0.49334 | val_1_rmse: 0.56936 |  0:00:13s
epoch 57 | loss: 0.26473 | val_0_rmse: 0.488   | val_1_rmse: 0.54872 |  0:00:13s
epoch 58 | loss: 0.25805 | val_0_rmse: 0.48773 | val_1_rmse: 0.55553 |  0:00:13s
epoch 59 | loss: 0.25652 | val_0_rmse: 0.48244 | val_1_rmse: 0.54394 |  0:00:13s
epoch 60 | loss: 0.24816 | val_0_rmse: 0.48343 | val_1_rmse: 0.55184 |  0:00:14s
epoch 61 | loss: 0.25543 | val_0_rmse: 0.48122 | val_1_rmse: 0.54902 |  0:00:14s
epoch 62 | loss: 0.24924 | val_0_rmse: 0.48038 | val_1_rmse: 0.55095 |  0:00:14s
epoch 63 | loss: 0.24794 | val_0_rmse: 0.47772 | val_1_rmse: 0.55045 |  0:00:14s
epoch 64 | loss: 0.24947 | val_0_rmse: 0.48124 | val_1_rmse: 0.54908 |  0:00:15s
epoch 65 | loss: 0.24351 | val_0_rmse: 0.48509 | val_1_rmse: 0.56234 |  0:00:15s
epoch 66 | loss: 0.25578 | val_0_rmse: 0.48421 | val_1_rmse: 0.55474 |  0:00:15s
epoch 67 | loss: 0.26539 | val_0_rmse: 0.4824  | val_1_rmse: 0.56073 |  0:00:15s
epoch 68 | loss: 0.25466 | val_0_rmse: 0.4848  | val_1_rmse: 0.56456 |  0:00:16s
epoch 69 | loss: 0.2638  | val_0_rmse: 0.48584 | val_1_rmse: 0.56602 |  0:00:16s
epoch 70 | loss: 0.25298 | val_0_rmse: 0.48764 | val_1_rmse: 0.56022 |  0:00:16s
epoch 71 | loss: 0.25093 | val_0_rmse: 0.48092 | val_1_rmse: 0.56112 |  0:00:16s
epoch 72 | loss: 0.24777 | val_0_rmse: 0.48833 | val_1_rmse: 0.56038 |  0:00:16s
epoch 73 | loss: 0.25847 | val_0_rmse: 0.48316 | val_1_rmse: 0.5636  |  0:00:17s
epoch 74 | loss: 0.26008 | val_0_rmse: 0.48964 | val_1_rmse: 0.5612  |  0:00:17s
epoch 75 | loss: 0.25834 | val_0_rmse: 0.48226 | val_1_rmse: 0.56185 |  0:00:17s
epoch 76 | loss: 0.25993 | val_0_rmse: 0.48828 | val_1_rmse: 0.56476 |  0:00:17s
epoch 77 | loss: 0.25321 | val_0_rmse: 0.48443 | val_1_rmse: 0.56115 |  0:00:18s
epoch 78 | loss: 0.24588 | val_0_rmse: 0.48801 | val_1_rmse: 0.55791 |  0:00:18s
epoch 79 | loss: 0.24636 | val_0_rmse: 0.47858 | val_1_rmse: 0.55337 |  0:00:18s
epoch 80 | loss: 0.2495  | val_0_rmse: 0.48877 | val_1_rmse: 0.55602 |  0:00:18s
epoch 81 | loss: 0.26252 | val_0_rmse: 0.48597 | val_1_rmse: 0.56315 |  0:00:19s
epoch 82 | loss: 0.26267 | val_0_rmse: 0.48338 | val_1_rmse: 0.55989 |  0:00:19s
epoch 83 | loss: 0.26251 | val_0_rmse: 0.48375 | val_1_rmse: 0.57014 |  0:00:19s
epoch 84 | loss: 0.25129 | val_0_rmse: 0.4813  | val_1_rmse: 0.57086 |  0:00:19s
epoch 85 | loss: 0.2495  | val_0_rmse: 0.47815 | val_1_rmse: 0.56489 |  0:00:19s
epoch 86 | loss: 0.24984 | val_0_rmse: 0.47621 | val_1_rmse: 0.56109 |  0:00:20s
epoch 87 | loss: 0.24818 | val_0_rmse: 0.47335 | val_1_rmse: 0.556   |  0:00:20s
epoch 88 | loss: 0.24174 | val_0_rmse: 0.47891 | val_1_rmse: 0.5661  |  0:00:20s
epoch 89 | loss: 0.2493  | val_0_rmse: 0.47385 | val_1_rmse: 0.56438 |  0:00:20s

Early stopping occured at epoch 89 with best_epoch = 59 and best_val_1_rmse = 0.54394
Best weights from best epoch are automatically used!
ended training at: 14:08:24
Feature importance:
[('Area', 0.15608704586326602), ('Baths', 0.01569283358397892), ('Beds', 0.22125001208619383), ('Latitude', 0.2896609972251387), ('Longitude', 0.24665698311740117), ('Month', 0.03923947647758746), ('Year', 0.03141265164643387)]
Mean squared error is of 23670535786.516438
Mean absolute error:112910.60366403784
MAPE:0.19221419371570433
R2 score:0.7107520657665067
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:08:24
epoch 0  | loss: 1.28476 | val_0_rmse: 1.06839 | val_1_rmse: 1.09461 |  0:00:00s
epoch 1  | loss: 0.67744 | val_0_rmse: 0.89937 | val_1_rmse: 0.92715 |  0:00:00s
epoch 2  | loss: 0.54943 | val_0_rmse: 0.9289  | val_1_rmse: 0.92634 |  0:00:00s
epoch 3  | loss: 0.50336 | val_0_rmse: 0.79125 | val_1_rmse: 0.80836 |  0:00:00s
epoch 4  | loss: 0.46032 | val_0_rmse: 0.78064 | val_1_rmse: 0.81773 |  0:00:01s
epoch 5  | loss: 0.43961 | val_0_rmse: 0.68056 | val_1_rmse: 0.72579 |  0:00:01s
epoch 6  | loss: 0.41423 | val_0_rmse: 0.75184 | val_1_rmse: 0.78415 |  0:00:01s
epoch 7  | loss: 0.39466 | val_0_rmse: 0.63046 | val_1_rmse: 0.68906 |  0:00:01s
epoch 8  | loss: 0.36855 | val_0_rmse: 0.69916 | val_1_rmse: 0.73538 |  0:00:02s
epoch 9  | loss: 0.34959 | val_0_rmse: 0.58586 | val_1_rmse: 0.62931 |  0:00:02s
epoch 10 | loss: 0.34935 | val_0_rmse: 0.6035  | val_1_rmse: 0.63981 |  0:00:02s
epoch 11 | loss: 0.3421  | val_0_rmse: 0.58571 | val_1_rmse: 0.61831 |  0:00:02s
epoch 12 | loss: 0.34314 | val_0_rmse: 0.59467 | val_1_rmse: 0.62176 |  0:00:03s
epoch 13 | loss: 0.33366 | val_0_rmse: 0.56987 | val_1_rmse: 0.60065 |  0:00:03s
epoch 14 | loss: 0.33886 | val_0_rmse: 0.57915 | val_1_rmse: 0.61342 |  0:00:03s
epoch 15 | loss: 0.32796 | val_0_rmse: 0.56274 | val_1_rmse: 0.5973  |  0:00:03s
epoch 16 | loss: 0.33206 | val_0_rmse: 0.55982 | val_1_rmse: 0.59336 |  0:00:03s
epoch 17 | loss: 0.31575 | val_0_rmse: 0.55578 | val_1_rmse: 0.58671 |  0:00:04s
epoch 18 | loss: 0.32215 | val_0_rmse: 0.55099 | val_1_rmse: 0.57246 |  0:00:04s
epoch 19 | loss: 0.31949 | val_0_rmse: 0.5467  | val_1_rmse: 0.576   |  0:00:04s
epoch 20 | loss: 0.31563 | val_0_rmse: 0.54718 | val_1_rmse: 0.57148 |  0:00:04s
epoch 21 | loss: 0.31695 | val_0_rmse: 0.53791 | val_1_rmse: 0.5656  |  0:00:05s
epoch 22 | loss: 0.31453 | val_0_rmse: 0.54381 | val_1_rmse: 0.57238 |  0:00:05s
epoch 23 | loss: 0.30454 | val_0_rmse: 0.53954 | val_1_rmse: 0.57461 |  0:00:05s
epoch 24 | loss: 0.29635 | val_0_rmse: 0.54064 | val_1_rmse: 0.57093 |  0:00:05s
epoch 25 | loss: 0.2949  | val_0_rmse: 0.53279 | val_1_rmse: 0.56658 |  0:00:06s
epoch 26 | loss: 0.30402 | val_0_rmse: 0.52734 | val_1_rmse: 0.56078 |  0:00:06s
epoch 27 | loss: 0.30882 | val_0_rmse: 0.53065 | val_1_rmse: 0.56481 |  0:00:06s
epoch 28 | loss: 0.29347 | val_0_rmse: 0.52418 | val_1_rmse: 0.55947 |  0:00:06s
epoch 29 | loss: 0.29299 | val_0_rmse: 0.52563 | val_1_rmse: 0.56154 |  0:00:06s
epoch 30 | loss: 0.29056 | val_0_rmse: 0.51671 | val_1_rmse: 0.54944 |  0:00:07s
epoch 31 | loss: 0.29121 | val_0_rmse: 0.52615 | val_1_rmse: 0.5661  |  0:00:07s
epoch 32 | loss: 0.28821 | val_0_rmse: 0.51976 | val_1_rmse: 0.55331 |  0:00:07s
epoch 33 | loss: 0.28777 | val_0_rmse: 0.51473 | val_1_rmse: 0.54635 |  0:00:07s
epoch 34 | loss: 0.28997 | val_0_rmse: 0.5165  | val_1_rmse: 0.54995 |  0:00:08s
epoch 35 | loss: 0.29082 | val_0_rmse: 0.51778 | val_1_rmse: 0.55091 |  0:00:08s
epoch 36 | loss: 0.28598 | val_0_rmse: 0.53285 | val_1_rmse: 0.55905 |  0:00:08s
epoch 37 | loss: 0.2808  | val_0_rmse: 0.51304 | val_1_rmse: 0.54475 |  0:00:08s
epoch 38 | loss: 0.2818  | val_0_rmse: 0.50804 | val_1_rmse: 0.54161 |  0:00:09s
epoch 39 | loss: 0.2831  | val_0_rmse: 0.5487  | val_1_rmse: 0.58553 |  0:00:09s
epoch 40 | loss: 0.2872  | val_0_rmse: 0.51204 | val_1_rmse: 0.5451  |  0:00:09s
epoch 41 | loss: 0.28887 | val_0_rmse: 0.50983 | val_1_rmse: 0.54114 |  0:00:09s
epoch 42 | loss: 0.284   | val_0_rmse: 0.50419 | val_1_rmse: 0.53688 |  0:00:09s
epoch 43 | loss: 0.2829  | val_0_rmse: 0.50906 | val_1_rmse: 0.53805 |  0:00:10s
epoch 44 | loss: 0.27413 | val_0_rmse: 0.51864 | val_1_rmse: 0.55226 |  0:00:10s
epoch 45 | loss: 0.28741 | val_0_rmse: 0.51382 | val_1_rmse: 0.53626 |  0:00:10s
epoch 46 | loss: 0.28616 | val_0_rmse: 0.49806 | val_1_rmse: 0.52573 |  0:00:10s
epoch 47 | loss: 0.27158 | val_0_rmse: 0.50644 | val_1_rmse: 0.53697 |  0:00:11s
epoch 48 | loss: 0.26637 | val_0_rmse: 0.499   | val_1_rmse: 0.5373  |  0:00:11s
epoch 49 | loss: 0.27891 | val_0_rmse: 0.49596 | val_1_rmse: 0.52878 |  0:00:11s
epoch 50 | loss: 0.26819 | val_0_rmse: 0.49634 | val_1_rmse: 0.53104 |  0:00:11s
epoch 51 | loss: 0.26197 | val_0_rmse: 0.49591 | val_1_rmse: 0.5273  |  0:00:12s
epoch 52 | loss: 0.2667  | val_0_rmse: 0.49864 | val_1_rmse: 0.5359  |  0:00:12s
epoch 53 | loss: 0.28007 | val_0_rmse: 0.49639 | val_1_rmse: 0.52838 |  0:00:12s
epoch 54 | loss: 0.27128 | val_0_rmse: 0.50638 | val_1_rmse: 0.54461 |  0:00:12s
epoch 55 | loss: 0.27423 | val_0_rmse: 0.51833 | val_1_rmse: 0.55108 |  0:00:12s
epoch 56 | loss: 0.26937 | val_0_rmse: 0.50399 | val_1_rmse: 0.54258 |  0:00:13s
epoch 57 | loss: 0.2682  | val_0_rmse: 0.51579 | val_1_rmse: 0.54687 |  0:00:13s
epoch 58 | loss: 0.28246 | val_0_rmse: 0.50957 | val_1_rmse: 0.54503 |  0:00:13s
epoch 59 | loss: 0.27676 | val_0_rmse: 0.51587 | val_1_rmse: 0.54061 |  0:00:13s
epoch 60 | loss: 0.26891 | val_0_rmse: 0.51273 | val_1_rmse: 0.54904 |  0:00:14s
epoch 61 | loss: 0.2738  | val_0_rmse: 0.52138 | val_1_rmse: 0.55135 |  0:00:14s
epoch 62 | loss: 0.27642 | val_0_rmse: 0.50653 | val_1_rmse: 0.53754 |  0:00:14s
epoch 63 | loss: 0.27118 | val_0_rmse: 0.50981 | val_1_rmse: 0.54194 |  0:00:14s
epoch 64 | loss: 0.26992 | val_0_rmse: 0.50413 | val_1_rmse: 0.54139 |  0:00:15s
epoch 65 | loss: 0.26229 | val_0_rmse: 0.49708 | val_1_rmse: 0.53363 |  0:00:15s
epoch 66 | loss: 0.2715  | val_0_rmse: 0.49785 | val_1_rmse: 0.5347  |  0:00:15s
epoch 67 | loss: 0.26105 | val_0_rmse: 0.49351 | val_1_rmse: 0.53654 |  0:00:15s
epoch 68 | loss: 0.25707 | val_0_rmse: 0.49243 | val_1_rmse: 0.53281 |  0:00:15s
epoch 69 | loss: 0.25075 | val_0_rmse: 0.49437 | val_1_rmse: 0.53396 |  0:00:16s
epoch 70 | loss: 0.26806 | val_0_rmse: 0.48866 | val_1_rmse: 0.52901 |  0:00:16s
epoch 71 | loss: 0.25716 | val_0_rmse: 0.48787 | val_1_rmse: 0.52505 |  0:00:16s
epoch 72 | loss: 0.25865 | val_0_rmse: 0.48745 | val_1_rmse: 0.52851 |  0:00:16s
epoch 73 | loss: 0.25725 | val_0_rmse: 0.48837 | val_1_rmse: 0.5293  |  0:00:17s
epoch 74 | loss: 0.2569  | val_0_rmse: 0.49762 | val_1_rmse: 0.54004 |  0:00:17s
epoch 75 | loss: 0.262   | val_0_rmse: 0.485   | val_1_rmse: 0.52728 |  0:00:17s
epoch 76 | loss: 0.2534  | val_0_rmse: 0.48588 | val_1_rmse: 0.52571 |  0:00:17s
epoch 77 | loss: 0.25088 | val_0_rmse: 0.48558 | val_1_rmse: 0.52757 |  0:00:17s
epoch 78 | loss: 0.26014 | val_0_rmse: 0.48835 | val_1_rmse: 0.52857 |  0:00:18s
epoch 79 | loss: 0.25756 | val_0_rmse: 0.48582 | val_1_rmse: 0.52624 |  0:00:18s
epoch 80 | loss: 0.26215 | val_0_rmse: 0.48357 | val_1_rmse: 0.5194  |  0:00:18s
epoch 81 | loss: 0.25334 | val_0_rmse: 0.48486 | val_1_rmse: 0.51658 |  0:00:18s
epoch 82 | loss: 0.25145 | val_0_rmse: 0.48582 | val_1_rmse: 0.52408 |  0:00:19s
epoch 83 | loss: 0.25818 | val_0_rmse: 0.48218 | val_1_rmse: 0.5196  |  0:00:19s
epoch 84 | loss: 0.25142 | val_0_rmse: 0.48828 | val_1_rmse: 0.52892 |  0:00:19s
epoch 85 | loss: 0.25225 | val_0_rmse: 0.49815 | val_1_rmse: 0.53576 |  0:00:19s
epoch 86 | loss: 0.28407 | val_0_rmse: 0.48683 | val_1_rmse: 0.53208 |  0:00:20s
epoch 87 | loss: 0.2699  | val_0_rmse: 0.4827  | val_1_rmse: 0.52838 |  0:00:20s
epoch 88 | loss: 0.2591  | val_0_rmse: 0.47576 | val_1_rmse: 0.51705 |  0:00:20s
epoch 89 | loss: 0.24926 | val_0_rmse: 0.47736 | val_1_rmse: 0.51416 |  0:00:20s
epoch 90 | loss: 0.25194 | val_0_rmse: 0.47798 | val_1_rmse: 0.51846 |  0:00:20s
epoch 91 | loss: 0.24807 | val_0_rmse: 0.47582 | val_1_rmse: 0.51756 |  0:00:21s
epoch 92 | loss: 0.25306 | val_0_rmse: 0.48244 | val_1_rmse: 0.52774 |  0:00:21s
epoch 93 | loss: 0.2669  | val_0_rmse: 0.49589 | val_1_rmse: 0.53514 |  0:00:21s
epoch 94 | loss: 0.24707 | val_0_rmse: 0.48177 | val_1_rmse: 0.52807 |  0:00:21s
epoch 95 | loss: 0.25744 | val_0_rmse: 0.48634 | val_1_rmse: 0.52938 |  0:00:22s
epoch 96 | loss: 0.25408 | val_0_rmse: 0.47895 | val_1_rmse: 0.52089 |  0:00:22s
epoch 97 | loss: 0.24986 | val_0_rmse: 0.47683 | val_1_rmse: 0.52114 |  0:00:22s
epoch 98 | loss: 0.24548 | val_0_rmse: 0.47539 | val_1_rmse: 0.51811 |  0:00:22s
epoch 99 | loss: 0.24361 | val_0_rmse: 0.47405 | val_1_rmse: 0.52108 |  0:00:23s
epoch 100| loss: 0.24326 | val_0_rmse: 0.47277 | val_1_rmse: 0.51962 |  0:00:23s
epoch 101| loss: 0.24985 | val_0_rmse: 0.47696 | val_1_rmse: 0.52151 |  0:00:23s
epoch 102| loss: 0.24098 | val_0_rmse: 0.47343 | val_1_rmse: 0.51917 |  0:00:23s
epoch 103| loss: 0.24419 | val_0_rmse: 0.48104 | val_1_rmse: 0.53287 |  0:00:23s
epoch 104| loss: 0.24616 | val_0_rmse: 0.48721 | val_1_rmse: 0.52659 |  0:00:24s
epoch 105| loss: 0.2496  | val_0_rmse: 0.47829 | val_1_rmse: 0.52473 |  0:00:24s
epoch 106| loss: 0.24763 | val_0_rmse: 0.49081 | val_1_rmse: 0.53588 |  0:00:24s
epoch 107| loss: 0.25919 | val_0_rmse: 0.48532 | val_1_rmse: 0.53207 |  0:00:24s
epoch 108| loss: 0.25037 | val_0_rmse: 0.47409 | val_1_rmse: 0.52136 |  0:00:25s
epoch 109| loss: 0.23841 | val_0_rmse: 0.48479 | val_1_rmse: 0.53132 |  0:00:25s
epoch 110| loss: 0.24724 | val_0_rmse: 0.47428 | val_1_rmse: 0.51778 |  0:00:25s
epoch 111| loss: 0.24323 | val_0_rmse: 0.47716 | val_1_rmse: 0.51683 |  0:00:25s
epoch 112| loss: 0.2462  | val_0_rmse: 0.47803 | val_1_rmse: 0.51754 |  0:00:26s
epoch 113| loss: 0.24197 | val_0_rmse: 0.47589 | val_1_rmse: 0.51732 |  0:00:26s
epoch 114| loss: 0.24815 | val_0_rmse: 0.47806 | val_1_rmse: 0.52146 |  0:00:26s
epoch 115| loss: 0.24278 | val_0_rmse: 0.47274 | val_1_rmse: 0.53305 |  0:00:26s
epoch 116| loss: 0.23946 | val_0_rmse: 0.47041 | val_1_rmse: 0.52494 |  0:00:26s
epoch 117| loss: 0.2462  | val_0_rmse: 0.47199 | val_1_rmse: 0.51893 |  0:00:27s
epoch 118| loss: 0.24518 | val_0_rmse: 0.47482 | val_1_rmse: 0.52384 |  0:00:27s
epoch 119| loss: 0.24767 | val_0_rmse: 0.47386 | val_1_rmse: 0.51628 |  0:00:27s

Early stopping occured at epoch 119 with best_epoch = 89 and best_val_1_rmse = 0.51416
Best weights from best epoch are automatically used!
ended training at: 14:08:52
Feature importance:
[('Area', 0.20446028663518653), ('Baths', 0.010427815075310837), ('Beds', 0.12870562249146084), ('Latitude', 0.36601695143339824), ('Longitude', 0.2059440868759785), ('Month', 0.06326230018794272), ('Year', 0.0211829373007223)]
Mean squared error is of 24688592826.070763
Mean absolute error:112827.51254304679
MAPE:0.18808235141359544
R2 score:0.6947336463439151
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:08:52
epoch 0  | loss: 1.30424 | val_0_rmse: 1.13412 | val_1_rmse: 1.0649  |  0:00:00s
epoch 1  | loss: 0.75346 | val_0_rmse: 0.88671 | val_1_rmse: 0.85437 |  0:00:00s
epoch 2  | loss: 0.61678 | val_0_rmse: 0.9442  | val_1_rmse: 0.90539 |  0:00:00s
epoch 3  | loss: 0.54635 | val_0_rmse: 0.83853 | val_1_rmse: 0.81025 |  0:00:00s
epoch 4  | loss: 0.51068 | val_0_rmse: 0.79192 | val_1_rmse: 0.7482  |  0:00:01s
epoch 5  | loss: 0.46193 | val_0_rmse: 0.72605 | val_1_rmse: 0.70576 |  0:00:01s
epoch 6  | loss: 0.4306  | val_0_rmse: 0.64937 | val_1_rmse: 0.62487 |  0:00:01s
epoch 7  | loss: 0.39317 | val_0_rmse: 0.64673 | val_1_rmse: 0.6406  |  0:00:01s
epoch 8  | loss: 0.37967 | val_0_rmse: 0.6179  | val_1_rmse: 0.62869 |  0:00:02s
epoch 9  | loss: 0.36981 | val_0_rmse: 0.58461 | val_1_rmse: 0.61841 |  0:00:02s
epoch 10 | loss: 0.3555  | val_0_rmse: 0.57707 | val_1_rmse: 0.60787 |  0:00:02s
epoch 11 | loss: 0.34472 | val_0_rmse: 0.56385 | val_1_rmse: 0.59694 |  0:00:02s
epoch 12 | loss: 0.33206 | val_0_rmse: 0.54536 | val_1_rmse: 0.5597  |  0:00:03s
epoch 13 | loss: 0.31263 | val_0_rmse: 0.54886 | val_1_rmse: 0.55663 |  0:00:03s
epoch 14 | loss: 0.30393 | val_0_rmse: 0.53653 | val_1_rmse: 0.54481 |  0:00:03s
epoch 15 | loss: 0.30671 | val_0_rmse: 0.5287  | val_1_rmse: 0.5334  |  0:00:03s
epoch 16 | loss: 0.30436 | val_0_rmse: 0.5289  | val_1_rmse: 0.52223 |  0:00:04s
epoch 17 | loss: 0.30119 | val_0_rmse: 0.52224 | val_1_rmse: 0.5232  |  0:00:04s
epoch 18 | loss: 0.30427 | val_0_rmse: 0.54386 | val_1_rmse: 0.54175 |  0:00:04s
epoch 19 | loss: 0.30918 | val_0_rmse: 0.5282  | val_1_rmse: 0.53147 |  0:00:04s
epoch 20 | loss: 0.2893  | val_0_rmse: 0.52472 | val_1_rmse: 0.52763 |  0:00:04s
epoch 21 | loss: 0.3037  | val_0_rmse: 0.52397 | val_1_rmse: 0.53289 |  0:00:05s
epoch 22 | loss: 0.2992  | val_0_rmse: 0.51744 | val_1_rmse: 0.51516 |  0:00:05s
epoch 23 | loss: 0.29053 | val_0_rmse: 0.51514 | val_1_rmse: 0.51822 |  0:00:05s
epoch 24 | loss: 0.28245 | val_0_rmse: 0.50874 | val_1_rmse: 0.5257  |  0:00:05s
epoch 25 | loss: 0.28598 | val_0_rmse: 0.52713 | val_1_rmse: 0.52035 |  0:00:06s
epoch 26 | loss: 0.29758 | val_0_rmse: 0.52093 | val_1_rmse: 0.51923 |  0:00:06s
epoch 27 | loss: 0.28546 | val_0_rmse: 0.51853 | val_1_rmse: 0.50902 |  0:00:06s
epoch 28 | loss: 0.28021 | val_0_rmse: 0.51128 | val_1_rmse: 0.52516 |  0:00:06s
epoch 29 | loss: 0.28779 | val_0_rmse: 0.50999 | val_1_rmse: 0.51161 |  0:00:07s
epoch 30 | loss: 0.28391 | val_0_rmse: 0.50414 | val_1_rmse: 0.51896 |  0:00:07s
epoch 31 | loss: 0.27697 | val_0_rmse: 0.50092 | val_1_rmse: 0.51649 |  0:00:07s
epoch 32 | loss: 0.27673 | val_0_rmse: 0.50277 | val_1_rmse: 0.51578 |  0:00:07s
epoch 33 | loss: 0.28683 | val_0_rmse: 0.50853 | val_1_rmse: 0.53218 |  0:00:07s
epoch 34 | loss: 0.27948 | val_0_rmse: 0.49456 | val_1_rmse: 0.50422 |  0:00:08s
epoch 35 | loss: 0.27231 | val_0_rmse: 0.49105 | val_1_rmse: 0.50751 |  0:00:08s
epoch 36 | loss: 0.27003 | val_0_rmse: 0.49614 | val_1_rmse: 0.49937 |  0:00:08s
epoch 37 | loss: 0.26664 | val_0_rmse: 0.49337 | val_1_rmse: 0.50919 |  0:00:08s
epoch 38 | loss: 0.26395 | val_0_rmse: 0.49147 | val_1_rmse: 0.51736 |  0:00:09s
epoch 39 | loss: 0.25112 | val_0_rmse: 0.48529 | val_1_rmse: 0.51091 |  0:00:09s
epoch 40 | loss: 0.25992 | val_0_rmse: 0.48969 | val_1_rmse: 0.50353 |  0:00:09s
epoch 41 | loss: 0.26533 | val_0_rmse: 0.48912 | val_1_rmse: 0.52022 |  0:00:09s
epoch 42 | loss: 0.26505 | val_0_rmse: 0.49333 | val_1_rmse: 0.50174 |  0:00:09s
epoch 43 | loss: 0.25374 | val_0_rmse: 0.48713 | val_1_rmse: 0.5241  |  0:00:10s
epoch 44 | loss: 0.25986 | val_0_rmse: 0.48031 | val_1_rmse: 0.50486 |  0:00:10s
epoch 45 | loss: 0.25455 | val_0_rmse: 0.48678 | val_1_rmse: 0.52297 |  0:00:10s
epoch 46 | loss: 0.24771 | val_0_rmse: 0.49033 | val_1_rmse: 0.50618 |  0:00:10s
epoch 47 | loss: 0.25913 | val_0_rmse: 0.49611 | val_1_rmse: 0.53397 |  0:00:11s
epoch 48 | loss: 0.27247 | val_0_rmse: 0.48854 | val_1_rmse: 0.50986 |  0:00:11s
epoch 49 | loss: 0.25588 | val_0_rmse: 0.48715 | val_1_rmse: 0.52136 |  0:00:11s
epoch 50 | loss: 0.25999 | val_0_rmse: 0.48659 | val_1_rmse: 0.51294 |  0:00:11s
epoch 51 | loss: 0.26133 | val_0_rmse: 0.48329 | val_1_rmse: 0.5113  |  0:00:12s
epoch 52 | loss: 0.25475 | val_0_rmse: 0.48086 | val_1_rmse: 0.51236 |  0:00:12s
epoch 53 | loss: 0.24377 | val_0_rmse: 0.47725 | val_1_rmse: 0.51575 |  0:00:12s
epoch 54 | loss: 0.26555 | val_0_rmse: 0.47948 | val_1_rmse: 0.51212 |  0:00:12s
epoch 55 | loss: 0.26025 | val_0_rmse: 0.48611 | val_1_rmse: 0.52773 |  0:00:12s
epoch 56 | loss: 0.26052 | val_0_rmse: 0.49764 | val_1_rmse: 0.52883 |  0:00:13s
epoch 57 | loss: 0.257   | val_0_rmse: 0.48128 | val_1_rmse: 0.52371 |  0:00:13s
epoch 58 | loss: 0.25851 | val_0_rmse: 0.49242 | val_1_rmse: 0.50273 |  0:00:13s
epoch 59 | loss: 0.2586  | val_0_rmse: 0.48435 | val_1_rmse: 0.52627 |  0:00:13s
epoch 60 | loss: 0.25399 | val_0_rmse: 0.48061 | val_1_rmse: 0.50139 |  0:00:14s
epoch 61 | loss: 0.25029 | val_0_rmse: 0.4882  | val_1_rmse: 0.53418 |  0:00:14s
epoch 62 | loss: 0.25459 | val_0_rmse: 0.49536 | val_1_rmse: 0.51725 |  0:00:14s
epoch 63 | loss: 0.26785 | val_0_rmse: 0.48064 | val_1_rmse: 0.52296 |  0:00:14s
epoch 64 | loss: 0.25235 | val_0_rmse: 0.48109 | val_1_rmse: 0.50305 |  0:00:14s
epoch 65 | loss: 0.25138 | val_0_rmse: 0.47841 | val_1_rmse: 0.5277  |  0:00:15s
epoch 66 | loss: 0.24807 | val_0_rmse: 0.47281 | val_1_rmse: 0.513   |  0:00:15s

Early stopping occured at epoch 66 with best_epoch = 36 and best_val_1_rmse = 0.49937
Best weights from best epoch are automatically used!
ended training at: 14:09:07
Feature importance:
[('Area', 0.20799588394102972), ('Baths', 0.06880856381238197), ('Beds', 0.07812601899796826), ('Latitude', 0.23047537733130546), ('Longitude', 0.3886542360763729), ('Month', 0.00892442426987104), ('Year', 0.017015495571070613)]
Mean squared error is of 22699420016.39621
Mean absolute error:111355.02204955311
MAPE:0.19594732575088475
R2 score:0.7229523621401828
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:09:07
epoch 0  | loss: 1.33991 | val_0_rmse: 1.11011 | val_1_rmse: 1.08265 |  0:00:00s
epoch 1  | loss: 0.77233 | val_0_rmse: 0.95719 | val_1_rmse: 0.95826 |  0:00:00s
epoch 2  | loss: 0.63167 | val_0_rmse: 0.92869 | val_1_rmse: 0.9319  |  0:00:00s
epoch 3  | loss: 0.54229 | val_0_rmse: 0.88397 | val_1_rmse: 0.89982 |  0:00:00s
epoch 4  | loss: 0.48469 | val_0_rmse: 0.85996 | val_1_rmse: 0.863   |  0:00:01s
epoch 5  | loss: 0.44347 | val_0_rmse: 0.79918 | val_1_rmse: 0.79229 |  0:00:01s
epoch 6  | loss: 0.42745 | val_0_rmse: 0.76641 | val_1_rmse: 0.74422 |  0:00:01s
epoch 7  | loss: 0.39112 | val_0_rmse: 0.66484 | val_1_rmse: 0.64486 |  0:00:01s
epoch 8  | loss: 0.39188 | val_0_rmse: 0.64233 | val_1_rmse: 0.63887 |  0:00:02s
epoch 9  | loss: 0.36784 | val_0_rmse: 0.61195 | val_1_rmse: 0.59681 |  0:00:02s
epoch 10 | loss: 0.34747 | val_0_rmse: 0.58905 | val_1_rmse: 0.56705 |  0:00:02s
epoch 11 | loss: 0.3385  | val_0_rmse: 0.57469 | val_1_rmse: 0.56092 |  0:00:02s
epoch 12 | loss: 0.3337  | val_0_rmse: 0.56709 | val_1_rmse: 0.56971 |  0:00:03s
epoch 13 | loss: 0.33223 | val_0_rmse: 0.55832 | val_1_rmse: 0.5665  |  0:00:03s
epoch 14 | loss: 0.32517 | val_0_rmse: 0.54812 | val_1_rmse: 0.54673 |  0:00:03s
epoch 15 | loss: 0.3226  | val_0_rmse: 0.54398 | val_1_rmse: 0.54417 |  0:00:03s
epoch 16 | loss: 0.31362 | val_0_rmse: 0.55073 | val_1_rmse: 0.55629 |  0:00:04s
epoch 17 | loss: 0.31227 | val_0_rmse: 0.54308 | val_1_rmse: 0.54528 |  0:00:04s
epoch 18 | loss: 0.31491 | val_0_rmse: 0.54867 | val_1_rmse: 0.54536 |  0:00:04s
epoch 19 | loss: 0.31363 | val_0_rmse: 0.53645 | val_1_rmse: 0.53365 |  0:00:04s
epoch 20 | loss: 0.30588 | val_0_rmse: 0.53453 | val_1_rmse: 0.5356  |  0:00:04s
epoch 21 | loss: 0.31039 | val_0_rmse: 0.54946 | val_1_rmse: 0.54143 |  0:00:05s
epoch 22 | loss: 0.30361 | val_0_rmse: 0.53409 | val_1_rmse: 0.52228 |  0:00:05s
epoch 23 | loss: 0.3055  | val_0_rmse: 0.5681  | val_1_rmse: 0.56364 |  0:00:05s
epoch 24 | loss: 0.29582 | val_0_rmse: 0.52695 | val_1_rmse: 0.53321 |  0:00:05s
epoch 25 | loss: 0.29672 | val_0_rmse: 0.52516 | val_1_rmse: 0.53042 |  0:00:06s
epoch 26 | loss: 0.28309 | val_0_rmse: 0.50869 | val_1_rmse: 0.52196 |  0:00:06s
epoch 27 | loss: 0.27594 | val_0_rmse: 0.5097  | val_1_rmse: 0.53022 |  0:00:06s
epoch 28 | loss: 0.27679 | val_0_rmse: 0.50048 | val_1_rmse: 0.51848 |  0:00:06s
epoch 29 | loss: 0.27782 | val_0_rmse: 0.50231 | val_1_rmse: 0.51764 |  0:00:07s
epoch 30 | loss: 0.2691  | val_0_rmse: 0.50178 | val_1_rmse: 0.51746 |  0:00:07s
epoch 31 | loss: 0.26841 | val_0_rmse: 0.49481 | val_1_rmse: 0.51715 |  0:00:07s
epoch 32 | loss: 0.2636  | val_0_rmse: 0.49074 | val_1_rmse: 0.50728 |  0:00:07s
epoch 33 | loss: 0.2643  | val_0_rmse: 0.49162 | val_1_rmse: 0.50996 |  0:00:07s
epoch 34 | loss: 0.26662 | val_0_rmse: 0.49599 | val_1_rmse: 0.51415 |  0:00:08s
epoch 35 | loss: 0.26917 | val_0_rmse: 0.48879 | val_1_rmse: 0.51389 |  0:00:08s
epoch 36 | loss: 0.26707 | val_0_rmse: 0.49162 | val_1_rmse: 0.51233 |  0:00:08s
epoch 37 | loss: 0.26771 | val_0_rmse: 0.48396 | val_1_rmse: 0.5094  |  0:00:08s
epoch 38 | loss: 0.25764 | val_0_rmse: 0.47945 | val_1_rmse: 0.50514 |  0:00:09s
epoch 39 | loss: 0.2517  | val_0_rmse: 0.47702 | val_1_rmse: 0.50499 |  0:00:09s
epoch 40 | loss: 0.253   | val_0_rmse: 0.47931 | val_1_rmse: 0.50548 |  0:00:09s
epoch 41 | loss: 0.26016 | val_0_rmse: 0.48111 | val_1_rmse: 0.50317 |  0:00:09s
epoch 42 | loss: 0.2577  | val_0_rmse: 0.47919 | val_1_rmse: 0.49334 |  0:00:10s
epoch 43 | loss: 0.25312 | val_0_rmse: 0.47963 | val_1_rmse: 0.49539 |  0:00:10s
epoch 44 | loss: 0.25728 | val_0_rmse: 0.48001 | val_1_rmse: 0.50122 |  0:00:10s
epoch 45 | loss: 0.2519  | val_0_rmse: 0.47362 | val_1_rmse: 0.5002  |  0:00:10s
epoch 46 | loss: 0.24787 | val_0_rmse: 0.47876 | val_1_rmse: 0.5079  |  0:00:10s
epoch 47 | loss: 0.25116 | val_0_rmse: 0.47489 | val_1_rmse: 0.50553 |  0:00:11s
epoch 48 | loss: 0.25419 | val_0_rmse: 0.47668 | val_1_rmse: 0.50424 |  0:00:11s
epoch 49 | loss: 0.25536 | val_0_rmse: 0.4876  | val_1_rmse: 0.51248 |  0:00:11s
epoch 50 | loss: 0.26201 | val_0_rmse: 0.47797 | val_1_rmse: 0.49392 |  0:00:11s
epoch 51 | loss: 0.26749 | val_0_rmse: 0.47713 | val_1_rmse: 0.50168 |  0:00:12s
epoch 52 | loss: 0.26554 | val_0_rmse: 0.47704 | val_1_rmse: 0.49761 |  0:00:12s
epoch 53 | loss: 0.26691 | val_0_rmse: 0.4824  | val_1_rmse: 0.49365 |  0:00:12s
epoch 54 | loss: 0.27548 | val_0_rmse: 0.47196 | val_1_rmse: 0.49311 |  0:00:12s
epoch 55 | loss: 0.25503 | val_0_rmse: 0.48584 | val_1_rmse: 0.4996  |  0:00:13s
epoch 56 | loss: 0.24203 | val_0_rmse: 0.48605 | val_1_rmse: 0.50218 |  0:00:13s
epoch 57 | loss: 0.25072 | val_0_rmse: 0.47742 | val_1_rmse: 0.49634 |  0:00:13s
epoch 58 | loss: 0.25488 | val_0_rmse: 0.47187 | val_1_rmse: 0.49519 |  0:00:13s
epoch 59 | loss: 0.25013 | val_0_rmse: 0.47155 | val_1_rmse: 0.48971 |  0:00:13s
epoch 60 | loss: 0.25429 | val_0_rmse: 0.4741  | val_1_rmse: 0.49434 |  0:00:14s
epoch 61 | loss: 0.25157 | val_0_rmse: 0.47045 | val_1_rmse: 0.49564 |  0:00:14s
epoch 62 | loss: 0.25378 | val_0_rmse: 0.47714 | val_1_rmse: 0.48881 |  0:00:14s
epoch 63 | loss: 0.23909 | val_0_rmse: 0.47331 | val_1_rmse: 0.49187 |  0:00:14s
epoch 64 | loss: 0.25311 | val_0_rmse: 0.46947 | val_1_rmse: 0.49363 |  0:00:15s
epoch 65 | loss: 0.24583 | val_0_rmse: 0.47676 | val_1_rmse: 0.50206 |  0:00:15s
epoch 66 | loss: 0.25237 | val_0_rmse: 0.47049 | val_1_rmse: 0.49526 |  0:00:15s
epoch 67 | loss: 0.25352 | val_0_rmse: 0.46993 | val_1_rmse: 0.4901  |  0:00:15s
epoch 68 | loss: 0.24945 | val_0_rmse: 0.46872 | val_1_rmse: 0.48585 |  0:00:16s
epoch 69 | loss: 0.24304 | val_0_rmse: 0.46592 | val_1_rmse: 0.48407 |  0:00:16s
epoch 70 | loss: 0.245   | val_0_rmse: 0.46972 | val_1_rmse: 0.49014 |  0:00:16s
epoch 71 | loss: 0.2499  | val_0_rmse: 0.46357 | val_1_rmse: 0.48327 |  0:00:16s
epoch 72 | loss: 0.24518 | val_0_rmse: 0.47115 | val_1_rmse: 0.49619 |  0:00:16s
epoch 73 | loss: 0.23789 | val_0_rmse: 0.46294 | val_1_rmse: 0.48936 |  0:00:17s
epoch 74 | loss: 0.24222 | val_0_rmse: 0.45853 | val_1_rmse: 0.4859  |  0:00:17s
epoch 75 | loss: 0.24056 | val_0_rmse: 0.47484 | val_1_rmse: 0.50199 |  0:00:17s
epoch 76 | loss: 0.24669 | val_0_rmse: 0.46959 | val_1_rmse: 0.49727 |  0:00:17s
epoch 77 | loss: 0.24318 | val_0_rmse: 0.46978 | val_1_rmse: 0.49629 |  0:00:18s
epoch 78 | loss: 0.2449  | val_0_rmse: 0.47016 | val_1_rmse: 0.49231 |  0:00:18s
epoch 79 | loss: 0.24732 | val_0_rmse: 0.46585 | val_1_rmse: 0.4905  |  0:00:18s
epoch 80 | loss: 0.2434  | val_0_rmse: 0.46313 | val_1_rmse: 0.48608 |  0:00:18s
epoch 81 | loss: 0.23123 | val_0_rmse: 0.46247 | val_1_rmse: 0.48849 |  0:00:18s
epoch 82 | loss: 0.22863 | val_0_rmse: 0.46147 | val_1_rmse: 0.49733 |  0:00:19s
epoch 83 | loss: 0.23774 | val_0_rmse: 0.4584  | val_1_rmse: 0.49097 |  0:00:19s
epoch 84 | loss: 0.24903 | val_0_rmse: 0.4619  | val_1_rmse: 0.48981 |  0:00:19s
epoch 85 | loss: 0.24081 | val_0_rmse: 0.46699 | val_1_rmse: 0.49917 |  0:00:19s
epoch 86 | loss: 0.22962 | val_0_rmse: 0.46104 | val_1_rmse: 0.49303 |  0:00:20s
epoch 87 | loss: 0.24146 | val_0_rmse: 0.46806 | val_1_rmse: 0.49264 |  0:00:20s
epoch 88 | loss: 0.23077 | val_0_rmse: 0.46303 | val_1_rmse: 0.48777 |  0:00:20s
epoch 89 | loss: 0.24263 | val_0_rmse: 0.46139 | val_1_rmse: 0.49148 |  0:00:20s
epoch 90 | loss: 0.23346 | val_0_rmse: 0.45843 | val_1_rmse: 0.49208 |  0:00:21s
epoch 91 | loss: 0.23794 | val_0_rmse: 0.46567 | val_1_rmse: 0.49521 |  0:00:21s
epoch 92 | loss: 0.22935 | val_0_rmse: 0.46208 | val_1_rmse: 0.49033 |  0:00:21s
epoch 93 | loss: 0.23734 | val_0_rmse: 0.45986 | val_1_rmse: 0.49979 |  0:00:21s
epoch 94 | loss: 0.23233 | val_0_rmse: 0.45658 | val_1_rmse: 0.4979  |  0:00:21s
epoch 95 | loss: 0.22353 | val_0_rmse: 0.45796 | val_1_rmse: 0.48737 |  0:00:22s
epoch 96 | loss: 0.23015 | val_0_rmse: 0.45119 | val_1_rmse: 0.48803 |  0:00:22s
epoch 97 | loss: 0.23349 | val_0_rmse: 0.44949 | val_1_rmse: 0.48881 |  0:00:22s
epoch 98 | loss: 0.23286 | val_0_rmse: 0.46139 | val_1_rmse: 0.495   |  0:00:22s
epoch 99 | loss: 0.23204 | val_0_rmse: 0.45433 | val_1_rmse: 0.49731 |  0:00:23s
epoch 100| loss: 0.22454 | val_0_rmse: 0.44953 | val_1_rmse: 0.49307 |  0:00:23s
epoch 101| loss: 0.22328 | val_0_rmse: 0.45057 | val_1_rmse: 0.48402 |  0:00:23s

Early stopping occured at epoch 101 with best_epoch = 71 and best_val_1_rmse = 0.48327
Best weights from best epoch are automatically used!
ended training at: 14:09:31
Feature importance:
[('Area', 0.20314577352758537), ('Baths', 0.050717806907985816), ('Beds', 0.0975451149526643), ('Latitude', 0.289430683059373), ('Longitude', 0.28544488658419714), ('Month', 0.016349230395251973), ('Year', 0.05736650457294242)]
Mean squared error is of 24228356586.25937
Mean absolute error:108777.15280454785
MAPE:0.18243000620924232
R2 score:0.7093691206226187
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:09:31
epoch 0  | loss: 1.35573 | val_0_rmse: 1.02302 | val_1_rmse: 0.97537 |  0:00:00s
epoch 1  | loss: 0.75545 | val_0_rmse: 0.99973 | val_1_rmse: 0.93741 |  0:00:00s
epoch 2  | loss: 0.6085  | val_0_rmse: 0.99255 | val_1_rmse: 0.92423 |  0:00:00s
epoch 3  | loss: 0.53263 | val_0_rmse: 0.85551 | val_1_rmse: 0.79905 |  0:00:00s
epoch 4  | loss: 0.50268 | val_0_rmse: 0.86034 | val_1_rmse: 0.81695 |  0:00:01s
epoch 5  | loss: 0.4624  | val_0_rmse: 0.79972 | val_1_rmse: 0.74819 |  0:00:01s
epoch 6  | loss: 0.43002 | val_0_rmse: 0.75488 | val_1_rmse: 0.7135  |  0:00:01s
epoch 7  | loss: 0.41958 | val_0_rmse: 0.72136 | val_1_rmse: 0.6765  |  0:00:01s
epoch 8  | loss: 0.40393 | val_0_rmse: 0.66965 | val_1_rmse: 0.61853 |  0:00:02s
epoch 9  | loss: 0.39806 | val_0_rmse: 0.6311  | val_1_rmse: 0.59237 |  0:00:02s
epoch 10 | loss: 0.38842 | val_0_rmse: 0.64864 | val_1_rmse: 0.62102 |  0:00:02s
epoch 11 | loss: 0.40324 | val_0_rmse: 0.6024  | val_1_rmse: 0.55725 |  0:00:02s
epoch 12 | loss: 0.37865 | val_0_rmse: 0.6086  | val_1_rmse: 0.56225 |  0:00:03s
epoch 13 | loss: 0.37479 | val_0_rmse: 0.59396 | val_1_rmse: 0.57733 |  0:00:03s
epoch 14 | loss: 0.36501 | val_0_rmse: 0.5987  | val_1_rmse: 0.5882  |  0:00:03s
epoch 15 | loss: 0.36398 | val_0_rmse: 0.58131 | val_1_rmse: 0.56907 |  0:00:03s
epoch 16 | loss: 0.35224 | val_0_rmse: 0.58209 | val_1_rmse: 0.5768  |  0:00:03s
epoch 17 | loss: 0.34616 | val_0_rmse: 0.57199 | val_1_rmse: 0.57418 |  0:00:04s
epoch 18 | loss: 0.35326 | val_0_rmse: 0.59458 | val_1_rmse: 0.56937 |  0:00:04s
epoch 19 | loss: 0.35025 | val_0_rmse: 0.56845 | val_1_rmse: 0.54948 |  0:00:04s
epoch 20 | loss: 0.34218 | val_0_rmse: 0.56908 | val_1_rmse: 0.54586 |  0:00:04s
epoch 21 | loss: 0.33568 | val_0_rmse: 0.55796 | val_1_rmse: 0.53662 |  0:00:05s
epoch 22 | loss: 0.32938 | val_0_rmse: 0.5696  | val_1_rmse: 0.53658 |  0:00:05s
epoch 23 | loss: 0.35086 | val_0_rmse: 0.5653  | val_1_rmse: 0.54272 |  0:00:05s
epoch 24 | loss: 0.34885 | val_0_rmse: 0.56853 | val_1_rmse: 0.54319 |  0:00:05s
epoch 25 | loss: 0.34328 | val_0_rmse: 0.57422 | val_1_rmse: 0.54742 |  0:00:05s
epoch 26 | loss: 0.33898 | val_0_rmse: 0.56678 | val_1_rmse: 0.5343  |  0:00:06s
epoch 27 | loss: 0.33603 | val_0_rmse: 0.57329 | val_1_rmse: 0.53928 |  0:00:06s
epoch 28 | loss: 0.31993 | val_0_rmse: 0.56039 | val_1_rmse: 0.52086 |  0:00:06s
epoch 29 | loss: 0.31716 | val_0_rmse: 0.56234 | val_1_rmse: 0.53009 |  0:00:06s
epoch 30 | loss: 0.31093 | val_0_rmse: 0.53861 | val_1_rmse: 0.50743 |  0:00:07s
epoch 31 | loss: 0.30854 | val_0_rmse: 0.54235 | val_1_rmse: 0.51316 |  0:00:07s
epoch 32 | loss: 0.30675 | val_0_rmse: 0.53727 | val_1_rmse: 0.51297 |  0:00:07s
epoch 33 | loss: 0.31379 | val_0_rmse: 0.53044 | val_1_rmse: 0.51562 |  0:00:07s
epoch 34 | loss: 0.29596 | val_0_rmse: 0.53853 | val_1_rmse: 0.50581 |  0:00:08s
epoch 35 | loss: 0.29784 | val_0_rmse: 0.52895 | val_1_rmse: 0.51586 |  0:00:08s
epoch 36 | loss: 0.30233 | val_0_rmse: 0.52597 | val_1_rmse: 0.49874 |  0:00:08s
epoch 37 | loss: 0.29708 | val_0_rmse: 0.53965 | val_1_rmse: 0.52217 |  0:00:08s
epoch 38 | loss: 0.30004 | val_0_rmse: 0.54214 | val_1_rmse: 0.52285 |  0:00:09s
epoch 39 | loss: 0.29702 | val_0_rmse: 0.55335 | val_1_rmse: 0.54652 |  0:00:09s
epoch 40 | loss: 0.31606 | val_0_rmse: 0.53658 | val_1_rmse: 0.51244 |  0:00:09s
epoch 41 | loss: 0.31864 | val_0_rmse: 0.53519 | val_1_rmse: 0.52637 |  0:00:09s
epoch 42 | loss: 0.2987  | val_0_rmse: 0.5211  | val_1_rmse: 0.5012  |  0:00:09s
epoch 43 | loss: 0.29421 | val_0_rmse: 0.52293 | val_1_rmse: 0.50602 |  0:00:10s
epoch 44 | loss: 0.28371 | val_0_rmse: 0.51699 | val_1_rmse: 0.49954 |  0:00:10s
epoch 45 | loss: 0.28963 | val_0_rmse: 0.52008 | val_1_rmse: 0.49957 |  0:00:10s
epoch 46 | loss: 0.28824 | val_0_rmse: 0.52307 | val_1_rmse: 0.50666 |  0:00:10s
epoch 47 | loss: 0.27964 | val_0_rmse: 0.52072 | val_1_rmse: 0.50153 |  0:00:11s
epoch 48 | loss: 0.28842 | val_0_rmse: 0.51362 | val_1_rmse: 0.49854 |  0:00:11s
epoch 49 | loss: 0.28879 | val_0_rmse: 0.52073 | val_1_rmse: 0.49258 |  0:00:11s
epoch 50 | loss: 0.27724 | val_0_rmse: 0.51484 | val_1_rmse: 0.50182 |  0:00:11s
epoch 51 | loss: 0.27867 | val_0_rmse: 0.50977 | val_1_rmse: 0.49252 |  0:00:11s
epoch 52 | loss: 0.27683 | val_0_rmse: 0.51999 | val_1_rmse: 0.51062 |  0:00:12s
epoch 53 | loss: 0.28706 | val_0_rmse: 0.51226 | val_1_rmse: 0.49924 |  0:00:12s
epoch 54 | loss: 0.27578 | val_0_rmse: 0.51236 | val_1_rmse: 0.50542 |  0:00:12s
epoch 55 | loss: 0.27798 | val_0_rmse: 0.50666 | val_1_rmse: 0.49398 |  0:00:12s
epoch 56 | loss: 0.27946 | val_0_rmse: 0.50947 | val_1_rmse: 0.50153 |  0:00:13s
epoch 57 | loss: 0.27373 | val_0_rmse: 0.50431 | val_1_rmse: 0.48801 |  0:00:13s
epoch 58 | loss: 0.26708 | val_0_rmse: 0.50807 | val_1_rmse: 0.5014  |  0:00:13s
epoch 59 | loss: 0.27311 | val_0_rmse: 0.50036 | val_1_rmse: 0.48697 |  0:00:13s
epoch 60 | loss: 0.27274 | val_0_rmse: 0.50039 | val_1_rmse: 0.48925 |  0:00:14s
epoch 61 | loss: 0.27467 | val_0_rmse: 0.49881 | val_1_rmse: 0.48642 |  0:00:14s
epoch 62 | loss: 0.27263 | val_0_rmse: 0.50365 | val_1_rmse: 0.49849 |  0:00:14s
epoch 63 | loss: 0.2746  | val_0_rmse: 0.50034 | val_1_rmse: 0.48755 |  0:00:14s
epoch 64 | loss: 0.27765 | val_0_rmse: 0.50701 | val_1_rmse: 0.50555 |  0:00:14s
epoch 65 | loss: 0.28337 | val_0_rmse: 0.50815 | val_1_rmse: 0.48563 |  0:00:15s
epoch 66 | loss: 0.27118 | val_0_rmse: 0.5022  | val_1_rmse: 0.50521 |  0:00:15s
epoch 67 | loss: 0.26291 | val_0_rmse: 0.50305 | val_1_rmse: 0.48142 |  0:00:15s
epoch 68 | loss: 0.27087 | val_0_rmse: 0.49444 | val_1_rmse: 0.49129 |  0:00:15s
epoch 69 | loss: 0.2638  | val_0_rmse: 0.49691 | val_1_rmse: 0.48834 |  0:00:16s
epoch 70 | loss: 0.26453 | val_0_rmse: 0.49003 | val_1_rmse: 0.48163 |  0:00:16s
epoch 71 | loss: 0.26401 | val_0_rmse: 0.49486 | val_1_rmse: 0.48264 |  0:00:16s
epoch 72 | loss: 0.26806 | val_0_rmse: 0.49196 | val_1_rmse: 0.4789  |  0:00:16s
epoch 73 | loss: 0.26285 | val_0_rmse: 0.49093 | val_1_rmse: 0.48596 |  0:00:17s
epoch 74 | loss: 0.26163 | val_0_rmse: 0.48856 | val_1_rmse: 0.48545 |  0:00:17s
epoch 75 | loss: 0.26203 | val_0_rmse: 0.489   | val_1_rmse: 0.48247 |  0:00:17s
epoch 76 | loss: 0.26167 | val_0_rmse: 0.4884  | val_1_rmse: 0.49256 |  0:00:17s
epoch 77 | loss: 0.25788 | val_0_rmse: 0.48428 | val_1_rmse: 0.48688 |  0:00:18s
epoch 78 | loss: 0.25124 | val_0_rmse: 0.49109 | val_1_rmse: 0.48479 |  0:00:18s
epoch 79 | loss: 0.25699 | val_0_rmse: 0.49335 | val_1_rmse: 0.48308 |  0:00:18s
epoch 80 | loss: 0.26025 | val_0_rmse: 0.49744 | val_1_rmse: 0.47788 |  0:00:18s
epoch 81 | loss: 0.27186 | val_0_rmse: 0.49928 | val_1_rmse: 0.49765 |  0:00:18s
epoch 82 | loss: 0.26783 | val_0_rmse: 0.49294 | val_1_rmse: 0.49731 |  0:00:19s
epoch 83 | loss: 0.26284 | val_0_rmse: 0.49817 | val_1_rmse: 0.49765 |  0:00:19s
epoch 84 | loss: 0.26601 | val_0_rmse: 0.49194 | val_1_rmse: 0.48236 |  0:00:19s
epoch 85 | loss: 0.26218 | val_0_rmse: 0.49724 | val_1_rmse: 0.50517 |  0:00:19s
epoch 86 | loss: 0.27016 | val_0_rmse: 0.48882 | val_1_rmse: 0.48107 |  0:00:20s
epoch 87 | loss: 0.27343 | val_0_rmse: 0.49449 | val_1_rmse: 0.49103 |  0:00:20s
epoch 88 | loss: 0.25588 | val_0_rmse: 0.49341 | val_1_rmse: 0.48242 |  0:00:20s
epoch 89 | loss: 0.26372 | val_0_rmse: 0.48778 | val_1_rmse: 0.4927  |  0:00:20s
epoch 90 | loss: 0.25739 | val_0_rmse: 0.4878  | val_1_rmse: 0.47813 |  0:00:20s
epoch 91 | loss: 0.25122 | val_0_rmse: 0.48592 | val_1_rmse: 0.48869 |  0:00:21s
epoch 92 | loss: 0.25767 | val_0_rmse: 0.48522 | val_1_rmse: 0.48041 |  0:00:21s
epoch 93 | loss: 0.25249 | val_0_rmse: 0.48637 | val_1_rmse: 0.49183 |  0:00:21s
epoch 94 | loss: 0.26358 | val_0_rmse: 0.48263 | val_1_rmse: 0.47452 |  0:00:21s
epoch 95 | loss: 0.26071 | val_0_rmse: 0.48286 | val_1_rmse: 0.49045 |  0:00:22s
epoch 96 | loss: 0.25328 | val_0_rmse: 0.49593 | val_1_rmse: 0.50444 |  0:00:22s
epoch 97 | loss: 0.26168 | val_0_rmse: 0.48233 | val_1_rmse: 0.48157 |  0:00:22s
epoch 98 | loss: 0.25718 | val_0_rmse: 0.48269 | val_1_rmse: 0.48225 |  0:00:22s
epoch 99 | loss: 0.25201 | val_0_rmse: 0.47956 | val_1_rmse: 0.4813  |  0:00:23s
epoch 100| loss: 0.25571 | val_0_rmse: 0.47906 | val_1_rmse: 0.49385 |  0:00:23s
epoch 101| loss: 0.25767 | val_0_rmse: 0.47953 | val_1_rmse: 0.48264 |  0:00:23s
epoch 102| loss: 0.25096 | val_0_rmse: 0.47539 | val_1_rmse: 0.48435 |  0:00:23s
epoch 103| loss: 0.25148 | val_0_rmse: 0.47646 | val_1_rmse: 0.48635 |  0:00:23s
epoch 104| loss: 0.24421 | val_0_rmse: 0.47996 | val_1_rmse: 0.48392 |  0:00:24s
epoch 105| loss: 0.25122 | val_0_rmse: 0.47749 | val_1_rmse: 0.49056 |  0:00:24s
epoch 106| loss: 0.26213 | val_0_rmse: 0.47918 | val_1_rmse: 0.48342 |  0:00:24s
epoch 107| loss: 0.25121 | val_0_rmse: 0.48099 | val_1_rmse: 0.50194 |  0:00:24s
epoch 108| loss: 0.2503  | val_0_rmse: 0.47323 | val_1_rmse: 0.49296 |  0:00:25s
epoch 109| loss: 0.25006 | val_0_rmse: 0.49443 | val_1_rmse: 0.5128  |  0:00:25s
epoch 110| loss: 0.25072 | val_0_rmse: 0.47929 | val_1_rmse: 0.49698 |  0:00:25s
epoch 111| loss: 0.25061 | val_0_rmse: 0.49664 | val_1_rmse: 0.5154  |  0:00:25s
epoch 112| loss: 0.25863 | val_0_rmse: 0.5052  | val_1_rmse: 0.51721 |  0:00:25s
epoch 113| loss: 0.25571 | val_0_rmse: 0.48278 | val_1_rmse: 0.50067 |  0:00:26s
epoch 114| loss: 0.25173 | val_0_rmse: 0.48946 | val_1_rmse: 0.48787 |  0:00:26s
epoch 115| loss: 0.25635 | val_0_rmse: 0.4763  | val_1_rmse: 0.49264 |  0:00:26s
epoch 116| loss: 0.24973 | val_0_rmse: 0.49162 | val_1_rmse: 0.49477 |  0:00:26s
epoch 117| loss: 0.25353 | val_0_rmse: 0.47632 | val_1_rmse: 0.48841 |  0:00:27s
epoch 118| loss: 0.2395  | val_0_rmse: 0.47021 | val_1_rmse: 0.4892  |  0:00:27s
epoch 119| loss: 0.24569 | val_0_rmse: 0.47191 | val_1_rmse: 0.49205 |  0:00:27s
epoch 120| loss: 0.24258 | val_0_rmse: 0.47056 | val_1_rmse: 0.49145 |  0:00:27s
epoch 121| loss: 0.24897 | val_0_rmse: 0.46638 | val_1_rmse: 0.4848  |  0:00:28s
epoch 122| loss: 0.2387  | val_0_rmse: 0.46976 | val_1_rmse: 0.50544 |  0:00:28s
epoch 123| loss: 0.24258 | val_0_rmse: 0.46283 | val_1_rmse: 0.4828  |  0:00:28s
epoch 124| loss: 0.23693 | val_0_rmse: 0.46764 | val_1_rmse: 0.47458 |  0:00:28s

Early stopping occured at epoch 124 with best_epoch = 94 and best_val_1_rmse = 0.47452
Best weights from best epoch are automatically used!
ended training at: 14:10:00
Feature importance:
[('Area', 0.2815510743922277), ('Baths', 0.025009668969510297), ('Beds', 0.09875771100577353), ('Latitude', 0.2575271926813115), ('Longitude', 0.20212284784197954), ('Month', 0.0376644448189762), ('Year', 0.09736706029022123)]
Mean squared error is of 18915622292.76435
Mean absolute error:100015.89019433492
MAPE:0.1764227128080694
R2 score:0.7576154327399722
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:10:00
epoch 0  | loss: 1.59341 | val_0_rmse: 1.16935 | val_1_rmse: 1.12292 |  0:00:00s
epoch 1  | loss: 0.86566 | val_0_rmse: 0.9257  | val_1_rmse: 1.03487 |  0:00:00s
epoch 2  | loss: 0.63772 | val_0_rmse: 1.18954 | val_1_rmse: 1.13391 |  0:00:00s
epoch 3  | loss: 0.55198 | val_0_rmse: 0.99065 | val_1_rmse: 0.97384 |  0:00:00s
epoch 4  | loss: 0.53142 | val_0_rmse: 0.99503 | val_1_rmse: 0.98576 |  0:00:00s
epoch 5  | loss: 0.50718 | val_0_rmse: 1.0065  | val_1_rmse: 1.01663 |  0:00:00s
epoch 6  | loss: 0.48791 | val_0_rmse: 0.87156 | val_1_rmse: 0.94277 |  0:00:00s
epoch 7  | loss: 0.4631  | val_0_rmse: 0.75735 | val_1_rmse: 0.85364 |  0:00:01s
epoch 8  | loss: 0.45903 | val_0_rmse: 0.72765 | val_1_rmse: 0.82496 |  0:00:01s
epoch 9  | loss: 0.46323 | val_0_rmse: 0.71915 | val_1_rmse: 0.82443 |  0:00:01s
epoch 10 | loss: 0.42937 | val_0_rmse: 0.70644 | val_1_rmse: 0.80429 |  0:00:01s
epoch 11 | loss: 0.42905 | val_0_rmse: 0.68542 | val_1_rmse: 0.77632 |  0:00:01s
epoch 12 | loss: 0.4337  | val_0_rmse: 0.694   | val_1_rmse: 0.77194 |  0:00:01s
epoch 13 | loss: 0.43192 | val_0_rmse: 0.67667 | val_1_rmse: 0.76056 |  0:00:01s
epoch 14 | loss: 0.425   | val_0_rmse: 0.65513 | val_1_rmse: 0.75871 |  0:00:02s
epoch 15 | loss: 0.42926 | val_0_rmse: 0.65104 | val_1_rmse: 0.75414 |  0:00:02s
epoch 16 | loss: 0.42387 | val_0_rmse: 0.64822 | val_1_rmse: 0.75195 |  0:00:02s
epoch 17 | loss: 0.41869 | val_0_rmse: 0.66769 | val_1_rmse: 0.76636 |  0:00:02s
epoch 18 | loss: 0.41999 | val_0_rmse: 0.63661 | val_1_rmse: 0.74822 |  0:00:02s
epoch 19 | loss: 0.4175  | val_0_rmse: 0.63957 | val_1_rmse: 0.74832 |  0:00:02s
epoch 20 | loss: 0.41076 | val_0_rmse: 0.62614 | val_1_rmse: 0.74245 |  0:00:02s
epoch 21 | loss: 0.39783 | val_0_rmse: 0.62937 | val_1_rmse: 0.74133 |  0:00:02s
epoch 22 | loss: 0.40644 | val_0_rmse: 0.63173 | val_1_rmse: 0.73779 |  0:00:03s
epoch 23 | loss: 0.40262 | val_0_rmse: 0.63478 | val_1_rmse: 0.7356  |  0:00:03s
epoch 24 | loss: 0.41403 | val_0_rmse: 0.64024 | val_1_rmse: 0.7422  |  0:00:03s
epoch 25 | loss: 0.40879 | val_0_rmse: 0.64488 | val_1_rmse: 0.75166 |  0:00:03s
epoch 26 | loss: 0.41666 | val_0_rmse: 0.6466  | val_1_rmse: 0.7545  |  0:00:03s
epoch 27 | loss: 0.4074  | val_0_rmse: 0.6477  | val_1_rmse: 0.74399 |  0:00:03s
epoch 28 | loss: 0.41507 | val_0_rmse: 0.63365 | val_1_rmse: 0.72446 |  0:00:03s
epoch 29 | loss: 0.40869 | val_0_rmse: 0.63514 | val_1_rmse: 0.72315 |  0:00:04s
epoch 30 | loss: 0.41005 | val_0_rmse: 0.63659 | val_1_rmse: 0.72739 |  0:00:04s
epoch 31 | loss: 0.41124 | val_0_rmse: 0.63793 | val_1_rmse: 0.72783 |  0:00:04s
epoch 32 | loss: 0.42091 | val_0_rmse: 0.63165 | val_1_rmse: 0.7228  |  0:00:04s
epoch 33 | loss: 0.40278 | val_0_rmse: 0.64239 | val_1_rmse: 0.72445 |  0:00:04s
epoch 34 | loss: 0.41927 | val_0_rmse: 0.62644 | val_1_rmse: 0.71937 |  0:00:04s
epoch 35 | loss: 0.406   | val_0_rmse: 0.63101 | val_1_rmse: 0.73159 |  0:00:04s
epoch 36 | loss: 0.40731 | val_0_rmse: 0.62872 | val_1_rmse: 0.73102 |  0:00:04s
epoch 37 | loss: 0.40725 | val_0_rmse: 0.62823 | val_1_rmse: 0.72945 |  0:00:05s
epoch 38 | loss: 0.39969 | val_0_rmse: 0.62744 | val_1_rmse: 0.73192 |  0:00:05s
epoch 39 | loss: 0.39824 | val_0_rmse: 0.62323 | val_1_rmse: 0.72188 |  0:00:05s
epoch 40 | loss: 0.38916 | val_0_rmse: 0.6218  | val_1_rmse: 0.71918 |  0:00:05s
epoch 41 | loss: 0.39729 | val_0_rmse: 0.61514 | val_1_rmse: 0.71533 |  0:00:05s
epoch 42 | loss: 0.39311 | val_0_rmse: 0.6144  | val_1_rmse: 0.71681 |  0:00:05s
epoch 43 | loss: 0.3827  | val_0_rmse: 0.61302 | val_1_rmse: 0.71548 |  0:00:05s
epoch 44 | loss: 0.39139 | val_0_rmse: 0.60817 | val_1_rmse: 0.71276 |  0:00:06s
epoch 45 | loss: 0.39142 | val_0_rmse: 0.60589 | val_1_rmse: 0.71509 |  0:00:06s
epoch 46 | loss: 0.38035 | val_0_rmse: 0.60773 | val_1_rmse: 0.71988 |  0:00:06s
epoch 47 | loss: 0.38811 | val_0_rmse: 0.60955 | val_1_rmse: 0.71957 |  0:00:06s
epoch 48 | loss: 0.39301 | val_0_rmse: 0.60576 | val_1_rmse: 0.71096 |  0:00:06s
epoch 49 | loss: 0.38243 | val_0_rmse: 0.60618 | val_1_rmse: 0.705   |  0:00:06s
epoch 50 | loss: 0.37512 | val_0_rmse: 0.60604 | val_1_rmse: 0.70532 |  0:00:06s
epoch 51 | loss: 0.37069 | val_0_rmse: 0.60787 | val_1_rmse: 0.7076  |  0:00:07s
epoch 52 | loss: 0.37739 | val_0_rmse: 0.60673 | val_1_rmse: 0.71091 |  0:00:07s
epoch 53 | loss: 0.38074 | val_0_rmse: 0.60335 | val_1_rmse: 0.70978 |  0:00:07s
epoch 54 | loss: 0.36866 | val_0_rmse: 0.60554 | val_1_rmse: 0.71142 |  0:00:07s
epoch 55 | loss: 0.38019 | val_0_rmse: 0.60361 | val_1_rmse: 0.70707 |  0:00:07s
epoch 56 | loss: 0.37443 | val_0_rmse: 0.59623 | val_1_rmse: 0.7048  |  0:00:07s
epoch 57 | loss: 0.37063 | val_0_rmse: 0.59852 | val_1_rmse: 0.70974 |  0:00:07s
epoch 58 | loss: 0.36661 | val_0_rmse: 0.59805 | val_1_rmse: 0.71045 |  0:00:07s
epoch 59 | loss: 0.38201 | val_0_rmse: 0.59294 | val_1_rmse: 0.70138 |  0:00:08s
epoch 60 | loss: 0.37144 | val_0_rmse: 0.59541 | val_1_rmse: 0.70331 |  0:00:08s
epoch 61 | loss: 0.3654  | val_0_rmse: 0.5972  | val_1_rmse: 0.70259 |  0:00:08s
epoch 62 | loss: 0.37461 | val_0_rmse: 0.59163 | val_1_rmse: 0.70111 |  0:00:08s
epoch 63 | loss: 0.3668  | val_0_rmse: 0.59463 | val_1_rmse: 0.70597 |  0:00:08s
epoch 64 | loss: 0.36598 | val_0_rmse: 0.60621 | val_1_rmse: 0.7164  |  0:00:08s
epoch 65 | loss: 0.36779 | val_0_rmse: 0.60406 | val_1_rmse: 0.71185 |  0:00:08s
epoch 66 | loss: 0.36867 | val_0_rmse: 0.59804 | val_1_rmse: 0.70622 |  0:00:08s
epoch 67 | loss: 0.36494 | val_0_rmse: 0.60628 | val_1_rmse: 0.7109  |  0:00:09s
epoch 68 | loss: 0.36711 | val_0_rmse: 0.59234 | val_1_rmse: 0.699   |  0:00:09s
epoch 69 | loss: 0.36176 | val_0_rmse: 0.59447 | val_1_rmse: 0.69948 |  0:00:09s
epoch 70 | loss: 0.35998 | val_0_rmse: 0.59009 | val_1_rmse: 0.70016 |  0:00:09s
epoch 71 | loss: 0.36922 | val_0_rmse: 0.58595 | val_1_rmse: 0.70031 |  0:00:09s
epoch 72 | loss: 0.35526 | val_0_rmse: 0.59012 | val_1_rmse: 0.70667 |  0:00:09s
epoch 73 | loss: 0.35896 | val_0_rmse: 0.59288 | val_1_rmse: 0.70751 |  0:00:09s
epoch 74 | loss: 0.34825 | val_0_rmse: 0.59129 | val_1_rmse: 0.70142 |  0:00:10s
epoch 75 | loss: 0.35163 | val_0_rmse: 0.60378 | val_1_rmse: 0.71127 |  0:00:10s
epoch 76 | loss: 0.35824 | val_0_rmse: 0.60609 | val_1_rmse: 0.71351 |  0:00:10s
epoch 77 | loss: 0.37117 | val_0_rmse: 0.60379 | val_1_rmse: 0.70852 |  0:00:10s
epoch 78 | loss: 0.36596 | val_0_rmse: 0.60145 | val_1_rmse: 0.7091  |  0:00:10s
epoch 79 | loss: 0.3835  | val_0_rmse: 0.61386 | val_1_rmse: 0.70462 |  0:00:10s
epoch 80 | loss: 0.37119 | val_0_rmse: 0.60633 | val_1_rmse: 0.69108 |  0:00:10s
epoch 81 | loss: 0.379   | val_0_rmse: 0.61638 | val_1_rmse: 0.7139  |  0:00:10s
epoch 82 | loss: 0.38827 | val_0_rmse: 0.60866 | val_1_rmse: 0.69779 |  0:00:11s
epoch 83 | loss: 0.37804 | val_0_rmse: 0.60876 | val_1_rmse: 0.69758 |  0:00:11s
epoch 84 | loss: 0.37447 | val_0_rmse: 0.61345 | val_1_rmse: 0.70225 |  0:00:11s
epoch 85 | loss: 0.3899  | val_0_rmse: 0.61953 | val_1_rmse: 0.7004  |  0:00:11s
epoch 86 | loss: 0.38609 | val_0_rmse: 0.60467 | val_1_rmse: 0.69025 |  0:00:11s
epoch 87 | loss: 0.37028 | val_0_rmse: 0.60058 | val_1_rmse: 0.70322 |  0:00:11s
epoch 88 | loss: 0.36788 | val_0_rmse: 0.60856 | val_1_rmse: 0.70758 |  0:00:11s
epoch 89 | loss: 0.37289 | val_0_rmse: 0.60559 | val_1_rmse: 0.7004  |  0:00:11s
epoch 90 | loss: 0.3892  | val_0_rmse: 0.58804 | val_1_rmse: 0.69306 |  0:00:12s
epoch 91 | loss: 0.36155 | val_0_rmse: 0.58976 | val_1_rmse: 0.69489 |  0:00:12s
epoch 92 | loss: 0.3626  | val_0_rmse: 0.59563 | val_1_rmse: 0.70517 |  0:00:12s
epoch 93 | loss: 0.37966 | val_0_rmse: 0.59215 | val_1_rmse: 0.70794 |  0:00:12s
epoch 94 | loss: 0.37824 | val_0_rmse: 0.60271 | val_1_rmse: 0.70185 |  0:00:12s
epoch 95 | loss: 0.38382 | val_0_rmse: 0.60801 | val_1_rmse: 0.70363 |  0:00:12s
epoch 96 | loss: 0.36578 | val_0_rmse: 0.60019 | val_1_rmse: 0.70483 |  0:00:12s
epoch 97 | loss: 0.38071 | val_0_rmse: 0.61714 | val_1_rmse: 0.7275  |  0:00:13s
epoch 98 | loss: 0.35985 | val_0_rmse: 0.59795 | val_1_rmse: 0.70474 |  0:00:13s
epoch 99 | loss: 0.37267 | val_0_rmse: 0.58997 | val_1_rmse: 0.689   |  0:00:13s
epoch 100| loss: 0.35568 | val_0_rmse: 0.59572 | val_1_rmse: 0.68703 |  0:00:13s
epoch 101| loss: 0.35927 | val_0_rmse: 0.59725 | val_1_rmse: 0.68775 |  0:00:13s
epoch 102| loss: 0.36209 | val_0_rmse: 0.58988 | val_1_rmse: 0.68832 |  0:00:13s
epoch 103| loss: 0.35801 | val_0_rmse: 0.57998 | val_1_rmse: 0.68943 |  0:00:13s
epoch 104| loss: 0.37138 | val_0_rmse: 0.57663 | val_1_rmse: 0.68422 |  0:00:13s
epoch 105| loss: 0.35633 | val_0_rmse: 0.60746 | val_1_rmse: 0.71031 |  0:00:14s
epoch 106| loss: 0.36986 | val_0_rmse: 0.58189 | val_1_rmse: 0.68967 |  0:00:14s
epoch 107| loss: 0.36192 | val_0_rmse: 0.59613 | val_1_rmse: 0.69141 |  0:00:14s
epoch 108| loss: 0.37092 | val_0_rmse: 0.60692 | val_1_rmse: 0.69299 |  0:00:14s
epoch 109| loss: 0.35308 | val_0_rmse: 0.58081 | val_1_rmse: 0.67913 |  0:00:14s
epoch 110| loss: 0.35619 | val_0_rmse: 0.57436 | val_1_rmse: 0.66784 |  0:00:14s
epoch 111| loss: 0.34186 | val_0_rmse: 0.58271 | val_1_rmse: 0.68006 |  0:00:14s
epoch 112| loss: 0.34137 | val_0_rmse: 0.57349 | val_1_rmse: 0.68429 |  0:00:15s
epoch 113| loss: 0.35422 | val_0_rmse: 0.57721 | val_1_rmse: 0.69225 |  0:00:15s
epoch 114| loss: 0.35138 | val_0_rmse: 0.59867 | val_1_rmse: 0.69743 |  0:00:15s
epoch 115| loss: 0.3448  | val_0_rmse: 0.59509 | val_1_rmse: 0.68131 |  0:00:15s
epoch 116| loss: 0.33988 | val_0_rmse: 0.59381 | val_1_rmse: 0.69062 |  0:00:15s
epoch 117| loss: 0.35247 | val_0_rmse: 0.58622 | val_1_rmse: 0.69234 |  0:00:15s
epoch 118| loss: 0.34508 | val_0_rmse: 0.58025 | val_1_rmse: 0.68665 |  0:00:15s
epoch 119| loss: 0.34349 | val_0_rmse: 0.60605 | val_1_rmse: 0.70718 |  0:00:15s
epoch 120| loss: 0.34391 | val_0_rmse: 0.60757 | val_1_rmse: 0.72028 |  0:00:16s
epoch 121| loss: 0.35173 | val_0_rmse: 0.58781 | val_1_rmse: 0.69821 |  0:00:16s
epoch 122| loss: 0.34783 | val_0_rmse: 0.58612 | val_1_rmse: 0.68854 |  0:00:16s
epoch 123| loss: 0.3398  | val_0_rmse: 0.60788 | val_1_rmse: 0.72041 |  0:00:16s
epoch 124| loss: 0.34378 | val_0_rmse: 0.63615 | val_1_rmse: 0.74773 |  0:00:16s
epoch 125| loss: 0.35203 | val_0_rmse: 0.73092 | val_1_rmse: 0.83141 |  0:00:16s
epoch 126| loss: 0.33799 | val_0_rmse: 0.67441 | val_1_rmse: 0.77497 |  0:00:16s
epoch 127| loss: 0.33817 | val_0_rmse: 0.60115 | val_1_rmse: 0.69643 |  0:00:16s
epoch 128| loss: 0.33238 | val_0_rmse: 0.60212 | val_1_rmse: 0.68819 |  0:00:17s
epoch 129| loss: 0.32707 | val_0_rmse: 0.60109 | val_1_rmse: 0.69059 |  0:00:17s
epoch 130| loss: 0.33337 | val_0_rmse: 0.55944 | val_1_rmse: 0.66404 |  0:00:17s
epoch 131| loss: 0.33238 | val_0_rmse: 0.55943 | val_1_rmse: 0.66743 |  0:00:17s
epoch 132| loss: 0.33179 | val_0_rmse: 0.55246 | val_1_rmse: 0.6566  |  0:00:17s
epoch 133| loss: 0.32377 | val_0_rmse: 0.57406 | val_1_rmse: 0.68283 |  0:00:17s
epoch 134| loss: 0.31751 | val_0_rmse: 0.60755 | val_1_rmse: 0.70606 |  0:00:17s
epoch 135| loss: 0.31912 | val_0_rmse: 0.58755 | val_1_rmse: 0.70207 |  0:00:18s
epoch 136| loss: 0.31536 | val_0_rmse: 0.56203 | val_1_rmse: 0.68397 |  0:00:18s
epoch 137| loss: 0.32748 | val_0_rmse: 0.57016 | val_1_rmse: 0.69052 |  0:00:18s
epoch 138| loss: 0.32405 | val_0_rmse: 0.6026  | val_1_rmse: 0.71194 |  0:00:18s
epoch 139| loss: 0.33412 | val_0_rmse: 0.58365 | val_1_rmse: 0.69804 |  0:00:18s
epoch 140| loss: 0.32029 | val_0_rmse: 0.55171 | val_1_rmse: 0.66324 |  0:00:18s
epoch 141| loss: 0.31452 | val_0_rmse: 0.56177 | val_1_rmse: 0.66612 |  0:00:18s
epoch 142| loss: 0.32176 | val_0_rmse: 0.56587 | val_1_rmse: 0.68036 |  0:00:18s
epoch 143| loss: 0.32499 | val_0_rmse: 0.55865 | val_1_rmse: 0.6824  |  0:00:19s
epoch 144| loss: 0.31931 | val_0_rmse: 0.58263 | val_1_rmse: 0.70589 |  0:00:19s
epoch 145| loss: 0.31823 | val_0_rmse: 0.58431 | val_1_rmse: 0.70147 |  0:00:19s
epoch 146| loss: 0.31413 | val_0_rmse: 0.58419 | val_1_rmse: 0.71002 |  0:00:19s
epoch 147| loss: 0.30272 | val_0_rmse: 0.57499 | val_1_rmse: 0.70624 |  0:00:19s
epoch 148| loss: 0.31661 | val_0_rmse: 0.56623 | val_1_rmse: 0.68258 |  0:00:19s
epoch 149| loss: 0.30799 | val_0_rmse: 0.57781 | val_1_rmse: 0.69706 |  0:00:19s
Stop training because you reached max_epochs = 150 with best_epoch = 132 and best_val_1_rmse = 0.6566
Best weights from best epoch are automatically used!
ended training at: 14:10:20
Feature importance:
[('Area', 0.31184944845652585), ('Baths', 0.16495280927564215), ('Beds', 0.02362749279451151), ('Latitude', 0.35900979962232815), ('Longitude', 0.04683075488402978), ('Month', 0.07212220179976385), ('Year', 0.02160749316719868)]
Mean squared error is of 2770746098.5579524
Mean absolute error:36571.65589244506
MAPE:0.3188376156548338
R2 score:0.5681472070636151
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:10:20
epoch 0  | loss: 1.70254 | val_0_rmse: 1.04359 | val_1_rmse: 0.99342 |  0:00:00s
epoch 1  | loss: 0.96834 | val_0_rmse: 1.00144 | val_1_rmse: 0.90873 |  0:00:00s
epoch 2  | loss: 0.76225 | val_0_rmse: 1.02539 | val_1_rmse: 0.92239 |  0:00:00s
epoch 3  | loss: 0.61687 | val_0_rmse: 0.91428 | val_1_rmse: 0.7945  |  0:00:00s
epoch 4  | loss: 0.5687  | val_0_rmse: 0.95638 | val_1_rmse: 0.73475 |  0:00:00s
epoch 5  | loss: 0.52005 | val_0_rmse: 0.96938 | val_1_rmse: 0.70864 |  0:00:00s
epoch 6  | loss: 0.51958 | val_0_rmse: 0.73495 | val_1_rmse: 0.69979 |  0:00:00s
epoch 7  | loss: 0.51021 | val_0_rmse: 0.74202 | val_1_rmse: 0.70125 |  0:00:01s
epoch 8  | loss: 0.4855  | val_0_rmse: 0.83576 | val_1_rmse: 0.6956  |  0:00:01s
epoch 9  | loss: 0.46545 | val_0_rmse: 0.91187 | val_1_rmse: 0.70406 |  0:00:01s
epoch 10 | loss: 0.47348 | val_0_rmse: 0.78624 | val_1_rmse: 0.72906 |  0:00:01s
epoch 11 | loss: 0.47889 | val_0_rmse: 0.72739 | val_1_rmse: 0.71914 |  0:00:01s
epoch 12 | loss: 0.49499 | val_0_rmse: 0.70026 | val_1_rmse: 0.71187 |  0:00:01s
epoch 13 | loss: 0.47315 | val_0_rmse: 0.70301 | val_1_rmse: 0.7031  |  0:00:01s
epoch 14 | loss: 0.46274 | val_0_rmse: 0.68548 | val_1_rmse: 0.68935 |  0:00:01s
epoch 15 | loss: 0.44532 | val_0_rmse: 0.68193 | val_1_rmse: 0.68751 |  0:00:02s
epoch 16 | loss: 0.44465 | val_0_rmse: 0.68375 | val_1_rmse: 0.68273 |  0:00:02s
epoch 17 | loss: 0.45426 | val_0_rmse: 0.66691 | val_1_rmse: 0.67605 |  0:00:02s
epoch 18 | loss: 0.45862 | val_0_rmse: 0.66283 | val_1_rmse: 0.67358 |  0:00:02s
epoch 19 | loss: 0.44059 | val_0_rmse: 0.68102 | val_1_rmse: 0.67877 |  0:00:02s
epoch 20 | loss: 0.45107 | val_0_rmse: 0.65937 | val_1_rmse: 0.67053 |  0:00:02s
epoch 21 | loss: 0.44047 | val_0_rmse: 0.65281 | val_1_rmse: 0.66278 |  0:00:02s
epoch 22 | loss: 0.43849 | val_0_rmse: 0.65286 | val_1_rmse: 0.6639  |  0:00:03s
epoch 23 | loss: 0.42759 | val_0_rmse: 0.65495 | val_1_rmse: 0.67308 |  0:00:03s
epoch 24 | loss: 0.43166 | val_0_rmse: 0.66129 | val_1_rmse: 0.6772  |  0:00:03s
epoch 25 | loss: 0.42738 | val_0_rmse: 0.68017 | val_1_rmse: 0.67588 |  0:00:03s
epoch 26 | loss: 0.42735 | val_0_rmse: 0.65779 | val_1_rmse: 0.66791 |  0:00:03s
epoch 27 | loss: 0.43242 | val_0_rmse: 0.65057 | val_1_rmse: 0.66509 |  0:00:03s
epoch 28 | loss: 0.41884 | val_0_rmse: 0.65554 | val_1_rmse: 0.67451 |  0:00:03s
epoch 29 | loss: 0.43293 | val_0_rmse: 0.66466 | val_1_rmse: 0.68241 |  0:00:03s
epoch 30 | loss: 0.42587 | val_0_rmse: 0.65124 | val_1_rmse: 0.67668 |  0:00:04s
epoch 31 | loss: 0.42445 | val_0_rmse: 0.67011 | val_1_rmse: 0.68031 |  0:00:04s
epoch 32 | loss: 0.4311  | val_0_rmse: 0.68461 | val_1_rmse: 0.68266 |  0:00:04s
epoch 33 | loss: 0.4265  | val_0_rmse: 0.65281 | val_1_rmse: 0.65875 |  0:00:04s
epoch 34 | loss: 0.42924 | val_0_rmse: 0.65706 | val_1_rmse: 0.66137 |  0:00:04s
epoch 35 | loss: 0.41911 | val_0_rmse: 0.67553 | val_1_rmse: 0.67108 |  0:00:04s
epoch 36 | loss: 0.43181 | val_0_rmse: 0.66053 | val_1_rmse: 0.66831 |  0:00:04s
epoch 37 | loss: 0.4129  | val_0_rmse: 0.63976 | val_1_rmse: 0.66832 |  0:00:05s
epoch 38 | loss: 0.43201 | val_0_rmse: 0.65067 | val_1_rmse: 0.67882 |  0:00:05s
epoch 39 | loss: 0.41842 | val_0_rmse: 0.66982 | val_1_rmse: 0.69638 |  0:00:05s
epoch 40 | loss: 0.43059 | val_0_rmse: 0.65269 | val_1_rmse: 0.68638 |  0:00:05s
epoch 41 | loss: 0.4162  | val_0_rmse: 0.64852 | val_1_rmse: 0.67368 |  0:00:05s
epoch 42 | loss: 0.41869 | val_0_rmse: 0.67048 | val_1_rmse: 0.67291 |  0:00:05s
epoch 43 | loss: 0.41424 | val_0_rmse: 0.65348 | val_1_rmse: 0.66871 |  0:00:05s
epoch 44 | loss: 0.41091 | val_0_rmse: 0.65825 | val_1_rmse: 0.67438 |  0:00:05s
epoch 45 | loss: 0.39857 | val_0_rmse: 0.66662 | val_1_rmse: 0.67793 |  0:00:06s
epoch 46 | loss: 0.40451 | val_0_rmse: 0.65794 | val_1_rmse: 0.67774 |  0:00:06s
epoch 47 | loss: 0.40622 | val_0_rmse: 0.63584 | val_1_rmse: 0.67211 |  0:00:06s
epoch 48 | loss: 0.39998 | val_0_rmse: 0.64457 | val_1_rmse: 0.68705 |  0:00:06s
epoch 49 | loss: 0.40272 | val_0_rmse: 0.65569 | val_1_rmse: 0.67872 |  0:00:06s
epoch 50 | loss: 0.39952 | val_0_rmse: 0.63435 | val_1_rmse: 0.65188 |  0:00:06s
epoch 51 | loss: 0.40002 | val_0_rmse: 0.62804 | val_1_rmse: 0.65628 |  0:00:06s
epoch 52 | loss: 0.39532 | val_0_rmse: 0.65839 | val_1_rmse: 0.68809 |  0:00:07s
epoch 53 | loss: 0.39797 | val_0_rmse: 0.66728 | val_1_rmse: 0.69628 |  0:00:07s
epoch 54 | loss: 0.404   | val_0_rmse: 0.63868 | val_1_rmse: 0.66351 |  0:00:07s
epoch 55 | loss: 0.39214 | val_0_rmse: 0.62402 | val_1_rmse: 0.6548  |  0:00:07s
epoch 56 | loss: 0.39776 | val_0_rmse: 0.63136 | val_1_rmse: 0.67167 |  0:00:07s
epoch 57 | loss: 0.38846 | val_0_rmse: 0.67185 | val_1_rmse: 0.713   |  0:00:07s
epoch 58 | loss: 0.39816 | val_0_rmse: 0.63857 | val_1_rmse: 0.6829  |  0:00:07s
epoch 59 | loss: 0.38544 | val_0_rmse: 0.61951 | val_1_rmse: 0.66108 |  0:00:07s
epoch 60 | loss: 0.39464 | val_0_rmse: 0.61645 | val_1_rmse: 0.66341 |  0:00:08s
epoch 61 | loss: 0.38057 | val_0_rmse: 0.6238  | val_1_rmse: 0.68032 |  0:00:08s
epoch 62 | loss: 0.39974 | val_0_rmse: 0.62938 | val_1_rmse: 0.69156 |  0:00:08s
epoch 63 | loss: 0.39094 | val_0_rmse: 0.61542 | val_1_rmse: 0.67819 |  0:00:08s
epoch 64 | loss: 0.38464 | val_0_rmse: 0.62192 | val_1_rmse: 0.67744 |  0:00:08s
epoch 65 | loss: 0.38259 | val_0_rmse: 0.61967 | val_1_rmse: 0.6723  |  0:00:08s
epoch 66 | loss: 0.37656 | val_0_rmse: 0.63602 | val_1_rmse: 0.68856 |  0:00:08s
epoch 67 | loss: 0.37361 | val_0_rmse: 0.64515 | val_1_rmse: 0.70218 |  0:00:09s
epoch 68 | loss: 0.38441 | val_0_rmse: 0.64363 | val_1_rmse: 0.70083 |  0:00:09s
epoch 69 | loss: 0.37675 | val_0_rmse: 0.6323  | val_1_rmse: 0.69054 |  0:00:09s
epoch 70 | loss: 0.38546 | val_0_rmse: 0.61837 | val_1_rmse: 0.67849 |  0:00:09s
epoch 71 | loss: 0.37548 | val_0_rmse: 0.64649 | val_1_rmse: 0.70429 |  0:00:09s
epoch 72 | loss: 0.37489 | val_0_rmse: 0.65047 | val_1_rmse: 0.70138 |  0:00:09s
epoch 73 | loss: 0.37905 | val_0_rmse: 0.64269 | val_1_rmse: 0.68258 |  0:00:09s
epoch 74 | loss: 0.36822 | val_0_rmse: 0.63188 | val_1_rmse: 0.66287 |  0:00:09s
epoch 75 | loss: 0.38306 | val_0_rmse: 0.61575 | val_1_rmse: 0.66054 |  0:00:10s
epoch 76 | loss: 0.37473 | val_0_rmse: 0.62913 | val_1_rmse: 0.67441 |  0:00:10s
epoch 77 | loss: 0.37912 | val_0_rmse: 0.64054 | val_1_rmse: 0.68531 |  0:00:10s
epoch 78 | loss: 0.3746  | val_0_rmse: 0.63731 | val_1_rmse: 0.67942 |  0:00:10s
epoch 79 | loss: 0.39606 | val_0_rmse: 0.63351 | val_1_rmse: 0.67695 |  0:00:10s
epoch 80 | loss: 0.38238 | val_0_rmse: 0.62045 | val_1_rmse: 0.66082 |  0:00:10s

Early stopping occured at epoch 80 with best_epoch = 50 and best_val_1_rmse = 0.65188
Best weights from best epoch are automatically used!
ended training at: 14:10:31
Feature importance:
[('Area', 0.32490648592751364), ('Baths', 0.1385208769663041), ('Beds', 0.16671563266003825), ('Latitude', 0.2504347024481563), ('Longitude', 0.008485902486825413), ('Month', 0.07638376816201822), ('Year', 0.03455263134914403)]
Mean squared error is of 3032575739.3387117
Mean absolute error:41858.04856771977
MAPE:0.40800261300136875
R2 score:0.5539914043069589
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:10:31
epoch 0  | loss: 1.50784 | val_0_rmse: 2.20106 | val_1_rmse: 1.10147 |  0:00:00s
epoch 1  | loss: 0.84511 | val_0_rmse: 0.91541 | val_1_rmse: 0.91494 |  0:00:00s
epoch 2  | loss: 0.61836 | val_0_rmse: 0.82277 | val_1_rmse: 0.83943 |  0:00:00s
epoch 3  | loss: 0.51906 | val_0_rmse: 0.85128 | val_1_rmse: 0.86732 |  0:00:00s
epoch 4  | loss: 0.49752 | val_0_rmse: 0.9094  | val_1_rmse: 0.93952 |  0:00:00s
epoch 5  | loss: 0.48116 | val_0_rmse: 0.88173 | val_1_rmse: 0.90948 |  0:00:00s
epoch 6  | loss: 0.49528 | val_0_rmse: 0.79819 | val_1_rmse: 0.80782 |  0:00:00s
epoch 7  | loss: 0.48194 | val_0_rmse: 0.74094 | val_1_rmse: 0.77294 |  0:00:01s
epoch 8  | loss: 0.4865  | val_0_rmse: 0.75277 | val_1_rmse: 0.78503 |  0:00:01s
epoch 9  | loss: 0.47756 | val_0_rmse: 0.73438 | val_1_rmse: 0.76459 |  0:00:01s
epoch 10 | loss: 0.46755 | val_0_rmse: 0.70389 | val_1_rmse: 0.73889 |  0:00:01s
epoch 11 | loss: 0.46317 | val_0_rmse: 0.6887  | val_1_rmse: 0.74965 |  0:00:01s
epoch 12 | loss: 0.45052 | val_0_rmse: 0.69069 | val_1_rmse: 0.75719 |  0:00:01s
epoch 13 | loss: 0.44643 | val_0_rmse: 0.69286 | val_1_rmse: 0.74689 |  0:00:01s
epoch 14 | loss: 0.45254 | val_0_rmse: 0.67712 | val_1_rmse: 0.72495 |  0:00:02s
epoch 15 | loss: 0.44911 | val_0_rmse: 0.68268 | val_1_rmse: 0.7321  |  0:00:02s
epoch 16 | loss: 0.45477 | val_0_rmse: 0.7068  | val_1_rmse: 0.77189 |  0:00:02s
epoch 17 | loss: 0.42976 | val_0_rmse: 0.67423 | val_1_rmse: 0.7537  |  0:00:02s
epoch 18 | loss: 0.44533 | val_0_rmse: 0.66487 | val_1_rmse: 0.7424  |  0:00:02s
epoch 19 | loss: 0.43268 | val_0_rmse: 0.68631 | val_1_rmse: 0.75054 |  0:00:02s
epoch 20 | loss: 0.42396 | val_0_rmse: 0.68116 | val_1_rmse: 0.73896 |  0:00:02s
epoch 21 | loss: 0.41405 | val_0_rmse: 0.6663  | val_1_rmse: 0.72688 |  0:00:02s
epoch 22 | loss: 0.40818 | val_0_rmse: 0.66816 | val_1_rmse: 0.72145 |  0:00:03s
epoch 23 | loss: 0.40361 | val_0_rmse: 0.68721 | val_1_rmse: 0.74063 |  0:00:03s
epoch 24 | loss: 0.40144 | val_0_rmse: 0.65828 | val_1_rmse: 0.71522 |  0:00:03s
epoch 25 | loss: 0.40565 | val_0_rmse: 0.63706 | val_1_rmse: 0.69727 |  0:00:03s
epoch 26 | loss: 0.38701 | val_0_rmse: 0.65545 | val_1_rmse: 0.71982 |  0:00:03s
epoch 27 | loss: 0.39068 | val_0_rmse: 0.64601 | val_1_rmse: 0.71872 |  0:00:03s
epoch 28 | loss: 0.37724 | val_0_rmse: 0.61164 | val_1_rmse: 0.67196 |  0:00:03s
epoch 29 | loss: 0.40033 | val_0_rmse: 0.61454 | val_1_rmse: 0.67494 |  0:00:04s
epoch 30 | loss: 0.38668 | val_0_rmse: 0.63237 | val_1_rmse: 0.70264 |  0:00:04s
epoch 31 | loss: 0.37891 | val_0_rmse: 0.64675 | val_1_rmse: 0.70721 |  0:00:04s
epoch 32 | loss: 0.38103 | val_0_rmse: 0.60265 | val_1_rmse: 0.66408 |  0:00:04s
epoch 33 | loss: 0.38547 | val_0_rmse: 0.59903 | val_1_rmse: 0.67351 |  0:00:04s
epoch 34 | loss: 0.38518 | val_0_rmse: 0.62201 | val_1_rmse: 0.71621 |  0:00:04s
epoch 35 | loss: 0.37491 | val_0_rmse: 0.62272 | val_1_rmse: 0.72518 |  0:00:04s
epoch 36 | loss: 0.39856 | val_0_rmse: 0.65092 | val_1_rmse: 0.74532 |  0:00:04s
epoch 37 | loss: 0.39444 | val_0_rmse: 0.6724  | val_1_rmse: 0.77778 |  0:00:05s
epoch 38 | loss: 0.40027 | val_0_rmse: 0.64162 | val_1_rmse: 0.73945 |  0:00:05s
epoch 39 | loss: 0.38714 | val_0_rmse: 0.63292 | val_1_rmse: 0.69453 |  0:00:05s
epoch 40 | loss: 0.38575 | val_0_rmse: 0.59358 | val_1_rmse: 0.6571  |  0:00:05s
epoch 41 | loss: 0.368   | val_0_rmse: 0.60765 | val_1_rmse: 0.67548 |  0:00:05s
epoch 42 | loss: 0.38391 | val_0_rmse: 0.6455  | val_1_rmse: 0.69074 |  0:00:05s
epoch 43 | loss: 0.37111 | val_0_rmse: 0.68494 | val_1_rmse: 0.70862 |  0:00:05s
epoch 44 | loss: 0.38297 | val_0_rmse: 0.70216 | val_1_rmse: 0.73207 |  0:00:06s
epoch 45 | loss: 0.37513 | val_0_rmse: 0.70833 | val_1_rmse: 0.73706 |  0:00:06s
epoch 46 | loss: 0.35945 | val_0_rmse: 0.64425 | val_1_rmse: 0.67125 |  0:00:06s
epoch 47 | loss: 0.3616  | val_0_rmse: 0.60859 | val_1_rmse: 0.63513 |  0:00:06s
epoch 48 | loss: 0.36278 | val_0_rmse: 0.63589 | val_1_rmse: 0.67862 |  0:00:06s
epoch 49 | loss: 0.35548 | val_0_rmse: 0.62481 | val_1_rmse: 0.6461  |  0:00:06s
epoch 50 | loss: 0.35472 | val_0_rmse: 0.61082 | val_1_rmse: 0.63521 |  0:00:06s
epoch 51 | loss: 0.35829 | val_0_rmse: 0.6009  | val_1_rmse: 0.63572 |  0:00:06s
epoch 52 | loss: 0.36728 | val_0_rmse: 0.65661 | val_1_rmse: 0.68673 |  0:00:07s
epoch 53 | loss: 0.37227 | val_0_rmse: 0.66251 | val_1_rmse: 0.69951 |  0:00:07s
epoch 54 | loss: 0.36732 | val_0_rmse: 0.61987 | val_1_rmse: 0.67389 |  0:00:07s
epoch 55 | loss: 0.36701 | val_0_rmse: 0.59066 | val_1_rmse: 0.65633 |  0:00:07s
epoch 56 | loss: 0.36788 | val_0_rmse: 0.63465 | val_1_rmse: 0.6957  |  0:00:07s
epoch 57 | loss: 0.3717  | val_0_rmse: 0.59603 | val_1_rmse: 0.64848 |  0:00:07s
epoch 58 | loss: 0.36138 | val_0_rmse: 0.58747 | val_1_rmse: 0.63836 |  0:00:07s
epoch 59 | loss: 0.3529  | val_0_rmse: 0.61235 | val_1_rmse: 0.6658  |  0:00:07s
epoch 60 | loss: 0.35769 | val_0_rmse: 0.64937 | val_1_rmse: 0.71267 |  0:00:08s
epoch 61 | loss: 0.35931 | val_0_rmse: 0.69526 | val_1_rmse: 0.77386 |  0:00:08s
epoch 62 | loss: 0.38397 | val_0_rmse: 0.66705 | val_1_rmse: 0.73033 |  0:00:08s
epoch 63 | loss: 0.35965 | val_0_rmse: 0.66868 | val_1_rmse: 0.72444 |  0:00:08s
epoch 64 | loss: 0.38346 | val_0_rmse: 0.60083 | val_1_rmse: 0.66252 |  0:00:08s
epoch 65 | loss: 0.39483 | val_0_rmse: 0.59569 | val_1_rmse: 0.66446 |  0:00:08s
epoch 66 | loss: 0.36345 | val_0_rmse: 0.63135 | val_1_rmse: 0.7031  |  0:00:08s
epoch 67 | loss: 0.3714  | val_0_rmse: 0.62282 | val_1_rmse: 0.69547 |  0:00:09s
epoch 68 | loss: 0.35257 | val_0_rmse: 0.59171 | val_1_rmse: 0.65649 |  0:00:09s
epoch 69 | loss: 0.34686 | val_0_rmse: 0.58309 | val_1_rmse: 0.64477 |  0:00:09s
epoch 70 | loss: 0.35042 | val_0_rmse: 0.58079 | val_1_rmse: 0.66055 |  0:00:09s
epoch 71 | loss: 0.34385 | val_0_rmse: 0.61809 | val_1_rmse: 0.7057  |  0:00:09s
epoch 72 | loss: 0.36158 | val_0_rmse: 0.58962 | val_1_rmse: 0.67261 |  0:00:09s
epoch 73 | loss: 0.34951 | val_0_rmse: 0.57379 | val_1_rmse: 0.6327  |  0:00:09s
epoch 74 | loss: 0.33749 | val_0_rmse: 0.58787 | val_1_rmse: 0.65687 |  0:00:09s
epoch 75 | loss: 0.33491 | val_0_rmse: 0.58287 | val_1_rmse: 0.64008 |  0:00:10s
epoch 76 | loss: 0.35515 | val_0_rmse: 0.57294 | val_1_rmse: 0.62737 |  0:00:10s
epoch 77 | loss: 0.3376  | val_0_rmse: 0.62262 | val_1_rmse: 0.68183 |  0:00:10s
epoch 78 | loss: 0.3532  | val_0_rmse: 0.66372 | val_1_rmse: 0.71668 |  0:00:10s
epoch 79 | loss: 0.34834 | val_0_rmse: 0.72842 | val_1_rmse: 0.78178 |  0:00:10s
epoch 80 | loss: 0.35448 | val_0_rmse: 0.79831 | val_1_rmse: 0.85409 |  0:00:10s
epoch 81 | loss: 0.36551 | val_0_rmse: 0.68531 | val_1_rmse: 0.74348 |  0:00:10s
epoch 82 | loss: 0.36161 | val_0_rmse: 0.59284 | val_1_rmse: 0.6443  |  0:00:10s
epoch 83 | loss: 0.3703  | val_0_rmse: 0.63393 | val_1_rmse: 0.69062 |  0:00:11s
epoch 84 | loss: 0.36412 | val_0_rmse: 0.59879 | val_1_rmse: 0.63926 |  0:00:11s
epoch 85 | loss: 0.37035 | val_0_rmse: 0.61135 | val_1_rmse: 0.65914 |  0:00:11s
epoch 86 | loss: 0.39277 | val_0_rmse: 0.67977 | val_1_rmse: 0.72168 |  0:00:11s
epoch 87 | loss: 0.36225 | val_0_rmse: 0.72874 | val_1_rmse: 0.76835 |  0:00:11s
epoch 88 | loss: 0.38258 | val_0_rmse: 0.62563 | val_1_rmse: 0.67951 |  0:00:11s
epoch 89 | loss: 0.3517  | val_0_rmse: 0.58989 | val_1_rmse: 0.64035 |  0:00:11s
epoch 90 | loss: 0.35649 | val_0_rmse: 0.5855  | val_1_rmse: 0.63369 |  0:00:12s
epoch 91 | loss: 0.34768 | val_0_rmse: 0.61216 | val_1_rmse: 0.66437 |  0:00:12s
epoch 92 | loss: 0.35008 | val_0_rmse: 0.64847 | val_1_rmse: 0.70163 |  0:00:12s
epoch 93 | loss: 0.3452  | val_0_rmse: 0.66854 | val_1_rmse: 0.73683 |  0:00:12s
epoch 94 | loss: 0.3591  | val_0_rmse: 0.65206 | val_1_rmse: 0.72337 |  0:00:12s
epoch 95 | loss: 0.33963 | val_0_rmse: 0.67822 | val_1_rmse: 0.74352 |  0:00:12s
epoch 96 | loss: 0.33717 | val_0_rmse: 0.69978 | val_1_rmse: 0.76747 |  0:00:12s
epoch 97 | loss: 0.33388 | val_0_rmse: 0.63011 | val_1_rmse: 0.7015  |  0:00:12s
epoch 98 | loss: 0.34175 | val_0_rmse: 0.57838 | val_1_rmse: 0.63942 |  0:00:13s
epoch 99 | loss: 0.33092 | val_0_rmse: 0.58009 | val_1_rmse: 0.62742 |  0:00:13s
epoch 100| loss: 0.34577 | val_0_rmse: 0.63486 | val_1_rmse: 0.67857 |  0:00:13s
epoch 101| loss: 0.34576 | val_0_rmse: 0.6638  | val_1_rmse: 0.71631 |  0:00:13s
epoch 102| loss: 0.3342  | val_0_rmse: 0.71065 | val_1_rmse: 0.77282 |  0:00:13s
epoch 103| loss: 0.3324  | val_0_rmse: 0.67624 | val_1_rmse: 0.73261 |  0:00:13s
epoch 104| loss: 0.33781 | val_0_rmse: 0.6137  | val_1_rmse: 0.66357 |  0:00:13s
epoch 105| loss: 0.32888 | val_0_rmse: 0.62264 | val_1_rmse: 0.6738  |  0:00:14s
epoch 106| loss: 0.34928 | val_0_rmse: 0.65069 | val_1_rmse: 0.70417 |  0:00:14s

Early stopping occured at epoch 106 with best_epoch = 76 and best_val_1_rmse = 0.62737
Best weights from best epoch are automatically used!
ended training at: 14:10:45
Feature importance:
[('Area', 0.40718462456640664), ('Baths', 0.20031413244272375), ('Beds', 0.01582708327620751), ('Latitude', 0.3092157006942438), ('Longitude', 0.03340210211869941), ('Month', 0.0286663946082445), ('Year', 0.005389962293474387)]
Mean squared error is of 2259659354.2369013
Mean absolute error:33941.22924336367
MAPE:0.32069783392864654
R2 score:0.6225514370453857
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:10:45
epoch 0  | loss: 1.6192  | val_0_rmse: 1.42212 | val_1_rmse: 1.57384 |  0:00:00s
epoch 1  | loss: 0.81037 | val_0_rmse: 1.01609 | val_1_rmse: 1.0862  |  0:00:00s
epoch 2  | loss: 0.60271 | val_0_rmse: 0.93234 | val_1_rmse: 0.94078 |  0:00:00s
epoch 3  | loss: 0.52164 | val_0_rmse: 0.8623  | val_1_rmse: 0.85221 |  0:00:00s
epoch 4  | loss: 0.51571 | val_0_rmse: 0.83119 | val_1_rmse: 0.81749 |  0:00:00s
epoch 5  | loss: 0.48746 | val_0_rmse: 0.80309 | val_1_rmse: 0.78498 |  0:00:00s
epoch 6  | loss: 0.46213 | val_0_rmse: 0.77009 | val_1_rmse: 0.75099 |  0:00:00s
epoch 7  | loss: 0.44319 | val_0_rmse: 0.77088 | val_1_rmse: 0.75699 |  0:00:01s
epoch 8  | loss: 0.43524 | val_0_rmse: 0.74314 | val_1_rmse: 0.73362 |  0:00:01s
epoch 9  | loss: 0.4349  | val_0_rmse: 0.73283 | val_1_rmse: 0.72177 |  0:00:01s
epoch 10 | loss: 0.43037 | val_0_rmse: 0.71931 | val_1_rmse: 0.69536 |  0:00:01s
epoch 11 | loss: 0.43176 | val_0_rmse: 0.70294 | val_1_rmse: 0.67916 |  0:00:01s
epoch 12 | loss: 0.42424 | val_0_rmse: 0.68599 | val_1_rmse: 0.67564 |  0:00:01s
epoch 13 | loss: 0.42765 | val_0_rmse: 0.6693  | val_1_rmse: 0.67659 |  0:00:01s
epoch 14 | loss: 0.4118  | val_0_rmse: 0.66327 | val_1_rmse: 0.67765 |  0:00:02s
epoch 15 | loss: 0.42872 | val_0_rmse: 0.638   | val_1_rmse: 0.64849 |  0:00:02s
epoch 16 | loss: 0.41832 | val_0_rmse: 0.63328 | val_1_rmse: 0.64319 |  0:00:02s
epoch 17 | loss: 0.41879 | val_0_rmse: 0.64341 | val_1_rmse: 0.63436 |  0:00:02s
epoch 18 | loss: 0.41257 | val_0_rmse: 0.6376  | val_1_rmse: 0.62309 |  0:00:02s
epoch 19 | loss: 0.41237 | val_0_rmse: 0.63121 | val_1_rmse: 0.63075 |  0:00:02s
epoch 20 | loss: 0.40941 | val_0_rmse: 0.63132 | val_1_rmse: 0.64196 |  0:00:02s
epoch 21 | loss: 0.40254 | val_0_rmse: 0.6301  | val_1_rmse: 0.6348  |  0:00:02s
epoch 22 | loss: 0.4035  | val_0_rmse: 0.62692 | val_1_rmse: 0.6288  |  0:00:03s
epoch 23 | loss: 0.39927 | val_0_rmse: 0.62755 | val_1_rmse: 0.62827 |  0:00:03s
epoch 24 | loss: 0.40374 | val_0_rmse: 0.63003 | val_1_rmse: 0.63285 |  0:00:03s
epoch 25 | loss: 0.40193 | val_0_rmse: 0.62157 | val_1_rmse: 0.62617 |  0:00:03s
epoch 26 | loss: 0.40196 | val_0_rmse: 0.62425 | val_1_rmse: 0.62923 |  0:00:03s
epoch 27 | loss: 0.39995 | val_0_rmse: 0.61951 | val_1_rmse: 0.6271  |  0:00:03s
epoch 28 | loss: 0.38841 | val_0_rmse: 0.6203  | val_1_rmse: 0.62755 |  0:00:03s
epoch 29 | loss: 0.39357 | val_0_rmse: 0.61775 | val_1_rmse: 0.62384 |  0:00:04s
epoch 30 | loss: 0.39275 | val_0_rmse: 0.61516 | val_1_rmse: 0.621   |  0:00:04s
epoch 31 | loss: 0.38409 | val_0_rmse: 0.62108 | val_1_rmse: 0.6271  |  0:00:04s
epoch 32 | loss: 0.39604 | val_0_rmse: 0.61679 | val_1_rmse: 0.63286 |  0:00:04s
epoch 33 | loss: 0.38926 | val_0_rmse: 0.61559 | val_1_rmse: 0.63442 |  0:00:04s
epoch 34 | loss: 0.37541 | val_0_rmse: 0.61246 | val_1_rmse: 0.61262 |  0:00:04s
epoch 35 | loss: 0.38748 | val_0_rmse: 0.61389 | val_1_rmse: 0.60595 |  0:00:04s
epoch 36 | loss: 0.38627 | val_0_rmse: 0.61209 | val_1_rmse: 0.60565 |  0:00:04s
epoch 37 | loss: 0.39073 | val_0_rmse: 0.60985 | val_1_rmse: 0.61172 |  0:00:05s
epoch 38 | loss: 0.37494 | val_0_rmse: 0.62035 | val_1_rmse: 0.62253 |  0:00:05s
epoch 39 | loss: 0.3889  | val_0_rmse: 0.6073  | val_1_rmse: 0.61251 |  0:00:05s
epoch 40 | loss: 0.37807 | val_0_rmse: 0.61676 | val_1_rmse: 0.61055 |  0:00:05s
epoch 41 | loss: 0.39354 | val_0_rmse: 0.61095 | val_1_rmse: 0.61344 |  0:00:05s
epoch 42 | loss: 0.38573 | val_0_rmse: 0.60725 | val_1_rmse: 0.61453 |  0:00:05s
epoch 43 | loss: 0.3843  | val_0_rmse: 0.60487 | val_1_rmse: 0.61695 |  0:00:05s
epoch 44 | loss: 0.38457 | val_0_rmse: 0.60926 | val_1_rmse: 0.63466 |  0:00:06s
epoch 45 | loss: 0.3836  | val_0_rmse: 0.60395 | val_1_rmse: 0.60912 |  0:00:06s
epoch 46 | loss: 0.37876 | val_0_rmse: 0.6081  | val_1_rmse: 0.60795 |  0:00:06s
epoch 47 | loss: 0.37543 | val_0_rmse: 0.60748 | val_1_rmse: 0.61672 |  0:00:06s
epoch 48 | loss: 0.38268 | val_0_rmse: 0.60986 | val_1_rmse: 0.62137 |  0:00:06s
epoch 49 | loss: 0.38447 | val_0_rmse: 0.60774 | val_1_rmse: 0.63217 |  0:00:06s
epoch 50 | loss: 0.37585 | val_0_rmse: 0.61532 | val_1_rmse: 0.63268 |  0:00:06s
epoch 51 | loss: 0.3826  | val_0_rmse: 0.62298 | val_1_rmse: 0.63219 |  0:00:06s
epoch 52 | loss: 0.38205 | val_0_rmse: 0.61637 | val_1_rmse: 0.62127 |  0:00:07s
epoch 53 | loss: 0.3858  | val_0_rmse: 0.60833 | val_1_rmse: 0.61641 |  0:00:07s
epoch 54 | loss: 0.38186 | val_0_rmse: 0.60393 | val_1_rmse: 0.61472 |  0:00:07s
epoch 55 | loss: 0.3801  | val_0_rmse: 0.60357 | val_1_rmse: 0.60633 |  0:00:07s
epoch 56 | loss: 0.37667 | val_0_rmse: 0.60602 | val_1_rmse: 0.60451 |  0:00:07s
epoch 57 | loss: 0.38343 | val_0_rmse: 0.60006 | val_1_rmse: 0.61209 |  0:00:07s
epoch 58 | loss: 0.36935 | val_0_rmse: 0.60007 | val_1_rmse: 0.61842 |  0:00:07s
epoch 59 | loss: 0.38139 | val_0_rmse: 0.60717 | val_1_rmse: 0.61894 |  0:00:08s
epoch 60 | loss: 0.37923 | val_0_rmse: 0.60374 | val_1_rmse: 0.61655 |  0:00:08s
epoch 61 | loss: 0.37151 | val_0_rmse: 0.60102 | val_1_rmse: 0.60977 |  0:00:08s
epoch 62 | loss: 0.36742 | val_0_rmse: 0.60794 | val_1_rmse: 0.62121 |  0:00:08s
epoch 63 | loss: 0.37546 | val_0_rmse: 0.59735 | val_1_rmse: 0.60925 |  0:00:08s
epoch 64 | loss: 0.37356 | val_0_rmse: 0.5954  | val_1_rmse: 0.61085 |  0:00:08s
epoch 65 | loss: 0.37482 | val_0_rmse: 0.59298 | val_1_rmse: 0.62397 |  0:00:08s
epoch 66 | loss: 0.3627  | val_0_rmse: 0.59863 | val_1_rmse: 0.64018 |  0:00:08s
epoch 67 | loss: 0.38878 | val_0_rmse: 0.61429 | val_1_rmse: 0.65204 |  0:00:09s
epoch 68 | loss: 0.37665 | val_0_rmse: 0.60074 | val_1_rmse: 0.63537 |  0:00:09s
epoch 69 | loss: 0.37887 | val_0_rmse: 0.5967  | val_1_rmse: 0.61349 |  0:00:09s
epoch 70 | loss: 0.37464 | val_0_rmse: 0.60417 | val_1_rmse: 0.6229  |  0:00:09s
epoch 71 | loss: 0.37934 | val_0_rmse: 0.61025 | val_1_rmse: 0.62688 |  0:00:09s
epoch 72 | loss: 0.36775 | val_0_rmse: 0.59818 | val_1_rmse: 0.60983 |  0:00:09s
epoch 73 | loss: 0.36309 | val_0_rmse: 0.59489 | val_1_rmse: 0.61124 |  0:00:09s
epoch 74 | loss: 0.35829 | val_0_rmse: 0.61016 | val_1_rmse: 0.62879 |  0:00:09s
epoch 75 | loss: 0.37027 | val_0_rmse: 0.59056 | val_1_rmse: 0.60643 |  0:00:10s
epoch 76 | loss: 0.37921 | val_0_rmse: 0.59273 | val_1_rmse: 0.60481 |  0:00:10s
epoch 77 | loss: 0.36077 | val_0_rmse: 0.59089 | val_1_rmse: 0.61912 |  0:00:10s
epoch 78 | loss: 0.35783 | val_0_rmse: 0.59416 | val_1_rmse: 0.60709 |  0:00:10s
epoch 79 | loss: 0.37055 | val_0_rmse: 0.61014 | val_1_rmse: 0.6175  |  0:00:10s
epoch 80 | loss: 0.36209 | val_0_rmse: 0.6177  | val_1_rmse: 0.61986 |  0:00:10s
epoch 81 | loss: 0.36205 | val_0_rmse: 0.59488 | val_1_rmse: 0.60311 |  0:00:10s
epoch 82 | loss: 0.34675 | val_0_rmse: 0.58807 | val_1_rmse: 0.6176  |  0:00:11s
epoch 83 | loss: 0.3455  | val_0_rmse: 0.57951 | val_1_rmse: 0.60692 |  0:00:11s
epoch 84 | loss: 0.35267 | val_0_rmse: 0.59125 | val_1_rmse: 0.60266 |  0:00:11s
epoch 85 | loss: 0.35733 | val_0_rmse: 0.59137 | val_1_rmse: 0.60359 |  0:00:11s
epoch 86 | loss: 0.3465  | val_0_rmse: 0.59176 | val_1_rmse: 0.61816 |  0:00:11s
epoch 87 | loss: 0.36664 | val_0_rmse: 0.61474 | val_1_rmse: 0.64209 |  0:00:11s
epoch 88 | loss: 0.35638 | val_0_rmse: 0.57978 | val_1_rmse: 0.60713 |  0:00:11s
epoch 89 | loss: 0.35198 | val_0_rmse: 0.63447 | val_1_rmse: 0.65993 |  0:00:11s
epoch 90 | loss: 0.34311 | val_0_rmse: 0.57816 | val_1_rmse: 0.59657 |  0:00:12s
epoch 91 | loss: 0.33703 | val_0_rmse: 0.7257  | val_1_rmse: 0.75391 |  0:00:12s
epoch 92 | loss: 0.33889 | val_0_rmse: 0.72035 | val_1_rmse: 0.75322 |  0:00:12s
epoch 93 | loss: 0.33143 | val_0_rmse: 0.59218 | val_1_rmse: 0.62782 |  0:00:12s
epoch 94 | loss: 0.35673 | val_0_rmse: 0.59424 | val_1_rmse: 0.60683 |  0:00:12s
epoch 95 | loss: 0.34549 | val_0_rmse: 0.58883 | val_1_rmse: 0.59344 |  0:00:12s
epoch 96 | loss: 0.34473 | val_0_rmse: 0.58935 | val_1_rmse: 0.61446 |  0:00:12s
epoch 97 | loss: 0.34433 | val_0_rmse: 0.65264 | val_1_rmse: 0.67617 |  0:00:12s
epoch 98 | loss: 0.3526  | val_0_rmse: 0.64283 | val_1_rmse: 0.64522 |  0:00:13s
epoch 99 | loss: 0.34737 | val_0_rmse: 0.60646 | val_1_rmse: 0.60269 |  0:00:13s
epoch 100| loss: 0.34456 | val_0_rmse: 0.61917 | val_1_rmse: 0.61296 |  0:00:13s
epoch 101| loss: 0.33777 | val_0_rmse: 0.66497 | val_1_rmse: 0.65609 |  0:00:13s
epoch 102| loss: 0.34036 | val_0_rmse: 0.5841  | val_1_rmse: 0.57845 |  0:00:13s
epoch 103| loss: 0.3318  | val_0_rmse: 0.58024 | val_1_rmse: 0.56845 |  0:00:13s
epoch 104| loss: 0.32961 | val_0_rmse: 0.57004 | val_1_rmse: 0.56064 |  0:00:13s
epoch 105| loss: 0.33612 | val_0_rmse: 0.57256 | val_1_rmse: 0.5819  |  0:00:14s
epoch 106| loss: 0.3269  | val_0_rmse: 0.56255 | val_1_rmse: 0.57593 |  0:00:14s
epoch 107| loss: 0.32451 | val_0_rmse: 0.56411 | val_1_rmse: 0.56937 |  0:00:14s
epoch 108| loss: 0.33094 | val_0_rmse: 0.55624 | val_1_rmse: 0.56438 |  0:00:14s
epoch 109| loss: 0.33002 | val_0_rmse: 0.58644 | val_1_rmse: 0.60269 |  0:00:14s
epoch 110| loss: 0.32555 | val_0_rmse: 0.59761 | val_1_rmse: 0.61591 |  0:00:14s
epoch 111| loss: 0.31807 | val_0_rmse: 0.55969 | val_1_rmse: 0.58025 |  0:00:14s
epoch 112| loss: 0.31725 | val_0_rmse: 0.56979 | val_1_rmse: 0.59234 |  0:00:15s
epoch 113| loss: 0.33629 | val_0_rmse: 0.55299 | val_1_rmse: 0.55989 |  0:00:15s
epoch 114| loss: 0.31811 | val_0_rmse: 0.60429 | val_1_rmse: 0.60785 |  0:00:15s
epoch 115| loss: 0.32785 | val_0_rmse: 0.584   | val_1_rmse: 0.56586 |  0:00:15s
epoch 116| loss: 0.32573 | val_0_rmse: 0.59392 | val_1_rmse: 0.57976 |  0:00:15s
epoch 117| loss: 0.30645 | val_0_rmse: 0.56658 | val_1_rmse: 0.55937 |  0:00:15s
epoch 118| loss: 0.3302  | val_0_rmse: 0.5977  | val_1_rmse: 0.6026  |  0:00:15s
epoch 119| loss: 0.32298 | val_0_rmse: 0.57458 | val_1_rmse: 0.56159 |  0:00:15s
epoch 120| loss: 0.31938 | val_0_rmse: 0.64008 | val_1_rmse: 0.63737 |  0:00:16s
epoch 121| loss: 0.31469 | val_0_rmse: 0.62133 | val_1_rmse: 0.62086 |  0:00:16s
epoch 122| loss: 0.32704 | val_0_rmse: 0.59605 | val_1_rmse: 0.58216 |  0:00:16s
epoch 123| loss: 0.31535 | val_0_rmse: 0.6266  | val_1_rmse: 0.59965 |  0:00:16s
epoch 124| loss: 0.32433 | val_0_rmse: 0.63314 | val_1_rmse: 0.60815 |  0:00:16s
epoch 125| loss: 0.34577 | val_0_rmse: 0.60802 | val_1_rmse: 0.56766 |  0:00:16s
epoch 126| loss: 0.33391 | val_0_rmse: 0.59534 | val_1_rmse: 0.55518 |  0:00:16s
epoch 127| loss: 0.3483  | val_0_rmse: 0.59581 | val_1_rmse: 0.56374 |  0:00:16s
epoch 128| loss: 0.37323 | val_0_rmse: 0.81411 | val_1_rmse: 0.80776 |  0:00:17s
epoch 129| loss: 0.34325 | val_0_rmse: 1.03178 | val_1_rmse: 1.02683 |  0:00:17s
epoch 130| loss: 0.35747 | val_0_rmse: 0.8678  | val_1_rmse: 0.85628 |  0:00:17s
epoch 131| loss: 0.34623 | val_0_rmse: 0.78098 | val_1_rmse: 0.76176 |  0:00:17s
epoch 132| loss: 0.3514  | val_0_rmse: 0.88821 | val_1_rmse: 0.86528 |  0:00:17s
epoch 133| loss: 0.33305 | val_0_rmse: 0.9824  | val_1_rmse: 0.95654 |  0:00:17s
epoch 134| loss: 0.35251 | val_0_rmse: 0.87664 | val_1_rmse: 0.86288 |  0:00:17s
epoch 135| loss: 0.33906 | val_0_rmse: 0.80973 | val_1_rmse: 0.8052  |  0:00:18s
epoch 136| loss: 0.34168 | val_0_rmse: 0.7812  | val_1_rmse: 0.79331 |  0:00:18s
epoch 137| loss: 0.33271 | val_0_rmse: 0.67702 | val_1_rmse: 0.67188 |  0:00:18s
epoch 138| loss: 0.33433 | val_0_rmse: 0.58051 | val_1_rmse: 0.5409  |  0:00:18s
epoch 139| loss: 0.33506 | val_0_rmse: 0.55876 | val_1_rmse: 0.53069 |  0:00:18s
epoch 140| loss: 0.34129 | val_0_rmse: 0.55945 | val_1_rmse: 0.5382  |  0:00:18s
epoch 141| loss: 0.32551 | val_0_rmse: 0.58411 | val_1_rmse: 0.56713 |  0:00:18s
epoch 142| loss: 0.34161 | val_0_rmse: 0.60821 | val_1_rmse: 0.59599 |  0:00:18s
epoch 143| loss: 0.32768 | val_0_rmse: 0.65316 | val_1_rmse: 0.65563 |  0:00:19s
epoch 144| loss: 0.31845 | val_0_rmse: 0.71724 | val_1_rmse: 0.71829 |  0:00:19s
epoch 145| loss: 0.32306 | val_0_rmse: 0.70433 | val_1_rmse: 0.70362 |  0:00:19s
epoch 146| loss: 0.32632 | val_0_rmse: 0.65379 | val_1_rmse: 0.65466 |  0:00:19s
epoch 147| loss: 0.32209 | val_0_rmse: 0.62038 | val_1_rmse: 0.62202 |  0:00:19s
epoch 148| loss: 0.33158 | val_0_rmse: 0.64472 | val_1_rmse: 0.65471 |  0:00:19s
epoch 149| loss: 0.35982 | val_0_rmse: 0.57542 | val_1_rmse: 0.55743 |  0:00:19s
Stop training because you reached max_epochs = 150 with best_epoch = 139 and best_val_1_rmse = 0.53069
Best weights from best epoch are automatically used!
ended training at: 14:11:05
Feature importance:
[('Area', 0.2504127753761936), ('Baths', 0.07197437372670645), ('Beds', 0.07352660566839667), ('Latitude', 0.39716118668755646), ('Longitude', 0.17232062329076364), ('Month', 0.03393819867251666), ('Year', 0.000666236577866531)]
Mean squared error is of 3186086383.7628264
Mean absolute error:39605.76780412087
MAPE:0.3643080296957011
R2 score:0.581204512162534
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:11:05
epoch 0  | loss: 1.55318 | val_0_rmse: 1.83221 | val_1_rmse: 1.18596 |  0:00:00s
epoch 1  | loss: 0.71822 | val_0_rmse: 1.26858 | val_1_rmse: 0.99981 |  0:00:00s
epoch 2  | loss: 0.63232 | val_0_rmse: 1.5244  | val_1_rmse: 0.84263 |  0:00:00s
epoch 3  | loss: 0.56131 | val_0_rmse: 1.35622 | val_1_rmse: 0.86425 |  0:00:00s
epoch 4  | loss: 0.52459 | val_0_rmse: 0.86834 | val_1_rmse: 0.84091 |  0:00:00s
epoch 5  | loss: 0.50305 | val_0_rmse: 0.82986 | val_1_rmse: 0.86001 |  0:00:00s
epoch 6  | loss: 0.48582 | val_0_rmse: 0.80332 | val_1_rmse: 0.84253 |  0:00:00s
epoch 7  | loss: 0.45376 | val_0_rmse: 0.80809 | val_1_rmse: 0.85511 |  0:00:01s
epoch 8  | loss: 0.44944 | val_0_rmse: 0.74038 | val_1_rmse: 0.80045 |  0:00:01s
epoch 9  | loss: 0.44551 | val_0_rmse: 0.69836 | val_1_rmse: 0.76322 |  0:00:01s
epoch 10 | loss: 0.4503  | val_0_rmse: 0.68842 | val_1_rmse: 0.77046 |  0:00:01s
epoch 11 | loss: 0.44888 | val_0_rmse: 0.6983  | val_1_rmse: 0.76495 |  0:00:01s
epoch 12 | loss: 0.43941 | val_0_rmse: 0.68527 | val_1_rmse: 0.74549 |  0:00:01s
epoch 13 | loss: 0.44685 | val_0_rmse: 0.66825 | val_1_rmse: 0.71834 |  0:00:01s
epoch 14 | loss: 0.41595 | val_0_rmse: 0.66082 | val_1_rmse: 0.70724 |  0:00:02s
epoch 15 | loss: 0.41709 | val_0_rmse: 0.65388 | val_1_rmse: 0.70501 |  0:00:02s
epoch 16 | loss: 0.41886 | val_0_rmse: 0.63838 | val_1_rmse: 0.69281 |  0:00:02s
epoch 17 | loss: 0.41469 | val_0_rmse: 0.63129 | val_1_rmse: 0.68434 |  0:00:02s
epoch 18 | loss: 0.41377 | val_0_rmse: 0.63135 | val_1_rmse: 0.686   |  0:00:02s
epoch 19 | loss: 0.40839 | val_0_rmse: 0.62956 | val_1_rmse: 0.69137 |  0:00:02s
epoch 20 | loss: 0.40557 | val_0_rmse: 0.63255 | val_1_rmse: 0.70312 |  0:00:02s
epoch 21 | loss: 0.39424 | val_0_rmse: 0.62634 | val_1_rmse: 0.69515 |  0:00:02s
epoch 22 | loss: 0.40756 | val_0_rmse: 0.62491 | val_1_rmse: 0.69336 |  0:00:03s
epoch 23 | loss: 0.39052 | val_0_rmse: 0.62326 | val_1_rmse: 0.69954 |  0:00:03s
epoch 24 | loss: 0.38556 | val_0_rmse: 0.62051 | val_1_rmse: 0.69649 |  0:00:03s
epoch 25 | loss: 0.38375 | val_0_rmse: 0.6204  | val_1_rmse: 0.68902 |  0:00:03s
epoch 26 | loss: 0.3913  | val_0_rmse: 0.61448 | val_1_rmse: 0.6834  |  0:00:03s
epoch 27 | loss: 0.3922  | val_0_rmse: 0.61301 | val_1_rmse: 0.6803  |  0:00:03s
epoch 28 | loss: 0.3891  | val_0_rmse: 0.61358 | val_1_rmse: 0.68249 |  0:00:03s
epoch 29 | loss: 0.39121 | val_0_rmse: 0.60926 | val_1_rmse: 0.68802 |  0:00:04s
epoch 30 | loss: 0.39539 | val_0_rmse: 0.61231 | val_1_rmse: 0.68643 |  0:00:04s
epoch 31 | loss: 0.39892 | val_0_rmse: 0.61468 | val_1_rmse: 0.68423 |  0:00:04s
epoch 32 | loss: 0.38441 | val_0_rmse: 0.61522 | val_1_rmse: 0.68478 |  0:00:04s
epoch 33 | loss: 0.39575 | val_0_rmse: 0.61507 | val_1_rmse: 0.68713 |  0:00:04s
epoch 34 | loss: 0.37331 | val_0_rmse: 0.61129 | val_1_rmse: 0.67869 |  0:00:04s
epoch 35 | loss: 0.37807 | val_0_rmse: 0.60572 | val_1_rmse: 0.67376 |  0:00:04s
epoch 36 | loss: 0.37457 | val_0_rmse: 0.60355 | val_1_rmse: 0.6747  |  0:00:04s
epoch 37 | loss: 0.3953  | val_0_rmse: 0.60804 | val_1_rmse: 0.68019 |  0:00:05s
epoch 38 | loss: 0.38731 | val_0_rmse: 0.61153 | val_1_rmse: 0.68686 |  0:00:05s
epoch 39 | loss: 0.38565 | val_0_rmse: 0.61082 | val_1_rmse: 0.68867 |  0:00:05s
epoch 40 | loss: 0.3921  | val_0_rmse: 0.60598 | val_1_rmse: 0.69211 |  0:00:05s
epoch 41 | loss: 0.38413 | val_0_rmse: 0.61234 | val_1_rmse: 0.68511 |  0:00:05s
epoch 42 | loss: 0.38102 | val_0_rmse: 0.60859 | val_1_rmse: 0.67254 |  0:00:05s
epoch 43 | loss: 0.38381 | val_0_rmse: 0.60243 | val_1_rmse: 0.67303 |  0:00:05s
epoch 44 | loss: 0.37122 | val_0_rmse: 0.60064 | val_1_rmse: 0.6796  |  0:00:06s
epoch 45 | loss: 0.38045 | val_0_rmse: 0.61815 | val_1_rmse: 0.68913 |  0:00:06s
epoch 46 | loss: 0.39319 | val_0_rmse: 0.61791 | val_1_rmse: 0.69456 |  0:00:06s
epoch 47 | loss: 0.39073 | val_0_rmse: 0.60793 | val_1_rmse: 0.69558 |  0:00:06s
epoch 48 | loss: 0.38684 | val_0_rmse: 0.60398 | val_1_rmse: 0.69012 |  0:00:06s
epoch 49 | loss: 0.3828  | val_0_rmse: 0.60694 | val_1_rmse: 0.68433 |  0:00:06s
epoch 50 | loss: 0.38247 | val_0_rmse: 0.60581 | val_1_rmse: 0.68253 |  0:00:06s
epoch 51 | loss: 0.3784  | val_0_rmse: 0.60533 | val_1_rmse: 0.68282 |  0:00:06s
epoch 52 | loss: 0.379   | val_0_rmse: 0.60927 | val_1_rmse: 0.68778 |  0:00:07s
epoch 53 | loss: 0.38227 | val_0_rmse: 0.6036  | val_1_rmse: 0.68266 |  0:00:07s
epoch 54 | loss: 0.36682 | val_0_rmse: 0.59872 | val_1_rmse: 0.68149 |  0:00:07s
epoch 55 | loss: 0.36735 | val_0_rmse: 0.6006  | val_1_rmse: 0.68428 |  0:00:07s
epoch 56 | loss: 0.36498 | val_0_rmse: 0.59875 | val_1_rmse: 0.69338 |  0:00:07s
epoch 57 | loss: 0.37513 | val_0_rmse: 0.59472 | val_1_rmse: 0.6911  |  0:00:07s
epoch 58 | loss: 0.37425 | val_0_rmse: 0.60224 | val_1_rmse: 0.69875 |  0:00:07s
epoch 59 | loss: 0.3779  | val_0_rmse: 0.59688 | val_1_rmse: 0.69964 |  0:00:07s
epoch 60 | loss: 0.38297 | val_0_rmse: 0.59377 | val_1_rmse: 0.70465 |  0:00:08s
epoch 61 | loss: 0.37213 | val_0_rmse: 0.60523 | val_1_rmse: 0.71789 |  0:00:08s
epoch 62 | loss: 0.37562 | val_0_rmse: 0.59827 | val_1_rmse: 0.70286 |  0:00:08s
epoch 63 | loss: 0.36762 | val_0_rmse: 0.59794 | val_1_rmse: 0.69551 |  0:00:08s
epoch 64 | loss: 0.3749  | val_0_rmse: 0.59648 | val_1_rmse: 0.68757 |  0:00:08s
epoch 65 | loss: 0.36014 | val_0_rmse: 0.59927 | val_1_rmse: 0.68399 |  0:00:08s
epoch 66 | loss: 0.36824 | val_0_rmse: 0.59669 | val_1_rmse: 0.68077 |  0:00:08s
epoch 67 | loss: 0.37223 | val_0_rmse: 0.59587 | val_1_rmse: 0.68805 |  0:00:09s
epoch 68 | loss: 0.36012 | val_0_rmse: 0.59496 | val_1_rmse: 0.68741 |  0:00:09s
epoch 69 | loss: 0.36082 | val_0_rmse: 0.58978 | val_1_rmse: 0.67865 |  0:00:09s
epoch 70 | loss: 0.36624 | val_0_rmse: 0.58656 | val_1_rmse: 0.68548 |  0:00:09s
epoch 71 | loss: 0.35758 | val_0_rmse: 0.59093 | val_1_rmse: 0.70002 |  0:00:09s
epoch 72 | loss: 0.36635 | val_0_rmse: 0.59634 | val_1_rmse: 0.69946 |  0:00:09s

Early stopping occured at epoch 72 with best_epoch = 42 and best_val_1_rmse = 0.67254
Best weights from best epoch are automatically used!
ended training at: 14:11:15
Feature importance:
[('Area', 0.37797603102143185), ('Baths', 0.13295408032944372), ('Beds', 0.12073935662211677), ('Latitude', 0.1449029866763443), ('Longitude', 0.14481612932898175), ('Month', 0.07378071536560717), ('Year', 0.004830700656074472)]
Mean squared error is of 3628538775.9289584
Mean absolute error:44857.46567561813
MAPE:0.3479034102700605
R2 score:0.4906449508825499
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:11:15
epoch 0  | loss: 1.60648 | val_0_rmse: 1.08891 | val_1_rmse: 1.07518 |  0:00:00s
epoch 1  | loss: 0.85583 | val_0_rmse: 1.13399 | val_1_rmse: 1.13677 |  0:00:00s
epoch 2  | loss: 0.67856 | val_0_rmse: 1.11424 | val_1_rmse: 1.18509 |  0:00:00s
epoch 3  | loss: 0.57515 | val_0_rmse: 0.94498 | val_1_rmse: 1.01711 |  0:00:00s
epoch 4  | loss: 0.5616  | val_0_rmse: 0.76315 | val_1_rmse: 0.92108 |  0:00:00s
epoch 5  | loss: 0.52748 | val_0_rmse: 0.73062 | val_1_rmse: 0.92736 |  0:00:00s
epoch 6  | loss: 0.4943  | val_0_rmse: 0.75853 | val_1_rmse: 1.01817 |  0:00:01s
epoch 7  | loss: 0.49399 | val_0_rmse: 0.70692 | val_1_rmse: 0.895   |  0:00:01s
epoch 8  | loss: 0.47689 | val_0_rmse: 0.69262 | val_1_rmse: 0.841   |  0:00:01s
epoch 9  | loss: 0.47486 | val_0_rmse: 0.69136 | val_1_rmse: 0.82261 |  0:00:01s
epoch 10 | loss: 0.45322 | val_0_rmse: 0.68373 | val_1_rmse: 0.82348 |  0:00:01s
epoch 11 | loss: 0.44734 | val_0_rmse: 0.68945 | val_1_rmse: 0.82303 |  0:00:01s
epoch 12 | loss: 0.4437  | val_0_rmse: 0.68731 | val_1_rmse: 0.83997 |  0:00:01s
epoch 13 | loss: 0.44491 | val_0_rmse: 0.67393 | val_1_rmse: 0.84959 |  0:00:02s
epoch 14 | loss: 0.42528 | val_0_rmse: 0.67192 | val_1_rmse: 0.84379 |  0:00:02s
epoch 15 | loss: 0.42074 | val_0_rmse: 0.66696 | val_1_rmse: 0.83285 |  0:00:02s
epoch 16 | loss: 0.4199  | val_0_rmse: 0.65374 | val_1_rmse: 0.80502 |  0:00:02s
epoch 17 | loss: 0.41386 | val_0_rmse: 0.64492 | val_1_rmse: 0.81111 |  0:00:02s
epoch 18 | loss: 0.41487 | val_0_rmse: 0.64508 | val_1_rmse: 0.85289 |  0:00:02s
epoch 19 | loss: 0.39933 | val_0_rmse: 0.66203 | val_1_rmse: 0.83649 |  0:00:03s
epoch 20 | loss: 0.38749 | val_0_rmse: 0.66534 | val_1_rmse: 0.84302 |  0:00:03s
epoch 21 | loss: 0.4069  | val_0_rmse: 0.65439 | val_1_rmse: 0.87674 |  0:00:03s
epoch 22 | loss: 0.38461 | val_0_rmse: 0.65492 | val_1_rmse: 0.89172 |  0:00:03s
epoch 23 | loss: 0.37879 | val_0_rmse: 0.64785 | val_1_rmse: 0.97089 |  0:00:03s
epoch 24 | loss: 0.37648 | val_0_rmse: 0.65095 | val_1_rmse: 1.01679 |  0:00:03s
epoch 25 | loss: 0.38256 | val_0_rmse: 0.65477 | val_1_rmse: 1.08686 |  0:00:03s
epoch 26 | loss: 0.39422 | val_0_rmse: 0.66111 | val_1_rmse: 1.07185 |  0:00:04s
epoch 27 | loss: 0.37513 | val_0_rmse: 0.65884 | val_1_rmse: 1.13898 |  0:00:04s
epoch 28 | loss: 0.39126 | val_0_rmse: 0.65922 | val_1_rmse: 1.10234 |  0:00:04s
epoch 29 | loss: 0.38209 | val_0_rmse: 0.65947 | val_1_rmse: 1.05835 |  0:00:04s
epoch 30 | loss: 0.38469 | val_0_rmse: 0.6611  | val_1_rmse: 1.10049 |  0:00:04s
epoch 31 | loss: 0.38898 | val_0_rmse: 0.64079 | val_1_rmse: 1.01339 |  0:00:04s
epoch 32 | loss: 0.36801 | val_0_rmse: 0.61687 | val_1_rmse: 0.78619 |  0:00:04s
epoch 33 | loss: 0.36703 | val_0_rmse: 0.62363 | val_1_rmse: 0.79499 |  0:00:05s
epoch 34 | loss: 0.35455 | val_0_rmse: 0.61833 | val_1_rmse: 0.74338 |  0:00:05s
epoch 35 | loss: 0.36058 | val_0_rmse: 0.62351 | val_1_rmse: 0.93507 |  0:00:05s
epoch 36 | loss: 0.35576 | val_0_rmse: 0.63337 | val_1_rmse: 0.98347 |  0:00:05s
epoch 37 | loss: 0.36673 | val_0_rmse: 0.64028 | val_1_rmse: 0.95786 |  0:00:05s
epoch 38 | loss: 0.36777 | val_0_rmse: 0.63228 | val_1_rmse: 0.97099 |  0:00:05s
epoch 39 | loss: 0.36476 | val_0_rmse: 0.61953 | val_1_rmse: 0.76579 |  0:00:06s
epoch 40 | loss: 0.35418 | val_0_rmse: 0.61795 | val_1_rmse: 0.75329 |  0:00:06s
epoch 41 | loss: 0.35684 | val_0_rmse: 0.62074 | val_1_rmse: 0.76484 |  0:00:06s
epoch 42 | loss: 0.35225 | val_0_rmse: 0.63071 | val_1_rmse: 0.69347 |  0:00:06s
epoch 43 | loss: 0.37395 | val_0_rmse: 0.6172  | val_1_rmse: 0.69108 |  0:00:06s
epoch 44 | loss: 0.37057 | val_0_rmse: 0.62128 | val_1_rmse: 0.71581 |  0:00:06s
epoch 45 | loss: 0.3766  | val_0_rmse: 0.61527 | val_1_rmse: 0.73623 |  0:00:06s
epoch 46 | loss: 0.35187 | val_0_rmse: 0.63137 | val_1_rmse: 0.9013  |  0:00:07s
epoch 47 | loss: 0.36175 | val_0_rmse: 0.62466 | val_1_rmse: 0.9153  |  0:00:07s
epoch 48 | loss: 0.34907 | val_0_rmse: 0.6137  | val_1_rmse: 0.8675  |  0:00:07s
epoch 49 | loss: 0.36508 | val_0_rmse: 0.61662 | val_1_rmse: 0.81824 |  0:00:07s
epoch 50 | loss: 0.35379 | val_0_rmse: 0.61994 | val_1_rmse: 0.81177 |  0:00:07s
epoch 51 | loss: 0.36024 | val_0_rmse: 0.64915 | val_1_rmse: 0.83021 |  0:00:07s
epoch 52 | loss: 0.37159 | val_0_rmse: 0.6458  | val_1_rmse: 0.86374 |  0:00:08s
epoch 53 | loss: 0.35435 | val_0_rmse: 0.69981 | val_1_rmse: 0.89003 |  0:00:08s
epoch 54 | loss: 0.36616 | val_0_rmse: 0.7362  | val_1_rmse: 0.84978 |  0:00:08s
epoch 55 | loss: 0.37608 | val_0_rmse: 0.65427 | val_1_rmse: 0.86551 |  0:00:08s
epoch 56 | loss: 0.39116 | val_0_rmse: 0.68078 | val_1_rmse: 0.87668 |  0:00:08s
epoch 57 | loss: 0.38914 | val_0_rmse: 0.62576 | val_1_rmse: 0.79523 |  0:00:08s
epoch 58 | loss: 0.37797 | val_0_rmse: 0.63346 | val_1_rmse: 0.745   |  0:00:08s
epoch 59 | loss: 0.37275 | val_0_rmse: 0.66389 | val_1_rmse: 0.78977 |  0:00:09s
epoch 60 | loss: 0.3984  | val_0_rmse: 0.66645 | val_1_rmse: 0.91042 |  0:00:09s
epoch 61 | loss: 0.40841 | val_0_rmse: 0.66104 | val_1_rmse: 0.9995  |  0:00:09s
epoch 62 | loss: 0.39807 | val_0_rmse: 0.66208 | val_1_rmse: 0.92677 |  0:00:09s
epoch 63 | loss: 0.39409 | val_0_rmse: 0.64726 | val_1_rmse: 0.96025 |  0:00:09s
epoch 64 | loss: 0.38769 | val_0_rmse: 0.6584  | val_1_rmse: 1.24107 |  0:00:09s
epoch 65 | loss: 0.38247 | val_0_rmse: 0.70894 | val_1_rmse: 1.19926 |  0:00:09s
epoch 66 | loss: 0.36155 | val_0_rmse: 0.67715 | val_1_rmse: 1.23235 |  0:00:10s
epoch 67 | loss: 0.37469 | val_0_rmse: 0.67242 | val_1_rmse: 1.26333 |  0:00:10s
epoch 68 | loss: 0.37566 | val_0_rmse: 0.69864 | val_1_rmse: 1.21454 |  0:00:10s
epoch 69 | loss: 0.35843 | val_0_rmse: 0.66327 | val_1_rmse: 0.90619 |  0:00:10s
epoch 70 | loss: 0.36048 | val_0_rmse: 0.65777 | val_1_rmse: 0.86979 |  0:00:10s
epoch 71 | loss: 0.3589  | val_0_rmse: 0.6794  | val_1_rmse: 0.83873 |  0:00:10s
epoch 72 | loss: 0.36317 | val_0_rmse: 0.63709 | val_1_rmse: 0.84104 |  0:00:10s
epoch 73 | loss: 0.36513 | val_0_rmse: 0.65444 | val_1_rmse: 0.80604 |  0:00:11s

Early stopping occured at epoch 73 with best_epoch = 43 and best_val_1_rmse = 0.69108
Best weights from best epoch are automatically used!
ended training at: 14:11:26
Feature importance:
[('Area', 0.23888041519630546), ('Baths', 0.29354195704691743), ('Beds', 0.0716805163133458), ('Latitude', 0.17304088965055003), ('Longitude', 0.17268138743087755), ('Month', 0.05016595080919776), ('Year', 8.88355280594563e-06)]
Mean squared error is of 3420552599.6465
Mean absolute error:39643.65926342317
MAPE:0.3102996246630343
R2 score:0.625850706637372
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:11:27
epoch 0  | loss: 1.57042 | val_0_rmse: 1.9665  | val_1_rmse: 2.00667 |  0:00:00s
epoch 1  | loss: 0.87866 | val_0_rmse: 1.35478 | val_1_rmse: 1.67064 |  0:00:00s
epoch 2  | loss: 0.63952 | val_0_rmse: 1.05805 | val_1_rmse: 0.97419 |  0:00:00s
epoch 3  | loss: 0.57799 | val_0_rmse: 0.84626 | val_1_rmse: 0.8415  |  0:00:00s
epoch 4  | loss: 0.53683 | val_0_rmse: 0.80988 | val_1_rmse: 0.82361 |  0:00:00s
epoch 5  | loss: 0.54918 | val_0_rmse: 0.78535 | val_1_rmse: 0.79231 |  0:00:00s
epoch 6  | loss: 0.5104  | val_0_rmse: 0.76875 | val_1_rmse: 0.78944 |  0:00:01s
epoch 7  | loss: 0.5248  | val_0_rmse: 0.73587 | val_1_rmse: 0.74294 |  0:00:01s
epoch 8  | loss: 0.52138 | val_0_rmse: 0.73859 | val_1_rmse: 0.73431 |  0:00:01s
epoch 9  | loss: 0.49055 | val_0_rmse: 0.71801 | val_1_rmse: 0.72105 |  0:00:01s
epoch 10 | loss: 0.46908 | val_0_rmse: 0.71135 | val_1_rmse: 0.72813 |  0:00:01s
epoch 11 | loss: 0.46343 | val_0_rmse: 0.6947  | val_1_rmse: 0.71225 |  0:00:01s
epoch 12 | loss: 0.45594 | val_0_rmse: 0.68011 | val_1_rmse: 0.69497 |  0:00:02s
epoch 13 | loss: 0.44199 | val_0_rmse: 0.69882 | val_1_rmse: 0.69905 |  0:00:02s
epoch 14 | loss: 0.441   | val_0_rmse: 0.71021 | val_1_rmse: 0.68842 |  0:00:02s
epoch 15 | loss: 0.43421 | val_0_rmse: 0.66536 | val_1_rmse: 0.65374 |  0:00:02s
epoch 16 | loss: 0.43326 | val_0_rmse: 0.65385 | val_1_rmse: 0.63643 |  0:00:02s
epoch 17 | loss: 0.43642 | val_0_rmse: 0.64863 | val_1_rmse: 0.64043 |  0:00:02s
epoch 18 | loss: 0.42325 | val_0_rmse: 0.65615 | val_1_rmse: 0.64867 |  0:00:02s
epoch 19 | loss: 0.42361 | val_0_rmse: 0.65673 | val_1_rmse: 0.67036 |  0:00:03s
epoch 20 | loss: 0.42417 | val_0_rmse: 0.65426 | val_1_rmse: 0.66304 |  0:00:03s
epoch 21 | loss: 0.42628 | val_0_rmse: 0.6527  | val_1_rmse: 0.66063 |  0:00:03s
epoch 22 | loss: 0.40819 | val_0_rmse: 0.64951 | val_1_rmse: 0.70898 |  0:00:03s
epoch 23 | loss: 0.42795 | val_0_rmse: 0.64893 | val_1_rmse: 0.70833 |  0:00:03s
epoch 24 | loss: 0.42002 | val_0_rmse: 0.64888 | val_1_rmse: 0.69532 |  0:00:03s
epoch 25 | loss: 0.4057  | val_0_rmse: 0.64789 | val_1_rmse: 0.68629 |  0:00:03s
epoch 26 | loss: 0.41307 | val_0_rmse: 0.62995 | val_1_rmse: 0.66538 |  0:00:04s
epoch 27 | loss: 0.40436 | val_0_rmse: 0.63832 | val_1_rmse: 0.67003 |  0:00:04s
epoch 28 | loss: 0.40946 | val_0_rmse: 0.63263 | val_1_rmse: 0.65509 |  0:00:04s
epoch 29 | loss: 0.40221 | val_0_rmse: 0.62536 | val_1_rmse: 0.65575 |  0:00:04s
epoch 30 | loss: 0.38272 | val_0_rmse: 0.63029 | val_1_rmse: 0.65376 |  0:00:04s
epoch 31 | loss: 0.39817 | val_0_rmse: 0.62778 | val_1_rmse: 0.65704 |  0:00:04s
epoch 32 | loss: 0.4321  | val_0_rmse: 0.63551 | val_1_rmse: 0.65995 |  0:00:05s
epoch 33 | loss: 0.41981 | val_0_rmse: 0.64802 | val_1_rmse: 0.67358 |  0:00:05s
epoch 34 | loss: 0.42027 | val_0_rmse: 0.66947 | val_1_rmse: 0.70034 |  0:00:05s
epoch 35 | loss: 0.41826 | val_0_rmse: 0.72242 | val_1_rmse: 0.75487 |  0:00:05s
epoch 36 | loss: 0.42746 | val_0_rmse: 0.64758 | val_1_rmse: 0.68122 |  0:00:05s
epoch 37 | loss: 0.40383 | val_0_rmse: 0.63696 | val_1_rmse: 0.64723 |  0:00:05s
epoch 38 | loss: 0.40461 | val_0_rmse: 0.65261 | val_1_rmse: 0.69358 |  0:00:06s
epoch 39 | loss: 0.40978 | val_0_rmse: 0.68672 | val_1_rmse: 0.76829 |  0:00:06s
epoch 40 | loss: 0.39805 | val_0_rmse: 0.68832 | val_1_rmse: 0.75739 |  0:00:06s
epoch 41 | loss: 0.40826 | val_0_rmse: 0.64671 | val_1_rmse: 0.69847 |  0:00:06s
epoch 42 | loss: 0.41123 | val_0_rmse: 0.63337 | val_1_rmse: 0.65586 |  0:00:06s
epoch 43 | loss: 0.43163 | val_0_rmse: 0.63456 | val_1_rmse: 0.66499 |  0:00:06s
epoch 44 | loss: 0.4189  | val_0_rmse: 0.65163 | val_1_rmse: 0.6944  |  0:00:06s
epoch 45 | loss: 0.42783 | val_0_rmse: 0.65313 | val_1_rmse: 0.6868  |  0:00:07s
epoch 46 | loss: 0.42065 | val_0_rmse: 0.64251 | val_1_rmse: 0.67741 |  0:00:07s

Early stopping occured at epoch 46 with best_epoch = 16 and best_val_1_rmse = 0.63643
Best weights from best epoch are automatically used!
ended training at: 14:11:34
Feature importance:
[('Area', 0.23153953090311516), ('Baths', 0.11516615627790137), ('Beds', 0.05631687568217497), ('Latitude', 0.11930051737896255), ('Longitude', 0.04740579758344817), ('Month', 0.1592269839582346), ('Year', 0.2710441382161632)]
Mean squared error is of 4294900419.6821976
Mean absolute error:44039.26428001645
MAPE:0.3621500180682697
R2 score:0.5141470734658743
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:11:34
epoch 0  | loss: 1.50988 | val_0_rmse: 1.60975 | val_1_rmse: 1.54598 |  0:00:00s
epoch 1  | loss: 0.88448 | val_0_rmse: 1.01153 | val_1_rmse: 0.811   |  0:00:00s
epoch 2  | loss: 0.6514  | val_0_rmse: 0.92078 | val_1_rmse: 0.83372 |  0:00:00s
epoch 3  | loss: 0.56107 | val_0_rmse: 0.82457 | val_1_rmse: 0.77595 |  0:00:00s
epoch 4  | loss: 0.54976 | val_0_rmse: 0.79616 | val_1_rmse: 0.72646 |  0:00:00s
epoch 5  | loss: 0.5161  | val_0_rmse: 0.82109 | val_1_rmse: 0.75014 |  0:00:00s
epoch 6  | loss: 0.52532 | val_0_rmse: 0.76395 | val_1_rmse: 0.70979 |  0:00:01s
epoch 7  | loss: 0.50573 | val_0_rmse: 0.74054 | val_1_rmse: 0.685   |  0:00:01s
epoch 8  | loss: 0.49531 | val_0_rmse: 0.73758 | val_1_rmse: 0.68385 |  0:00:01s
epoch 9  | loss: 0.49    | val_0_rmse: 0.72475 | val_1_rmse: 0.67875 |  0:00:01s
epoch 10 | loss: 0.47971 | val_0_rmse: 0.69535 | val_1_rmse: 0.64234 |  0:00:01s
epoch 11 | loss: 0.47536 | val_0_rmse: 0.69922 | val_1_rmse: 0.65947 |  0:00:01s
epoch 12 | loss: 0.48076 | val_0_rmse: 0.71636 | val_1_rmse: 0.65151 |  0:00:01s
epoch 13 | loss: 0.47705 | val_0_rmse: 0.71278 | val_1_rmse: 0.64903 |  0:00:02s
epoch 14 | loss: 0.47377 | val_0_rmse: 0.70921 | val_1_rmse: 0.64041 |  0:00:02s
epoch 15 | loss: 0.46887 | val_0_rmse: 0.68753 | val_1_rmse: 0.62624 |  0:00:02s
epoch 16 | loss: 0.45722 | val_0_rmse: 0.68154 | val_1_rmse: 0.627   |  0:00:02s
epoch 17 | loss: 0.45045 | val_0_rmse: 0.67989 | val_1_rmse: 0.63696 |  0:00:02s
epoch 18 | loss: 0.45725 | val_0_rmse: 0.67446 | val_1_rmse: 0.63486 |  0:00:02s
epoch 19 | loss: 0.43446 | val_0_rmse: 0.66273 | val_1_rmse: 0.63842 |  0:00:03s
epoch 20 | loss: 0.44812 | val_0_rmse: 0.66203 | val_1_rmse: 0.6297  |  0:00:03s
epoch 21 | loss: 0.44801 | val_0_rmse: 0.65528 | val_1_rmse: 0.61592 |  0:00:03s
epoch 22 | loss: 0.44298 | val_0_rmse: 0.67308 | val_1_rmse: 0.64717 |  0:00:03s
epoch 23 | loss: 0.43107 | val_0_rmse: 0.66798 | val_1_rmse: 0.64204 |  0:00:03s
epoch 24 | loss: 0.43796 | val_0_rmse: 0.65436 | val_1_rmse: 0.62823 |  0:00:03s
epoch 25 | loss: 0.44838 | val_0_rmse: 0.65012 | val_1_rmse: 0.6211  |  0:00:04s
epoch 26 | loss: 0.44471 | val_0_rmse: 0.65445 | val_1_rmse: 0.61828 |  0:00:04s
epoch 27 | loss: 0.43547 | val_0_rmse: 0.67672 | val_1_rmse: 0.64023 |  0:00:04s
epoch 28 | loss: 0.46683 | val_0_rmse: 0.67463 | val_1_rmse: 0.65393 |  0:00:04s
epoch 29 | loss: 0.44302 | val_0_rmse: 0.68339 | val_1_rmse: 0.64601 |  0:00:04s
epoch 30 | loss: 0.45878 | val_0_rmse: 0.67677 | val_1_rmse: 0.6444  |  0:00:04s
epoch 31 | loss: 0.45562 | val_0_rmse: 0.68186 | val_1_rmse: 0.65353 |  0:00:04s
epoch 32 | loss: 0.44198 | val_0_rmse: 0.67668 | val_1_rmse: 0.64697 |  0:00:05s
epoch 33 | loss: 0.4417  | val_0_rmse: 0.6637  | val_1_rmse: 0.63293 |  0:00:05s
epoch 34 | loss: 0.44837 | val_0_rmse: 0.6646  | val_1_rmse: 0.63632 |  0:00:05s
epoch 35 | loss: 0.42172 | val_0_rmse: 0.67731 | val_1_rmse: 0.658   |  0:00:05s
epoch 36 | loss: 0.44179 | val_0_rmse: 0.65479 | val_1_rmse: 0.63269 |  0:00:05s
epoch 37 | loss: 0.42755 | val_0_rmse: 0.64842 | val_1_rmse: 0.62742 |  0:00:05s
epoch 38 | loss: 0.4298  | val_0_rmse: 0.65082 | val_1_rmse: 0.63895 |  0:00:05s
epoch 39 | loss: 0.42213 | val_0_rmse: 0.65724 | val_1_rmse: 0.64699 |  0:00:06s
epoch 40 | loss: 0.41806 | val_0_rmse: 0.63699 | val_1_rmse: 0.63122 |  0:00:06s
epoch 41 | loss: 0.42583 | val_0_rmse: 0.63336 | val_1_rmse: 0.62648 |  0:00:06s
epoch 42 | loss: 0.40973 | val_0_rmse: 0.64519 | val_1_rmse: 0.63001 |  0:00:06s
epoch 43 | loss: 0.41949 | val_0_rmse: 0.63019 | val_1_rmse: 0.60387 |  0:00:06s
epoch 44 | loss: 0.39873 | val_0_rmse: 0.63106 | val_1_rmse: 0.5976  |  0:00:06s
epoch 45 | loss: 0.4084  | val_0_rmse: 0.62812 | val_1_rmse: 0.58935 |  0:00:06s
epoch 46 | loss: 0.40601 | val_0_rmse: 0.62356 | val_1_rmse: 0.59087 |  0:00:07s
epoch 47 | loss: 0.40129 | val_0_rmse: 0.62052 | val_1_rmse: 0.58539 |  0:00:07s
epoch 48 | loss: 0.3946  | val_0_rmse: 0.63649 | val_1_rmse: 0.60282 |  0:00:07s
epoch 49 | loss: 0.39364 | val_0_rmse: 0.62133 | val_1_rmse: 0.58612 |  0:00:07s
epoch 50 | loss: 0.39214 | val_0_rmse: 0.63418 | val_1_rmse: 0.59891 |  0:00:07s
epoch 51 | loss: 0.38384 | val_0_rmse: 0.62424 | val_1_rmse: 0.60454 |  0:00:07s
epoch 52 | loss: 0.39866 | val_0_rmse: 0.6533  | val_1_rmse: 0.61738 |  0:00:08s
epoch 53 | loss: 0.41022 | val_0_rmse: 0.72273 | val_1_rmse: 0.64499 |  0:00:08s
epoch 54 | loss: 0.43376 | val_0_rmse: 0.68872 | val_1_rmse: 0.66844 |  0:00:08s
epoch 55 | loss: 0.39822 | val_0_rmse: 0.65532 | val_1_rmse: 0.62341 |  0:00:08s
epoch 56 | loss: 0.40999 | val_0_rmse: 0.64049 | val_1_rmse: 0.60654 |  0:00:08s
epoch 57 | loss: 0.40569 | val_0_rmse: 0.64658 | val_1_rmse: 0.62242 |  0:00:08s
epoch 58 | loss: 0.38844 | val_0_rmse: 0.63293 | val_1_rmse: 0.60566 |  0:00:08s
epoch 59 | loss: 0.38523 | val_0_rmse: 0.63454 | val_1_rmse: 0.60916 |  0:00:09s
epoch 60 | loss: 0.39284 | val_0_rmse: 0.62745 | val_1_rmse: 0.60537 |  0:00:09s
epoch 61 | loss: 0.38443 | val_0_rmse: 0.61406 | val_1_rmse: 0.58727 |  0:00:09s
epoch 62 | loss: 0.37344 | val_0_rmse: 0.62026 | val_1_rmse: 0.59516 |  0:00:09s
epoch 63 | loss: 0.37626 | val_0_rmse: 0.61789 | val_1_rmse: 0.5863  |  0:00:09s
epoch 64 | loss: 0.36754 | val_0_rmse: 0.61227 | val_1_rmse: 0.58227 |  0:00:09s
epoch 65 | loss: 0.37238 | val_0_rmse: 0.6502  | val_1_rmse: 0.62162 |  0:00:10s
epoch 66 | loss: 0.35629 | val_0_rmse: 0.62483 | val_1_rmse: 0.59817 |  0:00:10s
epoch 67 | loss: 0.37341 | val_0_rmse: 0.6202  | val_1_rmse: 0.59001 |  0:00:10s
epoch 68 | loss: 0.36437 | val_0_rmse: 0.60954 | val_1_rmse: 0.5791  |  0:00:10s
epoch 69 | loss: 0.36296 | val_0_rmse: 0.59949 | val_1_rmse: 0.55662 |  0:00:10s
epoch 70 | loss: 0.36538 | val_0_rmse: 0.60821 | val_1_rmse: 0.57361 |  0:00:10s
epoch 71 | loss: 0.35729 | val_0_rmse: 0.59395 | val_1_rmse: 0.57168 |  0:00:10s
epoch 72 | loss: 0.36054 | val_0_rmse: 0.59692 | val_1_rmse: 0.56783 |  0:00:11s
epoch 73 | loss: 0.35143 | val_0_rmse: 0.60078 | val_1_rmse: 0.57047 |  0:00:11s
epoch 74 | loss: 0.35638 | val_0_rmse: 0.5956  | val_1_rmse: 0.56988 |  0:00:11s
epoch 75 | loss: 0.35957 | val_0_rmse: 0.59432 | val_1_rmse: 0.56523 |  0:00:11s
epoch 76 | loss: 0.3497  | val_0_rmse: 0.61325 | val_1_rmse: 0.58272 |  0:00:11s
epoch 77 | loss: 0.35032 | val_0_rmse: 0.61472 | val_1_rmse: 0.58973 |  0:00:11s
epoch 78 | loss: 0.36933 | val_0_rmse: 0.62057 | val_1_rmse: 0.59223 |  0:00:11s
epoch 79 | loss: 0.37836 | val_0_rmse: 0.64368 | val_1_rmse: 0.62013 |  0:00:12s
epoch 80 | loss: 0.37583 | val_0_rmse: 0.63066 | val_1_rmse: 0.60662 |  0:00:12s
epoch 81 | loss: 0.38371 | val_0_rmse: 0.61756 | val_1_rmse: 0.60359 |  0:00:12s
epoch 82 | loss: 0.37434 | val_0_rmse: 0.63115 | val_1_rmse: 0.62862 |  0:00:12s
epoch 83 | loss: 0.37998 | val_0_rmse: 0.63455 | val_1_rmse: 0.62099 |  0:00:12s
epoch 84 | loss: 0.37241 | val_0_rmse: 0.63326 | val_1_rmse: 0.60828 |  0:00:12s
epoch 85 | loss: 0.36748 | val_0_rmse: 0.70708 | val_1_rmse: 0.68831 |  0:00:13s
epoch 86 | loss: 0.38776 | val_0_rmse: 0.64186 | val_1_rmse: 0.61491 |  0:00:13s
epoch 87 | loss: 0.36886 | val_0_rmse: 0.62337 | val_1_rmse: 0.60657 |  0:00:13s
epoch 88 | loss: 0.35436 | val_0_rmse: 0.63824 | val_1_rmse: 0.6256  |  0:00:13s
epoch 89 | loss: 0.35623 | val_0_rmse: 0.59614 | val_1_rmse: 0.58316 |  0:00:13s
epoch 90 | loss: 0.35629 | val_0_rmse: 0.58696 | val_1_rmse: 0.57368 |  0:00:13s
epoch 91 | loss: 0.34132 | val_0_rmse: 0.59181 | val_1_rmse: 0.59206 |  0:00:13s
epoch 92 | loss: 0.3482  | val_0_rmse: 0.58463 | val_1_rmse: 0.57709 |  0:00:14s
epoch 93 | loss: 0.35764 | val_0_rmse: 0.59404 | val_1_rmse: 0.57862 |  0:00:14s
epoch 94 | loss: 0.35929 | val_0_rmse: 0.58972 | val_1_rmse: 0.59634 |  0:00:14s
epoch 95 | loss: 0.35046 | val_0_rmse: 0.59408 | val_1_rmse: 0.61209 |  0:00:14s
epoch 96 | loss: 0.379   | val_0_rmse: 0.67394 | val_1_rmse: 0.67829 |  0:00:14s
epoch 97 | loss: 0.39506 | val_0_rmse: 0.62003 | val_1_rmse: 0.62221 |  0:00:14s
epoch 98 | loss: 0.36181 | val_0_rmse: 0.60916 | val_1_rmse: 0.60535 |  0:00:14s
epoch 99 | loss: 0.35748 | val_0_rmse: 0.60312 | val_1_rmse: 0.59448 |  0:00:15s

Early stopping occured at epoch 99 with best_epoch = 69 and best_val_1_rmse = 0.55662
Best weights from best epoch are automatically used!
ended training at: 14:11:49
Feature importance:
[('Area', 0.20081417183159467), ('Baths', 0.20487055081708297), ('Beds', 0.07578445456851733), ('Latitude', 0.09363672281733022), ('Longitude', 0.2784803175013303), ('Month', 0.08491109764027112), ('Year', 0.061502684823873374)]
Mean squared error is of 3036729503.748754
Mean absolute error:37954.9153169567
MAPE:0.29480613801574507
R2 score:0.6736342523848177
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:11:49
epoch 0  | loss: 1.52023 | val_0_rmse: 1.14162 | val_1_rmse: 1.32238 |  0:00:00s
epoch 1  | loss: 0.79027 | val_0_rmse: 1.09095 | val_1_rmse: 1.21684 |  0:00:00s
epoch 2  | loss: 0.62128 | val_0_rmse: 1.07727 | val_1_rmse: 1.16521 |  0:00:00s
epoch 3  | loss: 0.59685 | val_0_rmse: 0.84932 | val_1_rmse: 0.93371 |  0:00:00s
epoch 4  | loss: 0.56736 | val_0_rmse: 0.83085 | val_1_rmse: 0.8837  |  0:00:00s
epoch 5  | loss: 0.50919 | val_0_rmse: 0.76925 | val_1_rmse: 0.83338 |  0:00:00s
epoch 6  | loss: 0.49182 | val_0_rmse: 0.72073 | val_1_rmse: 0.77664 |  0:00:01s
epoch 7  | loss: 0.48256 | val_0_rmse: 0.70407 | val_1_rmse: 0.76197 |  0:00:01s
epoch 8  | loss: 0.48816 | val_0_rmse: 0.72027 | val_1_rmse: 0.78194 |  0:00:01s
epoch 9  | loss: 0.49654 | val_0_rmse: 0.68727 | val_1_rmse: 0.75193 |  0:00:01s
epoch 10 | loss: 0.44552 | val_0_rmse: 0.66853 | val_1_rmse: 0.73146 |  0:00:01s
epoch 11 | loss: 0.46767 | val_0_rmse: 0.66358 | val_1_rmse: 0.73893 |  0:00:01s
epoch 12 | loss: 0.45111 | val_0_rmse: 0.66106 | val_1_rmse: 0.73033 |  0:00:02s
epoch 13 | loss: 0.43908 | val_0_rmse: 0.66361 | val_1_rmse: 0.72493 |  0:00:02s
epoch 14 | loss: 0.43664 | val_0_rmse: 0.67207 | val_1_rmse: 0.72989 |  0:00:02s
epoch 15 | loss: 0.44311 | val_0_rmse: 0.66778 | val_1_rmse: 0.73485 |  0:00:02s
epoch 16 | loss: 0.44113 | val_0_rmse: 0.65443 | val_1_rmse: 0.71785 |  0:00:02s
epoch 17 | loss: 0.41144 | val_0_rmse: 0.66089 | val_1_rmse: 0.7133  |  0:00:02s
epoch 18 | loss: 0.42846 | val_0_rmse: 0.66104 | val_1_rmse: 0.71382 |  0:00:03s
epoch 19 | loss: 0.42345 | val_0_rmse: 0.64807 | val_1_rmse: 0.70971 |  0:00:03s
epoch 20 | loss: 0.41129 | val_0_rmse: 0.64943 | val_1_rmse: 0.70638 |  0:00:03s
epoch 21 | loss: 0.41567 | val_0_rmse: 0.64663 | val_1_rmse: 0.71066 |  0:00:03s
epoch 22 | loss: 0.40842 | val_0_rmse: 0.64376 | val_1_rmse: 0.70817 |  0:00:03s
epoch 23 | loss: 0.40407 | val_0_rmse: 0.63767 | val_1_rmse: 0.7018  |  0:00:03s
epoch 24 | loss: 0.39115 | val_0_rmse: 0.63522 | val_1_rmse: 0.70434 |  0:00:03s
epoch 25 | loss: 0.40325 | val_0_rmse: 0.63314 | val_1_rmse: 0.70119 |  0:00:04s
epoch 26 | loss: 0.37479 | val_0_rmse: 0.62741 | val_1_rmse: 0.69986 |  0:00:04s
epoch 27 | loss: 0.40723 | val_0_rmse: 0.62659 | val_1_rmse: 0.70897 |  0:00:04s
epoch 28 | loss: 0.39444 | val_0_rmse: 0.6259  | val_1_rmse: 0.7227  |  0:00:04s
epoch 29 | loss: 0.38185 | val_0_rmse: 0.61991 | val_1_rmse: 0.72402 |  0:00:04s
epoch 30 | loss: 0.37787 | val_0_rmse: 0.61209 | val_1_rmse: 0.70756 |  0:00:04s
epoch 31 | loss: 0.39018 | val_0_rmse: 0.62009 | val_1_rmse: 0.74328 |  0:00:04s
epoch 32 | loss: 0.38247 | val_0_rmse: 0.61813 | val_1_rmse: 0.71965 |  0:00:05s
epoch 33 | loss: 0.38639 | val_0_rmse: 0.62154 | val_1_rmse: 0.69721 |  0:00:05s
epoch 34 | loss: 0.35964 | val_0_rmse: 0.61063 | val_1_rmse: 0.67912 |  0:00:05s
epoch 35 | loss: 0.36415 | val_0_rmse: 0.60064 | val_1_rmse: 0.67115 |  0:00:05s
epoch 36 | loss: 0.35885 | val_0_rmse: 0.60357 | val_1_rmse: 0.68528 |  0:00:05s
epoch 37 | loss: 0.35031 | val_0_rmse: 0.59857 | val_1_rmse: 0.67522 |  0:00:05s
epoch 38 | loss: 0.36336 | val_0_rmse: 0.60649 | val_1_rmse: 0.688   |  0:00:06s
epoch 39 | loss: 0.35437 | val_0_rmse: 0.60465 | val_1_rmse: 0.68263 |  0:00:06s
epoch 40 | loss: 0.38134 | val_0_rmse: 0.60575 | val_1_rmse: 0.71406 |  0:00:06s
epoch 41 | loss: 0.38826 | val_0_rmse: 0.60895 | val_1_rmse: 0.71461 |  0:00:06s
epoch 42 | loss: 0.37612 | val_0_rmse: 0.62553 | val_1_rmse: 0.69977 |  0:00:06s
epoch 43 | loss: 0.38908 | val_0_rmse: 0.61605 | val_1_rmse: 0.70075 |  0:00:06s
epoch 44 | loss: 0.38443 | val_0_rmse: 0.6514  | val_1_rmse: 0.73354 |  0:00:06s
epoch 45 | loss: 0.38057 | val_0_rmse: 0.62203 | val_1_rmse: 0.70221 |  0:00:07s
epoch 46 | loss: 0.39795 | val_0_rmse: 0.62886 | val_1_rmse: 0.72441 |  0:00:07s
epoch 47 | loss: 0.36737 | val_0_rmse: 0.6219  | val_1_rmse: 0.71303 |  0:00:07s
epoch 48 | loss: 0.35727 | val_0_rmse: 0.59788 | val_1_rmse: 0.67646 |  0:00:07s
epoch 49 | loss: 0.38063 | val_0_rmse: 0.60262 | val_1_rmse: 0.67393 |  0:00:07s
epoch 50 | loss: 0.35705 | val_0_rmse: 0.58987 | val_1_rmse: 0.67852 |  0:00:07s
epoch 51 | loss: 0.37222 | val_0_rmse: 0.59945 | val_1_rmse: 0.68228 |  0:00:08s
epoch 52 | loss: 0.33928 | val_0_rmse: 0.61702 | val_1_rmse: 0.68619 |  0:00:08s
epoch 53 | loss: 0.35186 | val_0_rmse: 0.59463 | val_1_rmse: 0.67631 |  0:00:08s
epoch 54 | loss: 0.36229 | val_0_rmse: 0.60597 | val_1_rmse: 0.67798 |  0:00:08s
epoch 55 | loss: 0.34342 | val_0_rmse: 0.58511 | val_1_rmse: 0.64524 |  0:00:08s
epoch 56 | loss: 0.33936 | val_0_rmse: 0.58543 | val_1_rmse: 0.6404  |  0:00:08s
epoch 57 | loss: 0.33683 | val_0_rmse: 0.58162 | val_1_rmse: 0.64161 |  0:00:08s
epoch 58 | loss: 0.33099 | val_0_rmse: 0.56871 | val_1_rmse: 0.63575 |  0:00:09s
epoch 59 | loss: 0.32971 | val_0_rmse: 0.56629 | val_1_rmse: 0.63248 |  0:00:09s
epoch 60 | loss: 0.33522 | val_0_rmse: 0.56621 | val_1_rmse: 0.62589 |  0:00:09s
epoch 61 | loss: 0.33789 | val_0_rmse: 0.57321 | val_1_rmse: 0.62735 |  0:00:09s
epoch 62 | loss: 0.32579 | val_0_rmse: 0.57195 | val_1_rmse: 0.62826 |  0:00:09s
epoch 63 | loss: 0.33434 | val_0_rmse: 0.5885  | val_1_rmse: 0.63699 |  0:00:09s
epoch 64 | loss: 0.3182  | val_0_rmse: 0.57852 | val_1_rmse: 0.63339 |  0:00:10s
epoch 65 | loss: 0.33017 | val_0_rmse: 0.57893 | val_1_rmse: 0.63845 |  0:00:10s
epoch 66 | loss: 0.31227 | val_0_rmse: 0.57132 | val_1_rmse: 0.63902 |  0:00:10s
epoch 67 | loss: 0.32327 | val_0_rmse: 0.59429 | val_1_rmse: 0.66187 |  0:00:10s
epoch 68 | loss: 0.31602 | val_0_rmse: 0.6123  | val_1_rmse: 0.68081 |  0:00:10s
epoch 69 | loss: 0.32934 | val_0_rmse: 0.57317 | val_1_rmse: 0.64602 |  0:00:10s
epoch 70 | loss: 0.32041 | val_0_rmse: 0.57969 | val_1_rmse: 0.64984 |  0:00:10s
epoch 71 | loss: 0.32064 | val_0_rmse: 0.58766 | val_1_rmse: 0.66612 |  0:00:11s
epoch 72 | loss: 0.31075 | val_0_rmse: 0.5855  | val_1_rmse: 0.6742  |  0:00:11s
epoch 73 | loss: 0.31361 | val_0_rmse: 0.58621 | val_1_rmse: 0.67859 |  0:00:11s
epoch 74 | loss: 0.33338 | val_0_rmse: 0.58126 | val_1_rmse: 0.68007 |  0:00:11s
epoch 75 | loss: 0.31982 | val_0_rmse: 0.57765 | val_1_rmse: 0.67039 |  0:00:11s
epoch 76 | loss: 0.32006 | val_0_rmse: 0.57589 | val_1_rmse: 0.65303 |  0:00:11s
epoch 77 | loss: 0.31398 | val_0_rmse: 0.57486 | val_1_rmse: 0.64777 |  0:00:11s
epoch 78 | loss: 0.31224 | val_0_rmse: 0.59516 | val_1_rmse: 0.66472 |  0:00:12s
epoch 79 | loss: 0.30811 | val_0_rmse: 0.57597 | val_1_rmse: 0.65165 |  0:00:12s
epoch 80 | loss: 0.30039 | val_0_rmse: 0.56756 | val_1_rmse: 0.64641 |  0:00:12s
epoch 81 | loss: 0.30761 | val_0_rmse: 0.5611  | val_1_rmse: 0.64205 |  0:00:12s
epoch 82 | loss: 0.3009  | val_0_rmse: 0.56373 | val_1_rmse: 0.64603 |  0:00:12s
epoch 83 | loss: 0.31152 | val_0_rmse: 0.56808 | val_1_rmse: 0.63857 |  0:00:12s
epoch 84 | loss: 0.34306 | val_0_rmse: 0.58057 | val_1_rmse: 0.6448  |  0:00:12s
epoch 85 | loss: 0.31884 | val_0_rmse: 0.5865  | val_1_rmse: 0.64494 |  0:00:13s
epoch 86 | loss: 0.30155 | val_0_rmse: 0.58559 | val_1_rmse: 0.64612 |  0:00:13s
epoch 87 | loss: 0.31445 | val_0_rmse: 0.56919 | val_1_rmse: 0.62866 |  0:00:13s
epoch 88 | loss: 0.30977 | val_0_rmse: 0.58864 | val_1_rmse: 0.65013 |  0:00:13s
epoch 89 | loss: 0.31083 | val_0_rmse: 0.58305 | val_1_rmse: 0.65274 |  0:00:13s
epoch 90 | loss: 0.31302 | val_0_rmse: 0.56625 | val_1_rmse: 0.66047 |  0:00:13s

Early stopping occured at epoch 90 with best_epoch = 60 and best_val_1_rmse = 0.62589
Best weights from best epoch are automatically used!
ended training at: 14:12:03
Feature importance:
[('Area', 0.3302321001762734), ('Baths', 0.11521307543278261), ('Beds', 0.007374580384392151), ('Latitude', 0.24079959210865232), ('Longitude', 0.15524029221269134), ('Month', 0.040504846353123215), ('Year', 0.11063551333208495)]
Mean squared error is of 3231858167.663308
Mean absolute error:39641.25987011885
MAPE:0.3468978715640128
R2 score:0.6579041435534562
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:12:03
epoch 0  | loss: 1.5801  | val_0_rmse: 1.15279 | val_1_rmse: 1.36726 |  0:00:00s
epoch 1  | loss: 0.77017 | val_0_rmse: 1.29764 | val_1_rmse: 1.62059 |  0:00:00s
epoch 2  | loss: 0.62756 | val_0_rmse: 0.84337 | val_1_rmse: 0.92732 |  0:00:00s
epoch 3  | loss: 0.6038  | val_0_rmse: 0.95025 | val_1_rmse: 0.90628 |  0:00:00s
epoch 4  | loss: 0.58403 | val_0_rmse: 0.92339 | val_1_rmse: 0.87209 |  0:00:00s
epoch 5  | loss: 0.57569 | val_0_rmse: 0.81544 | val_1_rmse: 0.8317  |  0:00:00s
epoch 6  | loss: 0.5384  | val_0_rmse: 0.98791 | val_1_rmse: 1.04172 |  0:00:01s
epoch 7  | loss: 0.51694 | val_0_rmse: 0.839   | val_1_rmse: 0.88342 |  0:00:01s
epoch 8  | loss: 0.48909 | val_0_rmse: 0.75524 | val_1_rmse: 0.81043 |  0:00:01s
epoch 9  | loss: 0.49113 | val_0_rmse: 0.73357 | val_1_rmse: 0.81564 |  0:00:01s
epoch 10 | loss: 0.4821  | val_0_rmse: 0.74607 | val_1_rmse: 0.87331 |  0:00:01s
epoch 11 | loss: 0.47881 | val_0_rmse: 0.73973 | val_1_rmse: 0.90687 |  0:00:01s
epoch 12 | loss: 0.48243 | val_0_rmse: 0.70454 | val_1_rmse: 0.68731 |  0:00:01s
epoch 13 | loss: 0.46509 | val_0_rmse: 0.67852 | val_1_rmse: 0.66363 |  0:00:02s
epoch 14 | loss: 0.47005 | val_0_rmse: 0.67333 | val_1_rmse: 0.65959 |  0:00:02s
epoch 15 | loss: 0.46023 | val_0_rmse: 0.66616 | val_1_rmse: 0.64361 |  0:00:02s
epoch 16 | loss: 0.43445 | val_0_rmse: 0.65625 | val_1_rmse: 0.63266 |  0:00:02s
epoch 17 | loss: 0.42567 | val_0_rmse: 0.65118 | val_1_rmse: 0.61797 |  0:00:02s
epoch 18 | loss: 0.42945 | val_0_rmse: 0.67195 | val_1_rmse: 0.61663 |  0:00:02s
epoch 19 | loss: 0.42176 | val_0_rmse: 0.7044  | val_1_rmse: 0.6271  |  0:00:03s
epoch 20 | loss: 0.43228 | val_0_rmse: 0.67652 | val_1_rmse: 0.64579 |  0:00:03s
epoch 21 | loss: 0.46452 | val_0_rmse: 0.68071 | val_1_rmse: 0.64042 |  0:00:03s
epoch 22 | loss: 0.468   | val_0_rmse: 0.68629 | val_1_rmse: 0.94898 |  0:00:03s
epoch 23 | loss: 0.46001 | val_0_rmse: 1.07745 | val_1_rmse: 1.90972 |  0:00:03s
epoch 24 | loss: 0.49024 | val_0_rmse: 0.97831 | val_1_rmse: 1.63738 |  0:00:03s
epoch 25 | loss: 0.47728 | val_0_rmse: 0.8295  | val_1_rmse: 1.22943 |  0:00:03s
epoch 26 | loss: 0.45972 | val_0_rmse: 0.74797 | val_1_rmse: 0.90478 |  0:00:04s
epoch 27 | loss: 0.44802 | val_0_rmse: 0.74244 | val_1_rmse: 0.85029 |  0:00:04s
epoch 28 | loss: 0.43857 | val_0_rmse: 0.73878 | val_1_rmse: 0.95146 |  0:00:04s
epoch 29 | loss: 0.44323 | val_0_rmse: 0.7545  | val_1_rmse: 1.02867 |  0:00:04s
epoch 30 | loss: 0.42808 | val_0_rmse: 0.76579 | val_1_rmse: 1.05999 |  0:00:04s
epoch 31 | loss: 0.43559 | val_0_rmse: 0.7922  | val_1_rmse: 1.17433 |  0:00:04s
epoch 32 | loss: 0.41776 | val_0_rmse: 0.83119 | val_1_rmse: 1.08267 |  0:00:05s
epoch 33 | loss: 0.4134  | val_0_rmse: 0.82488 | val_1_rmse: 1.12041 |  0:00:05s
epoch 34 | loss: 0.41577 | val_0_rmse: 0.81399 | val_1_rmse: 1.05245 |  0:00:05s
epoch 35 | loss: 0.40271 | val_0_rmse: 0.84639 | val_1_rmse: 1.22658 |  0:00:05s
epoch 36 | loss: 0.39871 | val_0_rmse: 0.86018 | val_1_rmse: 1.224   |  0:00:05s
epoch 37 | loss: 0.4064  | val_0_rmse: 0.87504 | val_1_rmse: 1.23665 |  0:00:05s
epoch 38 | loss: 0.40138 | val_0_rmse: 0.8945  | val_1_rmse: 1.08439 |  0:00:05s
epoch 39 | loss: 0.39137 | val_0_rmse: 0.86506 | val_1_rmse: 1.28151 |  0:00:06s
epoch 40 | loss: 0.39434 | val_0_rmse: 0.90536 | val_1_rmse: 1.58561 |  0:00:06s
epoch 41 | loss: 0.37769 | val_0_rmse: 0.90354 | val_1_rmse: 1.58608 |  0:00:06s
epoch 42 | loss: 0.37922 | val_0_rmse: 0.88267 | val_1_rmse: 1.52626 |  0:00:06s
epoch 43 | loss: 0.37779 | val_0_rmse: 0.77282 | val_1_rmse: 1.2259  |  0:00:06s
epoch 44 | loss: 0.36352 | val_0_rmse: 0.76707 | val_1_rmse: 1.21484 |  0:00:06s
epoch 45 | loss: 0.37805 | val_0_rmse: 0.72413 | val_1_rmse: 1.06919 |  0:00:07s
epoch 46 | loss: 0.35863 | val_0_rmse: 0.68863 | val_1_rmse: 1.00894 |  0:00:07s
epoch 47 | loss: 0.35992 | val_0_rmse: 0.78356 | val_1_rmse: 1.08902 |  0:00:07s
epoch 48 | loss: 0.36775 | val_0_rmse: 0.76239 | val_1_rmse: 1.22085 |  0:00:07s

Early stopping occured at epoch 48 with best_epoch = 18 and best_val_1_rmse = 0.61663
Best weights from best epoch are automatically used!
ended training at: 14:12:11
Feature importance:
[('Area', 0.3098423443953395), ('Baths', 0.17418394980756746), ('Beds', 0.08714935034381538), ('Latitude', 0.04189077608448255), ('Longitude', 0.14545821997390704), ('Month', 0.07446515967890328), ('Year', 0.16701019971598483)]
Mean squared error is of 4237067404.6830573
Mean absolute error:44901.78141065365
MAPE:0.36274935507114636
R2 score:0.5561665936431911
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:12:11
epoch 0  | loss: 0.51663 | val_0_rmse: 0.59559 | val_1_rmse: 0.61209 |  0:00:01s
epoch 1  | loss: 0.35622 | val_0_rmse: 0.57728 | val_1_rmse: 0.59455 |  0:00:03s
epoch 2  | loss: 0.34134 | val_0_rmse: 0.5847  | val_1_rmse: 0.60365 |  0:00:05s
epoch 3  | loss: 0.33629 | val_0_rmse: 0.57919 | val_1_rmse: 0.5948  |  0:00:07s
epoch 4  | loss: 0.33666 | val_0_rmse: 0.57441 | val_1_rmse: 0.59187 |  0:00:09s
epoch 5  | loss: 0.33573 | val_0_rmse: 0.58137 | val_1_rmse: 0.59623 |  0:00:11s
epoch 6  | loss: 0.33664 | val_0_rmse: 0.58342 | val_1_rmse: 0.59952 |  0:00:13s
epoch 7  | loss: 0.3482  | val_0_rmse: 0.59201 | val_1_rmse: 0.6128  |  0:00:15s
epoch 8  | loss: 0.33463 | val_0_rmse: 0.56783 | val_1_rmse: 0.58766 |  0:00:17s
epoch 9  | loss: 0.33544 | val_0_rmse: 0.60344 | val_1_rmse: 0.61982 |  0:00:19s
epoch 10 | loss: 0.33726 | val_0_rmse: 0.56463 | val_1_rmse: 0.58836 |  0:00:21s
epoch 11 | loss: 0.33025 | val_0_rmse: 0.56744 | val_1_rmse: 0.59012 |  0:00:23s
epoch 12 | loss: 0.32981 | val_0_rmse: 0.57037 | val_1_rmse: 0.58789 |  0:00:25s
epoch 13 | loss: 0.32724 | val_0_rmse: 0.5606  | val_1_rmse: 0.58841 |  0:00:26s
epoch 14 | loss: 0.32324 | val_0_rmse: 0.57608 | val_1_rmse: 0.63401 |  0:00:28s
epoch 15 | loss: 0.32313 | val_0_rmse: 0.56026 | val_1_rmse: 0.59645 |  0:00:30s
epoch 16 | loss: 0.32259 | val_0_rmse: 0.5862  | val_1_rmse: 0.62028 |  0:00:32s
epoch 17 | loss: 0.32114 | val_0_rmse: 0.57255 | val_1_rmse: 0.6126  |  0:00:34s
epoch 18 | loss: 0.3225  | val_0_rmse: 0.56446 | val_1_rmse: 0.61586 |  0:00:36s
epoch 19 | loss: 0.32027 | val_0_rmse: 0.56196 | val_1_rmse: 0.59585 |  0:00:38s
epoch 20 | loss: 0.31686 | val_0_rmse: 0.57374 | val_1_rmse: 0.65251 |  0:00:40s
epoch 21 | loss: 0.31912 | val_0_rmse: 0.55837 | val_1_rmse: 0.62614 |  0:00:42s
epoch 22 | loss: 0.31631 | val_0_rmse: 0.56121 | val_1_rmse: 0.64949 |  0:00:44s
epoch 23 | loss: 0.31032 | val_0_rmse: 0.64116 | val_1_rmse: 0.80788 |  0:00:46s
epoch 24 | loss: 0.30733 | val_0_rmse: 0.79118 | val_1_rmse: 0.97096 |  0:00:48s
epoch 25 | loss: 0.30215 | val_0_rmse: 0.53868 | val_1_rmse: 0.76042 |  0:00:50s
epoch 26 | loss: 0.3028  | val_0_rmse: 0.57838 | val_1_rmse: 0.75969 |  0:00:52s
epoch 27 | loss: 0.29514 | val_0_rmse: 0.64312 | val_1_rmse: 0.80953 |  0:00:53s
epoch 28 | loss: 0.29431 | val_0_rmse: 0.55683 | val_1_rmse: 0.80447 |  0:00:55s
epoch 29 | loss: 0.28881 | val_0_rmse: 0.54369 | val_1_rmse: 0.74415 |  0:00:57s
epoch 30 | loss: 0.29165 | val_0_rmse: 0.55436 | val_1_rmse: 0.83913 |  0:00:59s
epoch 31 | loss: 0.28964 | val_0_rmse: 0.55884 | val_1_rmse: 0.88084 |  0:01:01s
epoch 32 | loss: 0.29528 | val_0_rmse: 0.58277 | val_1_rmse: 0.7542  |  0:01:03s
epoch 33 | loss: 0.2965  | val_0_rmse: 0.60208 | val_1_rmse: 0.73136 |  0:01:05s
epoch 34 | loss: 0.28956 | val_0_rmse: 0.53246 | val_1_rmse: 0.70071 |  0:01:07s
epoch 35 | loss: 0.28821 | val_0_rmse: 0.53365 | val_1_rmse: 0.62153 |  0:01:09s
epoch 36 | loss: 0.28749 | val_0_rmse: 0.53332 | val_1_rmse: 0.55119 |  0:01:11s
epoch 37 | loss: 0.28592 | val_0_rmse: 0.59894 | val_1_rmse: 0.62215 |  0:01:13s
epoch 38 | loss: 0.28955 | val_0_rmse: 0.8179  | val_1_rmse: 0.82478 |  0:01:15s
epoch 39 | loss: 0.28532 | val_0_rmse: 0.55004 | val_1_rmse: 0.5715  |  0:01:17s
epoch 40 | loss: 0.28444 | val_0_rmse: 0.53572 | val_1_rmse: 0.55456 |  0:01:19s
epoch 41 | loss: 0.28513 | val_0_rmse: 0.53365 | val_1_rmse: 0.55446 |  0:01:21s
epoch 42 | loss: 0.284   | val_0_rmse: 0.61582 | val_1_rmse: 0.63969 |  0:01:22s
epoch 43 | loss: 0.28981 | val_0_rmse: 0.62385 | val_1_rmse: 0.64586 |  0:01:24s
epoch 44 | loss: 0.296   | val_0_rmse: 0.57996 | val_1_rmse: 0.58993 |  0:01:26s
epoch 45 | loss: 0.29211 | val_0_rmse: 0.58473 | val_1_rmse: 0.59533 |  0:01:28s
epoch 46 | loss: 0.2873  | val_0_rmse: 0.53184 | val_1_rmse: 0.55047 |  0:01:30s
epoch 47 | loss: 0.28397 | val_0_rmse: 0.52594 | val_1_rmse: 0.54292 |  0:01:32s
epoch 48 | loss: 0.28109 | val_0_rmse: 0.51981 | val_1_rmse: 0.53758 |  0:01:34s
epoch 49 | loss: 0.28274 | val_0_rmse: 0.57347 | val_1_rmse: 0.59586 |  0:01:36s
epoch 50 | loss: 0.28116 | val_0_rmse: 0.52505 | val_1_rmse: 0.54352 |  0:01:38s
epoch 51 | loss: 0.28092 | val_0_rmse: 0.52665 | val_1_rmse: 0.54756 |  0:01:40s
epoch 52 | loss: 0.28036 | val_0_rmse: 0.5214  | val_1_rmse: 0.5387  |  0:01:42s
epoch 53 | loss: 0.27836 | val_0_rmse: 0.54178 | val_1_rmse: 0.56112 |  0:01:44s
epoch 54 | loss: 0.28263 | val_0_rmse: 0.52575 | val_1_rmse: 0.54801 |  0:01:46s
epoch 55 | loss: 0.2794  | val_0_rmse: 0.51699 | val_1_rmse: 0.53593 |  0:01:47s
epoch 56 | loss: 0.28146 | val_0_rmse: 0.56164 | val_1_rmse: 0.58493 |  0:01:49s
epoch 57 | loss: 0.2781  | val_0_rmse: 0.58798 | val_1_rmse: 0.61284 |  0:01:51s
epoch 58 | loss: 0.27734 | val_0_rmse: 0.88273 | val_1_rmse: 0.88482 |  0:01:53s
epoch 59 | loss: 0.27958 | val_0_rmse: 0.60285 | val_1_rmse: 0.62715 |  0:01:55s
epoch 60 | loss: 0.27464 | val_0_rmse: 1.03025 | val_1_rmse: 1.03499 |  0:01:57s
epoch 61 | loss: 0.27451 | val_0_rmse: 0.55416 | val_1_rmse: 0.57932 |  0:01:59s
epoch 62 | loss: 0.27573 | val_0_rmse: 0.5556  | val_1_rmse: 0.57968 |  0:02:01s
epoch 63 | loss: 0.28351 | val_0_rmse: 0.57873 | val_1_rmse: 0.58979 |  0:02:03s
epoch 64 | loss: 0.28823 | val_0_rmse: 1.27726 | val_1_rmse: 1.28256 |  0:02:05s
epoch 65 | loss: 0.28508 | val_0_rmse: 0.61368 | val_1_rmse: 0.63545 |  0:02:07s
epoch 66 | loss: 0.28434 | val_0_rmse: 0.53357 | val_1_rmse: 0.55269 |  0:02:09s
epoch 67 | loss: 0.28073 | val_0_rmse: 0.55207 | val_1_rmse: 0.57439 |  0:02:11s
epoch 68 | loss: 0.27891 | val_0_rmse: 0.57684 | val_1_rmse: 0.59887 |  0:02:13s
epoch 69 | loss: 0.27953 | val_0_rmse: 0.51923 | val_1_rmse: 0.53578 |  0:02:15s
epoch 70 | loss: 0.27717 | val_0_rmse: 0.60085 | val_1_rmse: 0.62181 |  0:02:17s
epoch 71 | loss: 0.2746  | val_0_rmse: 0.58555 | val_1_rmse: 0.60839 |  0:02:18s
epoch 72 | loss: 0.28093 | val_0_rmse: 0.60712 | val_1_rmse: 0.63123 |  0:02:20s
epoch 73 | loss: 0.27498 | val_0_rmse: 0.60525 | val_1_rmse: 0.63073 |  0:02:22s
epoch 74 | loss: 0.27275 | val_0_rmse: 0.56721 | val_1_rmse: 0.59218 |  0:02:24s
epoch 75 | loss: 0.27198 | val_0_rmse: 0.53673 | val_1_rmse: 0.55884 |  0:02:26s
epoch 76 | loss: 0.27173 | val_0_rmse: 0.53592 | val_1_rmse: 0.55797 |  0:02:28s
epoch 77 | loss: 0.2694  | val_0_rmse: 0.54599 | val_1_rmse: 0.56138 |  0:02:30s
epoch 78 | loss: 0.26986 | val_0_rmse: 0.64537 | val_1_rmse: 0.66964 |  0:02:32s
epoch 79 | loss: 0.27083 | val_0_rmse: 0.53028 | val_1_rmse: 0.55218 |  0:02:34s
epoch 80 | loss: 0.26816 | val_0_rmse: 0.55971 | val_1_rmse: 0.58311 |  0:02:36s
epoch 81 | loss: 0.26655 | val_0_rmse: 0.55142 | val_1_rmse: 0.57463 |  0:02:38s
epoch 82 | loss: 0.2713  | val_0_rmse: 0.59899 | val_1_rmse: 0.62466 |  0:02:40s
epoch 83 | loss: 0.26733 | val_0_rmse: 0.67308 | val_1_rmse: 0.69743 |  0:02:41s
epoch 84 | loss: 0.26613 | val_0_rmse: 0.55727 | val_1_rmse: 0.57797 |  0:02:43s
epoch 85 | loss: 0.26469 | val_0_rmse: 0.51615 | val_1_rmse: 0.53203 |  0:02:45s
epoch 86 | loss: 0.26383 | val_0_rmse: 0.76722 | val_1_rmse: 0.77473 |  0:02:47s
epoch 87 | loss: 0.26574 | val_0_rmse: 0.5062  | val_1_rmse: 0.5247  |  0:02:49s
epoch 88 | loss: 0.2611  | val_0_rmse: 0.56431 | val_1_rmse: 0.58631 |  0:02:51s
epoch 89 | loss: 0.26086 | val_0_rmse: 0.53796 | val_1_rmse: 0.55125 |  0:02:53s
epoch 90 | loss: 0.26187 | val_0_rmse: 0.60597 | val_1_rmse: 0.62932 |  0:02:55s
epoch 91 | loss: 0.25941 | val_0_rmse: 0.63289 | val_1_rmse: 0.65569 |  0:02:57s
epoch 92 | loss: 0.26163 | val_0_rmse: 0.61675 | val_1_rmse: 0.64169 |  0:02:59s
epoch 93 | loss: 0.25781 | val_0_rmse: 0.58931 | val_1_rmse: 0.61233 |  0:03:01s
epoch 94 | loss: 0.25807 | val_0_rmse: 0.59362 | val_1_rmse: 0.6151  |  0:03:03s
epoch 95 | loss: 0.2578  | val_0_rmse: 0.54849 | val_1_rmse: 0.57269 |  0:03:05s
epoch 96 | loss: 0.25401 | val_0_rmse: 0.54325 | val_1_rmse: 0.5666  |  0:03:07s
epoch 97 | loss: 0.25552 | val_0_rmse: 0.54919 | val_1_rmse: 0.56839 |  0:03:09s
epoch 98 | loss: 0.25648 | val_0_rmse: 0.54438 | val_1_rmse: 0.55688 |  0:03:11s
epoch 99 | loss: 0.25857 | val_0_rmse: 0.57612 | val_1_rmse: 0.60041 |  0:03:13s
epoch 100| loss: 0.2542  | val_0_rmse: 0.58377 | val_1_rmse: 0.60571 |  0:03:14s
epoch 101| loss: 0.25524 | val_0_rmse: 0.52706 | val_1_rmse: 0.54478 |  0:03:16s
epoch 102| loss: 0.25683 | val_0_rmse: 0.59786 | val_1_rmse: 0.61927 |  0:03:18s
epoch 103| loss: 0.25521 | val_0_rmse: 0.67293 | val_1_rmse: 0.69757 |  0:03:20s
epoch 104| loss: 0.25466 | val_0_rmse: 0.52745 | val_1_rmse: 0.54635 |  0:03:22s
epoch 105| loss: 0.25027 | val_0_rmse: 0.53083 | val_1_rmse: 0.55192 |  0:03:24s
epoch 106| loss: 0.24949 | val_0_rmse: 0.6457  | val_1_rmse: 0.66981 |  0:03:26s
epoch 107| loss: 0.25261 | val_0_rmse: 0.65456 | val_1_rmse: 0.67805 |  0:03:28s
epoch 108| loss: 0.24862 | val_0_rmse: 0.52059 | val_1_rmse: 0.53948 |  0:03:30s
epoch 109| loss: 0.24746 | val_0_rmse: 0.66668 | val_1_rmse: 0.69334 |  0:03:32s
epoch 110| loss: 0.2484  | val_0_rmse: 0.57531 | val_1_rmse: 0.59757 |  0:03:34s
epoch 111| loss: 0.25151 | val_0_rmse: 0.63409 | val_1_rmse: 0.66123 |  0:03:36s
epoch 112| loss: 0.247   | val_0_rmse: 0.62617 | val_1_rmse: 0.64765 |  0:03:38s
epoch 113| loss: 0.25159 | val_0_rmse: 0.50351 | val_1_rmse: 0.52244 |  0:03:39s
epoch 114| loss: 0.25226 | val_0_rmse: 0.55297 | val_1_rmse: 0.57304 |  0:03:41s
epoch 115| loss: 0.25119 | val_0_rmse: 0.53024 | val_1_rmse: 0.54862 |  0:03:43s
epoch 116| loss: 0.24783 | val_0_rmse: 0.49269 | val_1_rmse: 0.51051 |  0:03:45s
epoch 117| loss: 0.2483  | val_0_rmse: 0.64    | val_1_rmse: 0.66471 |  0:03:47s
epoch 118| loss: 0.24952 | val_0_rmse: 0.59949 | val_1_rmse: 0.62093 |  0:03:49s
epoch 119| loss: 0.24618 | val_0_rmse: 0.66166 | val_1_rmse: 0.68598 |  0:03:51s
epoch 120| loss: 0.24628 | val_0_rmse: 0.64505 | val_1_rmse: 0.66755 |  0:03:53s
epoch 121| loss: 0.24263 | val_0_rmse: 0.60245 | val_1_rmse: 0.62253 |  0:03:55s
epoch 122| loss: 0.24632 | val_0_rmse: 0.54577 | val_1_rmse: 0.56794 |  0:03:57s
epoch 123| loss: 0.24664 | val_0_rmse: 0.57679 | val_1_rmse: 0.59616 |  0:03:59s
epoch 124| loss: 0.24329 | val_0_rmse: 0.58868 | val_1_rmse: 0.60789 |  0:04:01s
epoch 125| loss: 0.24262 | val_0_rmse: 0.59443 | val_1_rmse: 0.61658 |  0:04:03s
epoch 126| loss: 0.24235 | val_0_rmse: 0.52443 | val_1_rmse: 0.5456  |  0:04:04s
epoch 127| loss: 0.24508 | val_0_rmse: 0.57598 | val_1_rmse: 0.60007 |  0:04:06s
epoch 128| loss: 0.24308 | val_0_rmse: 0.56732 | val_1_rmse: 0.58973 |  0:04:08s
epoch 129| loss: 0.24412 | val_0_rmse: 0.58103 | val_1_rmse: 0.60576 |  0:04:10s
epoch 130| loss: 0.24653 | val_0_rmse: 0.61065 | val_1_rmse: 0.63158 |  0:04:12s
epoch 131| loss: 0.24166 | val_0_rmse: 0.55909 | val_1_rmse: 0.58221 |  0:04:14s
epoch 132| loss: 0.24139 | val_0_rmse: 0.6401  | val_1_rmse: 0.66498 |  0:04:16s
epoch 133| loss: 0.24064 | val_0_rmse: 0.51271 | val_1_rmse: 0.52994 |  0:04:18s
epoch 134| loss: 0.24415 | val_0_rmse: 0.52413 | val_1_rmse: 0.54446 |  0:04:20s
epoch 135| loss: 0.24122 | val_0_rmse: 0.62598 | val_1_rmse: 0.64962 |  0:04:22s
epoch 136| loss: 0.2422  | val_0_rmse: 0.51298 | val_1_rmse: 0.53477 |  0:04:24s
epoch 137| loss: 0.23895 | val_0_rmse: 0.54872 | val_1_rmse: 0.56991 |  0:04:26s
epoch 138| loss: 0.2419  | val_0_rmse: 0.57552 | val_1_rmse: 0.59738 |  0:04:27s
epoch 139| loss: 0.23964 | val_0_rmse: 0.50969 | val_1_rmse: 0.52959 |  0:04:29s
epoch 140| loss: 0.24057 | val_0_rmse: 0.52635 | val_1_rmse: 0.54658 |  0:04:31s
epoch 141| loss: 0.2379  | val_0_rmse: 0.53997 | val_1_rmse: 0.5618  |  0:04:33s
epoch 142| loss: 0.24023 | val_0_rmse: 0.70115 | val_1_rmse: 0.72684 |  0:04:35s
epoch 143| loss: 0.24428 | val_0_rmse: 0.58893 | val_1_rmse: 0.6146  |  0:04:37s
epoch 144| loss: 0.24241 | val_0_rmse: 0.60264 | val_1_rmse: 0.62411 |  0:04:39s
epoch 145| loss: 0.24153 | val_0_rmse: 0.70199 | val_1_rmse: 0.72763 |  0:04:41s
epoch 146| loss: 0.23925 | val_0_rmse: 0.56616 | val_1_rmse: 0.5919  |  0:04:43s

Early stopping occured at epoch 146 with best_epoch = 116 and best_val_1_rmse = 0.51051
Best weights from best epoch are automatically used!
ended training at: 14:16:55
Feature importance:
[('Area', 0.5105559333108307), ('Baths', 0.003863106681892873), ('Beds', 0.16204793581892335), ('Latitude', 0.04715049140176361), ('Longitude', 0.12899673250251217), ('Month', 0.1473714895744818), ('Year', 1.4310709595487211e-05)]
Mean squared error is of 894086276.1776185
Mean absolute error:20570.230599039263
MAPE:0.3190707791022758
R2 score:0.7291494949237463
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:16:55
epoch 0  | loss: 0.4996  | val_0_rmse: 0.66931 | val_1_rmse: 0.66697 |  0:00:01s
epoch 1  | loss: 0.35425 | val_0_rmse: 0.57605 | val_1_rmse: 0.56917 |  0:00:03s
epoch 2  | loss: 0.34169 | val_0_rmse: 0.58294 | val_1_rmse: 0.57803 |  0:00:05s
epoch 3  | loss: 0.34255 | val_0_rmse: 0.57437 | val_1_rmse: 0.56901 |  0:00:07s
epoch 4  | loss: 0.34401 | val_0_rmse: 0.5761  | val_1_rmse: 0.57236 |  0:00:09s
epoch 5  | loss: 0.34429 | val_0_rmse: 0.57376 | val_1_rmse: 0.56811 |  0:00:11s
epoch 6  | loss: 0.34048 | val_0_rmse: 0.57638 | val_1_rmse: 0.57529 |  0:00:13s
epoch 7  | loss: 0.33603 | val_0_rmse: 0.57676 | val_1_rmse: 0.57148 |  0:00:15s
epoch 8  | loss: 0.33347 | val_0_rmse: 0.5709  | val_1_rmse: 0.56735 |  0:00:17s
epoch 9  | loss: 0.33312 | val_0_rmse: 0.57072 | val_1_rmse: 0.56687 |  0:00:19s
epoch 10 | loss: 0.3346  | val_0_rmse: 0.56935 | val_1_rmse: 0.56416 |  0:00:21s
epoch 11 | loss: 0.33201 | val_0_rmse: 0.56932 | val_1_rmse: 0.56196 |  0:00:23s
epoch 12 | loss: 0.32433 | val_0_rmse: 0.56494 | val_1_rmse: 0.55821 |  0:00:25s
epoch 13 | loss: 0.32741 | val_0_rmse: 0.56775 | val_1_rmse: 0.56406 |  0:00:27s
epoch 14 | loss: 0.32794 | val_0_rmse: 0.5955  | val_1_rmse: 0.59087 |  0:00:29s
epoch 15 | loss: 0.33309 | val_0_rmse: 0.58463 | val_1_rmse: 0.58348 |  0:00:31s
epoch 16 | loss: 0.32996 | val_0_rmse: 0.56458 | val_1_rmse: 0.55983 |  0:00:32s
epoch 17 | loss: 0.32537 | val_0_rmse: 0.56311 | val_1_rmse: 0.55764 |  0:00:34s
epoch 18 | loss: 0.32147 | val_0_rmse: 0.56048 | val_1_rmse: 0.55504 |  0:00:36s
epoch 19 | loss: 0.32439 | val_0_rmse: 0.56804 | val_1_rmse: 0.55981 |  0:00:38s
epoch 20 | loss: 0.32092 | val_0_rmse: 0.56805 | val_1_rmse: 0.56387 |  0:00:40s
epoch 21 | loss: 0.32462 | val_0_rmse: 0.5586  | val_1_rmse: 0.55209 |  0:00:42s
epoch 22 | loss: 0.31999 | val_0_rmse: 0.56074 | val_1_rmse: 0.55429 |  0:00:44s
epoch 23 | loss: 0.31691 | val_0_rmse: 0.56718 | val_1_rmse: 0.5635  |  0:00:46s
epoch 24 | loss: 0.31994 | val_0_rmse: 0.55914 | val_1_rmse: 0.55116 |  0:00:48s
epoch 25 | loss: 0.3177  | val_0_rmse: 0.56402 | val_1_rmse: 0.56027 |  0:00:50s
epoch 26 | loss: 0.31495 | val_0_rmse: 0.62592 | val_1_rmse: 0.62561 |  0:00:52s
epoch 27 | loss: 0.31242 | val_0_rmse: 0.75171 | val_1_rmse: 0.75372 |  0:00:54s
epoch 28 | loss: 0.31276 | val_0_rmse: 0.57476 | val_1_rmse: 0.57292 |  0:00:56s
epoch 29 | loss: 0.30954 | val_0_rmse: 0.57995 | val_1_rmse: 0.57595 |  0:00:58s
epoch 30 | loss: 0.30543 | val_0_rmse: 0.61357 | val_1_rmse: 0.60952 |  0:01:00s
epoch 31 | loss: 0.29829 | val_0_rmse: 0.54023 | val_1_rmse: 0.53533 |  0:01:02s
epoch 32 | loss: 0.29902 | val_0_rmse: 0.57855 | val_1_rmse: 0.57784 |  0:01:03s
epoch 33 | loss: 0.30421 | val_0_rmse: 0.59934 | val_1_rmse: 0.59925 |  0:01:05s
epoch 34 | loss: 0.30205 | val_0_rmse: 0.67719 | val_1_rmse: 0.67737 |  0:01:07s
epoch 35 | loss: 0.29749 | val_0_rmse: 0.5493  | val_1_rmse: 0.54567 |  0:01:09s
epoch 36 | loss: 0.29322 | val_0_rmse: 0.6178  | val_1_rmse: 0.60496 |  0:01:11s
epoch 37 | loss: 0.29204 | val_0_rmse: 0.53193 | val_1_rmse: 0.52506 |  0:01:13s
epoch 38 | loss: 0.29051 | val_0_rmse: 0.52797 | val_1_rmse: 0.51769 |  0:01:15s
epoch 39 | loss: 0.28952 | val_0_rmse: 0.56591 | val_1_rmse: 0.56422 |  0:01:17s
epoch 40 | loss: 0.29048 | val_0_rmse: 0.53299 | val_1_rmse: 0.52731 |  0:01:19s
epoch 41 | loss: 0.28969 | val_0_rmse: 0.76126 | val_1_rmse: 0.76924 |  0:01:21s
epoch 42 | loss: 0.28889 | val_0_rmse: 0.56081 | val_1_rmse: 0.55702 |  0:01:23s
epoch 43 | loss: 0.28633 | val_0_rmse: 0.61525 | val_1_rmse: 0.60399 |  0:01:25s
epoch 44 | loss: 0.29074 | val_0_rmse: 0.56338 | val_1_rmse: 0.56253 |  0:01:27s
epoch 45 | loss: 0.28847 | val_0_rmse: 0.53283 | val_1_rmse: 0.52833 |  0:01:29s
epoch 46 | loss: 0.28763 | val_0_rmse: 0.60292 | val_1_rmse: 0.60124 |  0:01:31s
epoch 47 | loss: 0.28715 | val_0_rmse: 0.60429 | val_1_rmse: 0.6051  |  0:01:32s
epoch 48 | loss: 0.2845  | val_0_rmse: 0.5275  | val_1_rmse: 0.52153 |  0:01:34s
epoch 49 | loss: 0.2902  | val_0_rmse: 0.61333 | val_1_rmse: 0.61516 |  0:01:36s
epoch 50 | loss: 0.28373 | val_0_rmse: 0.56292 | val_1_rmse: 0.55661 |  0:01:38s
epoch 51 | loss: 0.28254 | val_0_rmse: 0.54056 | val_1_rmse: 0.53611 |  0:01:40s
epoch 52 | loss: 0.28877 | val_0_rmse: 0.52261 | val_1_rmse: 0.51506 |  0:01:42s
epoch 53 | loss: 0.28357 | val_0_rmse: 0.54318 | val_1_rmse: 0.53382 |  0:01:44s
epoch 54 | loss: 0.28205 | val_0_rmse: 0.59383 | val_1_rmse: 0.58235 |  0:01:46s
epoch 55 | loss: 0.27877 | val_0_rmse: 0.62469 | val_1_rmse: 0.61669 |  0:01:48s
epoch 56 | loss: 0.27972 | val_0_rmse: 0.52182 | val_1_rmse: 0.51571 |  0:01:50s
epoch 57 | loss: 0.27561 | val_0_rmse: 0.52351 | val_1_rmse: 0.51562 |  0:01:52s
epoch 58 | loss: 0.27722 | val_0_rmse: 0.55791 | val_1_rmse: 0.54672 |  0:01:54s
epoch 59 | loss: 0.28384 | val_0_rmse: 0.52523 | val_1_rmse: 0.51821 |  0:01:56s
epoch 60 | loss: 0.27646 | val_0_rmse: 0.56115 | val_1_rmse: 0.54851 |  0:01:57s
epoch 61 | loss: 0.276   | val_0_rmse: 0.52725 | val_1_rmse: 0.52146 |  0:01:59s
epoch 62 | loss: 0.27274 | val_0_rmse: 0.60368 | val_1_rmse: 0.59922 |  0:02:01s
epoch 63 | loss: 0.27577 | val_0_rmse: 0.56445 | val_1_rmse: 0.55972 |  0:02:03s
epoch 64 | loss: 0.28389 | val_0_rmse: 0.7551  | val_1_rmse: 0.75864 |  0:02:05s
epoch 65 | loss: 0.28357 | val_0_rmse: 0.63112 | val_1_rmse: 0.6291  |  0:02:07s
epoch 66 | loss: 0.27955 | val_0_rmse: 0.52449 | val_1_rmse: 0.51841 |  0:02:09s
epoch 67 | loss: 0.27674 | val_0_rmse: 0.62633 | val_1_rmse: 0.61168 |  0:02:11s
epoch 68 | loss: 0.2785  | val_0_rmse: 0.55818 | val_1_rmse: 0.54695 |  0:02:13s
epoch 69 | loss: 0.27685 | val_0_rmse: 0.55763 | val_1_rmse: 0.55399 |  0:02:15s
epoch 70 | loss: 0.27493 | val_0_rmse: 0.54457 | val_1_rmse: 0.53504 |  0:02:17s
epoch 71 | loss: 0.27827 | val_0_rmse: 0.52608 | val_1_rmse: 0.51947 |  0:02:19s
epoch 72 | loss: 0.27814 | val_0_rmse: 0.5214  | val_1_rmse: 0.5146  |  0:02:21s
epoch 73 | loss: 0.27676 | val_0_rmse: 0.55089 | val_1_rmse: 0.53711 |  0:02:23s
epoch 74 | loss: 0.27939 | val_0_rmse: 0.52643 | val_1_rmse: 0.51685 |  0:02:25s
epoch 75 | loss: 0.27665 | val_0_rmse: 0.53396 | val_1_rmse: 0.52827 |  0:02:27s
epoch 76 | loss: 0.2736  | val_0_rmse: 0.57665 | val_1_rmse: 0.56346 |  0:02:28s
epoch 77 | loss: 0.27249 | val_0_rmse: 0.51453 | val_1_rmse: 0.50907 |  0:02:30s
epoch 78 | loss: 0.2728  | val_0_rmse: 0.54021 | val_1_rmse: 0.52872 |  0:02:32s
epoch 79 | loss: 0.27825 | val_0_rmse: 0.5717  | val_1_rmse: 0.56842 |  0:02:34s
epoch 80 | loss: 0.27184 | val_0_rmse: 0.52955 | val_1_rmse: 0.52549 |  0:02:36s
epoch 81 | loss: 0.27121 | val_0_rmse: 0.53149 | val_1_rmse: 0.52761 |  0:02:38s
epoch 82 | loss: 0.27612 | val_0_rmse: 0.53381 | val_1_rmse: 0.52564 |  0:02:40s
epoch 83 | loss: 0.27185 | val_0_rmse: 0.51937 | val_1_rmse: 0.51122 |  0:02:42s
epoch 84 | loss: 0.27262 | val_0_rmse: 0.52276 | val_1_rmse: 0.51723 |  0:02:44s
epoch 85 | loss: 0.27285 | val_0_rmse: 0.55045 | val_1_rmse: 0.54168 |  0:02:46s
epoch 86 | loss: 0.27161 | val_0_rmse: 0.54499 | val_1_rmse: 0.53569 |  0:02:48s
epoch 87 | loss: 0.27113 | val_0_rmse: 0.51821 | val_1_rmse: 0.5112  |  0:02:50s
epoch 88 | loss: 0.27058 | val_0_rmse: 0.53337 | val_1_rmse: 0.52545 |  0:02:52s
epoch 89 | loss: 0.27247 | val_0_rmse: 0.61555 | val_1_rmse: 0.60909 |  0:02:53s
epoch 90 | loss: 0.27088 | val_0_rmse: 0.57241 | val_1_rmse: 0.55803 |  0:02:55s
epoch 91 | loss: 0.2676  | val_0_rmse: 0.54408 | val_1_rmse: 0.53427 |  0:02:57s
epoch 92 | loss: 0.27217 | val_0_rmse: 0.54603 | val_1_rmse: 0.53972 |  0:02:59s
epoch 93 | loss: 0.27377 | val_0_rmse: 0.51991 | val_1_rmse: 0.51326 |  0:03:01s
epoch 94 | loss: 0.27304 | val_0_rmse: 0.55761 | val_1_rmse: 0.54354 |  0:03:03s
epoch 95 | loss: 0.272   | val_0_rmse: 0.62487 | val_1_rmse: 0.61214 |  0:03:05s
epoch 96 | loss: 0.26938 | val_0_rmse: 0.53142 | val_1_rmse: 0.5272  |  0:03:07s
epoch 97 | loss: 0.27364 | val_0_rmse: 0.52606 | val_1_rmse: 0.52094 |  0:03:09s
epoch 98 | loss: 0.27156 | val_0_rmse: 0.5355  | val_1_rmse: 0.53    |  0:03:11s
epoch 99 | loss: 0.26618 | val_0_rmse: 0.52295 | val_1_rmse: 0.51949 |  0:03:13s
epoch 100| loss: 0.27054 | val_0_rmse: 0.60586 | val_1_rmse: 0.60208 |  0:03:15s
epoch 101| loss: 0.27032 | val_0_rmse: 0.61572 | val_1_rmse: 0.60357 |  0:03:17s
epoch 102| loss: 0.27421 | val_0_rmse: 0.55667 | val_1_rmse: 0.54906 |  0:03:19s
epoch 103| loss: 0.27374 | val_0_rmse: 0.52637 | val_1_rmse: 0.52455 |  0:03:21s
epoch 104| loss: 0.26881 | val_0_rmse: 0.51086 | val_1_rmse: 0.50482 |  0:03:22s
epoch 105| loss: 0.26807 | val_0_rmse: 0.62082 | val_1_rmse: 0.6178  |  0:03:24s
epoch 106| loss: 0.26545 | val_0_rmse: 0.5568  | val_1_rmse: 0.54866 |  0:03:26s
epoch 107| loss: 0.26556 | val_0_rmse: 0.52002 | val_1_rmse: 0.51621 |  0:03:28s
epoch 108| loss: 0.26698 | val_0_rmse: 0.50987 | val_1_rmse: 0.50752 |  0:03:30s
epoch 109| loss: 0.26486 | val_0_rmse: 0.55662 | val_1_rmse: 0.54336 |  0:03:32s
epoch 110| loss: 0.26342 | val_0_rmse: 0.59457 | val_1_rmse: 0.58373 |  0:03:34s
epoch 111| loss: 0.26609 | val_0_rmse: 0.51142 | val_1_rmse: 0.50776 |  0:03:36s
epoch 112| loss: 0.26895 | val_0_rmse: 0.52201 | val_1_rmse: 0.51542 |  0:03:38s
epoch 113| loss: 0.26954 | val_0_rmse: 0.51686 | val_1_rmse: 0.5129  |  0:03:40s
epoch 114| loss: 0.26501 | val_0_rmse: 0.52097 | val_1_rmse: 0.51392 |  0:03:42s
epoch 115| loss: 0.26615 | val_0_rmse: 0.54802 | val_1_rmse: 0.54251 |  0:03:44s
epoch 116| loss: 0.26735 | val_0_rmse: 0.58857 | val_1_rmse: 0.57408 |  0:03:46s
epoch 117| loss: 0.26814 | val_0_rmse: 0.5864  | val_1_rmse: 0.57756 |  0:03:48s
epoch 118| loss: 0.26302 | val_0_rmse: 0.52059 | val_1_rmse: 0.51517 |  0:03:49s
epoch 119| loss: 0.26554 | val_0_rmse: 0.60662 | val_1_rmse: 0.60192 |  0:03:51s
epoch 120| loss: 0.26844 | val_0_rmse: 0.52493 | val_1_rmse: 0.51913 |  0:03:53s
epoch 121| loss: 0.27124 | val_0_rmse: 0.54398 | val_1_rmse: 0.53367 |  0:03:55s
epoch 122| loss: 0.26675 | val_0_rmse: 0.59076 | val_1_rmse: 0.57733 |  0:03:57s
epoch 123| loss: 0.26445 | val_0_rmse: 0.53239 | val_1_rmse: 0.5286  |  0:03:59s
epoch 124| loss: 0.26333 | val_0_rmse: 0.52559 | val_1_rmse: 0.52123 |  0:04:01s
epoch 125| loss: 0.26106 | val_0_rmse: 0.5871  | val_1_rmse: 0.57526 |  0:04:03s
epoch 126| loss: 0.26722 | val_0_rmse: 0.57324 | val_1_rmse: 0.56763 |  0:04:05s
epoch 127| loss: 0.26514 | val_0_rmse: 0.52101 | val_1_rmse: 0.51907 |  0:04:07s
epoch 128| loss: 0.26352 | val_0_rmse: 0.50353 | val_1_rmse: 0.49976 |  0:04:09s
epoch 129| loss: 0.26151 | val_0_rmse: 0.54143 | val_1_rmse: 0.53715 |  0:04:11s
epoch 130| loss: 0.26289 | val_0_rmse: 0.59406 | val_1_rmse: 0.57971 |  0:04:13s
epoch 131| loss: 0.26105 | val_0_rmse: 0.60773 | val_1_rmse: 0.59521 |  0:04:14s
epoch 132| loss: 0.26781 | val_0_rmse: 0.55719 | val_1_rmse: 0.54873 |  0:04:16s
epoch 133| loss: 0.26196 | val_0_rmse: 0.51169 | val_1_rmse: 0.50723 |  0:04:18s
epoch 134| loss: 0.25733 | val_0_rmse: 0.53003 | val_1_rmse: 0.52205 |  0:04:20s
epoch 135| loss: 0.25703 | val_0_rmse: 0.5259  | val_1_rmse: 0.52141 |  0:04:22s
epoch 136| loss: 0.25905 | val_0_rmse: 0.52482 | val_1_rmse: 0.52284 |  0:04:24s
epoch 137| loss: 0.26187 | val_0_rmse: 0.54978 | val_1_rmse: 0.54327 |  0:04:26s
epoch 138| loss: 0.25865 | val_0_rmse: 0.57921 | val_1_rmse: 0.56679 |  0:04:28s
epoch 139| loss: 0.26    | val_0_rmse: 0.52324 | val_1_rmse: 0.52328 |  0:04:30s
epoch 140| loss: 0.25489 | val_0_rmse: 0.56851 | val_1_rmse: 0.5549  |  0:04:32s
epoch 141| loss: 0.25993 | val_0_rmse: 0.52065 | val_1_rmse: 0.51506 |  0:04:34s
epoch 142| loss: 0.25556 | val_0_rmse: 0.50707 | val_1_rmse: 0.5031  |  0:04:36s
epoch 143| loss: 0.2549  | val_0_rmse: 0.56941 | val_1_rmse: 0.56076 |  0:04:38s
epoch 144| loss: 0.25963 | val_0_rmse: 0.50323 | val_1_rmse: 0.50367 |  0:04:40s
epoch 145| loss: 0.25382 | val_0_rmse: 0.5621  | val_1_rmse: 0.55655 |  0:04:42s
epoch 146| loss: 0.25171 | val_0_rmse: 0.57439 | val_1_rmse: 0.5644  |  0:04:43s
epoch 147| loss: 0.25199 | val_0_rmse: 0.53064 | val_1_rmse: 0.52506 |  0:04:45s
epoch 148| loss: 0.24794 | val_0_rmse: 0.55044 | val_1_rmse: 0.54321 |  0:04:47s
epoch 149| loss: 0.25037 | val_0_rmse: 0.55672 | val_1_rmse: 0.5594  |  0:04:49s
Stop training because you reached max_epochs = 150 with best_epoch = 128 and best_val_1_rmse = 0.49976
Best weights from best epoch are automatically used!
ended training at: 14:21:46
Feature importance:
[('Area', 0.5579039485790837), ('Baths', 0.0), ('Beds', 0.1287807957875465), ('Latitude', 0.03910743842820255), ('Longitude', 0.0680738374063988), ('Month', 0.049653291043620804), ('Year', 0.15648068875514767)]
Mean squared error is of 884922162.3371586
Mean absolute error:20817.14811938881
MAPE:0.3479315936475162
R2 score:0.7333799948315189
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:21:46
epoch 0  | loss: 0.51644 | val_0_rmse: 0.61982 | val_1_rmse: 0.61438 |  0:00:01s
epoch 1  | loss: 0.35983 | val_0_rmse: 0.57966 | val_1_rmse: 0.57582 |  0:00:03s
epoch 2  | loss: 0.34837 | val_0_rmse: 0.57813 | val_1_rmse: 0.57233 |  0:00:05s
epoch 3  | loss: 0.34402 | val_0_rmse: 0.57887 | val_1_rmse: 0.57789 |  0:00:07s
epoch 4  | loss: 0.33572 | val_0_rmse: 0.57023 | val_1_rmse: 0.56704 |  0:00:09s
epoch 5  | loss: 0.3364  | val_0_rmse: 0.57677 | val_1_rmse: 0.57142 |  0:00:11s
epoch 6  | loss: 0.33625 | val_0_rmse: 0.56778 | val_1_rmse: 0.56296 |  0:00:13s
epoch 7  | loss: 0.33463 | val_0_rmse: 0.57217 | val_1_rmse: 0.5698  |  0:00:15s
epoch 8  | loss: 0.33431 | val_0_rmse: 0.56853 | val_1_rmse: 0.56493 |  0:00:17s
epoch 9  | loss: 0.33484 | val_0_rmse: 0.56631 | val_1_rmse: 0.55971 |  0:00:19s
epoch 10 | loss: 0.32761 | val_0_rmse: 0.56648 | val_1_rmse: 0.56207 |  0:00:21s
epoch 11 | loss: 0.33524 | val_0_rmse: 0.57942 | val_1_rmse: 0.57254 |  0:00:23s
epoch 12 | loss: 0.33509 | val_0_rmse: 0.57532 | val_1_rmse: 0.56776 |  0:00:25s
epoch 13 | loss: 0.33477 | val_0_rmse: 0.57057 | val_1_rmse: 0.56359 |  0:00:26s
epoch 14 | loss: 0.33464 | val_0_rmse: 0.57016 | val_1_rmse: 0.56342 |  0:00:28s
epoch 15 | loss: 0.33887 | val_0_rmse: 0.57273 | val_1_rmse: 0.5663  |  0:00:30s
epoch 16 | loss: 0.33273 | val_0_rmse: 0.56978 | val_1_rmse: 0.56342 |  0:00:32s
epoch 17 | loss: 0.33865 | val_0_rmse: 0.57258 | val_1_rmse: 0.56651 |  0:00:34s
epoch 18 | loss: 0.34314 | val_0_rmse: 0.57167 | val_1_rmse: 0.5669  |  0:00:36s
epoch 19 | loss: 0.33366 | val_0_rmse: 0.56332 | val_1_rmse: 0.55776 |  0:00:38s
epoch 20 | loss: 0.333   | val_0_rmse: 0.57089 | val_1_rmse: 0.56623 |  0:00:40s
epoch 21 | loss: 0.33757 | val_0_rmse: 0.58924 | val_1_rmse: 0.5863  |  0:00:42s
epoch 22 | loss: 0.34332 | val_0_rmse: 0.58831 | val_1_rmse: 0.58496 |  0:00:44s
epoch 23 | loss: 0.33566 | val_0_rmse: 0.56637 | val_1_rmse: 0.56279 |  0:00:46s
epoch 24 | loss: 0.32995 | val_0_rmse: 0.5634  | val_1_rmse: 0.55739 |  0:00:48s
epoch 25 | loss: 0.32947 | val_0_rmse: 0.56769 | val_1_rmse: 0.5612  |  0:00:50s
epoch 26 | loss: 0.33173 | val_0_rmse: 0.56468 | val_1_rmse: 0.55982 |  0:00:52s
epoch 27 | loss: 0.32892 | val_0_rmse: 0.57171 | val_1_rmse: 0.56691 |  0:00:53s
epoch 28 | loss: 0.32857 | val_0_rmse: 0.56617 | val_1_rmse: 0.55945 |  0:00:55s
epoch 29 | loss: 0.32944 | val_0_rmse: 0.56965 | val_1_rmse: 0.5622  |  0:00:57s
epoch 30 | loss: 0.32209 | val_0_rmse: 0.55938 | val_1_rmse: 0.5535  |  0:00:59s
epoch 31 | loss: 0.32085 | val_0_rmse: 0.56272 | val_1_rmse: 0.55719 |  0:01:01s
epoch 32 | loss: 0.32267 | val_0_rmse: 0.56178 | val_1_rmse: 0.55664 |  0:01:03s
epoch 33 | loss: 0.32095 | val_0_rmse: 0.56064 | val_1_rmse: 0.5537  |  0:01:05s
epoch 34 | loss: 0.32046 | val_0_rmse: 0.55946 | val_1_rmse: 0.55246 |  0:01:07s
epoch 35 | loss: 0.32173 | val_0_rmse: 0.55993 | val_1_rmse: 0.55342 |  0:01:09s
epoch 36 | loss: 0.31884 | val_0_rmse: 0.55829 | val_1_rmse: 0.55185 |  0:01:11s
epoch 37 | loss: 0.31922 | val_0_rmse: 0.57947 | val_1_rmse: 0.5676  |  0:01:13s
epoch 38 | loss: 0.31931 | val_0_rmse: 0.55741 | val_1_rmse: 0.55197 |  0:01:15s
epoch 39 | loss: 0.31876 | val_0_rmse: 0.55636 | val_1_rmse: 0.55086 |  0:01:17s
epoch 40 | loss: 0.31796 | val_0_rmse: 0.55598 | val_1_rmse: 0.54885 |  0:01:19s
epoch 41 | loss: 0.31786 | val_0_rmse: 0.55952 | val_1_rmse: 0.555   |  0:01:20s
epoch 42 | loss: 0.31423 | val_0_rmse: 0.55617 | val_1_rmse: 0.5517  |  0:01:22s
epoch 43 | loss: 0.31779 | val_0_rmse: 0.56259 | val_1_rmse: 0.55196 |  0:01:24s
epoch 44 | loss: 0.31825 | val_0_rmse: 0.56428 | val_1_rmse: 0.55552 |  0:01:26s
epoch 45 | loss: 0.31682 | val_0_rmse: 0.55837 | val_1_rmse: 0.55356 |  0:01:28s
epoch 46 | loss: 0.31797 | val_0_rmse: 0.55833 | val_1_rmse: 0.55487 |  0:01:30s
epoch 47 | loss: 0.31293 | val_0_rmse: 0.55452 | val_1_rmse: 0.54892 |  0:01:32s
epoch 48 | loss: 0.31729 | val_0_rmse: 0.55489 | val_1_rmse: 0.54991 |  0:01:34s
epoch 49 | loss: 0.31752 | val_0_rmse: 0.56691 | val_1_rmse: 0.55881 |  0:01:36s
epoch 50 | loss: 0.31951 | val_0_rmse: 0.55703 | val_1_rmse: 0.55364 |  0:01:38s
epoch 51 | loss: 0.32001 | val_0_rmse: 0.57258 | val_1_rmse: 0.56876 |  0:01:40s
epoch 52 | loss: 0.3164  | val_0_rmse: 0.55704 | val_1_rmse: 0.55039 |  0:01:42s
epoch 53 | loss: 0.31514 | val_0_rmse: 0.55959 | val_1_rmse: 0.55166 |  0:01:44s
epoch 54 | loss: 0.31467 | val_0_rmse: 0.56356 | val_1_rmse: 0.55477 |  0:01:46s
epoch 55 | loss: 0.31519 | val_0_rmse: 0.58322 | val_1_rmse: 0.5763  |  0:01:47s
epoch 56 | loss: 0.31213 | val_0_rmse: 0.58945 | val_1_rmse: 0.58026 |  0:01:49s
epoch 57 | loss: 0.31127 | val_0_rmse: 0.56511 | val_1_rmse: 0.56204 |  0:01:51s
epoch 58 | loss: 0.31601 | val_0_rmse: 0.56296 | val_1_rmse: 0.55924 |  0:01:53s
epoch 59 | loss: 0.31318 | val_0_rmse: 0.5785  | val_1_rmse: 0.55762 |  0:01:55s
epoch 60 | loss: 0.31094 | val_0_rmse: 0.55548 | val_1_rmse: 0.55145 |  0:01:57s
epoch 61 | loss: 0.31119 | val_0_rmse: 0.56134 | val_1_rmse: 0.54644 |  0:01:59s
epoch 62 | loss: 0.30982 | val_0_rmse: 0.59766 | val_1_rmse: 0.55762 |  0:02:01s
epoch 63 | loss: 0.30564 | val_0_rmse: 0.57316 | val_1_rmse: 0.55685 |  0:02:03s
epoch 64 | loss: 0.30657 | val_0_rmse: 0.63018 | val_1_rmse: 0.54875 |  0:02:05s
epoch 65 | loss: 0.30583 | val_0_rmse: 0.68571 | val_1_rmse: 0.54428 |  0:02:07s
epoch 66 | loss: 0.30339 | val_0_rmse: 0.62951 | val_1_rmse: 0.57717 |  0:02:09s
epoch 67 | loss: 0.30629 | val_0_rmse: 0.67644 | val_1_rmse: 0.60409 |  0:02:11s
epoch 68 | loss: 0.30357 | val_0_rmse: 0.63616 | val_1_rmse: 0.59105 |  0:02:13s
epoch 69 | loss: 0.30272 | val_0_rmse: 0.62876 | val_1_rmse: 0.54918 |  0:02:14s
epoch 70 | loss: 0.30066 | val_0_rmse: 0.65908 | val_1_rmse: 0.59683 |  0:02:16s
epoch 71 | loss: 0.29876 | val_0_rmse: 0.78577 | val_1_rmse: 0.66029 |  0:02:18s
epoch 72 | loss: 0.30507 | val_0_rmse: 0.60895 | val_1_rmse: 0.60125 |  0:02:20s
epoch 73 | loss: 0.30572 | val_0_rmse: 0.63398 | val_1_rmse: 0.54716 |  0:02:22s
epoch 74 | loss: 0.30413 | val_0_rmse: 0.73113 | val_1_rmse: 0.57608 |  0:02:24s
epoch 75 | loss: 0.30369 | val_0_rmse: 0.6262  | val_1_rmse: 0.61865 |  0:02:26s
epoch 76 | loss: 0.30256 | val_0_rmse: 0.62083 | val_1_rmse: 0.60983 |  0:02:28s
epoch 77 | loss: 0.30887 | val_0_rmse: 0.76576 | val_1_rmse: 0.6122  |  0:02:30s
epoch 78 | loss: 0.31387 | val_0_rmse: 0.76256 | val_1_rmse: 0.59197 |  0:02:32s
epoch 79 | loss: 0.32341 | val_0_rmse: 0.63774 | val_1_rmse: 0.57651 |  0:02:34s
epoch 80 | loss: 0.32341 | val_0_rmse: 0.62527 | val_1_rmse: 0.62272 |  0:02:36s
epoch 81 | loss: 0.32355 | val_0_rmse: 0.62398 | val_1_rmse: 0.58553 |  0:02:38s
epoch 82 | loss: 0.31528 | val_0_rmse: 0.63777 | val_1_rmse: 0.55369 |  0:02:39s
epoch 83 | loss: 0.3112  | val_0_rmse: 0.77566 | val_1_rmse: 0.72398 |  0:02:41s
epoch 84 | loss: 0.33096 | val_0_rmse: 0.57614 | val_1_rmse: 0.56926 |  0:02:43s
epoch 85 | loss: 0.33076 | val_0_rmse: 0.594   | val_1_rmse: 0.58342 |  0:02:45s
epoch 86 | loss: 0.33285 | val_0_rmse: 0.73667 | val_1_rmse: 0.73212 |  0:02:47s
epoch 87 | loss: 0.32735 | val_0_rmse: 0.59711 | val_1_rmse: 0.5945  |  0:02:49s
epoch 88 | loss: 0.32475 | val_0_rmse: 0.58203 | val_1_rmse: 0.58044 |  0:02:51s
epoch 89 | loss: 0.31965 | val_0_rmse: 0.63105 | val_1_rmse: 0.62937 |  0:02:53s
epoch 90 | loss: 0.31862 | val_0_rmse: 0.57479 | val_1_rmse: 0.56933 |  0:02:55s
epoch 91 | loss: 0.31966 | val_0_rmse: 0.56598 | val_1_rmse: 0.55819 |  0:02:57s
epoch 92 | loss: 0.31564 | val_0_rmse: 0.60199 | val_1_rmse: 0.59529 |  0:02:59s
epoch 93 | loss: 0.3175  | val_0_rmse: 0.60242 | val_1_rmse: 0.6021  |  0:03:01s
epoch 94 | loss: 0.3171  | val_0_rmse: 0.56383 | val_1_rmse: 0.56069 |  0:03:03s
epoch 95 | loss: 0.31372 | val_0_rmse: 0.56359 | val_1_rmse: 0.55546 |  0:03:05s

Early stopping occured at epoch 95 with best_epoch = 65 and best_val_1_rmse = 0.54428
Best weights from best epoch are automatically used!
ended training at: 14:24:52
Feature importance:
[('Area', 0.2707416207741742), ('Baths', 0.026439000793215563), ('Beds', 0.14464109147226994), ('Latitude', 0.04996706009208036), ('Longitude', 0.3432623923584463), ('Month', 0.13470022491516867), ('Year', 0.030248609594644968)]
Mean squared error is of 978466817.2983862
Mean absolute error:21783.98089204698
MAPE:0.3527695989920676
R2 score:0.7093927897178992
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:24:52
epoch 0  | loss: 0.49997 | val_0_rmse: 0.62534 | val_1_rmse: 0.62979 |  0:00:01s
epoch 1  | loss: 0.35163 | val_0_rmse: 0.59044 | val_1_rmse: 0.59839 |  0:00:03s
epoch 2  | loss: 0.33987 | val_0_rmse: 0.57132 | val_1_rmse: 0.5853  |  0:00:05s
epoch 3  | loss: 0.33644 | val_0_rmse: 0.57292 | val_1_rmse: 0.58526 |  0:00:07s
epoch 4  | loss: 0.33465 | val_0_rmse: 0.57861 | val_1_rmse: 0.58779 |  0:00:09s
epoch 5  | loss: 0.33513 | val_0_rmse: 0.57113 | val_1_rmse: 0.58127 |  0:00:11s
epoch 6  | loss: 0.32962 | val_0_rmse: 0.5706  | val_1_rmse: 0.58079 |  0:00:13s
epoch 7  | loss: 0.33476 | val_0_rmse: 0.57577 | val_1_rmse: 0.5884  |  0:00:15s
epoch 8  | loss: 0.33337 | val_0_rmse: 0.57193 | val_1_rmse: 0.58283 |  0:00:17s
epoch 9  | loss: 0.33121 | val_0_rmse: 0.56991 | val_1_rmse: 0.57916 |  0:00:19s
epoch 10 | loss: 0.32627 | val_0_rmse: 0.56617 | val_1_rmse: 0.57669 |  0:00:21s
epoch 11 | loss: 0.32673 | val_0_rmse: 0.56048 | val_1_rmse: 0.57099 |  0:00:23s
epoch 12 | loss: 0.32093 | val_0_rmse: 0.56417 | val_1_rmse: 0.57361 |  0:00:25s
epoch 13 | loss: 0.32315 | val_0_rmse: 0.55655 | val_1_rmse: 0.57052 |  0:00:27s
epoch 14 | loss: 0.32106 | val_0_rmse: 0.56254 | val_1_rmse: 0.5745  |  0:00:28s
epoch 15 | loss: 0.32164 | val_0_rmse: 0.56202 | val_1_rmse: 0.57442 |  0:00:30s
epoch 16 | loss: 0.32009 | val_0_rmse: 0.55576 | val_1_rmse: 0.5686  |  0:00:32s
epoch 17 | loss: 0.31692 | val_0_rmse: 0.55566 | val_1_rmse: 0.56961 |  0:00:34s
epoch 18 | loss: 0.31733 | val_0_rmse: 0.55773 | val_1_rmse: 0.56935 |  0:00:36s
epoch 19 | loss: 0.31564 | val_0_rmse: 0.55539 | val_1_rmse: 0.56876 |  0:00:38s
epoch 20 | loss: 0.31806 | val_0_rmse: 0.55267 | val_1_rmse: 0.56653 |  0:00:40s
epoch 21 | loss: 0.31392 | val_0_rmse: 0.56018 | val_1_rmse: 0.57349 |  0:00:42s
epoch 22 | loss: 0.3119  | val_0_rmse: 0.55918 | val_1_rmse: 0.57411 |  0:00:44s
epoch 23 | loss: 0.31699 | val_0_rmse: 0.55569 | val_1_rmse: 0.57105 |  0:00:46s
epoch 24 | loss: 0.31385 | val_0_rmse: 0.55494 | val_1_rmse: 0.56975 |  0:00:48s
epoch 25 | loss: 0.31456 | val_0_rmse: 0.54905 | val_1_rmse: 0.56202 |  0:00:50s
epoch 26 | loss: 0.3133  | val_0_rmse: 0.5594  | val_1_rmse: 0.57197 |  0:00:52s
epoch 27 | loss: 0.31302 | val_0_rmse: 0.55142 | val_1_rmse: 0.56561 |  0:00:53s
epoch 28 | loss: 0.31316 | val_0_rmse: 0.55577 | val_1_rmse: 0.56973 |  0:00:55s
epoch 29 | loss: 0.31056 | val_0_rmse: 0.55312 | val_1_rmse: 0.56875 |  0:00:57s
epoch 30 | loss: 0.31721 | val_0_rmse: 0.5503  | val_1_rmse: 0.56608 |  0:00:59s
epoch 31 | loss: 0.30545 | val_0_rmse: 0.55242 | val_1_rmse: 0.56456 |  0:01:01s
epoch 32 | loss: 0.30846 | val_0_rmse: 0.60212 | val_1_rmse: 0.61418 |  0:01:03s
epoch 33 | loss: 0.31046 | val_0_rmse: 0.55065 | val_1_rmse: 0.566   |  0:01:05s
epoch 34 | loss: 0.31097 | val_0_rmse: 0.58264 | val_1_rmse: 0.59023 |  0:01:07s
epoch 35 | loss: 0.31256 | val_0_rmse: 0.54784 | val_1_rmse: 0.56    |  0:01:09s
epoch 36 | loss: 0.29965 | val_0_rmse: 0.88154 | val_1_rmse: 0.88282 |  0:01:11s
epoch 37 | loss: 0.30015 | val_0_rmse: 1.93807 | val_1_rmse: 1.92364 |  0:01:13s
epoch 38 | loss: 0.29421 | val_0_rmse: 0.55764 | val_1_rmse: 0.57494 |  0:01:15s
epoch 39 | loss: 0.29424 | val_0_rmse: 0.88916 | val_1_rmse: 0.88288 |  0:01:17s
epoch 40 | loss: 0.29254 | val_0_rmse: 0.60453 | val_1_rmse: 0.61774 |  0:01:19s
epoch 41 | loss: 0.28827 | val_0_rmse: 0.54833 | val_1_rmse: 0.55899 |  0:01:21s
epoch 42 | loss: 0.28647 | val_0_rmse: 0.65693 | val_1_rmse: 0.66319 |  0:01:22s
epoch 43 | loss: 0.28414 | val_0_rmse: 0.64511 | val_1_rmse: 0.65543 |  0:01:24s
epoch 44 | loss: 0.28714 | val_0_rmse: 0.58932 | val_1_rmse: 0.59779 |  0:01:26s
epoch 45 | loss: 0.2811  | val_0_rmse: 0.5203  | val_1_rmse: 0.53224 |  0:01:28s
epoch 46 | loss: 0.28266 | val_0_rmse: 0.55054 | val_1_rmse: 0.56207 |  0:01:30s
epoch 47 | loss: 0.2878  | val_0_rmse: 0.58464 | val_1_rmse: 0.59421 |  0:01:32s
epoch 48 | loss: 0.28415 | val_0_rmse: 0.61041 | val_1_rmse: 0.62107 |  0:01:34s
epoch 49 | loss: 0.28296 | val_0_rmse: 0.52396 | val_1_rmse: 0.53766 |  0:01:36s
epoch 50 | loss: 0.28079 | val_0_rmse: 0.52695 | val_1_rmse: 0.53999 |  0:01:38s
epoch 51 | loss: 0.28037 | val_0_rmse: 0.62181 | val_1_rmse: 0.63013 |  0:01:40s
epoch 52 | loss: 0.28782 | val_0_rmse: 0.56246 | val_1_rmse: 0.56968 |  0:01:42s
epoch 53 | loss: 0.28128 | val_0_rmse: 0.55023 | val_1_rmse: 0.5628  |  0:01:44s
epoch 54 | loss: 0.28726 | val_0_rmse: 0.56539 | val_1_rmse: 0.58106 |  0:01:46s
epoch 55 | loss: 0.29013 | val_0_rmse: 0.58817 | val_1_rmse: 0.59682 |  0:01:48s
epoch 56 | loss: 0.28591 | val_0_rmse: 0.9802  | val_1_rmse: 0.96804 |  0:01:49s
epoch 57 | loss: 0.28062 | val_0_rmse: 0.52059 | val_1_rmse: 0.53339 |  0:01:51s
epoch 58 | loss: 0.27734 | val_0_rmse: 0.55155 | val_1_rmse: 0.56453 |  0:01:53s
epoch 59 | loss: 0.27826 | val_0_rmse: 0.52812 | val_1_rmse: 0.53994 |  0:01:55s
epoch 60 | loss: 0.2778  | val_0_rmse: 0.53561 | val_1_rmse: 0.54937 |  0:01:57s
epoch 61 | loss: 0.28427 | val_0_rmse: 0.5446  | val_1_rmse: 0.56165 |  0:01:59s
epoch 62 | loss: 0.27547 | val_0_rmse: 0.64806 | val_1_rmse: 0.65537 |  0:02:01s
epoch 63 | loss: 0.28312 | val_0_rmse: 0.66916 | val_1_rmse: 0.66986 |  0:02:03s
epoch 64 | loss: 0.28591 | val_0_rmse: 0.62545 | val_1_rmse: 0.63348 |  0:02:05s
epoch 65 | loss: 0.28173 | val_0_rmse: 0.52385 | val_1_rmse: 0.53721 |  0:02:07s
epoch 66 | loss: 0.27385 | val_0_rmse: 0.53262 | val_1_rmse: 0.54795 |  0:02:09s
epoch 67 | loss: 0.27821 | val_0_rmse: 0.5563  | val_1_rmse: 0.56453 |  0:02:11s
epoch 68 | loss: 0.27744 | val_0_rmse: 0.57587 | val_1_rmse: 0.58445 |  0:02:13s
epoch 69 | loss: 0.27626 | val_0_rmse: 0.60357 | val_1_rmse: 0.61535 |  0:02:15s
epoch 70 | loss: 0.2788  | val_0_rmse: 0.56674 | val_1_rmse: 0.58146 |  0:02:16s
epoch 71 | loss: 0.27677 | val_0_rmse: 0.56135 | val_1_rmse: 0.57445 |  0:02:18s
epoch 72 | loss: 0.28543 | val_0_rmse: 0.63996 | val_1_rmse: 0.64875 |  0:02:20s
epoch 73 | loss: 0.27805 | val_0_rmse: 0.54068 | val_1_rmse: 0.55559 |  0:02:22s
epoch 74 | loss: 0.27953 | val_0_rmse: 1.02275 | val_1_rmse: 1.01155 |  0:02:24s
epoch 75 | loss: 0.28876 | val_0_rmse: 1.79803 | val_1_rmse: 1.76979 |  0:02:26s

Early stopping occured at epoch 75 with best_epoch = 45 and best_val_1_rmse = 0.53224
Best weights from best epoch are automatically used!
ended training at: 14:27:19
Feature importance:
[('Area', 0.4198586751367014), ('Baths', 0.05735646164799121), ('Beds', 0.22642406803689039), ('Latitude', 0.1902977597887024), ('Longitude', 0.09281052039802035), ('Month', 0.009128512557724436), ('Year', 0.004124002433969875)]
Mean squared error is of 931707952.4601896
Mean absolute error:21316.498888551305
MAPE:0.3551833715691903
R2 score:0.7172996831605873
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:27:19
epoch 0  | loss: 0.50795 | val_0_rmse: 0.59729 | val_1_rmse: 0.62065 |  0:00:01s
epoch 1  | loss: 0.34628 | val_0_rmse: 0.60053 | val_1_rmse: 0.6224  |  0:00:03s
epoch 2  | loss: 0.33144 | val_0_rmse: 0.56509 | val_1_rmse: 0.58699 |  0:00:05s
epoch 3  | loss: 0.33488 | val_0_rmse: 0.56737 | val_1_rmse: 0.59078 |  0:00:07s
epoch 4  | loss: 0.32986 | val_0_rmse: 0.56523 | val_1_rmse: 0.58748 |  0:00:09s
epoch 5  | loss: 0.32694 | val_0_rmse: 0.56343 | val_1_rmse: 0.58832 |  0:00:11s
epoch 6  | loss: 0.32336 | val_0_rmse: 0.56001 | val_1_rmse: 0.5864  |  0:00:13s
epoch 7  | loss: 0.32258 | val_0_rmse: 0.5644  | val_1_rmse: 0.58729 |  0:00:15s
epoch 8  | loss: 0.32468 | val_0_rmse: 0.56436 | val_1_rmse: 0.59265 |  0:00:17s
epoch 9  | loss: 0.31954 | val_0_rmse: 0.56063 | val_1_rmse: 0.58667 |  0:00:19s
epoch 10 | loss: 0.3213  | val_0_rmse: 0.5703  | val_1_rmse: 0.59789 |  0:00:21s
epoch 11 | loss: 0.31889 | val_0_rmse: 0.56468 | val_1_rmse: 0.58984 |  0:00:23s
epoch 12 | loss: 0.31951 | val_0_rmse: 0.56992 | val_1_rmse: 0.59625 |  0:00:25s
epoch 13 | loss: 0.31913 | val_0_rmse: 0.55713 | val_1_rmse: 0.58435 |  0:00:27s
epoch 14 | loss: 0.31682 | val_0_rmse: 0.55295 | val_1_rmse: 0.58182 |  0:00:28s
epoch 15 | loss: 0.31647 | val_0_rmse: 0.55501 | val_1_rmse: 0.583   |  0:00:30s
epoch 16 | loss: 0.31253 | val_0_rmse: 0.55191 | val_1_rmse: 0.57888 |  0:00:32s
epoch 17 | loss: 0.31185 | val_0_rmse: 0.55326 | val_1_rmse: 0.58003 |  0:00:34s
epoch 18 | loss: 0.31345 | val_0_rmse: 0.55776 | val_1_rmse: 0.58365 |  0:00:36s
epoch 19 | loss: 0.3154  | val_0_rmse: 0.55514 | val_1_rmse: 0.58175 |  0:00:38s
epoch 20 | loss: 0.31188 | val_0_rmse: 0.5501  | val_1_rmse: 0.5779  |  0:00:40s
epoch 21 | loss: 0.31153 | val_0_rmse: 0.55925 | val_1_rmse: 0.58624 |  0:00:42s
epoch 22 | loss: 0.31333 | val_0_rmse: 0.55945 | val_1_rmse: 0.58315 |  0:00:44s
epoch 23 | loss: 0.31301 | val_0_rmse: 0.55657 | val_1_rmse: 0.58174 |  0:00:46s
epoch 24 | loss: 0.31261 | val_0_rmse: 0.55465 | val_1_rmse: 0.58134 |  0:00:48s
epoch 25 | loss: 0.311   | val_0_rmse: 0.54967 | val_1_rmse: 0.57653 |  0:00:50s
epoch 26 | loss: 0.31461 | val_0_rmse: 0.5475  | val_1_rmse: 0.57376 |  0:00:52s
epoch 27 | loss: 0.31364 | val_0_rmse: 0.55809 | val_1_rmse: 0.58171 |  0:00:54s
epoch 28 | loss: 0.31133 | val_0_rmse: 0.55523 | val_1_rmse: 0.57855 |  0:00:56s
epoch 29 | loss: 0.31555 | val_0_rmse: 0.57061 | val_1_rmse: 0.59647 |  0:00:58s
epoch 30 | loss: 0.31516 | val_0_rmse: 0.54768 | val_1_rmse: 0.57659 |  0:00:59s
epoch 31 | loss: 0.30535 | val_0_rmse: 0.56381 | val_1_rmse: 0.58785 |  0:01:01s
epoch 32 | loss: 0.32113 | val_0_rmse: 0.56156 | val_1_rmse: 0.5865  |  0:01:03s
epoch 33 | loss: 0.32492 | val_0_rmse: 0.57552 | val_1_rmse: 0.58778 |  0:01:05s
epoch 34 | loss: 0.31366 | val_0_rmse: 0.72724 | val_1_rmse: 0.59303 |  0:01:07s
epoch 35 | loss: 0.31667 | val_0_rmse: 0.55116 | val_1_rmse: 0.57571 |  0:01:09s
epoch 36 | loss: 0.30873 | val_0_rmse: 0.58355 | val_1_rmse: 0.5815  |  0:01:11s
epoch 37 | loss: 0.30938 | val_0_rmse: 0.72382 | val_1_rmse: 0.59335 |  0:01:13s
epoch 38 | loss: 0.3116  | val_0_rmse: 0.76534 | val_1_rmse: 0.57958 |  0:01:15s
epoch 39 | loss: 0.31523 | val_0_rmse: 0.57541 | val_1_rmse: 0.57468 |  0:01:17s
epoch 40 | loss: 0.3104  | val_0_rmse: 0.66928 | val_1_rmse: 0.58462 |  0:01:19s
epoch 41 | loss: 0.31319 | val_0_rmse: 0.66829 | val_1_rmse: 0.58103 |  0:01:21s
epoch 42 | loss: 0.30966 | val_0_rmse: 0.71757 | val_1_rmse: 0.58299 |  0:01:23s
epoch 43 | loss: 0.30729 | val_0_rmse: 0.69608 | val_1_rmse: 0.58818 |  0:01:25s
epoch 44 | loss: 0.31487 | val_0_rmse: 0.9028  | val_1_rmse: 0.57503 |  0:01:27s
epoch 45 | loss: 0.30524 | val_0_rmse: 0.84332 | val_1_rmse: 0.62315 |  0:01:28s
epoch 46 | loss: 0.30311 | val_0_rmse: 0.79655 | val_1_rmse: 0.58918 |  0:01:30s
epoch 47 | loss: 0.3008  | val_0_rmse: 0.57002 | val_1_rmse: 0.59485 |  0:01:32s
epoch 48 | loss: 0.30096 | val_0_rmse: 0.83161 | val_1_rmse: 0.58294 |  0:01:34s
epoch 49 | loss: 0.30445 | val_0_rmse: 0.73478 | val_1_rmse: 0.5803  |  0:01:36s
epoch 50 | loss: 0.3016  | val_0_rmse: 0.80445 | val_1_rmse: 0.59002 |  0:01:38s
epoch 51 | loss: 0.30198 | val_0_rmse: 0.78018 | val_1_rmse: 0.65576 |  0:01:40s
epoch 52 | loss: 0.30029 | val_0_rmse: 0.60536 | val_1_rmse: 0.58379 |  0:01:42s
epoch 53 | loss: 0.30369 | val_0_rmse: 0.5583  | val_1_rmse: 0.5782  |  0:01:44s
epoch 54 | loss: 0.29673 | val_0_rmse: 0.78803 | val_1_rmse: 0.62208 |  0:01:46s
epoch 55 | loss: 0.29891 | val_0_rmse: 0.97915 | val_1_rmse: 0.82182 |  0:01:48s
epoch 56 | loss: 0.29707 | val_0_rmse: 0.53196 | val_1_rmse: 0.55836 |  0:01:50s
epoch 57 | loss: 0.29369 | val_0_rmse: 0.5325  | val_1_rmse: 0.55985 |  0:01:52s
epoch 58 | loss: 0.29871 | val_0_rmse: 0.5541  | val_1_rmse: 0.57714 |  0:01:54s
epoch 59 | loss: 0.30219 | val_0_rmse: 0.57079 | val_1_rmse: 0.59944 |  0:01:56s
epoch 60 | loss: 0.29636 | val_0_rmse: 0.54879 | val_1_rmse: 0.57339 |  0:01:57s
epoch 61 | loss: 0.29026 | val_0_rmse: 0.54464 | val_1_rmse: 0.56963 |  0:01:59s
epoch 62 | loss: 0.29589 | val_0_rmse: 0.54111 | val_1_rmse: 0.56491 |  0:02:01s
epoch 63 | loss: 0.29679 | val_0_rmse: 0.54175 | val_1_rmse: 0.57187 |  0:02:03s
epoch 64 | loss: 0.29088 | val_0_rmse: 0.54486 | val_1_rmse: 0.57038 |  0:02:05s
epoch 65 | loss: 0.28774 | val_0_rmse: 0.53894 | val_1_rmse: 0.56473 |  0:02:07s
epoch 66 | loss: 0.28987 | val_0_rmse: 0.56863 | val_1_rmse: 0.59425 |  0:02:09s
epoch 67 | loss: 0.28464 | val_0_rmse: 0.53905 | val_1_rmse: 0.5666  |  0:02:11s
epoch 68 | loss: 0.28665 | val_0_rmse: 0.5297  | val_1_rmse: 0.55411 |  0:02:13s
epoch 69 | loss: 0.28591 | val_0_rmse: 0.54864 | val_1_rmse: 0.57639 |  0:02:15s
epoch 70 | loss: 0.28646 | val_0_rmse: 0.57305 | val_1_rmse: 0.59577 |  0:02:17s
epoch 71 | loss: 0.2831  | val_0_rmse: 0.56406 | val_1_rmse: 0.58681 |  0:02:19s
epoch 72 | loss: 0.28126 | val_0_rmse: 0.55953 | val_1_rmse: 0.58124 |  0:02:21s
epoch 73 | loss: 0.27666 | val_0_rmse: 0.57889 | val_1_rmse: 0.60109 |  0:02:23s
epoch 74 | loss: 0.2742  | val_0_rmse: 0.52921 | val_1_rmse: 0.55185 |  0:02:25s
epoch 75 | loss: 0.27678 | val_0_rmse: 0.54436 | val_1_rmse: 0.56662 |  0:02:26s
epoch 76 | loss: 0.27735 | val_0_rmse: 0.52921 | val_1_rmse: 0.55574 |  0:02:28s
epoch 77 | loss: 0.28434 | val_0_rmse: 0.5544  | val_1_rmse: 0.57743 |  0:02:30s
epoch 78 | loss: 0.27631 | val_0_rmse: 0.52061 | val_1_rmse: 0.54699 |  0:02:32s
epoch 79 | loss: 0.27584 | val_0_rmse: 0.5361  | val_1_rmse: 0.55797 |  0:02:34s
epoch 80 | loss: 0.27724 | val_0_rmse: 0.60879 | val_1_rmse: 0.62845 |  0:02:36s
epoch 81 | loss: 0.2739  | val_0_rmse: 0.54013 | val_1_rmse: 0.5603  |  0:02:38s
epoch 82 | loss: 0.2747  | val_0_rmse: 0.5314  | val_1_rmse: 0.55751 |  0:02:40s
epoch 83 | loss: 0.27426 | val_0_rmse: 0.53143 | val_1_rmse: 0.55534 |  0:02:42s
epoch 84 | loss: 0.276   | val_0_rmse: 0.53592 | val_1_rmse: 0.5576  |  0:02:44s
epoch 85 | loss: 0.27366 | val_0_rmse: 0.51186 | val_1_rmse: 0.53533 |  0:02:46s
epoch 86 | loss: 0.27636 | val_0_rmse: 0.60225 | val_1_rmse: 0.61955 |  0:02:48s
epoch 87 | loss: 0.27131 | val_0_rmse: 0.56895 | val_1_rmse: 0.58913 |  0:02:50s
epoch 88 | loss: 0.27378 | val_0_rmse: 0.51462 | val_1_rmse: 0.54151 |  0:02:52s
epoch 89 | loss: 0.27352 | val_0_rmse: 0.51141 | val_1_rmse: 0.53625 |  0:02:54s
epoch 90 | loss: 0.2725  | val_0_rmse: 0.55812 | val_1_rmse: 0.57808 |  0:02:56s
epoch 91 | loss: 0.27422 | val_0_rmse: 0.5975  | val_1_rmse: 0.62137 |  0:02:57s
epoch 92 | loss: 0.27324 | val_0_rmse: 0.54408 | val_1_rmse: 0.57075 |  0:03:00s
epoch 93 | loss: 0.2752  | val_0_rmse: 0.54392 | val_1_rmse: 0.56468 |  0:03:02s
epoch 94 | loss: 0.27795 | val_0_rmse: 0.54943 | val_1_rmse: 0.57    |  0:03:03s
epoch 95 | loss: 0.28329 | val_0_rmse: 0.53052 | val_1_rmse: 0.55153 |  0:03:05s
epoch 96 | loss: 0.28413 | val_0_rmse: 0.58206 | val_1_rmse: 0.59822 |  0:03:07s
epoch 97 | loss: 0.28074 | val_0_rmse: 0.62113 | val_1_rmse: 0.63877 |  0:03:09s
epoch 98 | loss: 0.27873 | val_0_rmse: 0.57712 | val_1_rmse: 0.594   |  0:03:11s
epoch 99 | loss: 0.28621 | val_0_rmse: 0.60496 | val_1_rmse: 0.62627 |  0:03:13s
epoch 100| loss: 0.28907 | val_0_rmse: 0.55135 | val_1_rmse: 0.56953 |  0:03:15s
epoch 101| loss: 0.28595 | val_0_rmse: 0.53063 | val_1_rmse: 0.55139 |  0:03:17s
epoch 102| loss: 0.28844 | val_0_rmse: 0.54243 | val_1_rmse: 0.56128 |  0:03:19s
epoch 103| loss: 0.3046  | val_0_rmse: 0.59208 | val_1_rmse: 0.61618 |  0:03:21s
epoch 104| loss: 0.2958  | val_0_rmse: 0.57196 | val_1_rmse: 0.59373 |  0:03:23s
epoch 105| loss: 0.29705 | val_0_rmse: 0.87762 | val_1_rmse: 0.899   |  0:03:25s
epoch 106| loss: 0.29602 | val_0_rmse: 0.67601 | val_1_rmse: 0.69768 |  0:03:27s
epoch 107| loss: 0.29202 | val_0_rmse: 0.52997 | val_1_rmse: 0.5502  |  0:03:29s
epoch 108| loss: 0.29355 | val_0_rmse: 0.59774 | val_1_rmse: 0.61367 |  0:03:30s
epoch 109| loss: 0.29031 | val_0_rmse: 0.53313 | val_1_rmse: 0.55511 |  0:03:32s
epoch 110| loss: 0.29084 | val_0_rmse: 0.57884 | val_1_rmse: 0.59536 |  0:03:34s
epoch 111| loss: 0.29025 | val_0_rmse: 0.53561 | val_1_rmse: 0.55778 |  0:03:36s
epoch 112| loss: 0.29059 | val_0_rmse: 0.59935 | val_1_rmse: 0.61505 |  0:03:38s
epoch 113| loss: 0.28468 | val_0_rmse: 0.52789 | val_1_rmse: 0.55059 |  0:03:40s
epoch 114| loss: 0.28195 | val_0_rmse: 0.63778 | val_1_rmse: 0.65468 |  0:03:42s
epoch 115| loss: 0.28543 | val_0_rmse: 0.52682 | val_1_rmse: 0.54404 |  0:03:44s

Early stopping occured at epoch 115 with best_epoch = 85 and best_val_1_rmse = 0.53533
Best weights from best epoch are automatically used!
ended training at: 14:31:04
Feature importance:
[('Area', 0.5241873818431495), ('Baths', 0.024867776973107124), ('Beds', 0.22721395636115568), ('Latitude', 0.03717431504124061), ('Longitude', 0.16579267746102885), ('Month', 0.015464832212685282), ('Year', 0.005299060107632917)]
Mean squared error is of 911412734.7898248
Mean absolute error:20892.436844618052
MAPE:0.3540614979738415
R2 score:0.7274978006588924
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:31:05
epoch 0  | loss: 0.42676 | val_0_rmse: 0.63241 | val_1_rmse: 0.64067 |  0:00:09s
epoch 1  | loss: 0.352   | val_0_rmse: 0.56494 | val_1_rmse: 0.56414 |  0:00:17s
epoch 2  | loss: 0.32242 | val_0_rmse: 0.58463 | val_1_rmse: 0.58629 |  0:00:27s
epoch 3  | loss: 0.32372 | val_0_rmse: 0.61701 | val_1_rmse: 0.61993 |  0:00:35s
epoch 4  | loss: 0.31756 | val_0_rmse: 0.59569 | val_1_rmse: 0.60302 |  0:00:44s
epoch 5  | loss: 0.31581 | val_0_rmse: 0.57638 | val_1_rmse: 0.58271 |  0:00:53s
epoch 6  | loss: 0.30867 | val_0_rmse: 0.55058 | val_1_rmse: 0.55393 |  0:01:02s
epoch 7  | loss: 0.31053 | val_0_rmse: 0.55517 | val_1_rmse: 0.55454 |  0:01:11s
epoch 8  | loss: 0.30653 | val_0_rmse: 0.54932 | val_1_rmse: 0.55107 |  0:01:20s
epoch 9  | loss: 0.30646 | val_0_rmse: 0.54782 | val_1_rmse: 0.54766 |  0:01:29s
epoch 10 | loss: 0.30743 | val_0_rmse: 0.58116 | val_1_rmse: 0.57792 |  0:01:38s
epoch 11 | loss: 0.31264 | val_0_rmse: 0.56578 | val_1_rmse: 0.56895 |  0:01:47s
epoch 12 | loss: 0.30889 | val_0_rmse: 0.54768 | val_1_rmse: 0.54902 |  0:01:57s
epoch 13 | loss: 0.30056 | val_0_rmse: 0.56118 | val_1_rmse: 0.55906 |  0:02:05s
epoch 14 | loss: 0.30292 | val_0_rmse: 0.55603 | val_1_rmse: 0.55326 |  0:02:14s
epoch 15 | loss: 0.30186 | val_0_rmse: 0.55555 | val_1_rmse: 0.55955 |  0:02:23s
epoch 16 | loss: 0.299   | val_0_rmse: 0.54432 | val_1_rmse: 0.54465 |  0:02:32s
epoch 17 | loss: 0.29945 | val_0_rmse: 0.54908 | val_1_rmse: 0.55213 |  0:02:41s
epoch 18 | loss: 0.29869 | val_0_rmse: 0.5494  | val_1_rmse: 0.55214 |  0:02:50s
epoch 19 | loss: 0.29772 | val_0_rmse: 0.5437  | val_1_rmse: 0.54781 |  0:02:59s
epoch 20 | loss: 0.29772 | val_0_rmse: 0.5656  | val_1_rmse: 0.56759 |  0:03:08s
epoch 21 | loss: 0.30159 | val_0_rmse: 0.55541 | val_1_rmse: 0.56104 |  0:03:17s
epoch 22 | loss: 0.29556 | val_0_rmse: 0.53999 | val_1_rmse: 0.54515 |  0:03:26s
epoch 23 | loss: 0.29627 | val_0_rmse: 0.54632 | val_1_rmse: 0.54959 |  0:03:35s
epoch 24 | loss: 0.29427 | val_0_rmse: 0.54588 | val_1_rmse: 0.54875 |  0:03:44s
epoch 25 | loss: 0.29457 | val_0_rmse: 0.55671 | val_1_rmse: 0.55533 |  0:03:53s
epoch 26 | loss: 0.3016  | val_0_rmse: 0.53739 | val_1_rmse: 0.54053 |  0:04:02s
epoch 27 | loss: 0.29553 | val_0_rmse: 0.59735 | val_1_rmse: 0.59488 |  0:04:11s
epoch 28 | loss: 0.29757 | val_0_rmse: 0.54311 | val_1_rmse: 0.54327 |  0:04:20s
epoch 29 | loss: 0.29338 | val_0_rmse: 0.54233 | val_1_rmse: 0.54175 |  0:04:29s
epoch 30 | loss: 0.295   | val_0_rmse: 0.54808 | val_1_rmse: 0.55021 |  0:04:38s
epoch 31 | loss: 0.29665 | val_0_rmse: 0.55184 | val_1_rmse: 0.55734 |  0:04:47s
epoch 32 | loss: 0.29292 | val_0_rmse: 0.53955 | val_1_rmse: 0.54136 |  0:04:56s
epoch 33 | loss: 0.29435 | val_0_rmse: 0.57569 | val_1_rmse: 0.58168 |  0:05:05s
epoch 34 | loss: 0.294   | val_0_rmse: 0.55796 | val_1_rmse: 0.56183 |  0:05:14s
epoch 35 | loss: 0.29321 | val_0_rmse: 0.56149 | val_1_rmse: 0.5603  |  0:05:23s
epoch 36 | loss: 0.2985  | val_0_rmse: 0.53917 | val_1_rmse: 0.5449  |  0:05:32s
epoch 37 | loss: 0.29426 | val_0_rmse: 0.53267 | val_1_rmse: 0.53401 |  0:05:41s
epoch 38 | loss: 0.29271 | val_0_rmse: 0.59913 | val_1_rmse: 0.60059 |  0:05:50s
epoch 39 | loss: 0.29217 | val_0_rmse: 0.56221 | val_1_rmse: 0.56618 |  0:05:59s
epoch 40 | loss: 0.29721 | val_0_rmse: 0.53898 | val_1_rmse: 0.54291 |  0:06:08s
epoch 41 | loss: 0.29277 | val_0_rmse: 0.54729 | val_1_rmse: 0.55201 |  0:06:17s
epoch 42 | loss: 0.29214 | val_0_rmse: 0.53974 | val_1_rmse: 0.5403  |  0:06:26s
epoch 43 | loss: 0.28984 | val_0_rmse: 0.53557 | val_1_rmse: 0.5391  |  0:06:35s
epoch 44 | loss: 0.2953  | val_0_rmse: 0.53961 | val_1_rmse: 0.54286 |  0:06:44s
epoch 45 | loss: 0.29121 | val_0_rmse: 0.54104 | val_1_rmse: 0.542   |  0:06:53s
epoch 46 | loss: 0.29088 | val_0_rmse: 0.54412 | val_1_rmse: 0.54692 |  0:07:02s
epoch 47 | loss: 0.28918 | val_0_rmse: 0.53441 | val_1_rmse: 0.53535 |  0:07:11s
epoch 48 | loss: 0.28754 | val_0_rmse: 0.53914 | val_1_rmse: 0.54054 |  0:07:20s
epoch 49 | loss: 0.29015 | val_0_rmse: 0.56134 | val_1_rmse: 0.56775 |  0:07:29s
epoch 50 | loss: 0.28803 | val_0_rmse: 0.52852 | val_1_rmse: 0.53376 |  0:07:38s
epoch 51 | loss: 0.29028 | val_0_rmse: 0.54154 | val_1_rmse: 0.54424 |  0:07:47s
epoch 52 | loss: 0.28948 | val_0_rmse: 0.55236 | val_1_rmse: 0.55587 |  0:07:56s
epoch 53 | loss: 0.28989 | val_0_rmse: 0.53319 | val_1_rmse: 0.5341  |  0:08:05s
epoch 54 | loss: 0.28847 | val_0_rmse: 0.53909 | val_1_rmse: 0.54544 |  0:08:14s
epoch 55 | loss: 0.28891 | val_0_rmse: 0.55744 | val_1_rmse: 0.56423 |  0:08:23s
epoch 56 | loss: 0.29153 | val_0_rmse: 0.54705 | val_1_rmse: 0.55016 |  0:08:32s
epoch 57 | loss: 0.28809 | val_0_rmse: 0.56545 | val_1_rmse: 0.57588 |  0:08:41s
epoch 58 | loss: 0.28927 | val_0_rmse: 0.54184 | val_1_rmse: 0.54249 |  0:08:50s
epoch 59 | loss: 0.29009 | val_0_rmse: 0.53674 | val_1_rmse: 0.54088 |  0:08:59s
epoch 60 | loss: 0.28582 | val_0_rmse: 0.53635 | val_1_rmse: 0.53964 |  0:09:08s
epoch 61 | loss: 0.2883  | val_0_rmse: 0.54405 | val_1_rmse: 0.54663 |  0:09:17s
epoch 62 | loss: 0.28894 | val_0_rmse: 0.53818 | val_1_rmse: 0.5409  |  0:09:26s
epoch 63 | loss: 0.28907 | val_0_rmse: 0.55697 | val_1_rmse: 0.56296 |  0:09:35s
epoch 64 | loss: 0.28987 | val_0_rmse: 0.53899 | val_1_rmse: 0.53994 |  0:09:44s
epoch 65 | loss: 0.28739 | val_0_rmse: 0.56065 | val_1_rmse: 0.57142 |  0:09:53s
epoch 66 | loss: 0.29019 | val_0_rmse: 0.5368  | val_1_rmse: 0.54015 |  0:10:02s
epoch 67 | loss: 0.28549 | val_0_rmse: 0.55821 | val_1_rmse: 0.55982 |  0:10:11s
epoch 68 | loss: 0.28712 | val_0_rmse: 0.52698 | val_1_rmse: 0.53203 |  0:10:20s
epoch 69 | loss: 0.28649 | val_0_rmse: 0.53126 | val_1_rmse: 0.53533 |  0:10:29s
epoch 70 | loss: 0.28623 | val_0_rmse: 0.52573 | val_1_rmse: 0.53089 |  0:10:38s
epoch 71 | loss: 0.28574 | val_0_rmse: 0.5508  | val_1_rmse: 0.55413 |  0:10:47s
epoch 72 | loss: 0.28621 | val_0_rmse: 0.52653 | val_1_rmse: 0.5297  |  0:10:56s
epoch 73 | loss: 0.28624 | val_0_rmse: 0.52991 | val_1_rmse: 0.5327  |  0:11:05s
epoch 74 | loss: 0.28513 | val_0_rmse: 0.52773 | val_1_rmse: 0.53416 |  0:11:14s
epoch 75 | loss: 0.28663 | val_0_rmse: 0.52692 | val_1_rmse: 0.53246 |  0:11:23s
epoch 76 | loss: 0.28611 | val_0_rmse: 0.54113 | val_1_rmse: 0.54664 |  0:11:32s
epoch 77 | loss: 0.28461 | val_0_rmse: 0.55663 | val_1_rmse: 0.56425 |  0:11:41s
epoch 78 | loss: 0.28582 | val_0_rmse: 0.53217 | val_1_rmse: 0.53704 |  0:11:50s
epoch 79 | loss: 0.28568 | val_0_rmse: 0.55578 | val_1_rmse: 0.56313 |  0:11:59s
epoch 80 | loss: 0.28855 | val_0_rmse: 0.58191 | val_1_rmse: 0.58587 |  0:12:08s
epoch 81 | loss: 0.2883  | val_0_rmse: 0.53167 | val_1_rmse: 0.53662 |  0:12:17s
epoch 82 | loss: 0.28575 | val_0_rmse: 0.56433 | val_1_rmse: 0.57314 |  0:12:26s
epoch 83 | loss: 0.28423 | val_0_rmse: 0.52802 | val_1_rmse: 0.53403 |  0:12:35s
epoch 84 | loss: 0.28461 | val_0_rmse: 0.53079 | val_1_rmse: 0.53483 |  0:12:44s
epoch 85 | loss: 0.28486 | val_0_rmse: 0.53038 | val_1_rmse: 0.53688 |  0:12:53s
epoch 86 | loss: 0.28603 | val_0_rmse: 0.54709 | val_1_rmse: 0.55552 |  0:13:02s
epoch 87 | loss: 0.28672 | val_0_rmse: 0.54434 | val_1_rmse: 0.5504  |  0:13:11s
epoch 88 | loss: 0.28402 | val_0_rmse: 0.53282 | val_1_rmse: 0.54073 |  0:13:20s
epoch 89 | loss: 0.28462 | val_0_rmse: 0.53788 | val_1_rmse: 0.54348 |  0:13:29s
epoch 90 | loss: 0.28577 | val_0_rmse: 0.53392 | val_1_rmse: 0.53936 |  0:13:38s
epoch 91 | loss: 0.28349 | val_0_rmse: 0.5387  | val_1_rmse: 0.54714 |  0:13:47s
epoch 92 | loss: 0.28503 | val_0_rmse: 0.53125 | val_1_rmse: 0.53517 |  0:13:56s
epoch 93 | loss: 0.28601 | val_0_rmse: 0.52978 | val_1_rmse: 0.53236 |  0:14:05s
epoch 94 | loss: 0.2862  | val_0_rmse: 0.56954 | val_1_rmse: 0.57578 |  0:14:14s
epoch 95 | loss: 0.28692 | val_0_rmse: 0.56629 | val_1_rmse: 0.5669  |  0:14:23s
epoch 96 | loss: 0.28572 | val_0_rmse: 0.53336 | val_1_rmse: 0.53973 |  0:14:32s
epoch 97 | loss: 0.28402 | val_0_rmse: 0.55002 | val_1_rmse: 0.55815 |  0:14:41s
epoch 98 | loss: 0.28341 | val_0_rmse: 0.53195 | val_1_rmse: 0.5388  |  0:14:50s
epoch 99 | loss: 0.2839  | val_0_rmse: 0.59987 | val_1_rmse: 0.60673 |  0:14:59s
epoch 100| loss: 0.28546 | val_0_rmse: 0.52937 | val_1_rmse: 0.53624 |  0:15:08s
epoch 101| loss: 0.28601 | val_0_rmse: 0.56126 | val_1_rmse: 0.56497 |  0:15:17s
epoch 102| loss: 0.28247 | val_0_rmse: 0.53669 | val_1_rmse: 0.5431  |  0:15:26s

Early stopping occured at epoch 102 with best_epoch = 72 and best_val_1_rmse = 0.5297
Best weights from best epoch are automatically used!
ended training at: 14:46:34
Feature importance:
[('Area', 0.33299538539093165), ('Baths', 0.15358050472519022), ('Beds', 0.07308860642972459), ('Latitude', 0.30886635462065476), ('Longitude', 0.10303678983300417), ('Month', 0.0), ('Year', 0.02843235900049463)]
Mean squared error is of 10941901796.611332
Mean absolute error:66509.41119875653
MAPE:0.4642349355670724
R2 score:0.7208865662054542
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:46:36
epoch 0  | loss: 0.41544 | val_0_rmse: 0.59979 | val_1_rmse: 0.60581 |  0:00:09s
epoch 1  | loss: 0.35075 | val_0_rmse: 0.58793 | val_1_rmse: 0.59291 |  0:00:18s
epoch 2  | loss: 0.34311 | val_0_rmse: 0.57941 | val_1_rmse: 0.5896  |  0:00:27s
epoch 3  | loss: 0.33424 | val_0_rmse: 0.62214 | val_1_rmse: 0.62876 |  0:00:36s
epoch 4  | loss: 0.32564 | val_0_rmse: 0.56774 | val_1_rmse: 0.57401 |  0:00:45s
epoch 5  | loss: 0.3219  | val_0_rmse: 0.61358 | val_1_rmse: 0.61541 |  0:00:54s
epoch 6  | loss: 0.3183  | val_0_rmse: 0.55885 | val_1_rmse: 0.56658 |  0:01:03s
epoch 7  | loss: 0.32049 | val_0_rmse: 0.58436 | val_1_rmse: 0.59384 |  0:01:12s
epoch 8  | loss: 0.32149 | val_0_rmse: 0.55892 | val_1_rmse: 0.56692 |  0:01:21s
epoch 9  | loss: 0.31412 | val_0_rmse: 0.77833 | val_1_rmse: 0.77819 |  0:01:30s
epoch 10 | loss: 0.31222 | val_0_rmse: 0.60141 | val_1_rmse: 0.61195 |  0:01:39s
epoch 11 | loss: 0.30809 | val_0_rmse: 0.59254 | val_1_rmse: 0.59885 |  0:01:48s
epoch 12 | loss: 0.31086 | val_0_rmse: 0.5659  | val_1_rmse: 0.57485 |  0:01:57s
epoch 13 | loss: 0.31049 | val_0_rmse: 0.5578  | val_1_rmse: 0.56713 |  0:02:06s
epoch 14 | loss: 0.30601 | val_0_rmse: 0.61565 | val_1_rmse: 0.62852 |  0:02:15s
epoch 15 | loss: 0.30435 | val_0_rmse: 0.548   | val_1_rmse: 0.55821 |  0:02:24s
epoch 16 | loss: 0.30144 | val_0_rmse: 0.54876 | val_1_rmse: 0.55554 |  0:02:33s
epoch 17 | loss: 0.29862 | val_0_rmse: 0.55294 | val_1_rmse: 0.56067 |  0:02:42s
epoch 18 | loss: 0.29851 | val_0_rmse: 0.54533 | val_1_rmse: 0.55248 |  0:02:51s
epoch 19 | loss: 0.29585 | val_0_rmse: 0.55507 | val_1_rmse: 0.56337 |  0:03:00s
epoch 20 | loss: 0.29818 | val_0_rmse: 0.63012 | val_1_rmse: 0.64281 |  0:03:09s
epoch 21 | loss: 0.30475 | val_0_rmse: 0.81248 | val_1_rmse: 0.81585 |  0:03:18s
epoch 22 | loss: 0.32609 | val_0_rmse: 0.55139 | val_1_rmse: 0.55932 |  0:03:27s
epoch 23 | loss: 0.30467 | val_0_rmse: 0.54179 | val_1_rmse: 0.55103 |  0:03:36s
epoch 24 | loss: 0.30446 | val_0_rmse: 0.55264 | val_1_rmse: 0.56398 |  0:03:45s
epoch 25 | loss: 0.30259 | val_0_rmse: 0.57152 | val_1_rmse: 0.57665 |  0:03:54s
epoch 26 | loss: 0.3035  | val_0_rmse: 0.5441  | val_1_rmse: 0.55032 |  0:04:03s
epoch 27 | loss: 0.30086 | val_0_rmse: 0.55476 | val_1_rmse: 0.56428 |  0:04:12s
epoch 28 | loss: 0.29767 | val_0_rmse: 0.54015 | val_1_rmse: 0.54911 |  0:04:21s
epoch 29 | loss: 0.29642 | val_0_rmse: 0.56607 | val_1_rmse: 0.57234 |  0:04:29s
epoch 30 | loss: 0.29648 | val_0_rmse: 0.54665 | val_1_rmse: 0.55447 |  0:04:39s
epoch 31 | loss: 0.29518 | val_0_rmse: 0.54603 | val_1_rmse: 0.55338 |  0:04:47s
epoch 32 | loss: 0.29578 | val_0_rmse: 0.56091 | val_1_rmse: 0.56859 |  0:04:56s
epoch 33 | loss: 0.29653 | val_0_rmse: 0.54169 | val_1_rmse: 0.55148 |  0:05:05s
epoch 34 | loss: 0.29468 | val_0_rmse: 0.54692 | val_1_rmse: 0.55763 |  0:05:14s
epoch 35 | loss: 0.29406 | val_0_rmse: 0.55712 | val_1_rmse: 0.56737 |  0:05:23s
epoch 36 | loss: 0.29706 | val_0_rmse: 0.5962  | val_1_rmse: 0.60577 |  0:05:32s
epoch 37 | loss: 0.29471 | val_0_rmse: 0.60846 | val_1_rmse: 0.61549 |  0:05:41s
epoch 38 | loss: 0.29838 | val_0_rmse: 0.57667 | val_1_rmse: 0.58676 |  0:05:50s
epoch 39 | loss: 0.294   | val_0_rmse: 0.54216 | val_1_rmse: 0.55272 |  0:05:59s
epoch 40 | loss: 0.29264 | val_0_rmse: 0.53743 | val_1_rmse: 0.54723 |  0:06:08s
epoch 41 | loss: 0.29186 | val_0_rmse: 0.53546 | val_1_rmse: 0.54471 |  0:06:17s
epoch 42 | loss: 0.29302 | val_0_rmse: 0.56701 | val_1_rmse: 0.57346 |  0:06:26s
epoch 43 | loss: 0.31667 | val_0_rmse: 0.55327 | val_1_rmse: 0.56005 |  0:06:35s
epoch 44 | loss: 0.30283 | val_0_rmse: 0.5685  | val_1_rmse: 0.57598 |  0:06:44s
epoch 45 | loss: 0.29791 | val_0_rmse: 0.54789 | val_1_rmse: 0.55705 |  0:06:53s
epoch 46 | loss: 0.30916 | val_0_rmse: 0.54051 | val_1_rmse: 0.5491  |  0:07:02s
epoch 47 | loss: 0.29717 | val_0_rmse: 0.56482 | val_1_rmse: 0.57578 |  0:07:11s
epoch 48 | loss: 0.2939  | val_0_rmse: 0.54227 | val_1_rmse: 0.55358 |  0:07:20s
epoch 49 | loss: 0.29433 | val_0_rmse: 0.88919 | val_1_rmse: 0.89451 |  0:07:29s
epoch 50 | loss: 0.30957 | val_0_rmse: 0.55037 | val_1_rmse: 0.55877 |  0:07:38s
epoch 51 | loss: 0.29425 | val_0_rmse: 0.5419  | val_1_rmse: 0.55085 |  0:07:47s
epoch 52 | loss: 0.29464 | val_0_rmse: 0.58154 | val_1_rmse: 0.58737 |  0:07:56s
epoch 53 | loss: 0.29504 | val_0_rmse: 0.54805 | val_1_rmse: 0.55779 |  0:08:05s
epoch 54 | loss: 0.2936  | val_0_rmse: 0.5383  | val_1_rmse: 0.54643 |  0:08:14s
epoch 55 | loss: 0.29319 | val_0_rmse: 0.5363  | val_1_rmse: 0.54662 |  0:08:23s
epoch 56 | loss: 0.29073 | val_0_rmse: 0.53955 | val_1_rmse: 0.54783 |  0:08:32s
epoch 57 | loss: 0.2911  | val_0_rmse: 0.54185 | val_1_rmse: 0.55167 |  0:08:41s
epoch 58 | loss: 0.29166 | val_0_rmse: 0.60596 | val_1_rmse: 0.62032 |  0:08:50s
epoch 59 | loss: 0.29207 | val_0_rmse: 0.53669 | val_1_rmse: 0.54736 |  0:08:59s
epoch 60 | loss: 0.29049 | val_0_rmse: 0.59467 | val_1_rmse: 0.60373 |  0:09:08s
epoch 61 | loss: 0.2898  | val_0_rmse: 0.52927 | val_1_rmse: 0.53882 |  0:09:17s
epoch 62 | loss: 0.28941 | val_0_rmse: 0.55243 | val_1_rmse: 0.5603  |  0:09:26s
epoch 63 | loss: 0.28758 | val_0_rmse: 0.53072 | val_1_rmse: 0.54051 |  0:09:35s
epoch 64 | loss: 0.29397 | val_0_rmse: 0.53321 | val_1_rmse: 0.54254 |  0:09:44s
epoch 65 | loss: 0.29028 | val_0_rmse: 0.53727 | val_1_rmse: 0.54638 |  0:09:53s
epoch 66 | loss: 0.2885  | val_0_rmse: 0.53861 | val_1_rmse: 0.54879 |  0:10:02s
epoch 67 | loss: 0.28936 | val_0_rmse: 0.53462 | val_1_rmse: 0.54406 |  0:10:11s
epoch 68 | loss: 0.28596 | val_0_rmse: 0.54666 | val_1_rmse: 0.55768 |  0:10:20s
epoch 69 | loss: 0.29087 | val_0_rmse: 0.60935 | val_1_rmse: 0.61901 |  0:10:29s
epoch 70 | loss: 0.29191 | val_0_rmse: 0.54057 | val_1_rmse: 0.54941 |  0:10:38s
epoch 71 | loss: 0.28941 | val_0_rmse: 0.54448 | val_1_rmse: 0.55286 |  0:10:47s
epoch 72 | loss: 0.28763 | val_0_rmse: 0.54491 | val_1_rmse: 0.55409 |  0:10:56s
epoch 73 | loss: 0.28711 | val_0_rmse: 0.54224 | val_1_rmse: 0.5541  |  0:11:05s
epoch 74 | loss: 0.28766 | val_0_rmse: 0.53452 | val_1_rmse: 0.54403 |  0:11:14s
epoch 75 | loss: 0.28783 | val_0_rmse: 0.53202 | val_1_rmse: 0.54203 |  0:11:23s
epoch 76 | loss: 0.2874  | val_0_rmse: 0.53796 | val_1_rmse: 0.54904 |  0:11:32s
epoch 77 | loss: 0.2898  | val_0_rmse: 0.55262 | val_1_rmse: 0.56171 |  0:11:41s
epoch 78 | loss: 0.29011 | val_0_rmse: 0.52974 | val_1_rmse: 0.53914 |  0:11:50s
epoch 79 | loss: 0.28638 | val_0_rmse: 0.64338 | val_1_rmse: 0.65066 |  0:11:59s
epoch 80 | loss: 0.28718 | val_0_rmse: 0.53845 | val_1_rmse: 0.54817 |  0:12:08s
epoch 81 | loss: 0.28838 | val_0_rmse: 0.59435 | val_1_rmse: 0.60336 |  0:12:17s
epoch 82 | loss: 0.29039 | val_0_rmse: 0.77408 | val_1_rmse: 0.7873  |  0:12:26s
epoch 83 | loss: 0.30221 | val_0_rmse: 0.53706 | val_1_rmse: 0.54781 |  0:12:35s
epoch 84 | loss: 0.29264 | val_0_rmse: 0.56413 | val_1_rmse: 0.5715  |  0:12:44s
epoch 85 | loss: 0.2895  | val_0_rmse: 0.53126 | val_1_rmse: 0.5407  |  0:12:53s
epoch 86 | loss: 0.28916 | val_0_rmse: 0.54318 | val_1_rmse: 0.55362 |  0:13:02s
epoch 87 | loss: 0.2886  | val_0_rmse: 0.56276 | val_1_rmse: 0.57281 |  0:13:11s
epoch 88 | loss: 0.28602 | val_0_rmse: 0.58051 | val_1_rmse: 0.58967 |  0:13:20s
epoch 89 | loss: 0.28801 | val_0_rmse: 0.53809 | val_1_rmse: 0.5503  |  0:13:29s
epoch 90 | loss: 0.29603 | val_0_rmse: 0.5324  | val_1_rmse: 0.54263 |  0:13:38s
epoch 91 | loss: 0.28862 | val_0_rmse: 0.55663 | val_1_rmse: 0.56691 |  0:13:48s

Early stopping occured at epoch 91 with best_epoch = 61 and best_val_1_rmse = 0.53882
Best weights from best epoch are automatically used!
ended training at: 15:00:27
Feature importance:
[('Area', 0.2270781370910023), ('Baths', 0.05935326295642101), ('Beds', 0.05823930875661848), ('Latitude', 0.20659670879579922), ('Longitude', 0.10082989994500546), ('Month', 0.0), ('Year', 0.34790268245515354)]
Mean squared error is of 11114423876.852457
Mean absolute error:66999.86319413233
MAPE:0.4500510559130616
R2 score:0.7137268674972463
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:00:28
epoch 0  | loss: 0.4465  | val_0_rmse: 0.6176  | val_1_rmse: 0.62515 |  0:00:09s
epoch 1  | loss: 0.34641 | val_0_rmse: 0.59544 | val_1_rmse: 0.60168 |  0:00:18s
epoch 2  | loss: 0.33637 | val_0_rmse: 0.60534 | val_1_rmse: 0.61282 |  0:00:27s
epoch 3  | loss: 0.32862 | val_0_rmse: 0.56315 | val_1_rmse: 0.56904 |  0:00:36s
epoch 4  | loss: 0.31542 | val_0_rmse: 0.55305 | val_1_rmse: 0.55786 |  0:00:45s
epoch 5  | loss: 0.31755 | val_0_rmse: 0.57524 | val_1_rmse: 0.57892 |  0:00:54s
epoch 6  | loss: 0.31695 | val_0_rmse: 0.5848  | val_1_rmse: 0.583   |  0:01:03s
epoch 7  | loss: 0.32002 | val_0_rmse: 0.55781 | val_1_rmse: 0.56013 |  0:01:12s
epoch 8  | loss: 0.3199  | val_0_rmse: 0.56011 | val_1_rmse: 0.56474 |  0:01:21s
epoch 9  | loss: 0.33811 | val_0_rmse: 0.59902 | val_1_rmse: 0.59757 |  0:01:30s
epoch 10 | loss: 0.36828 | val_0_rmse: 0.62408 | val_1_rmse: 0.62929 |  0:01:39s
epoch 11 | loss: 0.36628 | val_0_rmse: 0.59555 | val_1_rmse: 0.60089 |  0:01:48s
epoch 12 | loss: 0.35788 | val_0_rmse: 0.58916 | val_1_rmse: 0.59035 |  0:01:57s
epoch 13 | loss: 0.34099 | val_0_rmse: 0.57787 | val_1_rmse: 0.58073 |  0:02:06s
epoch 14 | loss: 0.33679 | val_0_rmse: 0.57815 | val_1_rmse: 0.58461 |  0:02:15s
epoch 15 | loss: 0.33465 | val_0_rmse: 0.56605 | val_1_rmse: 0.57072 |  0:02:24s
epoch 16 | loss: 0.34042 | val_0_rmse: 0.57545 | val_1_rmse: 0.58116 |  0:02:33s
epoch 17 | loss: 0.33442 | val_0_rmse: 0.5763  | val_1_rmse: 0.57996 |  0:02:42s
epoch 18 | loss: 0.33607 | val_0_rmse: 0.59153 | val_1_rmse: 0.59316 |  0:02:51s
epoch 19 | loss: 0.35184 | val_0_rmse: 0.57061 | val_1_rmse: 0.57113 |  0:03:00s
epoch 20 | loss: 0.32715 | val_0_rmse: 0.56056 | val_1_rmse: 0.5637  |  0:03:08s
epoch 21 | loss: 0.31809 | val_0_rmse: 0.5652  | val_1_rmse: 0.57205 |  0:03:17s
epoch 22 | loss: 0.31555 | val_0_rmse: 0.56746 | val_1_rmse: 0.56945 |  0:03:26s
epoch 23 | loss: 0.31656 | val_0_rmse: 0.54772 | val_1_rmse: 0.54994 |  0:03:35s
epoch 24 | loss: 0.3202  | val_0_rmse: 0.55783 | val_1_rmse: 0.55987 |  0:03:44s
epoch 25 | loss: 0.31369 | val_0_rmse: 0.54796 | val_1_rmse: 0.55045 |  0:03:53s
epoch 26 | loss: 0.31005 | val_0_rmse: 0.55751 | val_1_rmse: 0.56301 |  0:04:02s
epoch 27 | loss: 0.30933 | val_0_rmse: 0.63231 | val_1_rmse: 0.63407 |  0:04:11s
epoch 28 | loss: 0.31053 | val_0_rmse: 0.54557 | val_1_rmse: 0.54949 |  0:04:20s
epoch 29 | loss: 0.33689 | val_0_rmse: 0.60123 | val_1_rmse: 0.60671 |  0:04:29s
epoch 30 | loss: 0.33072 | val_0_rmse: 0.67513 | val_1_rmse: 0.67413 |  0:04:39s
epoch 31 | loss: 0.32231 | val_0_rmse: 0.63697 | val_1_rmse: 0.64724 |  0:04:48s
epoch 32 | loss: 0.35798 | val_0_rmse: 0.71361 | val_1_rmse: 0.71265 |  0:04:57s
epoch 33 | loss: 0.37618 | val_0_rmse: 0.61558 | val_1_rmse: 0.61991 |  0:05:06s
epoch 34 | loss: 0.34292 | val_0_rmse: 0.56613 | val_1_rmse: 0.56846 |  0:05:15s
epoch 35 | loss: 0.33274 | val_0_rmse: 0.58052 | val_1_rmse: 0.58027 |  0:05:24s
epoch 36 | loss: 0.3284  | val_0_rmse: 0.56005 | val_1_rmse: 0.56309 |  0:05:33s
epoch 37 | loss: 0.32487 | val_0_rmse: 0.55748 | val_1_rmse: 0.55934 |  0:05:42s
epoch 38 | loss: 0.32203 | val_0_rmse: 0.59843 | val_1_rmse: 0.59994 |  0:05:51s
epoch 39 | loss: 0.32462 | val_0_rmse: 0.56189 | val_1_rmse: 0.56287 |  0:06:00s
epoch 40 | loss: 0.32484 | val_0_rmse: 0.57565 | val_1_rmse: 0.57803 |  0:06:09s
epoch 41 | loss: 0.31796 | val_0_rmse: 0.5592  | val_1_rmse: 0.56028 |  0:06:18s
epoch 42 | loss: 0.31397 | val_0_rmse: 0.57243 | val_1_rmse: 0.57308 |  0:06:27s
epoch 43 | loss: 0.31608 | val_0_rmse: 0.55459 | val_1_rmse: 0.55732 |  0:06:36s
epoch 44 | loss: 0.32037 | val_0_rmse: 0.59114 | val_1_rmse: 0.59602 |  0:06:45s
epoch 45 | loss: 0.31275 | val_0_rmse: 0.57547 | val_1_rmse: 0.57697 |  0:06:54s
epoch 46 | loss: 0.31305 | val_0_rmse: 0.55608 | val_1_rmse: 0.55695 |  0:07:03s
epoch 47 | loss: 0.31364 | val_0_rmse: 0.57956 | val_1_rmse: 0.58352 |  0:07:12s
epoch 48 | loss: 0.31245 | val_0_rmse: 0.54987 | val_1_rmse: 0.55108 |  0:07:21s
epoch 49 | loss: 0.3092  | val_0_rmse: 0.56314 | val_1_rmse: 0.56196 |  0:07:30s
epoch 50 | loss: 0.31058 | val_0_rmse: 0.55061 | val_1_rmse: 0.55021 |  0:07:39s
epoch 51 | loss: 0.3115  | val_0_rmse: 0.5646  | val_1_rmse: 0.56785 |  0:07:48s
epoch 52 | loss: 0.31064 | val_0_rmse: 0.58128 | val_1_rmse: 0.58434 |  0:07:57s
epoch 53 | loss: 0.3166  | val_0_rmse: 0.56236 | val_1_rmse: 0.56762 |  0:08:06s
epoch 54 | loss: 0.3139  | val_0_rmse: 0.56706 | val_1_rmse: 0.56843 |  0:08:15s
epoch 55 | loss: 0.31326 | val_0_rmse: 0.5744  | val_1_rmse: 0.5783  |  0:08:24s
epoch 56 | loss: 0.31433 | val_0_rmse: 0.56678 | val_1_rmse: 0.5668  |  0:08:33s
epoch 57 | loss: 0.31144 | val_0_rmse: 0.55455 | val_1_rmse: 0.55647 |  0:08:42s
epoch 58 | loss: 0.3128  | val_0_rmse: 0.57891 | val_1_rmse: 0.58205 |  0:08:51s

Early stopping occured at epoch 58 with best_epoch = 28 and best_val_1_rmse = 0.54949
Best weights from best epoch are automatically used!
ended training at: 15:09:23
Feature importance:
[('Area', 0.354970636354742), ('Baths', 0.07098386500657194), ('Beds', 0.0), ('Latitude', 0.16867084287074807), ('Longitude', 0.11790161761236684), ('Month', 0.0), ('Year', 0.2874730381555711)]
Mean squared error is of 11312519327.421694
Mean absolute error:68672.75540255765
MAPE:0.48630817737938686
R2 score:0.7069081580890508
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:09:24
epoch 0  | loss: 0.4023  | val_0_rmse: 0.59167 | val_1_rmse: 0.59899 |  0:00:09s
epoch 1  | loss: 0.33971 | val_0_rmse: 0.57476 | val_1_rmse: 0.58334 |  0:00:18s
epoch 2  | loss: 0.33518 | val_0_rmse: 0.56183 | val_1_rmse: 0.57029 |  0:00:27s
epoch 3  | loss: 0.3293  | val_0_rmse: 0.57465 | val_1_rmse: 0.58193 |  0:00:36s
epoch 4  | loss: 0.32532 | val_0_rmse: 0.57824 | val_1_rmse: 0.58496 |  0:00:45s
epoch 5  | loss: 0.31707 | val_0_rmse: 0.55822 | val_1_rmse: 0.5659  |  0:00:54s
epoch 6  | loss: 0.31905 | val_0_rmse: 0.55062 | val_1_rmse: 0.55859 |  0:01:03s
epoch 7  | loss: 0.31492 | val_0_rmse: 0.56048 | val_1_rmse: 0.56878 |  0:01:12s
epoch 8  | loss: 0.31195 | val_0_rmse: 0.5547  | val_1_rmse: 0.56294 |  0:01:21s
epoch 9  | loss: 0.30935 | val_0_rmse: 0.54799 | val_1_rmse: 0.55636 |  0:01:30s
epoch 10 | loss: 0.30622 | val_0_rmse: 0.55378 | val_1_rmse: 0.56025 |  0:01:39s
epoch 11 | loss: 0.30738 | val_0_rmse: 0.56469 | val_1_rmse: 0.57309 |  0:01:48s
epoch 12 | loss: 0.30483 | val_0_rmse: 0.55008 | val_1_rmse: 0.55954 |  0:01:57s
epoch 13 | loss: 0.30263 | val_0_rmse: 0.53968 | val_1_rmse: 0.54826 |  0:02:06s
epoch 14 | loss: 0.32591 | val_0_rmse: 0.57438 | val_1_rmse: 0.58239 |  0:02:15s
epoch 15 | loss: 0.33129 | val_0_rmse: 0.56972 | val_1_rmse: 0.57834 |  0:02:24s
epoch 16 | loss: 0.33033 | val_0_rmse: 0.56624 | val_1_rmse: 0.57464 |  0:02:33s
epoch 17 | loss: 0.32705 | val_0_rmse: 0.63255 | val_1_rmse: 0.63997 |  0:02:42s
epoch 18 | loss: 0.33095 | val_0_rmse: 0.56338 | val_1_rmse: 0.57239 |  0:02:51s
epoch 19 | loss: 0.32697 | val_0_rmse: 0.57488 | val_1_rmse: 0.58434 |  0:03:00s
epoch 20 | loss: 0.32432 | val_0_rmse: 0.57697 | val_1_rmse: 0.58462 |  0:03:09s
epoch 21 | loss: 0.32267 | val_0_rmse: 0.58672 | val_1_rmse: 0.59491 |  0:03:18s
epoch 22 | loss: 0.32265 | val_0_rmse: 0.57428 | val_1_rmse: 0.58338 |  0:03:27s
epoch 23 | loss: 0.32264 | val_0_rmse: 0.5692  | val_1_rmse: 0.57817 |  0:03:36s
epoch 24 | loss: 0.32519 | val_0_rmse: 0.5726  | val_1_rmse: 0.58093 |  0:03:44s
epoch 25 | loss: 0.32134 | val_0_rmse: 0.56512 | val_1_rmse: 0.57466 |  0:03:53s
epoch 26 | loss: 0.32105 | val_0_rmse: 0.56945 | val_1_rmse: 0.57941 |  0:04:02s
epoch 27 | loss: 0.32348 | val_0_rmse: 0.57632 | val_1_rmse: 0.58512 |  0:04:11s
epoch 28 | loss: 0.32092 | val_0_rmse: 0.57587 | val_1_rmse: 0.58566 |  0:04:20s
epoch 29 | loss: 0.3199  | val_0_rmse: 0.56556 | val_1_rmse: 0.57512 |  0:04:29s
epoch 30 | loss: 0.32025 | val_0_rmse: 0.56166 | val_1_rmse: 0.57184 |  0:04:38s
epoch 31 | loss: 0.3206  | val_0_rmse: 0.56291 | val_1_rmse: 0.57296 |  0:04:47s
epoch 32 | loss: 0.31808 | val_0_rmse: 0.56249 | val_1_rmse: 0.57208 |  0:04:56s
epoch 33 | loss: 0.3201  | val_0_rmse: 0.56809 | val_1_rmse: 0.57657 |  0:05:05s
epoch 34 | loss: 0.32    | val_0_rmse: 0.59685 | val_1_rmse: 0.60588 |  0:05:14s
epoch 35 | loss: 0.3186  | val_0_rmse: 0.55576 | val_1_rmse: 0.56614 |  0:05:23s
epoch 36 | loss: 0.31669 | val_0_rmse: 0.55885 | val_1_rmse: 0.56907 |  0:05:32s
epoch 37 | loss: 0.31612 | val_0_rmse: 0.55665 | val_1_rmse: 0.56602 |  0:05:41s
epoch 38 | loss: 0.31824 | val_0_rmse: 0.57006 | val_1_rmse: 0.58079 |  0:05:50s
epoch 39 | loss: 0.31931 | val_0_rmse: 0.57653 | val_1_rmse: 0.58646 |  0:05:59s
epoch 40 | loss: 0.31684 | val_0_rmse: 0.57034 | val_1_rmse: 0.58071 |  0:06:08s
epoch 41 | loss: 0.31393 | val_0_rmse: 0.57474 | val_1_rmse: 0.58492 |  0:06:17s
epoch 42 | loss: 0.3195  | val_0_rmse: 0.56475 | val_1_rmse: 0.57521 |  0:06:26s
epoch 43 | loss: 0.3154  | val_0_rmse: 0.55696 | val_1_rmse: 0.56726 |  0:06:35s

Early stopping occured at epoch 43 with best_epoch = 13 and best_val_1_rmse = 0.54826
Best weights from best epoch are automatically used!
ended training at: 15:16:02
Feature importance:
[('Area', 0.23194880784074587), ('Baths', 0.14610134420917534), ('Beds', 0.030487056379353686), ('Latitude', 0.30040534478314823), ('Longitude', 0.10171444051636133), ('Month', 0.0), ('Year', 0.18934300627121553)]
Mean squared error is of 11613385875.09209
Mean absolute error:68518.84664590108
MAPE:0.4742274919620492
R2 score:0.7058593580363054
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:16:04
epoch 0  | loss: 0.4229  | val_0_rmse: 0.59877 | val_1_rmse: 0.5999  |  0:00:09s
epoch 1  | loss: 0.33579 | val_0_rmse: 0.5667  | val_1_rmse: 0.56691 |  0:00:18s
epoch 2  | loss: 0.32499 | val_0_rmse: 0.60915 | val_1_rmse: 0.61452 |  0:00:27s
epoch 3  | loss: 0.32547 | val_0_rmse: 0.58315 | val_1_rmse: 0.58155 |  0:00:36s
epoch 4  | loss: 0.31722 | val_0_rmse: 0.54967 | val_1_rmse: 0.54931 |  0:00:45s
epoch 5  | loss: 0.31544 | val_0_rmse: 0.55754 | val_1_rmse: 0.55797 |  0:00:54s
epoch 6  | loss: 0.31331 | val_0_rmse: 0.55215 | val_1_rmse: 0.55366 |  0:01:03s
epoch 7  | loss: 0.31306 | val_0_rmse: 0.60783 | val_1_rmse: 0.60756 |  0:01:12s
epoch 8  | loss: 0.3086  | val_0_rmse: 0.55687 | val_1_rmse: 0.55684 |  0:01:21s
epoch 9  | loss: 0.30988 | val_0_rmse: 0.55064 | val_1_rmse: 0.55463 |  0:01:30s
epoch 10 | loss: 0.30589 | val_0_rmse: 0.56267 | val_1_rmse: 0.56507 |  0:01:39s
epoch 11 | loss: 0.30486 | val_0_rmse: 0.54815 | val_1_rmse: 0.54923 |  0:01:48s
epoch 12 | loss: 0.30232 | val_0_rmse: 0.56512 | val_1_rmse: 0.56641 |  0:01:57s
epoch 13 | loss: 0.30602 | val_0_rmse: 0.56222 | val_1_rmse: 0.5617  |  0:02:06s
epoch 14 | loss: 0.307   | val_0_rmse: 0.7385  | val_1_rmse: 0.74325 |  0:02:15s
epoch 15 | loss: 0.30477 | val_0_rmse: 0.62277 | val_1_rmse: 0.61737 |  0:02:24s
epoch 16 | loss: 0.30699 | val_0_rmse: 0.58246 | val_1_rmse: 0.58412 |  0:02:33s
epoch 17 | loss: 0.3025  | val_0_rmse: 0.56018 | val_1_rmse: 0.56185 |  0:02:42s
epoch 18 | loss: 0.29919 | val_0_rmse: 0.55219 | val_1_rmse: 0.55264 |  0:02:51s
epoch 19 | loss: 0.2989  | val_0_rmse: 0.58051 | val_1_rmse: 0.57993 |  0:03:00s
epoch 20 | loss: 0.2972  | val_0_rmse: 0.54097 | val_1_rmse: 0.54413 |  0:03:09s
epoch 21 | loss: 0.29942 | val_0_rmse: 0.54146 | val_1_rmse: 0.54092 |  0:03:18s
epoch 22 | loss: 0.29724 | val_0_rmse: 0.54716 | val_1_rmse: 0.5467  |  0:03:27s
epoch 23 | loss: 0.30066 | val_0_rmse: 0.55153 | val_1_rmse: 0.54926 |  0:03:36s
epoch 24 | loss: 0.29557 | val_0_rmse: 0.54459 | val_1_rmse: 0.54591 |  0:03:45s
epoch 25 | loss: 0.29815 | val_0_rmse: 0.54667 | val_1_rmse: 0.54856 |  0:03:54s
epoch 26 | loss: 0.2958  | val_0_rmse: 0.55023 | val_1_rmse: 0.55046 |  0:04:03s
epoch 27 | loss: 0.30194 | val_0_rmse: 0.54575 | val_1_rmse: 0.54748 |  0:04:12s
epoch 28 | loss: 0.29578 | val_0_rmse: 0.5466  | val_1_rmse: 0.54929 |  0:04:22s
epoch 29 | loss: 0.29687 | val_0_rmse: 0.5374  | val_1_rmse: 0.54035 |  0:04:31s
epoch 30 | loss: 0.29386 | val_0_rmse: 0.58818 | val_1_rmse: 0.58864 |  0:04:40s
epoch 31 | loss: 0.29557 | val_0_rmse: 0.54733 | val_1_rmse: 0.54828 |  0:04:49s
epoch 32 | loss: 0.29321 | val_0_rmse: 0.53782 | val_1_rmse: 0.54086 |  0:04:58s
epoch 33 | loss: 0.29203 | val_0_rmse: 0.53566 | val_1_rmse: 0.53894 |  0:05:07s
epoch 34 | loss: 0.29257 | val_0_rmse: 0.5433  | val_1_rmse: 0.54389 |  0:05:16s
epoch 35 | loss: 0.29204 | val_0_rmse: 0.54697 | val_1_rmse: 0.54758 |  0:05:25s
epoch 36 | loss: 0.30535 | val_0_rmse: 0.64867 | val_1_rmse: 0.6491  |  0:05:34s
epoch 37 | loss: 0.31478 | val_0_rmse: 0.67332 | val_1_rmse: 0.66867 |  0:05:43s
epoch 38 | loss: 0.32194 | val_0_rmse: 0.55149 | val_1_rmse: 0.5512  |  0:05:52s
epoch 39 | loss: 0.36089 | val_0_rmse: 0.68285 | val_1_rmse: 0.67986 |  0:06:01s
epoch 40 | loss: 0.40375 | val_0_rmse: 0.62322 | val_1_rmse: 0.61953 |  0:06:10s
epoch 41 | loss: 0.36805 | val_0_rmse: 0.6632  | val_1_rmse: 0.66617 |  0:06:19s
epoch 42 | loss: 0.41247 | val_0_rmse: 0.61402 | val_1_rmse: 0.61364 |  0:06:28s
epoch 43 | loss: 0.39113 | val_0_rmse: 0.60892 | val_1_rmse: 0.60948 |  0:06:37s
epoch 44 | loss: 0.38484 | val_0_rmse: 0.61929 | val_1_rmse: 0.61895 |  0:06:46s
epoch 45 | loss: 0.38424 | val_0_rmse: 0.61618 | val_1_rmse: 0.61328 |  0:06:55s
epoch 46 | loss: 0.36998 | val_0_rmse: 0.60144 | val_1_rmse: 0.60036 |  0:07:04s
epoch 47 | loss: 0.35824 | val_0_rmse: 0.57514 | val_1_rmse: 0.57623 |  0:07:13s
epoch 48 | loss: 0.3383  | val_0_rmse: 0.57946 | val_1_rmse: 0.58068 |  0:07:22s
epoch 49 | loss: 0.33135 | val_0_rmse: 0.5663  | val_1_rmse: 0.56868 |  0:07:31s
epoch 50 | loss: 0.32539 | val_0_rmse: 0.58243 | val_1_rmse: 0.58506 |  0:07:40s
epoch 51 | loss: 0.32522 | val_0_rmse: 0.58559 | val_1_rmse: 0.58783 |  0:07:49s
epoch 52 | loss: 0.31944 | val_0_rmse: 0.57915 | val_1_rmse: 0.58224 |  0:07:58s
epoch 53 | loss: 0.3159  | val_0_rmse: 0.55233 | val_1_rmse: 0.55281 |  0:08:07s
epoch 54 | loss: 0.30692 | val_0_rmse: 0.54268 | val_1_rmse: 0.5428  |  0:08:17s
epoch 55 | loss: 0.30112 | val_0_rmse: 0.55248 | val_1_rmse: 0.55319 |  0:08:26s
epoch 56 | loss: 0.29804 | val_0_rmse: 0.55181 | val_1_rmse: 0.55319 |  0:08:35s
epoch 57 | loss: 0.29536 | val_0_rmse: 0.53733 | val_1_rmse: 0.54024 |  0:08:44s
epoch 58 | loss: 0.29569 | val_0_rmse: 0.56306 | val_1_rmse: 0.5657  |  0:08:53s
epoch 59 | loss: 0.29459 | val_0_rmse: 0.54642 | val_1_rmse: 0.54845 |  0:09:02s
epoch 60 | loss: 0.29721 | val_0_rmse: 0.53378 | val_1_rmse: 0.53512 |  0:09:11s
epoch 61 | loss: 0.2944  | val_0_rmse: 0.54161 | val_1_rmse: 0.54361 |  0:09:20s
epoch 62 | loss: 0.29754 | val_0_rmse: 0.56072 | val_1_rmse: 0.56061 |  0:09:29s
epoch 63 | loss: 0.293   | val_0_rmse: 0.54341 | val_1_rmse: 0.54468 |  0:09:38s
epoch 64 | loss: 0.29494 | val_0_rmse: 0.54003 | val_1_rmse: 0.54237 |  0:09:47s
epoch 65 | loss: 0.29505 | val_0_rmse: 0.53246 | val_1_rmse: 0.53502 |  0:09:56s
epoch 66 | loss: 0.29252 | val_0_rmse: 0.56248 | val_1_rmse: 0.56322 |  0:10:05s
epoch 67 | loss: 0.29274 | val_0_rmse: 0.53957 | val_1_rmse: 0.54106 |  0:10:14s
epoch 68 | loss: 0.29417 | val_0_rmse: 0.53384 | val_1_rmse: 0.53544 |  0:10:23s
epoch 69 | loss: 0.29071 | val_0_rmse: 0.5548  | val_1_rmse: 0.55666 |  0:10:32s
epoch 70 | loss: 0.28965 | val_0_rmse: 0.54314 | val_1_rmse: 0.54513 |  0:10:41s
epoch 71 | loss: 0.28982 | val_0_rmse: 0.56069 | val_1_rmse: 0.56181 |  0:10:50s
epoch 72 | loss: 0.29611 | val_0_rmse: 0.57338 | val_1_rmse: 0.57246 |  0:10:59s
epoch 73 | loss: 0.29396 | val_0_rmse: 0.55476 | val_1_rmse: 0.55701 |  0:11:08s
epoch 74 | loss: 0.29234 | val_0_rmse: 0.55277 | val_1_rmse: 0.55608 |  0:11:17s
epoch 75 | loss: 0.29145 | val_0_rmse: 0.53524 | val_1_rmse: 0.53673 |  0:11:26s
epoch 76 | loss: 0.2917  | val_0_rmse: 0.54591 | val_1_rmse: 0.54836 |  0:11:35s
epoch 77 | loss: 0.28913 | val_0_rmse: 0.53673 | val_1_rmse: 0.53952 |  0:11:44s
epoch 78 | loss: 0.29093 | val_0_rmse: 0.54767 | val_1_rmse: 0.55022 |  0:11:53s
epoch 79 | loss: 0.28937 | val_0_rmse: 0.54784 | val_1_rmse: 0.55076 |  0:12:02s
epoch 80 | loss: 0.29018 | val_0_rmse: 0.52997 | val_1_rmse: 0.53224 |  0:12:11s
epoch 81 | loss: 0.28884 | val_0_rmse: 0.61657 | val_1_rmse: 0.61721 |  0:12:20s
epoch 82 | loss: 0.29012 | val_0_rmse: 0.54721 | val_1_rmse: 0.55037 |  0:12:29s
epoch 83 | loss: 0.29296 | val_0_rmse: 0.55882 | val_1_rmse: 0.56157 |  0:12:38s
epoch 84 | loss: 0.28961 | val_0_rmse: 0.5405  | val_1_rmse: 0.54543 |  0:12:47s
epoch 85 | loss: 0.29106 | val_0_rmse: 0.54102 | val_1_rmse: 0.54302 |  0:12:56s
epoch 86 | loss: 0.28835 | val_0_rmse: 0.53023 | val_1_rmse: 0.53208 |  0:13:05s
epoch 87 | loss: 0.28905 | val_0_rmse: 0.5333  | val_1_rmse: 0.53666 |  0:13:14s
epoch 88 | loss: 0.28867 | val_0_rmse: 0.56023 | val_1_rmse: 0.56427 |  0:13:23s
epoch 89 | loss: 0.2872  | val_0_rmse: 0.55511 | val_1_rmse: 0.55913 |  0:13:32s
epoch 90 | loss: 0.28924 | val_0_rmse: 0.53519 | val_1_rmse: 0.53787 |  0:13:41s
epoch 91 | loss: 0.28658 | val_0_rmse: 0.5302  | val_1_rmse: 0.53253 |  0:13:50s
epoch 92 | loss: 0.2889  | val_0_rmse: 0.53631 | val_1_rmse: 0.53971 |  0:13:59s
epoch 93 | loss: 0.28826 | val_0_rmse: 0.53654 | val_1_rmse: 0.53987 |  0:14:08s
epoch 94 | loss: 0.2869  | val_0_rmse: 0.55809 | val_1_rmse: 0.56191 |  0:14:17s
epoch 95 | loss: 0.28808 | val_0_rmse: 0.5346  | val_1_rmse: 0.53722 |  0:14:27s
epoch 96 | loss: 0.28762 | val_0_rmse: 0.54047 | val_1_rmse: 0.54323 |  0:14:36s
epoch 97 | loss: 0.28716 | val_0_rmse: 0.53644 | val_1_rmse: 0.54066 |  0:14:45s
epoch 98 | loss: 0.28791 | val_0_rmse: 0.53235 | val_1_rmse: 0.53519 |  0:14:54s
epoch 99 | loss: 0.29057 | val_0_rmse: 0.52971 | val_1_rmse: 0.53352 |  0:15:03s
epoch 100| loss: 0.28893 | val_0_rmse: 0.52933 | val_1_rmse: 0.53283 |  0:15:12s
epoch 101| loss: 0.28683 | val_0_rmse: 0.53202 | val_1_rmse: 0.53432 |  0:15:21s
epoch 102| loss: 0.28802 | val_0_rmse: 0.53544 | val_1_rmse: 0.53887 |  0:15:30s
epoch 103| loss: 0.28862 | val_0_rmse: 0.5758  | val_1_rmse: 0.58007 |  0:15:39s
epoch 104| loss: 0.28651 | val_0_rmse: 0.5349  | val_1_rmse: 0.53776 |  0:15:48s
epoch 105| loss: 0.28784 | val_0_rmse: 0.53016 | val_1_rmse: 0.53387 |  0:15:57s
epoch 106| loss: 0.28737 | val_0_rmse: 0.53051 | val_1_rmse: 0.5348  |  0:16:06s
epoch 107| loss: 0.28698 | val_0_rmse: 0.58481 | val_1_rmse: 0.58574 |  0:16:15s
epoch 108| loss: 0.29161 | val_0_rmse: 0.53457 | val_1_rmse: 0.5389  |  0:16:24s
epoch 109| loss: 0.28751 | val_0_rmse: 0.55578 | val_1_rmse: 0.55927 |  0:16:33s
epoch 110| loss: 0.29255 | val_0_rmse: 0.52528 | val_1_rmse: 0.53005 |  0:16:42s
epoch 111| loss: 0.28815 | val_0_rmse: 0.54344 | val_1_rmse: 0.54801 |  0:16:51s
epoch 112| loss: 0.28584 | val_0_rmse: 0.57473 | val_1_rmse: 0.57605 |  0:17:00s
epoch 113| loss: 0.28731 | val_0_rmse: 0.53914 | val_1_rmse: 0.54226 |  0:17:10s
epoch 114| loss: 0.28671 | val_0_rmse: 0.5441  | val_1_rmse: 0.54804 |  0:17:18s
epoch 115| loss: 0.28599 | val_0_rmse: 0.53156 | val_1_rmse: 0.53621 |  0:17:28s
epoch 116| loss: 0.28673 | val_0_rmse: 0.53009 | val_1_rmse: 0.53437 |  0:17:37s
epoch 117| loss: 0.28531 | val_0_rmse: 0.53262 | val_1_rmse: 0.53603 |  0:17:46s
epoch 118| loss: 0.28524 | val_0_rmse: 0.53632 | val_1_rmse: 0.5419  |  0:17:55s
epoch 119| loss: 0.28665 | val_0_rmse: 0.53701 | val_1_rmse: 0.53941 |  0:18:04s
epoch 120| loss: 0.28564 | val_0_rmse: 0.53209 | val_1_rmse: 0.53654 |  0:18:13s
epoch 121| loss: 0.28642 | val_0_rmse: 0.5711  | val_1_rmse: 0.57271 |  0:18:22s
epoch 122| loss: 0.28746 | val_0_rmse: 0.57038 | val_1_rmse: 0.57616 |  0:18:31s
epoch 123| loss: 0.28644 | val_0_rmse: 0.52914 | val_1_rmse: 0.5342  |  0:18:40s
epoch 124| loss: 0.28621 | val_0_rmse: 0.53411 | val_1_rmse: 0.53899 |  0:18:49s
epoch 125| loss: 0.287   | val_0_rmse: 0.59074 | val_1_rmse: 0.59277 |  0:18:58s
epoch 126| loss: 0.28443 | val_0_rmse: 0.5409  | val_1_rmse: 0.54517 |  0:19:07s
epoch 127| loss: 0.28541 | val_0_rmse: 0.58013 | val_1_rmse: 0.58175 |  0:19:16s
epoch 128| loss: 0.28564 | val_0_rmse: 0.57936 | val_1_rmse: 0.58421 |  0:19:25s
epoch 129| loss: 0.28425 | val_0_rmse: 0.5773  | val_1_rmse: 0.57986 |  0:19:34s
epoch 130| loss: 0.28504 | val_0_rmse: 0.55583 | val_1_rmse: 0.56094 |  0:19:43s
epoch 131| loss: 0.28554 | val_0_rmse: 0.52645 | val_1_rmse: 0.53063 |  0:19:53s
epoch 132| loss: 0.28387 | val_0_rmse: 0.60778 | val_1_rmse: 0.60765 |  0:20:02s
epoch 133| loss: 0.28974 | val_0_rmse: 0.53028 | val_1_rmse: 0.53501 |  0:20:11s
epoch 134| loss: 0.28659 | val_0_rmse: 0.55385 | val_1_rmse: 0.55727 |  0:20:20s
epoch 135| loss: 0.2867  | val_0_rmse: 0.53208 | val_1_rmse: 0.53583 |  0:20:29s
epoch 136| loss: 0.28595 | val_0_rmse: 0.59596 | val_1_rmse: 0.59602 |  0:20:38s
epoch 137| loss: 0.28577 | val_0_rmse: 0.52931 | val_1_rmse: 0.5323  |  0:20:47s
epoch 138| loss: 0.28571 | val_0_rmse: 0.6037  | val_1_rmse: 0.60269 |  0:20:56s
epoch 139| loss: 0.28636 | val_0_rmse: 0.52784 | val_1_rmse: 0.5318  |  0:21:05s
epoch 140| loss: 0.28475 | val_0_rmse: 0.54387 | val_1_rmse: 0.54704 |  0:21:14s

Early stopping occured at epoch 140 with best_epoch = 110 and best_val_1_rmse = 0.53005
Best weights from best epoch are automatically used!
ended training at: 15:37:21
Feature importance:
[('Area', 0.05982903724712869), ('Baths', 0.20691354114179408), ('Beds', 0.0), ('Latitude', 0.10891821085743811), ('Longitude', 0.381594447736886), ('Month', 0.0), ('Year', 0.24274476301675313)]
Mean squared error is of 11020377582.542782
Mean absolute error:65731.8921133783
MAPE:0.40861552257293177
R2 score:0.7214760027798652
------------------------------------------------------------------
