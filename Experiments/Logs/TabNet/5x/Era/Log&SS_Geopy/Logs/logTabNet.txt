TabNet Logs:

Saving copy of script...
In this script only the augmented Era dataset is used
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: DataBase_Era_augmented.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:22:55
epoch 0  | loss: 2.5419  | val_0_rmse: 0.99198 | val_1_rmse: 0.98583 |  0:00:02s
epoch 1  | loss: 1.12041 | val_0_rmse: 0.99363 | val_1_rmse: 0.98756 |  0:00:03s
epoch 2  | loss: 0.97145 | val_0_rmse: 0.98294 | val_1_rmse: 0.97902 |  0:00:04s
epoch 3  | loss: 0.83289 | val_0_rmse: 0.85679 | val_1_rmse: 0.85374 |  0:00:04s
epoch 4  | loss: 0.64139 | val_0_rmse: 0.80437 | val_1_rmse: 0.7973  |  0:00:05s
epoch 5  | loss: 0.58824 | val_0_rmse: 0.76956 | val_1_rmse: 0.75697 |  0:00:05s
epoch 6  | loss: 0.56498 | val_0_rmse: 0.75793 | val_1_rmse: 0.73843 |  0:00:06s
epoch 7  | loss: 0.53398 | val_0_rmse: 0.76879 | val_1_rmse: 0.74813 |  0:00:07s
epoch 8  | loss: 0.49857 | val_0_rmse: 0.72892 | val_1_rmse: 0.7201  |  0:00:07s
epoch 9  | loss: 0.45826 | val_0_rmse: 0.7175  | val_1_rmse: 0.71264 |  0:00:08s
epoch 10 | loss: 0.42589 | val_0_rmse: 0.71982 | val_1_rmse: 0.71738 |  0:00:09s
epoch 11 | loss: 0.40604 | val_0_rmse: 0.65831 | val_1_rmse: 0.64255 |  0:00:09s
epoch 12 | loss: 0.38237 | val_0_rmse: 0.65893 | val_1_rmse: 0.65164 |  0:00:10s
epoch 13 | loss: 0.3917  | val_0_rmse: 0.64737 | val_1_rmse: 0.64246 |  0:00:11s
epoch 14 | loss: 0.37961 | val_0_rmse: 0.63016 | val_1_rmse: 0.6162  |  0:00:11s
epoch 15 | loss: 0.35812 | val_0_rmse: 0.61879 | val_1_rmse: 0.60295 |  0:00:12s
epoch 16 | loss: 0.34907 | val_0_rmse: 0.61048 | val_1_rmse: 0.59997 |  0:00:13s
epoch 17 | loss: 0.34344 | val_0_rmse: 0.60717 | val_1_rmse: 0.59747 |  0:00:13s
epoch 18 | loss: 0.32129 | val_0_rmse: 0.60566 | val_1_rmse: 0.59338 |  0:00:14s
epoch 19 | loss: 0.32389 | val_0_rmse: 0.56444 | val_1_rmse: 0.55215 |  0:00:15s
epoch 20 | loss: 0.30277 | val_0_rmse: 0.57989 | val_1_rmse: 0.57751 |  0:00:15s
epoch 21 | loss: 0.28211 | val_0_rmse: 0.55543 | val_1_rmse: 0.55217 |  0:00:16s
epoch 22 | loss: 0.27074 | val_0_rmse: 0.56523 | val_1_rmse: 0.5691  |  0:00:16s
epoch 23 | loss: 0.25832 | val_0_rmse: 0.54393 | val_1_rmse: 0.54703 |  0:00:17s
epoch 24 | loss: 0.25086 | val_0_rmse: 0.53664 | val_1_rmse: 0.53515 |  0:00:18s
epoch 25 | loss: 0.24745 | val_0_rmse: 0.54529 | val_1_rmse: 0.54414 |  0:00:18s
epoch 26 | loss: 0.24312 | val_0_rmse: 0.53337 | val_1_rmse: 0.53111 |  0:00:19s
epoch 27 | loss: 0.22775 | val_0_rmse: 0.52808 | val_1_rmse: 0.53208 |  0:00:20s
epoch 28 | loss: 0.22741 | val_0_rmse: 0.52638 | val_1_rmse: 0.53606 |  0:00:20s
epoch 29 | loss: 0.22634 | val_0_rmse: 0.51971 | val_1_rmse: 0.5256  |  0:00:21s
epoch 30 | loss: 0.21708 | val_0_rmse: 0.5087  | val_1_rmse: 0.5083  |  0:00:22s
epoch 31 | loss: 0.20923 | val_0_rmse: 0.50755 | val_1_rmse: 0.50594 |  0:00:22s
epoch 32 | loss: 0.20596 | val_0_rmse: 0.49814 | val_1_rmse: 0.49881 |  0:00:23s
epoch 33 | loss: 0.19631 | val_0_rmse: 0.51619 | val_1_rmse: 0.5222  |  0:00:24s
epoch 34 | loss: 0.20252 | val_0_rmse: 0.51652 | val_1_rmse: 0.52193 |  0:00:25s
epoch 35 | loss: 0.19929 | val_0_rmse: 0.4883  | val_1_rmse: 0.48751 |  0:00:25s
epoch 36 | loss: 0.19064 | val_0_rmse: 0.49222 | val_1_rmse: 0.49579 |  0:00:26s
epoch 37 | loss: 0.18135 | val_0_rmse: 0.47022 | val_1_rmse: 0.47873 |  0:00:27s
epoch 38 | loss: 0.17451 | val_0_rmse: 0.46525 | val_1_rmse: 0.47286 |  0:00:27s
epoch 39 | loss: 0.16545 | val_0_rmse: 0.47554 | val_1_rmse: 0.48641 |  0:00:28s
epoch 40 | loss: 0.16433 | val_0_rmse: 0.47799 | val_1_rmse: 0.49102 |  0:00:29s
epoch 41 | loss: 0.15985 | val_0_rmse: 0.46938 | val_1_rmse: 0.47868 |  0:00:29s
epoch 42 | loss: 0.15731 | val_0_rmse: 0.46276 | val_1_rmse: 0.47131 |  0:00:30s
epoch 43 | loss: 0.15709 | val_0_rmse: 0.47827 | val_1_rmse: 0.49502 |  0:00:31s
epoch 44 | loss: 0.15255 | val_0_rmse: 0.46096 | val_1_rmse: 0.47341 |  0:00:31s
epoch 45 | loss: 0.15076 | val_0_rmse: 0.4646  | val_1_rmse: 0.47977 |  0:00:32s
epoch 46 | loss: 0.15442 | val_0_rmse: 0.45979 | val_1_rmse: 0.48106 |  0:00:32s
epoch 47 | loss: 0.15251 | val_0_rmse: 0.45534 | val_1_rmse: 0.47303 |  0:00:33s
epoch 48 | loss: 0.15103 | val_0_rmse: 0.45956 | val_1_rmse: 0.47777 |  0:00:34s
epoch 49 | loss: 0.14419 | val_0_rmse: 0.46475 | val_1_rmse: 0.48176 |  0:00:34s
epoch 50 | loss: 0.14793 | val_0_rmse: 0.453   | val_1_rmse: 0.46931 |  0:00:35s
epoch 51 | loss: 0.14609 | val_0_rmse: 0.44487 | val_1_rmse: 0.46016 |  0:00:36s
epoch 52 | loss: 0.15311 | val_0_rmse: 0.44477 | val_1_rmse: 0.46723 |  0:00:36s
epoch 53 | loss: 0.15672 | val_0_rmse: 0.43417 | val_1_rmse: 0.45745 |  0:00:37s
epoch 54 | loss: 0.14856 | val_0_rmse: 0.41573 | val_1_rmse: 0.43848 |  0:00:38s
epoch 55 | loss: 0.14367 | val_0_rmse: 0.41532 | val_1_rmse: 0.43925 |  0:00:38s
epoch 56 | loss: 0.14225 | val_0_rmse: 0.41058 | val_1_rmse: 0.43355 |  0:00:39s
epoch 57 | loss: 0.13959 | val_0_rmse: 0.4077  | val_1_rmse: 0.438   |  0:00:40s
epoch 58 | loss: 0.14633 | val_0_rmse: 0.41236 | val_1_rmse: 0.43374 |  0:00:40s
epoch 59 | loss: 0.14247 | val_0_rmse: 0.41393 | val_1_rmse: 0.43817 |  0:00:41s
epoch 60 | loss: 0.13928 | val_0_rmse: 0.41428 | val_1_rmse: 0.4437  |  0:00:42s
epoch 61 | loss: 0.15373 | val_0_rmse: 0.41368 | val_1_rmse: 0.4469  |  0:00:42s
epoch 62 | loss: 0.1507  | val_0_rmse: 0.41958 | val_1_rmse: 0.45089 |  0:00:43s
epoch 63 | loss: 0.15417 | val_0_rmse: 0.4366  | val_1_rmse: 0.45933 |  0:00:43s
epoch 64 | loss: 0.15373 | val_0_rmse: 0.4149  | val_1_rmse: 0.44519 |  0:00:44s
epoch 65 | loss: 0.14546 | val_0_rmse: 0.41069 | val_1_rmse: 0.44133 |  0:00:45s
epoch 66 | loss: 0.13971 | val_0_rmse: 0.40475 | val_1_rmse: 0.4417  |  0:00:45s
epoch 67 | loss: 0.12825 | val_0_rmse: 0.38086 | val_1_rmse: 0.42514 |  0:00:46s
epoch 68 | loss: 0.12896 | val_0_rmse: 0.38907 | val_1_rmse: 0.42747 |  0:00:47s
epoch 69 | loss: 0.12966 | val_0_rmse: 0.39141 | val_1_rmse: 0.43736 |  0:00:47s
epoch 70 | loss: 0.13823 | val_0_rmse: 0.41685 | val_1_rmse: 0.4507  |  0:00:48s
epoch 71 | loss: 0.14483 | val_0_rmse: 0.41596 | val_1_rmse: 0.45339 |  0:00:49s
epoch 72 | loss: 0.1536  | val_0_rmse: 0.43568 | val_1_rmse: 0.47365 |  0:00:49s
epoch 73 | loss: 0.18914 | val_0_rmse: 0.43026 | val_1_rmse: 0.47456 |  0:00:50s
epoch 74 | loss: 0.16587 | val_0_rmse: 0.41177 | val_1_rmse: 0.45386 |  0:00:51s
epoch 75 | loss: 0.15011 | val_0_rmse: 0.39976 | val_1_rmse: 0.44487 |  0:00:51s
epoch 76 | loss: 0.15123 | val_0_rmse: 0.41515 | val_1_rmse: 0.46096 |  0:00:52s
epoch 77 | loss: 0.14816 | val_0_rmse: 0.40482 | val_1_rmse: 0.4617  |  0:00:53s
epoch 78 | loss: 0.13905 | val_0_rmse: 0.39422 | val_1_rmse: 0.43442 |  0:00:53s
epoch 79 | loss: 0.14303 | val_0_rmse: 0.37984 | val_1_rmse: 0.41865 |  0:00:54s
epoch 80 | loss: 0.14175 | val_0_rmse: 0.38217 | val_1_rmse: 0.42803 |  0:00:54s
epoch 81 | loss: 0.14108 | val_0_rmse: 0.37916 | val_1_rmse: 0.42282 |  0:00:55s
epoch 82 | loss: 0.15558 | val_0_rmse: 0.41157 | val_1_rmse: 0.44215 |  0:00:56s
epoch 83 | loss: 0.1423  | val_0_rmse: 0.38404 | val_1_rmse: 0.41189 |  0:00:56s
epoch 84 | loss: 0.13879 | val_0_rmse: 0.37882 | val_1_rmse: 0.41653 |  0:00:57s
epoch 85 | loss: 0.14172 | val_0_rmse: 0.38177 | val_1_rmse: 0.42113 |  0:00:58s
epoch 86 | loss: 0.15028 | val_0_rmse: 0.40085 | val_1_rmse: 0.43377 |  0:00:58s
epoch 87 | loss: 0.14332 | val_0_rmse: 0.38113 | val_1_rmse: 0.41435 |  0:00:59s
epoch 88 | loss: 0.14007 | val_0_rmse: 0.37565 | val_1_rmse: 0.42047 |  0:01:00s
epoch 89 | loss: 0.14345 | val_0_rmse: 0.35319 | val_1_rmse: 0.39927 |  0:01:00s
epoch 90 | loss: 0.13696 | val_0_rmse: 0.35694 | val_1_rmse: 0.40613 |  0:01:01s
epoch 91 | loss: 0.13663 | val_0_rmse: 0.352   | val_1_rmse: 0.40336 |  0:01:02s
epoch 92 | loss: 0.13046 | val_0_rmse: 0.33981 | val_1_rmse: 0.39711 |  0:01:02s
epoch 93 | loss: 0.12318 | val_0_rmse: 0.33333 | val_1_rmse: 0.39536 |  0:01:03s
epoch 94 | loss: 0.12245 | val_0_rmse: 0.33154 | val_1_rmse: 0.39207 |  0:01:04s
epoch 95 | loss: 0.12291 | val_0_rmse: 0.33406 | val_1_rmse: 0.39317 |  0:01:04s
epoch 96 | loss: 0.12507 | val_0_rmse: 0.35711 | val_1_rmse: 0.40432 |  0:01:05s
epoch 97 | loss: 0.1241  | val_0_rmse: 0.34028 | val_1_rmse: 0.40841 |  0:01:06s
epoch 98 | loss: 0.13409 | val_0_rmse: 0.354   | val_1_rmse: 0.41275 |  0:01:06s
epoch 99 | loss: 0.13425 | val_0_rmse: 0.33895 | val_1_rmse: 0.39535 |  0:01:07s
epoch 100| loss: 0.13731 | val_0_rmse: 0.38896 | val_1_rmse: 0.41888 |  0:01:07s
epoch 101| loss: 0.16732 | val_0_rmse: 0.3702  | val_1_rmse: 0.40584 |  0:01:08s
epoch 102| loss: 0.15097 | val_0_rmse: 0.41466 | val_1_rmse: 0.45794 |  0:01:09s
epoch 103| loss: 0.15127 | val_0_rmse: 0.37857 | val_1_rmse: 0.41709 |  0:01:09s
epoch 104| loss: 0.14524 | val_0_rmse: 0.35816 | val_1_rmse: 0.40711 |  0:01:10s
epoch 105| loss: 0.13683 | val_0_rmse: 0.34834 | val_1_rmse: 0.39542 |  0:01:11s
epoch 106| loss: 0.13377 | val_0_rmse: 0.33832 | val_1_rmse: 0.3948  |  0:01:11s
epoch 107| loss: 0.1281  | val_0_rmse: 0.32837 | val_1_rmse: 0.39093 |  0:01:12s
epoch 108| loss: 0.12136 | val_0_rmse: 0.32124 | val_1_rmse: 0.38899 |  0:01:13s
epoch 109| loss: 0.11311 | val_0_rmse: 0.31475 | val_1_rmse: 0.38673 |  0:01:13s
epoch 110| loss: 0.11398 | val_0_rmse: 0.31114 | val_1_rmse: 0.37697 |  0:01:14s
epoch 111| loss: 0.11263 | val_0_rmse: 0.30883 | val_1_rmse: 0.37851 |  0:01:15s
epoch 112| loss: 0.1105  | val_0_rmse: 0.31234 | val_1_rmse: 0.38356 |  0:01:15s
epoch 113| loss: 0.10788 | val_0_rmse: 0.2991  | val_1_rmse: 0.37298 |  0:01:16s
epoch 114| loss: 0.10854 | val_0_rmse: 0.30286 | val_1_rmse: 0.37296 |  0:01:17s
epoch 115| loss: 0.10558 | val_0_rmse: 0.29949 | val_1_rmse: 0.37427 |  0:01:17s
epoch 116| loss: 0.09992 | val_0_rmse: 0.31038 | val_1_rmse: 0.38755 |  0:01:18s
epoch 117| loss: 0.1069  | val_0_rmse: 0.29444 | val_1_rmse: 0.367   |  0:01:19s
epoch 118| loss: 0.10205 | val_0_rmse: 0.29816 | val_1_rmse: 0.37493 |  0:01:19s
epoch 119| loss: 0.10289 | val_0_rmse: 0.28444 | val_1_rmse: 0.36426 |  0:01:20s
epoch 120| loss: 0.09696 | val_0_rmse: 0.28618 | val_1_rmse: 0.36686 |  0:01:20s
epoch 121| loss: 0.0952  | val_0_rmse: 0.28321 | val_1_rmse: 0.36765 |  0:01:21s
epoch 122| loss: 0.09778 | val_0_rmse: 0.27732 | val_1_rmse: 0.36138 |  0:01:22s
epoch 123| loss: 0.09467 | val_0_rmse: 0.27848 | val_1_rmse: 0.36007 |  0:01:22s
epoch 124| loss: 0.09584 | val_0_rmse: 0.27468 | val_1_rmse: 0.36114 |  0:01:23s
epoch 125| loss: 0.08991 | val_0_rmse: 0.27552 | val_1_rmse: 0.3642  |  0:01:24s
epoch 126| loss: 0.09664 | val_0_rmse: 0.27369 | val_1_rmse: 0.36296 |  0:01:24s
epoch 127| loss: 0.08975 | val_0_rmse: 0.27504 | val_1_rmse: 0.36292 |  0:01:25s
epoch 128| loss: 0.08946 | val_0_rmse: 0.27035 | val_1_rmse: 0.35945 |  0:01:26s
epoch 129| loss: 0.08914 | val_0_rmse: 0.28083 | val_1_rmse: 0.37339 |  0:01:26s
epoch 130| loss: 0.08828 | val_0_rmse: 0.26695 | val_1_rmse: 0.36029 |  0:01:27s
epoch 131| loss: 0.09093 | val_0_rmse: 0.27208 | val_1_rmse: 0.36411 |  0:01:28s
epoch 132| loss: 0.08679 | val_0_rmse: 0.26564 | val_1_rmse: 0.35985 |  0:01:28s
epoch 133| loss: 0.08766 | val_0_rmse: 0.26601 | val_1_rmse: 0.35601 |  0:01:29s
epoch 134| loss: 0.08383 | val_0_rmse: 0.26737 | val_1_rmse: 0.35908 |  0:01:30s
epoch 135| loss: 0.087   | val_0_rmse: 0.27108 | val_1_rmse: 0.35935 |  0:01:30s
epoch 136| loss: 0.08748 | val_0_rmse: 0.27335 | val_1_rmse: 0.37039 |  0:01:31s
epoch 137| loss: 0.1276  | val_0_rmse: 0.38011 | val_1_rmse: 0.44366 |  0:01:31s
epoch 138| loss: 0.13883 | val_0_rmse: 0.36599 | val_1_rmse: 0.59456 |  0:01:32s
epoch 139| loss: 0.16625 | val_0_rmse: 0.42363 | val_1_rmse: 0.5731  |  0:01:33s
epoch 140| loss: 0.17265 | val_0_rmse: 0.43012 | val_1_rmse: 0.47357 |  0:01:33s
epoch 141| loss: 0.15597 | val_0_rmse: 0.38067 | val_1_rmse: 0.42556 |  0:01:34s
epoch 142| loss: 0.14341 | val_0_rmse: 0.35203 | val_1_rmse: 0.40377 |  0:01:35s
epoch 143| loss: 0.13321 | val_0_rmse: 0.34515 | val_1_rmse: 0.40653 |  0:01:35s
epoch 144| loss: 0.1281  | val_0_rmse: 0.36675 | val_1_rmse: 0.42671 |  0:01:36s
epoch 145| loss: 0.12686 | val_0_rmse: 0.49623 | val_1_rmse: 0.53764 |  0:01:37s
epoch 146| loss: 0.12542 | val_0_rmse: 0.37686 | val_1_rmse: 0.44278 |  0:01:37s
epoch 147| loss: 0.12171 | val_0_rmse: 0.33196 | val_1_rmse: 0.40792 |  0:01:38s
epoch 148| loss: 0.13287 | val_0_rmse: 0.34221 | val_1_rmse: 0.41599 |  0:01:39s
epoch 149| loss: 0.12716 | val_0_rmse: 0.33251 | val_1_rmse: 0.40872 |  0:01:39s
Stop training because you reached max_epochs = 150 with best_epoch = 133 and best_val_1_rmse = 0.35601
Best weights from best epoch are automatically used!
ended training at: 04:24:35
Feature importance:
Mean squared error is of 3350584952.5854764
Mean absolute error:34768.0166263285
MAPE:0.21999445119677505
R2 score:0.8245124254182299
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Era\Log&SS_Geopy\Logs\\\tabnet_model_iter_0.zip
------------------------------------------------------------------
Using the dataset: DataBase_Era_augmented.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:24:36
epoch 0  | loss: 2.54192 | val_0_rmse: 1.00195 | val_1_rmse: 1.01061 |  0:00:00s
epoch 1  | loss: 1.18243 | val_0_rmse: 0.99041 | val_1_rmse: 1.00904 |  0:00:01s
epoch 2  | loss: 0.98244 | val_0_rmse: 0.94478 | val_1_rmse: 0.95348 |  0:00:01s
epoch 3  | loss: 0.87983 | val_0_rmse: 0.91733 | val_1_rmse: 0.91815 |  0:00:02s
epoch 4  | loss: 0.74476 | val_0_rmse: 0.88956 | val_1_rmse: 0.84146 |  0:00:03s
epoch 5  | loss: 0.62245 | val_0_rmse: 0.79104 | val_1_rmse: 0.77263 |  0:00:03s
epoch 6  | loss: 0.56015 | val_0_rmse: 0.78308 | val_1_rmse: 0.76997 |  0:00:04s
epoch 7  | loss: 0.53033 | val_0_rmse: 0.75073 | val_1_rmse: 0.74174 |  0:00:05s
epoch 8  | loss: 0.49906 | val_0_rmse: 0.71789 | val_1_rmse: 0.71185 |  0:00:05s
epoch 9  | loss: 0.47635 | val_0_rmse: 0.70745 | val_1_rmse: 0.69921 |  0:00:06s
epoch 10 | loss: 0.45138 | val_0_rmse: 0.70069 | val_1_rmse: 0.68517 |  0:00:07s
epoch 11 | loss: 0.42726 | val_0_rmse: 0.68044 | val_1_rmse: 0.66812 |  0:00:07s
epoch 12 | loss: 0.39298 | val_0_rmse: 0.64784 | val_1_rmse: 0.64683 |  0:00:08s
epoch 13 | loss: 0.37767 | val_0_rmse: 0.65428 | val_1_rmse: 0.66495 |  0:00:09s
epoch 14 | loss: 0.3705  | val_0_rmse: 0.63992 | val_1_rmse: 0.6544  |  0:00:09s
epoch 15 | loss: 0.36121 | val_0_rmse: 0.62226 | val_1_rmse: 0.62651 |  0:00:10s
epoch 16 | loss: 0.32822 | val_0_rmse: 0.61877 | val_1_rmse: 0.62067 |  0:00:11s
epoch 17 | loss: 0.3167  | val_0_rmse: 0.61211 | val_1_rmse: 0.62748 |  0:00:11s
epoch 18 | loss: 0.30356 | val_0_rmse: 0.60676 | val_1_rmse: 0.61871 |  0:00:12s
epoch 19 | loss: 0.30016 | val_0_rmse: 0.62205 | val_1_rmse: 0.62641 |  0:00:13s
epoch 20 | loss: 0.28216 | val_0_rmse: 0.63685 | val_1_rmse: 0.64385 |  0:00:13s
epoch 21 | loss: 0.28469 | val_0_rmse: 0.61305 | val_1_rmse: 0.61935 |  0:00:14s
epoch 22 | loss: 0.27874 | val_0_rmse: 0.60776 | val_1_rmse: 0.60723 |  0:00:15s
epoch 23 | loss: 0.2677  | val_0_rmse: 0.57839 | val_1_rmse: 0.58705 |  0:00:15s
epoch 24 | loss: 0.25057 | val_0_rmse: 0.57364 | val_1_rmse: 0.59248 |  0:00:16s
epoch 25 | loss: 0.24876 | val_0_rmse: 0.55641 | val_1_rmse: 0.57216 |  0:00:17s
epoch 26 | loss: 0.23743 | val_0_rmse: 0.57185 | val_1_rmse: 0.59041 |  0:00:17s
epoch 27 | loss: 0.2296  | val_0_rmse: 0.55943 | val_1_rmse: 0.58018 |  0:00:18s
epoch 28 | loss: 0.2271  | val_0_rmse: 0.55374 | val_1_rmse: 0.5684  |  0:00:18s
epoch 29 | loss: 0.22318 | val_0_rmse: 0.56465 | val_1_rmse: 0.57512 |  0:00:19s
epoch 30 | loss: 0.21793 | val_0_rmse: 0.55413 | val_1_rmse: 0.56471 |  0:00:20s
epoch 31 | loss: 0.20251 | val_0_rmse: 0.53882 | val_1_rmse: 0.56366 |  0:00:20s
epoch 32 | loss: 0.19933 | val_0_rmse: 0.51572 | val_1_rmse: 0.53671 |  0:00:21s
epoch 33 | loss: 0.18812 | val_0_rmse: 0.52525 | val_1_rmse: 0.54051 |  0:00:22s
epoch 34 | loss: 0.19316 | val_0_rmse: 0.51971 | val_1_rmse: 0.53296 |  0:00:22s
epoch 35 | loss: 0.1801  | val_0_rmse: 0.51923 | val_1_rmse: 0.53776 |  0:00:23s
epoch 36 | loss: 0.17429 | val_0_rmse: 0.50965 | val_1_rmse: 0.5284  |  0:00:24s
epoch 37 | loss: 0.17458 | val_0_rmse: 0.50325 | val_1_rmse: 0.52686 |  0:00:24s
epoch 38 | loss: 0.16786 | val_0_rmse: 0.49285 | val_1_rmse: 0.51909 |  0:00:25s
epoch 39 | loss: 0.15872 | val_0_rmse: 0.48126 | val_1_rmse: 0.50093 |  0:00:26s
epoch 40 | loss: 0.15909 | val_0_rmse: 0.50099 | val_1_rmse: 0.5226  |  0:00:26s
epoch 41 | loss: 0.15781 | val_0_rmse: 0.49148 | val_1_rmse: 0.51308 |  0:00:27s
epoch 42 | loss: 0.14626 | val_0_rmse: 0.46519 | val_1_rmse: 0.488   |  0:00:28s
epoch 43 | loss: 0.1456  | val_0_rmse: 0.46593 | val_1_rmse: 0.4778  |  0:00:28s
epoch 44 | loss: 0.14025 | val_0_rmse: 0.46384 | val_1_rmse: 0.48051 |  0:00:29s
epoch 45 | loss: 0.14931 | val_0_rmse: 0.45582 | val_1_rmse: 0.46296 |  0:00:29s
epoch 46 | loss: 0.1457  | val_0_rmse: 0.45897 | val_1_rmse: 0.46987 |  0:00:30s
epoch 47 | loss: 0.14249 | val_0_rmse: 0.4443  | val_1_rmse: 0.44855 |  0:00:31s
epoch 48 | loss: 0.13754 | val_0_rmse: 0.43962 | val_1_rmse: 0.448   |  0:00:31s
epoch 49 | loss: 0.13894 | val_0_rmse: 0.4296  | val_1_rmse: 0.44038 |  0:00:32s
epoch 50 | loss: 0.12956 | val_0_rmse: 0.43731 | val_1_rmse: 0.44427 |  0:00:33s
epoch 51 | loss: 0.12725 | val_0_rmse: 0.42549 | val_1_rmse: 0.43247 |  0:00:33s
epoch 52 | loss: 0.12443 | val_0_rmse: 0.42383 | val_1_rmse: 0.43362 |  0:00:34s
epoch 53 | loss: 0.12081 | val_0_rmse: 0.42387 | val_1_rmse: 0.43912 |  0:00:35s
epoch 54 | loss: 0.12557 | val_0_rmse: 0.40696 | val_1_rmse: 0.42451 |  0:00:35s
epoch 55 | loss: 0.11957 | val_0_rmse: 0.4198  | val_1_rmse: 0.43521 |  0:00:36s
epoch 56 | loss: 0.11881 | val_0_rmse: 0.4053  | val_1_rmse: 0.42549 |  0:00:37s
epoch 57 | loss: 0.12367 | val_0_rmse: 0.401   | val_1_rmse: 0.41583 |  0:00:37s
epoch 58 | loss: 0.11494 | val_0_rmse: 0.39594 | val_1_rmse: 0.41661 |  0:00:38s
epoch 59 | loss: 0.11467 | val_0_rmse: 0.3927  | val_1_rmse: 0.41743 |  0:00:39s
epoch 60 | loss: 0.11509 | val_0_rmse: 0.38348 | val_1_rmse: 0.40566 |  0:00:39s
epoch 61 | loss: 0.10905 | val_0_rmse: 0.37759 | val_1_rmse: 0.39918 |  0:00:40s
epoch 62 | loss: 0.11125 | val_0_rmse: 0.37943 | val_1_rmse: 0.39411 |  0:00:40s
epoch 63 | loss: 0.10638 | val_0_rmse: 0.37501 | val_1_rmse: 0.39902 |  0:00:41s
epoch 64 | loss: 0.10776 | val_0_rmse: 0.36941 | val_1_rmse: 0.3949  |  0:00:42s
epoch 65 | loss: 0.10871 | val_0_rmse: 0.36637 | val_1_rmse: 0.38863 |  0:00:42s
epoch 66 | loss: 0.10729 | val_0_rmse: 0.3674  | val_1_rmse: 0.38504 |  0:00:43s
epoch 67 | loss: 0.10548 | val_0_rmse: 0.35737 | val_1_rmse: 0.38077 |  0:00:44s
epoch 68 | loss: 0.10488 | val_0_rmse: 0.36286 | val_1_rmse: 0.39128 |  0:00:44s
epoch 69 | loss: 0.10901 | val_0_rmse: 0.35782 | val_1_rmse: 0.37351 |  0:00:45s
epoch 70 | loss: 0.10531 | val_0_rmse: 0.36816 | val_1_rmse: 0.38163 |  0:00:46s
epoch 71 | loss: 0.10988 | val_0_rmse: 0.34421 | val_1_rmse: 0.37157 |  0:00:46s
epoch 72 | loss: 0.09947 | val_0_rmse: 0.34153 | val_1_rmse: 0.36324 |  0:00:47s
epoch 73 | loss: 0.09925 | val_0_rmse: 0.34958 | val_1_rmse: 0.36989 |  0:00:48s
epoch 74 | loss: 0.09609 | val_0_rmse: 0.33767 | val_1_rmse: 0.35897 |  0:00:48s
epoch 75 | loss: 0.09588 | val_0_rmse: 0.32699 | val_1_rmse: 0.36181 |  0:00:49s
epoch 76 | loss: 0.09526 | val_0_rmse: 0.32852 | val_1_rmse: 0.36055 |  0:00:50s
epoch 77 | loss: 0.09772 | val_0_rmse: 0.3301  | val_1_rmse: 0.36244 |  0:00:50s
epoch 78 | loss: 0.0925  | val_0_rmse: 0.32601 | val_1_rmse: 0.36194 |  0:00:51s
epoch 79 | loss: 0.09757 | val_0_rmse: 0.31492 | val_1_rmse: 0.3451  |  0:00:52s
epoch 80 | loss: 0.10031 | val_0_rmse: 0.31074 | val_1_rmse: 0.34915 |  0:00:52s
epoch 81 | loss: 0.09466 | val_0_rmse: 0.3044  | val_1_rmse: 0.34122 |  0:00:53s
epoch 82 | loss: 0.09476 | val_0_rmse: 0.30087 | val_1_rmse: 0.3491  |  0:00:54s
epoch 83 | loss: 0.09039 | val_0_rmse: 0.29914 | val_1_rmse: 0.34478 |  0:00:54s
epoch 84 | loss: 0.09128 | val_0_rmse: 0.30078 | val_1_rmse: 0.34163 |  0:00:55s
epoch 85 | loss: 0.09551 | val_0_rmse: 0.30149 | val_1_rmse: 0.34408 |  0:00:55s
epoch 86 | loss: 0.09607 | val_0_rmse: 0.29525 | val_1_rmse: 0.34288 |  0:00:56s
epoch 87 | loss: 0.09061 | val_0_rmse: 0.30479 | val_1_rmse: 0.35039 |  0:00:57s
epoch 88 | loss: 0.08949 | val_0_rmse: 0.28706 | val_1_rmse: 0.34372 |  0:00:57s
epoch 89 | loss: 0.08844 | val_0_rmse: 0.28854 | val_1_rmse: 0.33995 |  0:00:58s
epoch 90 | loss: 0.08756 | val_0_rmse: 0.28616 | val_1_rmse: 0.34237 |  0:00:59s
epoch 91 | loss: 0.08657 | val_0_rmse: 0.28402 | val_1_rmse: 0.35549 |  0:00:59s
epoch 92 | loss: 0.0862  | val_0_rmse: 0.27871 | val_1_rmse: 0.35366 |  0:01:00s
epoch 93 | loss: 0.08286 | val_0_rmse: 0.27155 | val_1_rmse: 0.34565 |  0:01:01s
epoch 94 | loss: 0.08675 | val_0_rmse: 0.26693 | val_1_rmse: 0.34034 |  0:01:01s
epoch 95 | loss: 0.08858 | val_0_rmse: 0.27529 | val_1_rmse: 0.35011 |  0:01:02s
epoch 96 | loss: 0.0829  | val_0_rmse: 0.26501 | val_1_rmse: 0.3453  |  0:01:03s
epoch 97 | loss: 0.08755 | val_0_rmse: 0.26756 | val_1_rmse: 0.34884 |  0:01:03s
epoch 98 | loss: 0.09199 | val_0_rmse: 0.27494 | val_1_rmse: 0.34351 |  0:01:04s
epoch 99 | loss: 0.08475 | val_0_rmse: 0.27084 | val_1_rmse: 0.34777 |  0:01:05s
epoch 100| loss: 0.08779 | val_0_rmse: 0.26352 | val_1_rmse: 0.34184 |  0:01:05s
epoch 101| loss: 0.08374 | val_0_rmse: 0.26648 | val_1_rmse: 0.34685 |  0:01:06s
epoch 102| loss: 0.08087 | val_0_rmse: 0.25602 | val_1_rmse: 0.33583 |  0:01:06s
epoch 103| loss: 0.08068 | val_0_rmse: 0.25803 | val_1_rmse: 0.3336  |  0:01:07s
epoch 104| loss: 0.0853  | val_0_rmse: 0.26319 | val_1_rmse: 0.33525 |  0:01:08s
epoch 105| loss: 0.08355 | val_0_rmse: 0.26318 | val_1_rmse: 0.33323 |  0:01:08s
epoch 106| loss: 0.08382 | val_0_rmse: 0.26782 | val_1_rmse: 0.34842 |  0:01:09s
epoch 107| loss: 0.08247 | val_0_rmse: 0.258   | val_1_rmse: 0.33966 |  0:01:10s
epoch 108| loss: 0.08214 | val_0_rmse: 0.25256 | val_1_rmse: 0.33792 |  0:01:10s
epoch 109| loss: 0.0791  | val_0_rmse: 0.25076 | val_1_rmse: 0.33229 |  0:01:11s
epoch 110| loss: 0.0764  | val_0_rmse: 0.24718 | val_1_rmse: 0.33541 |  0:01:12s
epoch 111| loss: 0.07418 | val_0_rmse: 0.26252 | val_1_rmse: 0.35786 |  0:01:12s
epoch 112| loss: 0.07697 | val_0_rmse: 0.24149 | val_1_rmse: 0.33674 |  0:01:13s
epoch 113| loss: 0.07509 | val_0_rmse: 0.24016 | val_1_rmse: 0.33158 |  0:01:14s
epoch 114| loss: 0.07563 | val_0_rmse: 0.24744 | val_1_rmse: 0.34386 |  0:01:14s
epoch 115| loss: 0.07362 | val_0_rmse: 0.24138 | val_1_rmse: 0.33569 |  0:01:15s
epoch 116| loss: 0.07294 | val_0_rmse: 0.23714 | val_1_rmse: 0.34184 |  0:01:15s
epoch 117| loss: 0.06925 | val_0_rmse: 0.24438 | val_1_rmse: 0.34012 |  0:01:16s
epoch 118| loss: 0.07368 | val_0_rmse: 0.25446 | val_1_rmse: 0.33792 |  0:01:17s
epoch 119| loss: 0.06972 | val_0_rmse: 0.26573 | val_1_rmse: 0.3606  |  0:01:17s
epoch 120| loss: 0.07279 | val_0_rmse: 0.2439  | val_1_rmse: 0.34309 |  0:01:18s
epoch 121| loss: 0.07528 | val_0_rmse: 0.24262 | val_1_rmse: 0.34055 |  0:01:19s
epoch 122| loss: 0.07391 | val_0_rmse: 0.24198 | val_1_rmse: 0.34149 |  0:01:19s
epoch 123| loss: 0.07134 | val_0_rmse: 0.27088 | val_1_rmse: 0.35713 |  0:01:20s
epoch 124| loss: 0.07602 | val_0_rmse: 0.24268 | val_1_rmse: 0.34181 |  0:01:21s
epoch 125| loss: 0.07451 | val_0_rmse: 0.25696 | val_1_rmse: 0.35629 |  0:01:21s
epoch 126| loss: 0.07749 | val_0_rmse: 0.25392 | val_1_rmse: 0.34184 |  0:01:22s
epoch 127| loss: 0.07505 | val_0_rmse: 0.2504  | val_1_rmse: 0.3432  |  0:01:23s
epoch 128| loss: 0.07301 | val_0_rmse: 0.24165 | val_1_rmse: 0.33297 |  0:01:23s
epoch 129| loss: 0.06941 | val_0_rmse: 0.23951 | val_1_rmse: 0.33204 |  0:01:24s
epoch 130| loss: 0.07029 | val_0_rmse: 0.23201 | val_1_rmse: 0.32469 |  0:01:25s
epoch 131| loss: 0.06882 | val_0_rmse: 0.22892 | val_1_rmse: 0.33008 |  0:01:25s
epoch 132| loss: 0.06913 | val_0_rmse: 0.23585 | val_1_rmse: 0.33013 |  0:01:26s
epoch 133| loss: 0.06892 | val_0_rmse: 0.22962 | val_1_rmse: 0.32726 |  0:01:27s
epoch 134| loss: 0.06574 | val_0_rmse: 0.22923 | val_1_rmse: 0.33449 |  0:01:27s
epoch 135| loss: 0.06806 | val_0_rmse: 0.22962 | val_1_rmse: 0.326   |  0:01:28s
epoch 136| loss: 0.06825 | val_0_rmse: 0.23208 | val_1_rmse: 0.32451 |  0:01:28s
epoch 137| loss: 0.06469 | val_0_rmse: 0.22553 | val_1_rmse: 0.32489 |  0:01:29s
epoch 138| loss: 0.06366 | val_0_rmse: 0.24734 | val_1_rmse: 0.35059 |  0:01:30s
epoch 139| loss: 0.06695 | val_0_rmse: 0.26447 | val_1_rmse: 0.34986 |  0:01:30s
epoch 140| loss: 0.07117 | val_0_rmse: 0.23851 | val_1_rmse: 0.33833 |  0:01:31s
epoch 141| loss: 0.06871 | val_0_rmse: 0.23875 | val_1_rmse: 0.32722 |  0:01:32s
epoch 142| loss: 0.06878 | val_0_rmse: 0.22943 | val_1_rmse: 0.32195 |  0:01:32s
epoch 143| loss: 0.0681  | val_0_rmse: 0.23762 | val_1_rmse: 0.33264 |  0:01:33s
epoch 144| loss: 0.06837 | val_0_rmse: 0.23177 | val_1_rmse: 0.32699 |  0:01:34s
epoch 145| loss: 0.06526 | val_0_rmse: 0.23738 | val_1_rmse: 0.33934 |  0:01:34s
epoch 146| loss: 0.0704  | val_0_rmse: 0.26558 | val_1_rmse: 0.35181 |  0:01:35s
epoch 147| loss: 0.07055 | val_0_rmse: 0.24678 | val_1_rmse: 0.33955 |  0:01:36s
epoch 148| loss: 0.06706 | val_0_rmse: 0.23702 | val_1_rmse: 0.32764 |  0:01:36s
epoch 149| loss: 0.07976 | val_0_rmse: 0.28083 | val_1_rmse: 0.36145 |  0:01:37s
Stop training because you reached max_epochs = 150 with best_epoch = 142 and best_val_1_rmse = 0.32195
Best weights from best epoch are automatically used!
ended training at: 04:26:14
Feature importance:
Mean squared error is of 3117394445.8967547
Mean absolute error:32857.49988268736
MAPE:0.20923230189274522
R2 score:0.8416610262585358
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Era\Log&SS_Geopy\Logs\\\tabnet_model_iter_1.zip
------------------------------------------------------------------
Using the dataset: DataBase_Era_augmented.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:26:14
epoch 0  | loss: 2.68808 | val_0_rmse: 1.00165 | val_1_rmse: 0.98023 |  0:00:00s
epoch 1  | loss: 1.26106 | val_0_rmse: 0.99666 | val_1_rmse: 0.97439 |  0:00:01s
epoch 2  | loss: 1.05571 | val_0_rmse: 1.00188 | val_1_rmse: 0.97909 |  0:00:01s
epoch 3  | loss: 0.97157 | val_0_rmse: 0.91075 | val_1_rmse: 0.88789 |  0:00:02s
epoch 4  | loss: 0.80308 | val_0_rmse: 1.00293 | val_1_rmse: 1.00496 |  0:00:03s
epoch 5  | loss: 0.66846 | val_0_rmse: 0.81569 | val_1_rmse: 0.79249 |  0:00:03s
epoch 6  | loss: 0.6007  | val_0_rmse: 0.779   | val_1_rmse: 0.77211 |  0:00:04s
epoch 7  | loss: 0.55888 | val_0_rmse: 0.77787 | val_1_rmse: 0.76027 |  0:00:05s
epoch 8  | loss: 0.51315 | val_0_rmse: 0.75691 | val_1_rmse: 0.74732 |  0:00:05s
epoch 9  | loss: 0.50339 | val_0_rmse: 0.73597 | val_1_rmse: 0.72704 |  0:00:06s
epoch 10 | loss: 0.47928 | val_0_rmse: 0.72356 | val_1_rmse: 0.71091 |  0:00:07s
epoch 11 | loss: 0.48529 | val_0_rmse: 0.72786 | val_1_rmse: 0.72169 |  0:00:07s
epoch 12 | loss: 0.47054 | val_0_rmse: 0.68555 | val_1_rmse: 0.67751 |  0:00:08s
epoch 13 | loss: 0.44244 | val_0_rmse: 0.68608 | val_1_rmse: 0.68033 |  0:00:09s
epoch 14 | loss: 0.43389 | val_0_rmse: 0.6753  | val_1_rmse: 0.67724 |  0:00:09s
epoch 15 | loss: 0.41359 | val_0_rmse: 0.67362 | val_1_rmse: 0.66936 |  0:00:10s
epoch 16 | loss: 0.40965 | val_0_rmse: 0.68187 | val_1_rmse: 0.67934 |  0:00:11s
epoch 17 | loss: 0.40132 | val_0_rmse: 0.68569 | val_1_rmse: 0.67723 |  0:00:11s
epoch 18 | loss: 0.39311 | val_0_rmse: 0.67063 | val_1_rmse: 0.66713 |  0:00:12s
epoch 19 | loss: 0.36835 | val_0_rmse: 0.65495 | val_1_rmse: 0.65548 |  0:00:13s
epoch 20 | loss: 0.36319 | val_0_rmse: 0.65353 | val_1_rmse: 0.65463 |  0:00:13s
epoch 21 | loss: 0.34455 | val_0_rmse: 0.6456  | val_1_rmse: 0.64878 |  0:00:14s
epoch 22 | loss: 0.33286 | val_0_rmse: 0.63927 | val_1_rmse: 0.63909 |  0:00:15s
epoch 23 | loss: 0.32425 | val_0_rmse: 0.6521  | val_1_rmse: 0.6551  |  0:00:15s
epoch 24 | loss: 0.31143 | val_0_rmse: 0.62843 | val_1_rmse: 0.63094 |  0:00:16s
epoch 25 | loss: 0.30482 | val_0_rmse: 0.6168  | val_1_rmse: 0.62459 |  0:00:16s
epoch 26 | loss: 0.29937 | val_0_rmse: 0.60586 | val_1_rmse: 0.61824 |  0:00:17s
epoch 27 | loss: 0.29439 | val_0_rmse: 0.59465 | val_1_rmse: 0.61134 |  0:00:18s
epoch 28 | loss: 0.29582 | val_0_rmse: 0.58065 | val_1_rmse: 0.59701 |  0:00:18s
epoch 29 | loss: 0.2765  | val_0_rmse: 0.59035 | val_1_rmse: 0.60792 |  0:00:19s
epoch 30 | loss: 0.26394 | val_0_rmse: 0.57605 | val_1_rmse: 0.59856 |  0:00:20s
epoch 31 | loss: 0.25986 | val_0_rmse: 0.57717 | val_1_rmse: 0.59924 |  0:00:20s
epoch 32 | loss: 0.25258 | val_0_rmse: 0.57294 | val_1_rmse: 0.59375 |  0:00:21s
epoch 33 | loss: 0.24341 | val_0_rmse: 0.57255 | val_1_rmse: 0.60061 |  0:00:22s
epoch 34 | loss: 0.23116 | val_0_rmse: 0.57458 | val_1_rmse: 0.59779 |  0:00:22s
epoch 35 | loss: 0.22889 | val_0_rmse: 0.56721 | val_1_rmse: 0.59138 |  0:00:23s
epoch 36 | loss: 0.22477 | val_0_rmse: 0.56338 | val_1_rmse: 0.57799 |  0:00:24s
epoch 37 | loss: 0.22429 | val_0_rmse: 0.57946 | val_1_rmse: 0.59318 |  0:00:24s
epoch 38 | loss: 0.21063 | val_0_rmse: 0.54903 | val_1_rmse: 0.56361 |  0:00:25s
epoch 39 | loss: 0.21033 | val_0_rmse: 0.54917 | val_1_rmse: 0.56587 |  0:00:26s
epoch 40 | loss: 0.2033  | val_0_rmse: 0.53296 | val_1_rmse: 0.54281 |  0:00:26s
epoch 41 | loss: 0.19726 | val_0_rmse: 0.52819 | val_1_rmse: 0.53725 |  0:00:27s
epoch 42 | loss: 0.1944  | val_0_rmse: 0.53356 | val_1_rmse: 0.53973 |  0:00:28s
epoch 43 | loss: 0.19175 | val_0_rmse: 0.52245 | val_1_rmse: 0.53425 |  0:00:28s
epoch 44 | loss: 0.19158 | val_0_rmse: 0.52742 | val_1_rmse: 0.53721 |  0:00:29s
epoch 45 | loss: 0.18787 | val_0_rmse: 0.52494 | val_1_rmse: 0.53462 |  0:00:30s
epoch 46 | loss: 0.18783 | val_0_rmse: 0.51825 | val_1_rmse: 0.52057 |  0:00:30s
epoch 47 | loss: 0.18382 | val_0_rmse: 0.52124 | val_1_rmse: 0.53124 |  0:00:31s
epoch 48 | loss: 0.1765  | val_0_rmse: 0.50221 | val_1_rmse: 0.51845 |  0:00:31s
epoch 49 | loss: 0.17661 | val_0_rmse: 0.50902 | val_1_rmse: 0.52271 |  0:00:32s
epoch 50 | loss: 0.17277 | val_0_rmse: 0.50407 | val_1_rmse: 0.5155  |  0:00:33s
epoch 51 | loss: 0.16667 | val_0_rmse: 0.4822  | val_1_rmse: 0.49212 |  0:00:33s
epoch 52 | loss: 0.16628 | val_0_rmse: 0.47519 | val_1_rmse: 0.48109 |  0:00:34s
epoch 53 | loss: 0.16558 | val_0_rmse: 0.47633 | val_1_rmse: 0.48456 |  0:00:35s
epoch 54 | loss: 0.15393 | val_0_rmse: 0.47371 | val_1_rmse: 0.48286 |  0:00:35s
epoch 55 | loss: 0.14613 | val_0_rmse: 0.46258 | val_1_rmse: 0.47568 |  0:00:36s
epoch 56 | loss: 0.15523 | val_0_rmse: 0.45723 | val_1_rmse: 0.47249 |  0:00:37s
epoch 57 | loss: 0.14473 | val_0_rmse: 0.45763 | val_1_rmse: 0.4724  |  0:00:37s
epoch 58 | loss: 0.14443 | val_0_rmse: 0.44707 | val_1_rmse: 0.46647 |  0:00:38s
epoch 59 | loss: 0.14431 | val_0_rmse: 0.44315 | val_1_rmse: 0.4627  |  0:00:39s
epoch 60 | loss: 0.141   | val_0_rmse: 0.43346 | val_1_rmse: 0.45368 |  0:00:39s
epoch 61 | loss: 0.13467 | val_0_rmse: 0.43927 | val_1_rmse: 0.46012 |  0:00:40s
epoch 62 | loss: 0.13417 | val_0_rmse: 0.41505 | val_1_rmse: 0.44986 |  0:00:41s
epoch 63 | loss: 0.13474 | val_0_rmse: 0.4113  | val_1_rmse: 0.43817 |  0:00:41s
epoch 64 | loss: 0.13708 | val_0_rmse: 0.42172 | val_1_rmse: 0.45364 |  0:00:42s
epoch 65 | loss: 0.1336  | val_0_rmse: 0.4175  | val_1_rmse: 0.44733 |  0:00:43s
epoch 66 | loss: 0.12799 | val_0_rmse: 0.39915 | val_1_rmse: 0.44141 |  0:00:43s
epoch 67 | loss: 0.12606 | val_0_rmse: 0.4002  | val_1_rmse: 0.44989 |  0:00:44s
epoch 68 | loss: 0.125   | val_0_rmse: 0.39897 | val_1_rmse: 0.44662 |  0:00:45s
epoch 69 | loss: 0.12364 | val_0_rmse: 0.3837  | val_1_rmse: 0.44341 |  0:00:45s
epoch 70 | loss: 0.121   | val_0_rmse: 0.39584 | val_1_rmse: 0.44406 |  0:00:46s
epoch 71 | loss: 0.11847 | val_0_rmse: 0.37549 | val_1_rmse: 0.43301 |  0:00:46s
epoch 72 | loss: 0.1181  | val_0_rmse: 0.36729 | val_1_rmse: 0.41862 |  0:00:47s
epoch 73 | loss: 0.11414 | val_0_rmse: 0.36339 | val_1_rmse: 0.41188 |  0:00:48s
epoch 74 | loss: 0.11183 | val_0_rmse: 0.35962 | val_1_rmse: 0.41297 |  0:00:48s
epoch 75 | loss: 0.11205 | val_0_rmse: 0.36486 | val_1_rmse: 0.41174 |  0:00:49s
epoch 76 | loss: 0.10801 | val_0_rmse: 0.35688 | val_1_rmse: 0.40877 |  0:00:50s
epoch 77 | loss: 0.11107 | val_0_rmse: 0.36728 | val_1_rmse: 0.41533 |  0:00:50s
epoch 78 | loss: 0.10894 | val_0_rmse: 0.34627 | val_1_rmse: 0.40573 |  0:00:51s
epoch 79 | loss: 0.10882 | val_0_rmse: 0.35009 | val_1_rmse: 0.41092 |  0:00:52s
epoch 80 | loss: 0.10425 | val_0_rmse: 0.34033 | val_1_rmse: 0.39941 |  0:00:52s
epoch 81 | loss: 0.10208 | val_0_rmse: 0.32684 | val_1_rmse: 0.391   |  0:00:53s
epoch 82 | loss: 0.10162 | val_0_rmse: 0.32408 | val_1_rmse: 0.38934 |  0:00:54s
epoch 83 | loss: 0.10069 | val_0_rmse: 0.32389 | val_1_rmse: 0.39469 |  0:00:54s
epoch 84 | loss: 0.10178 | val_0_rmse: 0.31625 | val_1_rmse: 0.39304 |  0:00:55s
epoch 85 | loss: 0.09632 | val_0_rmse: 0.32119 | val_1_rmse: 0.38989 |  0:00:56s
epoch 86 | loss: 0.10293 | val_0_rmse: 0.32504 | val_1_rmse: 0.39497 |  0:00:56s
epoch 87 | loss: 0.10636 | val_0_rmse: 0.32873 | val_1_rmse: 0.39881 |  0:00:57s
epoch 88 | loss: 0.1029  | val_0_rmse: 0.31072 | val_1_rmse: 0.39194 |  0:00:58s
epoch 89 | loss: 0.10365 | val_0_rmse: 0.30305 | val_1_rmse: 0.38585 |  0:00:58s
epoch 90 | loss: 0.09698 | val_0_rmse: 0.31036 | val_1_rmse: 0.39287 |  0:00:59s
epoch 91 | loss: 0.10252 | val_0_rmse: 0.29722 | val_1_rmse: 0.3955  |  0:00:59s
epoch 92 | loss: 0.0984  | val_0_rmse: 0.30798 | val_1_rmse: 0.39341 |  0:01:00s
epoch 93 | loss: 0.09821 | val_0_rmse: 0.28572 | val_1_rmse: 0.38294 |  0:01:01s
epoch 94 | loss: 0.10051 | val_0_rmse: 0.29616 | val_1_rmse: 0.38719 |  0:01:01s
epoch 95 | loss: 0.09401 | val_0_rmse: 0.28878 | val_1_rmse: 0.3795  |  0:01:02s
epoch 96 | loss: 0.09153 | val_0_rmse: 0.28414 | val_1_rmse: 0.39019 |  0:01:03s
epoch 97 | loss: 0.09519 | val_0_rmse: 0.29184 | val_1_rmse: 0.37987 |  0:01:03s
epoch 98 | loss: 0.0999  | val_0_rmse: 0.27593 | val_1_rmse: 0.37486 |  0:01:04s
epoch 99 | loss: 0.09213 | val_0_rmse: 0.2834  | val_1_rmse: 0.37669 |  0:01:05s
epoch 100| loss: 0.09254 | val_0_rmse: 0.2807  | val_1_rmse: 0.38199 |  0:01:05s
epoch 101| loss: 0.08955 | val_0_rmse: 0.27461 | val_1_rmse: 0.38173 |  0:01:06s
epoch 102| loss: 0.09057 | val_0_rmse: 0.27182 | val_1_rmse: 0.37464 |  0:01:07s
epoch 103| loss: 0.08682 | val_0_rmse: 0.27048 | val_1_rmse: 0.37239 |  0:01:07s
epoch 104| loss: 0.08593 | val_0_rmse: 0.26959 | val_1_rmse: 0.38027 |  0:01:08s
epoch 105| loss: 0.08729 | val_0_rmse: 0.27618 | val_1_rmse: 0.38749 |  0:01:09s
epoch 106| loss: 0.08749 | val_0_rmse: 0.27583 | val_1_rmse: 0.39937 |  0:01:09s
epoch 107| loss: 0.09148 | val_0_rmse: 0.27674 | val_1_rmse: 0.38564 |  0:01:10s
epoch 108| loss: 0.09034 | val_0_rmse: 0.27246 | val_1_rmse: 0.3857  |  0:01:11s
epoch 109| loss: 0.09044 | val_0_rmse: 0.28134 | val_1_rmse: 0.38675 |  0:01:11s
epoch 110| loss: 0.08841 | val_0_rmse: 0.27484 | val_1_rmse: 0.37944 |  0:01:12s
epoch 111| loss: 0.09148 | val_0_rmse: 0.27765 | val_1_rmse: 0.38254 |  0:01:12s
epoch 112| loss: 0.09154 | val_0_rmse: 0.27751 | val_1_rmse: 0.38959 |  0:01:13s
epoch 113| loss: 0.09064 | val_0_rmse: 0.28511 | val_1_rmse: 0.40361 |  0:01:14s
epoch 114| loss: 0.09166 | val_0_rmse: 0.27555 | val_1_rmse: 0.40105 |  0:01:14s
epoch 115| loss: 0.08944 | val_0_rmse: 0.28719 | val_1_rmse: 0.40765 |  0:01:15s
epoch 116| loss: 0.0886  | val_0_rmse: 0.26714 | val_1_rmse: 0.3895  |  0:01:16s
epoch 117| loss: 0.09348 | val_0_rmse: 0.29356 | val_1_rmse: 0.40495 |  0:01:16s
epoch 118| loss: 0.10141 | val_0_rmse: 0.54695 | val_1_rmse: 0.58247 |  0:01:17s
epoch 119| loss: 0.09397 | val_0_rmse: 0.31917 | val_1_rmse: 0.41104 |  0:01:18s
epoch 120| loss: 0.10363 | val_0_rmse: 0.3038  | val_1_rmse: 0.41576 |  0:01:18s
epoch 121| loss: 0.10235 | val_0_rmse: 0.29169 | val_1_rmse: 0.40701 |  0:01:19s
epoch 122| loss: 0.09737 | val_0_rmse: 0.3243  | val_1_rmse: 0.44142 |  0:01:20s
epoch 123| loss: 0.0937  | val_0_rmse: 0.27783 | val_1_rmse: 0.39329 |  0:01:20s
epoch 124| loss: 0.08739 | val_0_rmse: 0.27184 | val_1_rmse: 0.3932  |  0:01:21s
epoch 125| loss: 0.08675 | val_0_rmse: 0.2745  | val_1_rmse: 0.39869 |  0:01:22s
epoch 126| loss: 0.08398 | val_0_rmse: 0.26493 | val_1_rmse: 0.38663 |  0:01:22s
epoch 127| loss: 0.08487 | val_0_rmse: 0.25917 | val_1_rmse: 0.39515 |  0:01:23s
epoch 128| loss: 0.08127 | val_0_rmse: 0.26826 | val_1_rmse: 0.39117 |  0:01:23s
epoch 129| loss: 0.07825 | val_0_rmse: 0.2637  | val_1_rmse: 0.39255 |  0:01:24s
epoch 130| loss: 0.08261 | val_0_rmse: 0.24953 | val_1_rmse: 0.37971 |  0:01:25s
epoch 131| loss: 0.0768  | val_0_rmse: 0.27432 | val_1_rmse: 0.39572 |  0:01:25s
epoch 132| loss: 0.08137 | val_0_rmse: 0.25023 | val_1_rmse: 0.38769 |  0:01:26s
epoch 133| loss: 0.07647 | val_0_rmse: 0.25231 | val_1_rmse: 0.38922 |  0:01:27s

Early stopping occured at epoch 133 with best_epoch = 103 and best_val_1_rmse = 0.37239
Best weights from best epoch are automatically used!
ended training at: 04:27:42
Feature importance:
Mean squared error is of 3554095694.4340496
Mean absolute error:34664.7238210341
MAPE:0.19589070471985545
R2 score:0.8147509919249656
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Era\Log&SS_Geopy\Logs\\\tabnet_model_iter_2.zip
------------------------------------------------------------------
Using the dataset: DataBase_Era_augmented.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:27:43
epoch 0  | loss: 2.61711 | val_0_rmse: 0.9958  | val_1_rmse: 0.99956 |  0:00:00s
epoch 1  | loss: 1.13471 | val_0_rmse: 0.99415 | val_1_rmse: 0.99396 |  0:00:01s
epoch 2  | loss: 0.93373 | val_0_rmse: 0.94074 | val_1_rmse: 0.94356 |  0:00:02s
epoch 3  | loss: 0.79686 | val_0_rmse: 0.90611 | val_1_rmse: 0.91986 |  0:00:02s
epoch 4  | loss: 0.66291 | val_0_rmse: 0.85279 | val_1_rmse: 0.86571 |  0:00:03s
epoch 5  | loss: 0.60018 | val_0_rmse: 0.76281 | val_1_rmse: 0.77038 |  0:00:03s
epoch 6  | loss: 0.5447  | val_0_rmse: 0.72012 | val_1_rmse: 0.73421 |  0:00:04s
epoch 7  | loss: 0.51645 | val_0_rmse: 0.7376  | val_1_rmse: 0.76403 |  0:00:05s
epoch 8  | loss: 0.50288 | val_0_rmse: 0.69045 | val_1_rmse: 0.71563 |  0:00:05s
epoch 9  | loss: 0.46588 | val_0_rmse: 0.71368 | val_1_rmse: 0.73562 |  0:00:06s
epoch 10 | loss: 0.43748 | val_0_rmse: 0.69191 | val_1_rmse: 0.71633 |  0:00:07s
epoch 11 | loss: 0.42596 | val_0_rmse: 0.69109 | val_1_rmse: 0.70869 |  0:00:07s
epoch 12 | loss: 0.41554 | val_0_rmse: 0.69894 | val_1_rmse: 0.71365 |  0:00:08s
epoch 13 | loss: 0.3908  | val_0_rmse: 0.65918 | val_1_rmse: 0.67334 |  0:00:09s
epoch 14 | loss: 0.36781 | val_0_rmse: 0.66454 | val_1_rmse: 0.68078 |  0:00:09s
epoch 15 | loss: 0.34797 | val_0_rmse: 0.65626 | val_1_rmse: 0.67621 |  0:00:10s
epoch 16 | loss: 0.33278 | val_0_rmse: 0.64985 | val_1_rmse: 0.67218 |  0:00:11s
epoch 17 | loss: 0.32825 | val_0_rmse: 0.6381  | val_1_rmse: 0.65128 |  0:00:11s
epoch 18 | loss: 0.3168  | val_0_rmse: 0.64767 | val_1_rmse: 0.64609 |  0:00:12s
epoch 19 | loss: 0.30387 | val_0_rmse: 0.61616 | val_1_rmse: 0.61752 |  0:00:13s
epoch 20 | loss: 0.28374 | val_0_rmse: 0.62286 | val_1_rmse: 0.62578 |  0:00:13s
epoch 21 | loss: 0.29174 | val_0_rmse: 0.6249  | val_1_rmse: 0.63227 |  0:00:14s
epoch 22 | loss: 0.29404 | val_0_rmse: 0.60621 | val_1_rmse: 0.61457 |  0:00:15s
epoch 23 | loss: 0.28528 | val_0_rmse: 0.58158 | val_1_rmse: 0.59251 |  0:00:15s
epoch 24 | loss: 0.27422 | val_0_rmse: 0.60161 | val_1_rmse: 0.61023 |  0:00:16s
epoch 25 | loss: 0.26243 | val_0_rmse: 0.57135 | val_1_rmse: 0.57927 |  0:00:17s
epoch 26 | loss: 0.25922 | val_0_rmse: 0.56969 | val_1_rmse: 0.58274 |  0:00:17s
epoch 27 | loss: 0.24988 | val_0_rmse: 0.57123 | val_1_rmse: 0.58515 |  0:00:18s
epoch 28 | loss: 0.24269 | val_0_rmse: 0.56749 | val_1_rmse: 0.57897 |  0:00:19s
epoch 29 | loss: 0.23678 | val_0_rmse: 0.55247 | val_1_rmse: 0.56201 |  0:00:19s
epoch 30 | loss: 0.22591 | val_0_rmse: 0.53341 | val_1_rmse: 0.54617 |  0:00:20s
epoch 31 | loss: 0.22158 | val_0_rmse: 0.51204 | val_1_rmse: 0.52017 |  0:00:21s
epoch 32 | loss: 0.21991 | val_0_rmse: 0.53954 | val_1_rmse: 0.55508 |  0:00:21s
epoch 33 | loss: 0.21546 | val_0_rmse: 0.53222 | val_1_rmse: 0.54066 |  0:00:22s
epoch 34 | loss: 0.20522 | val_0_rmse: 0.51166 | val_1_rmse: 0.52075 |  0:00:23s
epoch 35 | loss: 0.20388 | val_0_rmse: 0.53327 | val_1_rmse: 0.5417  |  0:00:23s
epoch 36 | loss: 0.20961 | val_0_rmse: 0.52563 | val_1_rmse: 0.53946 |  0:00:24s
epoch 37 | loss: 0.19577 | val_0_rmse: 0.52997 | val_1_rmse: 0.54037 |  0:00:25s
epoch 38 | loss: 0.19177 | val_0_rmse: 0.52462 | val_1_rmse: 0.52806 |  0:00:25s
epoch 39 | loss: 0.20731 | val_0_rmse: 0.52448 | val_1_rmse: 0.52913 |  0:00:26s
epoch 40 | loss: 0.2073  | val_0_rmse: 0.52976 | val_1_rmse: 0.52876 |  0:00:27s
epoch 41 | loss: 0.20388 | val_0_rmse: 0.52975 | val_1_rmse: 0.53467 |  0:00:27s
epoch 42 | loss: 0.19946 | val_0_rmse: 0.52424 | val_1_rmse: 0.52861 |  0:00:28s
epoch 43 | loss: 0.19607 | val_0_rmse: 0.51723 | val_1_rmse: 0.52907 |  0:00:28s
epoch 44 | loss: 0.18818 | val_0_rmse: 0.49164 | val_1_rmse: 0.50653 |  0:00:29s
epoch 45 | loss: 0.18357 | val_0_rmse: 0.48091 | val_1_rmse: 0.50357 |  0:00:30s
epoch 46 | loss: 0.17833 | val_0_rmse: 0.49453 | val_1_rmse: 0.50865 |  0:00:30s
epoch 47 | loss: 0.18633 | val_0_rmse: 0.49392 | val_1_rmse: 0.51199 |  0:00:31s
epoch 48 | loss: 0.17008 | val_0_rmse: 0.46291 | val_1_rmse: 0.48244 |  0:00:32s
epoch 49 | loss: 0.1648  | val_0_rmse: 0.46291 | val_1_rmse: 0.47919 |  0:00:32s
epoch 50 | loss: 0.1625  | val_0_rmse: 0.45282 | val_1_rmse: 0.46538 |  0:00:33s
epoch 51 | loss: 0.15623 | val_0_rmse: 0.47184 | val_1_rmse: 0.48672 |  0:00:34s
epoch 52 | loss: 0.15673 | val_0_rmse: 0.45211 | val_1_rmse: 0.47835 |  0:00:34s
epoch 53 | loss: 0.16197 | val_0_rmse: 0.43073 | val_1_rmse: 0.45111 |  0:00:35s
epoch 54 | loss: 0.15665 | val_0_rmse: 0.45401 | val_1_rmse: 0.47335 |  0:00:36s
epoch 55 | loss: 0.1517  | val_0_rmse: 0.4343  | val_1_rmse: 0.45502 |  0:00:36s
epoch 56 | loss: 0.15065 | val_0_rmse: 0.43991 | val_1_rmse: 0.46389 |  0:00:37s
epoch 57 | loss: 0.14952 | val_0_rmse: 0.42427 | val_1_rmse: 0.44929 |  0:00:38s
epoch 58 | loss: 0.14682 | val_0_rmse: 0.42768 | val_1_rmse: 0.45784 |  0:00:38s
epoch 59 | loss: 0.13937 | val_0_rmse: 0.41401 | val_1_rmse: 0.43879 |  0:00:39s
epoch 60 | loss: 0.1434  | val_0_rmse: 0.40794 | val_1_rmse: 0.43464 |  0:00:40s
epoch 61 | loss: 0.13904 | val_0_rmse: 0.40632 | val_1_rmse: 0.43225 |  0:00:40s
epoch 62 | loss: 0.13734 | val_0_rmse: 0.42527 | val_1_rmse: 0.4524  |  0:00:41s
epoch 63 | loss: 0.14351 | val_0_rmse: 0.4223  | val_1_rmse: 0.46039 |  0:00:42s
epoch 64 | loss: 0.15396 | val_0_rmse: 0.41672 | val_1_rmse: 0.45876 |  0:00:42s
epoch 65 | loss: 0.14851 | val_0_rmse: 0.41048 | val_1_rmse: 0.44727 |  0:00:43s
epoch 66 | loss: 0.14218 | val_0_rmse: 0.41118 | val_1_rmse: 0.43691 |  0:00:44s
epoch 67 | loss: 0.1433  | val_0_rmse: 0.40418 | val_1_rmse: 0.43516 |  0:00:44s
epoch 68 | loss: 0.13887 | val_0_rmse: 0.39704 | val_1_rmse: 0.43549 |  0:00:45s
epoch 69 | loss: 0.13249 | val_0_rmse: 0.38769 | val_1_rmse: 0.4214  |  0:00:46s
epoch 70 | loss: 0.12949 | val_0_rmse: 0.37666 | val_1_rmse: 0.41952 |  0:00:46s
epoch 71 | loss: 0.12759 | val_0_rmse: 0.37464 | val_1_rmse: 0.42543 |  0:00:47s
epoch 72 | loss: 0.1268  | val_0_rmse: 0.37558 | val_1_rmse: 0.42007 |  0:00:48s
epoch 73 | loss: 0.12582 | val_0_rmse: 0.37223 | val_1_rmse: 0.41939 |  0:00:48s
epoch 74 | loss: 0.12235 | val_0_rmse: 0.36096 | val_1_rmse: 0.41852 |  0:00:49s
epoch 75 | loss: 0.1329  | val_0_rmse: 0.36653 | val_1_rmse: 0.41999 |  0:00:50s
epoch 76 | loss: 0.12787 | val_0_rmse: 0.35883 | val_1_rmse: 0.43292 |  0:00:50s
epoch 77 | loss: 0.1281  | val_0_rmse: 0.36214 | val_1_rmse: 0.42981 |  0:00:51s
epoch 78 | loss: 0.13214 | val_0_rmse: 0.35207 | val_1_rmse: 0.42258 |  0:00:51s
epoch 79 | loss: 0.12633 | val_0_rmse: 0.35831 | val_1_rmse: 0.42471 |  0:00:52s
epoch 80 | loss: 0.13865 | val_0_rmse: 0.36267 | val_1_rmse: 0.43147 |  0:00:53s
epoch 81 | loss: 0.13081 | val_0_rmse: 0.37067 | val_1_rmse: 0.44559 |  0:00:53s
epoch 82 | loss: 0.13648 | val_0_rmse: 0.34867 | val_1_rmse: 0.41285 |  0:00:54s
epoch 83 | loss: 0.1295  | val_0_rmse: 0.35236 | val_1_rmse: 0.41102 |  0:00:55s
epoch 84 | loss: 0.1353  | val_0_rmse: 0.33878 | val_1_rmse: 0.40692 |  0:00:55s
epoch 85 | loss: 0.1225  | val_0_rmse: 0.3345  | val_1_rmse: 0.39852 |  0:00:56s
epoch 86 | loss: 0.11524 | val_0_rmse: 0.32966 | val_1_rmse: 0.3939  |  0:00:57s
epoch 87 | loss: 0.11653 | val_0_rmse: 0.33062 | val_1_rmse: 0.39694 |  0:00:57s
epoch 88 | loss: 0.11393 | val_0_rmse: 0.31684 | val_1_rmse: 0.39367 |  0:00:58s
epoch 89 | loss: 0.11456 | val_0_rmse: 0.32026 | val_1_rmse: 0.39341 |  0:00:59s
epoch 90 | loss: 0.10893 | val_0_rmse: 0.31246 | val_1_rmse: 0.39795 |  0:00:59s
epoch 91 | loss: 0.10742 | val_0_rmse: 0.32313 | val_1_rmse: 0.4056  |  0:01:00s
epoch 92 | loss: 0.11089 | val_0_rmse: 0.31592 | val_1_rmse: 0.3948  |  0:01:01s
epoch 93 | loss: 0.1148  | val_0_rmse: 0.32941 | val_1_rmse: 0.40405 |  0:01:01s
epoch 94 | loss: 0.12344 | val_0_rmse: 0.34348 | val_1_rmse: 0.41407 |  0:01:02s
epoch 95 | loss: 0.1301  | val_0_rmse: 0.32909 | val_1_rmse: 0.41159 |  0:01:03s
epoch 96 | loss: 0.12249 | val_0_rmse: 0.33333 | val_1_rmse: 0.40921 |  0:01:03s
epoch 97 | loss: 0.12381 | val_0_rmse: 0.32868 | val_1_rmse: 0.40684 |  0:01:04s
epoch 98 | loss: 0.11568 | val_0_rmse: 0.31387 | val_1_rmse: 0.39843 |  0:01:05s
epoch 99 | loss: 0.11598 | val_0_rmse: 0.30176 | val_1_rmse: 0.38849 |  0:01:05s
epoch 100| loss: 0.11019 | val_0_rmse: 0.29941 | val_1_rmse: 0.39238 |  0:01:06s
epoch 101| loss: 0.11181 | val_0_rmse: 0.30374 | val_1_rmse: 0.39433 |  0:01:07s
epoch 102| loss: 0.11066 | val_0_rmse: 0.29797 | val_1_rmse: 0.39417 |  0:01:07s
epoch 103| loss: 0.10811 | val_0_rmse: 0.3327  | val_1_rmse: 0.40478 |  0:01:08s
epoch 104| loss: 0.10797 | val_0_rmse: 0.30856 | val_1_rmse: 0.39784 |  0:01:09s
epoch 105| loss: 0.10872 | val_0_rmse: 0.29333 | val_1_rmse: 0.40225 |  0:01:09s
epoch 106| loss: 0.10621 | val_0_rmse: 0.29977 | val_1_rmse: 0.39386 |  0:01:10s
epoch 107| loss: 0.10486 | val_0_rmse: 0.30881 | val_1_rmse: 0.40505 |  0:01:10s
epoch 108| loss: 0.12793 | val_0_rmse: 0.35731 | val_1_rmse: 0.43069 |  0:01:11s
epoch 109| loss: 0.1277  | val_0_rmse: 0.34201 | val_1_rmse: 0.43615 |  0:01:12s
epoch 110| loss: 0.12574 | val_0_rmse: 0.329   | val_1_rmse: 0.42752 |  0:01:12s
epoch 111| loss: 0.12109 | val_0_rmse: 0.31469 | val_1_rmse: 0.41524 |  0:01:13s
epoch 112| loss: 0.11162 | val_0_rmse: 0.30667 | val_1_rmse: 0.41245 |  0:01:14s
epoch 113| loss: 0.11133 | val_0_rmse: 0.30553 | val_1_rmse: 0.42077 |  0:01:14s
epoch 114| loss: 0.11551 | val_0_rmse: 0.3149  | val_1_rmse: 0.41807 |  0:01:15s
epoch 115| loss: 0.1187  | val_0_rmse: 0.31146 | val_1_rmse: 0.42215 |  0:01:16s
epoch 116| loss: 0.11039 | val_0_rmse: 0.30272 | val_1_rmse: 0.41622 |  0:01:16s
epoch 117| loss: 0.1095  | val_0_rmse: 0.29776 | val_1_rmse: 0.40832 |  0:01:17s
epoch 118| loss: 0.10832 | val_0_rmse: 0.30819 | val_1_rmse: 0.40957 |  0:01:18s
epoch 119| loss: 0.10961 | val_0_rmse: 0.30142 | val_1_rmse: 0.40569 |  0:01:18s
epoch 120| loss: 0.10452 | val_0_rmse: 0.29907 | val_1_rmse: 0.40703 |  0:01:19s
epoch 121| loss: 0.10573 | val_0_rmse: 0.29412 | val_1_rmse: 0.4073  |  0:01:20s
epoch 122| loss: 0.1056  | val_0_rmse: 0.29161 | val_1_rmse: 0.40987 |  0:01:20s
epoch 123| loss: 0.10047 | val_0_rmse: 0.28881 | val_1_rmse: 0.40424 |  0:01:21s
epoch 124| loss: 0.09671 | val_0_rmse: 0.28333 | val_1_rmse: 0.39911 |  0:01:22s
epoch 125| loss: 0.10137 | val_0_rmse: 0.28865 | val_1_rmse: 0.40102 |  0:01:22s
epoch 126| loss: 0.10362 | val_0_rmse: 0.28649 | val_1_rmse: 0.40693 |  0:01:23s
epoch 127| loss: 0.09933 | val_0_rmse: 0.29663 | val_1_rmse: 0.40844 |  0:01:24s
epoch 128| loss: 0.10676 | val_0_rmse: 0.30551 | val_1_rmse: 0.41775 |  0:01:24s
epoch 129| loss: 0.10253 | val_0_rmse: 0.28763 | val_1_rmse: 0.40493 |  0:01:25s

Early stopping occured at epoch 129 with best_epoch = 99 and best_val_1_rmse = 0.38849
Best weights from best epoch are automatically used!
ended training at: 04:29:08
Feature importance:
Mean squared error is of 3489004067.6907015
Mean absolute error:37758.056618726645
MAPE:0.23427931462178353
R2 score:0.8239132612284698
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Era\Log&SS_Geopy\Logs\\\tabnet_model_iter_3.zip
------------------------------------------------------------------
Using the dataset: DataBase_Era_augmented.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:29:09
epoch 0  | loss: 2.7564  | val_0_rmse: 0.99441 | val_1_rmse: 0.99851 |  0:00:00s
epoch 1  | loss: 1.3082  | val_0_rmse: 0.99097 | val_1_rmse: 0.99443 |  0:00:01s
epoch 2  | loss: 0.96857 | val_0_rmse: 0.85276 | val_1_rmse: 0.82165 |  0:00:01s
epoch 3  | loss: 0.75766 | val_0_rmse: 0.889   | val_1_rmse: 0.87708 |  0:00:02s
epoch 4  | loss: 0.67794 | val_0_rmse: 0.90119 | val_1_rmse: 0.89669 |  0:00:03s
epoch 5  | loss: 0.62014 | val_0_rmse: 0.82565 | val_1_rmse: 0.80207 |  0:00:03s
epoch 6  | loss: 0.59033 | val_0_rmse: 0.78983 | val_1_rmse: 0.7679  |  0:00:04s
epoch 7  | loss: 0.54982 | val_0_rmse: 0.75146 | val_1_rmse: 0.73752 |  0:00:05s
epoch 8  | loss: 0.5081  | val_0_rmse: 0.73025 | val_1_rmse: 0.71298 |  0:00:05s
epoch 9  | loss: 0.48129 | val_0_rmse: 0.7133  | val_1_rmse: 0.70409 |  0:00:06s
epoch 10 | loss: 0.46239 | val_0_rmse: 0.68908 | val_1_rmse: 0.68241 |  0:00:07s
epoch 11 | loss: 0.44048 | val_0_rmse: 0.68318 | val_1_rmse: 0.67133 |  0:00:07s
epoch 12 | loss: 0.41468 | val_0_rmse: 0.67108 | val_1_rmse: 0.66645 |  0:00:08s
epoch 13 | loss: 0.39307 | val_0_rmse: 0.68514 | val_1_rmse: 0.67664 |  0:00:09s
epoch 14 | loss: 0.37473 | val_0_rmse: 0.64578 | val_1_rmse: 0.63447 |  0:00:09s
epoch 15 | loss: 0.35913 | val_0_rmse: 0.64441 | val_1_rmse: 0.63899 |  0:00:10s
epoch 16 | loss: 0.36638 | val_0_rmse: 0.65865 | val_1_rmse: 0.64332 |  0:00:11s
epoch 17 | loss: 0.33197 | val_0_rmse: 0.60497 | val_1_rmse: 0.59698 |  0:00:11s
epoch 18 | loss: 0.31293 | val_0_rmse: 0.59566 | val_1_rmse: 0.58108 |  0:00:12s
epoch 19 | loss: 0.29298 | val_0_rmse: 0.59478 | val_1_rmse: 0.58268 |  0:00:13s
epoch 20 | loss: 0.28601 | val_0_rmse: 0.57119 | val_1_rmse: 0.56868 |  0:00:13s
epoch 21 | loss: 0.26635 | val_0_rmse: 0.56515 | val_1_rmse: 0.56541 |  0:00:14s
epoch 22 | loss: 0.26791 | val_0_rmse: 0.57041 | val_1_rmse: 0.5783  |  0:00:15s
epoch 23 | loss: 0.25793 | val_0_rmse: 0.53954 | val_1_rmse: 0.54877 |  0:00:15s
epoch 24 | loss: 0.26216 | val_0_rmse: 0.57033 | val_1_rmse: 0.56769 |  0:00:16s
epoch 25 | loss: 0.26415 | val_0_rmse: 0.55214 | val_1_rmse: 0.55896 |  0:00:17s
epoch 26 | loss: 0.25134 | val_0_rmse: 0.53953 | val_1_rmse: 0.55205 |  0:00:17s
epoch 27 | loss: 0.24017 | val_0_rmse: 0.5456  | val_1_rmse: 0.55693 |  0:00:18s
epoch 28 | loss: 0.23983 | val_0_rmse: 0.53878 | val_1_rmse: 0.55174 |  0:00:19s
epoch 29 | loss: 0.22927 | val_0_rmse: 0.55924 | val_1_rmse: 0.56935 |  0:00:19s
epoch 30 | loss: 0.22098 | val_0_rmse: 0.54471 | val_1_rmse: 0.55523 |  0:00:20s
epoch 31 | loss: 0.21059 | val_0_rmse: 0.523   | val_1_rmse: 0.53823 |  0:00:20s
epoch 32 | loss: 0.20389 | val_0_rmse: 0.51764 | val_1_rmse: 0.53247 |  0:00:21s
epoch 33 | loss: 0.20385 | val_0_rmse: 0.51629 | val_1_rmse: 0.53144 |  0:00:22s
epoch 34 | loss: 0.20199 | val_0_rmse: 0.52944 | val_1_rmse: 0.54077 |  0:00:23s
epoch 35 | loss: 0.1974  | val_0_rmse: 0.52348 | val_1_rmse: 0.53969 |  0:00:23s
epoch 36 | loss: 0.19388 | val_0_rmse: 0.50041 | val_1_rmse: 0.51873 |  0:00:24s
epoch 37 | loss: 0.1844  | val_0_rmse: 0.49552 | val_1_rmse: 0.51836 |  0:00:24s
epoch 38 | loss: 0.18646 | val_0_rmse: 0.50239 | val_1_rmse: 0.52379 |  0:00:25s
epoch 39 | loss: 0.18437 | val_0_rmse: 0.50247 | val_1_rmse: 0.52542 |  0:00:26s
epoch 40 | loss: 0.18143 | val_0_rmse: 0.49447 | val_1_rmse: 0.51993 |  0:00:26s
epoch 41 | loss: 0.18961 | val_0_rmse: 0.47839 | val_1_rmse: 0.50148 |  0:00:27s
epoch 42 | loss: 0.17847 | val_0_rmse: 0.49379 | val_1_rmse: 0.51756 |  0:00:28s
epoch 43 | loss: 0.17449 | val_0_rmse: 0.49026 | val_1_rmse: 0.51273 |  0:00:28s
epoch 44 | loss: 0.16929 | val_0_rmse: 0.47896 | val_1_rmse: 0.50082 |  0:00:29s
epoch 45 | loss: 0.17804 | val_0_rmse: 0.48744 | val_1_rmse: 0.50036 |  0:00:30s
epoch 46 | loss: 0.19232 | val_0_rmse: 0.4985  | val_1_rmse: 0.50742 |  0:00:31s
epoch 47 | loss: 0.1869  | val_0_rmse: 0.48304 | val_1_rmse: 0.50567 |  0:00:31s
epoch 48 | loss: 0.18109 | val_0_rmse: 0.4732  | val_1_rmse: 0.50247 |  0:00:32s
epoch 49 | loss: 0.17592 | val_0_rmse: 0.45981 | val_1_rmse: 0.48975 |  0:00:33s
epoch 50 | loss: 0.17762 | val_0_rmse: 0.47868 | val_1_rmse: 0.50185 |  0:00:33s
epoch 51 | loss: 0.17767 | val_0_rmse: 0.44822 | val_1_rmse: 0.47899 |  0:00:34s
epoch 52 | loss: 0.167   | val_0_rmse: 0.43478 | val_1_rmse: 0.46386 |  0:00:35s
epoch 53 | loss: 0.16548 | val_0_rmse: 0.43198 | val_1_rmse: 0.45529 |  0:00:35s
epoch 54 | loss: 0.16671 | val_0_rmse: 0.43241 | val_1_rmse: 0.45944 |  0:00:36s
epoch 55 | loss: 0.15654 | val_0_rmse: 0.41926 | val_1_rmse: 0.44624 |  0:00:37s
epoch 56 | loss: 0.15059 | val_0_rmse: 0.42958 | val_1_rmse: 0.45349 |  0:00:38s
epoch 57 | loss: 0.1522  | val_0_rmse: 0.43052 | val_1_rmse: 0.45592 |  0:00:38s
epoch 58 | loss: 0.14974 | val_0_rmse: 0.43158 | val_1_rmse: 0.468   |  0:00:39s
epoch 59 | loss: 0.14701 | val_0_rmse: 0.42257 | val_1_rmse: 0.45557 |  0:00:40s
epoch 60 | loss: 0.14572 | val_0_rmse: 0.40619 | val_1_rmse: 0.44612 |  0:00:41s
epoch 61 | loss: 0.13758 | val_0_rmse: 0.39912 | val_1_rmse: 0.44309 |  0:00:41s
epoch 62 | loss: 0.14337 | val_0_rmse: 0.40641 | val_1_rmse: 0.44538 |  0:00:42s
epoch 63 | loss: 0.14802 | val_0_rmse: 0.41323 | val_1_rmse: 0.45041 |  0:00:43s
epoch 64 | loss: 0.15857 | val_0_rmse: 0.39733 | val_1_rmse: 0.44205 |  0:00:43s
epoch 65 | loss: 0.14389 | val_0_rmse: 0.40132 | val_1_rmse: 0.44876 |  0:00:44s
epoch 66 | loss: 0.1411  | val_0_rmse: 0.38022 | val_1_rmse: 0.42655 |  0:00:45s
epoch 67 | loss: 0.14255 | val_0_rmse: 0.37761 | val_1_rmse: 0.42565 |  0:00:45s
epoch 68 | loss: 0.13741 | val_0_rmse: 0.36947 | val_1_rmse: 0.41384 |  0:00:46s
epoch 69 | loss: 0.14071 | val_0_rmse: 0.3743  | val_1_rmse: 0.41662 |  0:00:47s
epoch 70 | loss: 0.13353 | val_0_rmse: 0.36523 | val_1_rmse: 0.40998 |  0:00:47s
epoch 71 | loss: 0.133   | val_0_rmse: 0.36802 | val_1_rmse: 0.41477 |  0:00:48s
epoch 72 | loss: 0.13106 | val_0_rmse: 0.36988 | val_1_rmse: 0.41886 |  0:00:49s
epoch 73 | loss: 0.13363 | val_0_rmse: 0.37154 | val_1_rmse: 0.42372 |  0:00:49s
epoch 74 | loss: 0.12827 | val_0_rmse: 0.35918 | val_1_rmse: 0.41029 |  0:00:50s
epoch 75 | loss: 0.12704 | val_0_rmse: 0.35185 | val_1_rmse: 0.39974 |  0:00:51s
epoch 76 | loss: 0.12394 | val_0_rmse: 0.3473  | val_1_rmse: 0.40491 |  0:00:52s
epoch 77 | loss: 0.12319 | val_0_rmse: 0.33855 | val_1_rmse: 0.39702 |  0:00:52s
epoch 78 | loss: 0.11972 | val_0_rmse: 0.33547 | val_1_rmse: 0.39375 |  0:00:53s
epoch 79 | loss: 0.12578 | val_0_rmse: 0.34416 | val_1_rmse: 0.39473 |  0:00:54s
epoch 80 | loss: 0.12229 | val_0_rmse: 0.33433 | val_1_rmse: 0.39361 |  0:00:54s
epoch 81 | loss: 0.12178 | val_0_rmse: 0.33643 | val_1_rmse: 0.3937  |  0:00:55s
epoch 82 | loss: 0.12101 | val_0_rmse: 0.327   | val_1_rmse: 0.39095 |  0:00:56s
epoch 83 | loss: 0.11706 | val_0_rmse: 0.3409  | val_1_rmse: 0.40697 |  0:00:56s
epoch 84 | loss: 0.11454 | val_0_rmse: 0.33128 | val_1_rmse: 0.39217 |  0:00:57s
epoch 85 | loss: 0.11759 | val_0_rmse: 0.3247  | val_1_rmse: 0.38629 |  0:00:58s
epoch 86 | loss: 0.11586 | val_0_rmse: 0.33235 | val_1_rmse: 0.39277 |  0:00:58s
epoch 87 | loss: 0.1171  | val_0_rmse: 0.31305 | val_1_rmse: 0.38123 |  0:00:59s
epoch 88 | loss: 0.11176 | val_0_rmse: 0.31972 | val_1_rmse: 0.38778 |  0:01:00s
epoch 89 | loss: 0.11308 | val_0_rmse: 0.31756 | val_1_rmse: 0.38404 |  0:01:01s
epoch 90 | loss: 0.11191 | val_0_rmse: 0.31507 | val_1_rmse: 0.38733 |  0:01:01s
epoch 91 | loss: 0.10679 | val_0_rmse: 0.31019 | val_1_rmse: 0.3851  |  0:01:02s
epoch 92 | loss: 0.10439 | val_0_rmse: 0.30322 | val_1_rmse: 0.37845 |  0:01:03s
epoch 93 | loss: 0.10455 | val_0_rmse: 0.29966 | val_1_rmse: 0.3797  |  0:01:03s
epoch 94 | loss: 0.10586 | val_0_rmse: 0.31656 | val_1_rmse: 0.38708 |  0:01:04s
epoch 95 | loss: 0.10376 | val_0_rmse: 0.30155 | val_1_rmse: 0.37695 |  0:01:05s
epoch 96 | loss: 0.10624 | val_0_rmse: 0.30977 | val_1_rmse: 0.38626 |  0:01:05s
epoch 97 | loss: 0.10276 | val_0_rmse: 0.29867 | val_1_rmse: 0.37788 |  0:01:06s
epoch 98 | loss: 0.097   | val_0_rmse: 0.3209  | val_1_rmse: 0.40038 |  0:01:06s
epoch 99 | loss: 0.10192 | val_0_rmse: 0.29435 | val_1_rmse: 0.38084 |  0:01:07s
epoch 100| loss: 0.09782 | val_0_rmse: 0.29308 | val_1_rmse: 0.38205 |  0:01:08s
epoch 101| loss: 0.10038 | val_0_rmse: 0.28703 | val_1_rmse: 0.37811 |  0:01:08s
epoch 102| loss: 0.10117 | val_0_rmse: 0.29738 | val_1_rmse: 0.3846  |  0:01:09s
epoch 103| loss: 0.10183 | val_0_rmse: 0.29153 | val_1_rmse: 0.38133 |  0:01:10s
epoch 104| loss: 0.09873 | val_0_rmse: 0.2863  | val_1_rmse: 0.37959 |  0:01:10s
epoch 105| loss: 0.0946  | val_0_rmse: 0.28127 | val_1_rmse: 0.37207 |  0:01:11s
epoch 106| loss: 0.0939  | val_0_rmse: 0.27721 | val_1_rmse: 0.3641  |  0:01:12s
epoch 107| loss: 0.09314 | val_0_rmse: 0.27773 | val_1_rmse: 0.36637 |  0:01:12s
epoch 108| loss: 0.09194 | val_0_rmse: 0.27972 | val_1_rmse: 0.37321 |  0:01:13s
epoch 109| loss: 0.09364 | val_0_rmse: 0.27868 | val_1_rmse: 0.37136 |  0:01:14s
epoch 110| loss: 0.09358 | val_0_rmse: 0.28576 | val_1_rmse: 0.3778  |  0:01:14s
epoch 111| loss: 0.0938  | val_0_rmse: 0.28833 | val_1_rmse: 0.37819 |  0:01:15s
epoch 112| loss: 0.09748 | val_0_rmse: 0.29766 | val_1_rmse: 0.3961  |  0:01:15s
epoch 113| loss: 0.0985  | val_0_rmse: 0.3053  | val_1_rmse: 0.40383 |  0:01:16s
epoch 114| loss: 0.09792 | val_0_rmse: 0.35899 | val_1_rmse: 0.44874 |  0:01:17s
epoch 115| loss: 0.0957  | val_0_rmse: 0.3391  | val_1_rmse: 0.43549 |  0:01:17s
epoch 116| loss: 0.09509 | val_0_rmse: 0.28791 | val_1_rmse: 0.38402 |  0:01:18s
epoch 117| loss: 0.09302 | val_0_rmse: 0.28661 | val_1_rmse: 0.38771 |  0:01:19s
epoch 118| loss: 0.09356 | val_0_rmse: 0.27386 | val_1_rmse: 0.37674 |  0:01:19s
epoch 119| loss: 0.08824 | val_0_rmse: 0.27582 | val_1_rmse: 0.37939 |  0:01:20s
epoch 120| loss: 0.08726 | val_0_rmse: 0.27251 | val_1_rmse: 0.38198 |  0:01:21s
epoch 121| loss: 0.08768 | val_0_rmse: 0.281   | val_1_rmse: 0.38247 |  0:01:21s
epoch 122| loss: 0.08793 | val_0_rmse: 0.27302 | val_1_rmse: 0.37782 |  0:01:22s
epoch 123| loss: 0.09035 | val_0_rmse: 0.27469 | val_1_rmse: 0.37925 |  0:01:23s
epoch 124| loss: 0.09277 | val_0_rmse: 0.2994  | val_1_rmse: 0.38864 |  0:01:23s
epoch 125| loss: 0.09956 | val_0_rmse: 0.28816 | val_1_rmse: 0.37536 |  0:01:24s
epoch 126| loss: 0.10069 | val_0_rmse: 0.31472 | val_1_rmse: 0.40232 |  0:01:25s
epoch 127| loss: 0.09424 | val_0_rmse: 0.30915 | val_1_rmse: 0.39918 |  0:01:25s
epoch 128| loss: 0.09309 | val_0_rmse: 0.28211 | val_1_rmse: 0.37593 |  0:01:26s
epoch 129| loss: 0.09056 | val_0_rmse: 0.27897 | val_1_rmse: 0.38382 |  0:01:26s
epoch 130| loss: 0.09499 | val_0_rmse: 0.27917 | val_1_rmse: 0.38982 |  0:01:27s
epoch 131| loss: 0.09254 | val_0_rmse: 0.26995 | val_1_rmse: 0.37951 |  0:01:28s
epoch 132| loss: 0.08549 | val_0_rmse: 0.27882 | val_1_rmse: 0.38444 |  0:01:28s
epoch 133| loss: 0.08388 | val_0_rmse: 0.26831 | val_1_rmse: 0.38394 |  0:01:29s
epoch 134| loss: 0.08546 | val_0_rmse: 0.26881 | val_1_rmse: 0.37095 |  0:01:30s
epoch 135| loss: 0.08074 | val_0_rmse: 0.27068 | val_1_rmse: 0.36901 |  0:01:30s
epoch 136| loss: 0.08151 | val_0_rmse: 0.26087 | val_1_rmse: 0.36897 |  0:01:31s

Early stopping occured at epoch 136 with best_epoch = 106 and best_val_1_rmse = 0.3641
Best weights from best epoch are automatically used!
ended training at: 04:30:41
Feature importance:
Mean squared error is of 3071469553.601206
Mean absolute error:34629.6353553165
MAPE:0.22093962446137463
R2 score:0.8429952725905847
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Era\Log&SS_Geopy\Logs\\\tabnet_model_iter_4.zip
