TabNet Logs:

Saving copy of script...
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:14:46
epoch 0  | loss: 0.14464 | val_0_rmse: 0.18279 | val_1_rmse: 0.18104 |  0:00:05s
epoch 1  | loss: 0.02221 | val_0_rmse: 0.18307 | val_1_rmse: 0.18287 |  0:00:07s
epoch 2  | loss: 0.01987 | val_0_rmse: 0.17968 | val_1_rmse: 0.18    |  0:00:09s
epoch 3  | loss: 0.0188  | val_0_rmse: 0.17914 | val_1_rmse: 0.17972 |  0:00:10s
epoch 4  | loss: 0.01756 | val_0_rmse: 0.18718 | val_1_rmse: 0.18818 |  0:00:12s
epoch 5  | loss: 0.0173  | val_0_rmse: 0.18618 | val_1_rmse: 0.18661 |  0:00:13s
epoch 6  | loss: 0.01651 | val_0_rmse: 0.18437 | val_1_rmse: 0.1846  |  0:00:15s
epoch 7  | loss: 0.01584 | val_0_rmse: 0.18636 | val_1_rmse: 0.18672 |  0:00:16s
epoch 8  | loss: 0.01591 | val_0_rmse: 0.16169 | val_1_rmse: 0.16111 |  0:00:18s
epoch 9  | loss: 0.01613 | val_0_rmse: 0.17326 | val_1_rmse: 0.17259 |  0:00:19s
epoch 10 | loss: 0.01574 | val_0_rmse: 0.16078 | val_1_rmse: 0.16006 |  0:00:21s
epoch 11 | loss: 0.0154  | val_0_rmse: 0.15395 | val_1_rmse: 0.15299 |  0:00:22s
epoch 12 | loss: 0.01533 | val_0_rmse: 0.15923 | val_1_rmse: 0.15873 |  0:00:24s
epoch 13 | loss: 0.01567 | val_0_rmse: 0.14649 | val_1_rmse: 0.14616 |  0:00:25s
epoch 14 | loss: 0.01505 | val_0_rmse: 0.14162 | val_1_rmse: 0.1411  |  0:00:27s
epoch 15 | loss: 0.01457 | val_0_rmse: 0.13911 | val_1_rmse: 0.13801 |  0:00:28s
epoch 16 | loss: 0.01498 | val_0_rmse: 0.13667 | val_1_rmse: 0.1368  |  0:00:29s
epoch 17 | loss: 0.01511 | val_0_rmse: 0.1311  | val_1_rmse: 0.1302  |  0:00:31s
epoch 18 | loss: 0.01518 | val_0_rmse: 0.12796 | val_1_rmse: 0.12818 |  0:00:32s
epoch 19 | loss: 0.01572 | val_0_rmse: 0.12491 | val_1_rmse: 0.12488 |  0:00:34s
epoch 20 | loss: 0.01544 | val_0_rmse: 0.12441 | val_1_rmse: 0.12432 |  0:00:35s
epoch 21 | loss: 0.01496 | val_0_rmse: 0.12417 | val_1_rmse: 0.12431 |  0:00:37s
epoch 22 | loss: 0.01453 | val_0_rmse: 0.11705 | val_1_rmse: 0.11708 |  0:00:38s
epoch 23 | loss: 0.01454 | val_0_rmse: 0.11992 | val_1_rmse: 0.11997 |  0:00:40s
epoch 24 | loss: 0.01442 | val_0_rmse: 0.11662 | val_1_rmse: 0.11656 |  0:00:41s
epoch 25 | loss: 0.01379 | val_0_rmse: 0.11534 | val_1_rmse: 0.1155  |  0:00:43s
epoch 26 | loss: 0.01389 | val_0_rmse: 0.11407 | val_1_rmse: 0.11509 |  0:00:44s
epoch 27 | loss: 0.01462 | val_0_rmse: 0.11557 | val_1_rmse: 0.11611 |  0:00:46s
epoch 28 | loss: 0.01481 | val_0_rmse: 0.11371 | val_1_rmse: 0.11444 |  0:00:47s
epoch 29 | loss: 0.01413 | val_0_rmse: 0.1138  | val_1_rmse: 0.11474 |  0:00:48s
epoch 30 | loss: 0.01419 | val_0_rmse: 0.11269 | val_1_rmse: 0.11347 |  0:00:50s
epoch 31 | loss: 0.01355 | val_0_rmse: 0.11424 | val_1_rmse: 0.1158  |  0:00:51s
epoch 32 | loss: 0.01367 | val_0_rmse: 0.11259 | val_1_rmse: 0.11345 |  0:00:53s
epoch 33 | loss: 0.01379 | val_0_rmse: 0.11819 | val_1_rmse: 0.11976 |  0:00:54s
epoch 34 | loss: 0.01419 | val_0_rmse: 0.11081 | val_1_rmse: 0.11117 |  0:00:56s
epoch 35 | loss: 0.01345 | val_0_rmse: 0.11066 | val_1_rmse: 0.11142 |  0:00:57s
epoch 36 | loss: 0.01381 | val_0_rmse: 0.1181  | val_1_rmse: 0.11955 |  0:00:59s
epoch 37 | loss: 0.0137  | val_0_rmse: 0.11188 | val_1_rmse: 0.1121  |  0:01:00s
epoch 38 | loss: 0.01327 | val_0_rmse: 0.1149  | val_1_rmse: 0.11589 |  0:01:02s
epoch 39 | loss: 0.01376 | val_0_rmse: 0.11324 | val_1_rmse: 0.11387 |  0:01:03s
epoch 40 | loss: 0.01464 | val_0_rmse: 0.1127  | val_1_rmse: 0.1141  |  0:01:04s
epoch 41 | loss: 0.01369 | val_0_rmse: 0.11259 | val_1_rmse: 0.11268 |  0:01:06s
epoch 42 | loss: 0.01318 | val_0_rmse: 0.11153 | val_1_rmse: 0.11248 |  0:01:07s
epoch 43 | loss: 0.01304 | val_0_rmse: 0.10953 | val_1_rmse: 0.11027 |  0:01:09s
epoch 44 | loss: 0.01299 | val_0_rmse: 0.10941 | val_1_rmse: 0.11016 |  0:01:10s
epoch 45 | loss: 0.01304 | val_0_rmse: 0.11185 | val_1_rmse: 0.11266 |  0:01:12s
epoch 46 | loss: 0.013   | val_0_rmse: 0.10928 | val_1_rmse: 0.11067 |  0:01:13s
epoch 47 | loss: 0.01295 | val_0_rmse: 0.1102  | val_1_rmse: 0.1111  |  0:01:15s
epoch 48 | loss: 0.01327 | val_0_rmse: 0.1086  | val_1_rmse: 0.10907 |  0:01:16s
epoch 49 | loss: 0.01304 | val_0_rmse: 0.11374 | val_1_rmse: 0.11497 |  0:01:18s
epoch 50 | loss: 0.01324 | val_0_rmse: 0.11039 | val_1_rmse: 0.1111  |  0:01:19s
epoch 51 | loss: 0.01307 | val_0_rmse: 0.11213 | val_1_rmse: 0.11353 |  0:01:20s
epoch 52 | loss: 0.01336 | val_0_rmse: 0.10889 | val_1_rmse: 0.11006 |  0:01:22s
epoch 53 | loss: 0.01318 | val_0_rmse: 0.10872 | val_1_rmse: 0.10905 |  0:01:23s
epoch 54 | loss: 0.01264 | val_0_rmse: 0.11051 | val_1_rmse: 0.11167 |  0:01:25s
epoch 55 | loss: 0.01284 | val_0_rmse: 0.11178 | val_1_rmse: 0.11218 |  0:01:26s
epoch 56 | loss: 0.01297 | val_0_rmse: 0.1084  | val_1_rmse: 0.10952 |  0:01:28s
epoch 57 | loss: 0.01267 | val_0_rmse: 0.10689 | val_1_rmse: 0.10769 |  0:01:29s
epoch 58 | loss: 0.01261 | val_0_rmse: 0.10821 | val_1_rmse: 0.10842 |  0:01:31s
epoch 59 | loss: 0.01308 | val_0_rmse: 0.1079  | val_1_rmse: 0.10849 |  0:01:32s
epoch 60 | loss: 0.01262 | val_0_rmse: 0.10794 | val_1_rmse: 0.10908 |  0:01:34s
epoch 61 | loss: 0.01297 | val_0_rmse: 0.10698 | val_1_rmse: 0.10783 |  0:01:35s
epoch 62 | loss: 0.01279 | val_0_rmse: 0.10734 | val_1_rmse: 0.10798 |  0:01:36s
epoch 63 | loss: 0.01264 | val_0_rmse: 0.10872 | val_1_rmse: 0.11027 |  0:01:38s
epoch 64 | loss: 0.01262 | val_0_rmse: 0.1111  | val_1_rmse: 0.11212 |  0:01:39s
epoch 65 | loss: 0.01248 | val_0_rmse: 0.10784 | val_1_rmse: 0.10963 |  0:01:41s
epoch 66 | loss: 0.01253 | val_0_rmse: 0.10944 | val_1_rmse: 0.11087 |  0:01:42s
epoch 67 | loss: 0.0131  | val_0_rmse: 0.11055 | val_1_rmse: 0.11183 |  0:01:44s
epoch 68 | loss: 0.0128  | val_0_rmse: 0.10675 | val_1_rmse: 0.10767 |  0:01:45s
epoch 69 | loss: 0.0126  | val_0_rmse: 0.1091  | val_1_rmse: 0.10888 |  0:01:47s
epoch 70 | loss: 0.01235 | val_0_rmse: 0.1076  | val_1_rmse: 0.10898 |  0:01:48s
epoch 71 | loss: 0.01294 | val_0_rmse: 0.10722 | val_1_rmse: 0.10839 |  0:01:50s
epoch 72 | loss: 0.01233 | val_0_rmse: 0.10686 | val_1_rmse: 0.1084  |  0:01:51s
epoch 73 | loss: 0.01219 | val_0_rmse: 0.11253 | val_1_rmse: 0.11425 |  0:01:52s
epoch 74 | loss: 0.0125  | val_0_rmse: 0.10945 | val_1_rmse: 0.11063 |  0:01:54s
epoch 75 | loss: 0.01318 | val_0_rmse: 0.1109  | val_1_rmse: 0.11256 |  0:01:55s
epoch 76 | loss: 0.01272 | val_0_rmse: 0.10746 | val_1_rmse: 0.10703 |  0:01:57s
epoch 77 | loss: 0.01256 | val_0_rmse: 0.10532 | val_1_rmse: 0.10705 |  0:01:58s
epoch 78 | loss: 0.01247 | val_0_rmse: 0.10591 | val_1_rmse: 0.10688 |  0:02:00s
epoch 79 | loss: 0.01267 | val_0_rmse: 0.1075  | val_1_rmse: 0.10975 |  0:02:01s
epoch 80 | loss: 0.01222 | val_0_rmse: 0.10708 | val_1_rmse: 0.10914 |  0:02:03s
epoch 81 | loss: 0.01214 | val_0_rmse: 0.10958 | val_1_rmse: 0.11046 |  0:02:04s
epoch 82 | loss: 0.01241 | val_0_rmse: 0.10513 | val_1_rmse: 0.10692 |  0:02:06s
epoch 83 | loss: 0.01204 | val_0_rmse: 0.10462 | val_1_rmse: 0.10715 |  0:02:07s
epoch 84 | loss: 0.01181 | val_0_rmse: 0.10383 | val_1_rmse: 0.10613 |  0:02:09s
epoch 85 | loss: 0.01215 | val_0_rmse: 0.10886 | val_1_rmse: 0.11063 |  0:02:10s
epoch 86 | loss: 0.01238 | val_0_rmse: 0.10506 | val_1_rmse: 0.10626 |  0:02:11s
epoch 87 | loss: 0.01196 | val_0_rmse: 0.10326 | val_1_rmse: 0.10478 |  0:02:13s
epoch 88 | loss: 0.01188 | val_0_rmse: 0.1045  | val_1_rmse: 0.10533 |  0:02:14s
epoch 89 | loss: 0.01178 | val_0_rmse: 0.11027 | val_1_rmse: 0.10971 |  0:02:16s
epoch 90 | loss: 0.01279 | val_0_rmse: 0.10869 | val_1_rmse: 0.10968 |  0:02:17s
epoch 91 | loss: 0.01251 | val_0_rmse: 0.10626 | val_1_rmse: 0.10736 |  0:02:19s
epoch 92 | loss: 0.01203 | val_0_rmse: 0.10611 | val_1_rmse: 0.10795 |  0:02:20s
epoch 93 | loss: 0.01225 | val_0_rmse: 0.10682 | val_1_rmse: 0.10826 |  0:02:22s
epoch 94 | loss: 0.01205 | val_0_rmse: 0.10662 | val_1_rmse: 0.10766 |  0:02:23s
epoch 95 | loss: 0.01216 | val_0_rmse: 0.10736 | val_1_rmse: 0.1081  |  0:02:25s
epoch 96 | loss: 0.01175 | val_0_rmse: 0.10405 | val_1_rmse: 0.10593 |  0:02:26s
epoch 97 | loss: 0.01199 | val_0_rmse: 0.10419 | val_1_rmse: 0.10577 |  0:02:28s
epoch 98 | loss: 0.01244 | val_0_rmse: 0.10312 | val_1_rmse: 0.10384 |  0:02:29s
epoch 99 | loss: 0.01255 | val_0_rmse: 0.10489 | val_1_rmse: 0.10701 |  0:02:30s
epoch 100| loss: 0.01232 | val_0_rmse: 0.10658 | val_1_rmse: 0.10738 |  0:02:32s
epoch 101| loss: 0.01266 | val_0_rmse: 0.10793 | val_1_rmse: 0.10723 |  0:02:33s
epoch 102| loss: 0.01227 | val_0_rmse: 0.1054  | val_1_rmse: 0.10655 |  0:02:35s
epoch 103| loss: 0.01198 | val_0_rmse: 0.10967 | val_1_rmse: 0.1103  |  0:02:36s
epoch 104| loss: 0.01191 | val_0_rmse: 0.10596 | val_1_rmse: 0.10663 |  0:02:38s
epoch 105| loss: 0.01204 | val_0_rmse: 0.10306 | val_1_rmse: 0.10462 |  0:02:39s
epoch 106| loss: 0.01197 | val_0_rmse: 0.10878 | val_1_rmse: 0.11173 |  0:02:41s
epoch 107| loss: 0.01158 | val_0_rmse: 0.10857 | val_1_rmse: 0.11112 |  0:02:42s
epoch 108| loss: 0.01175 | val_0_rmse: 0.10614 | val_1_rmse: 0.10759 |  0:02:43s
epoch 109| loss: 0.01244 | val_0_rmse: 0.10366 | val_1_rmse: 0.10568 |  0:02:45s
epoch 110| loss: 0.01217 | val_0_rmse: 0.10722 | val_1_rmse: 0.10742 |  0:02:46s
epoch 111| loss: 0.01193 | val_0_rmse: 0.10478 | val_1_rmse: 0.10492 |  0:02:48s
epoch 112| loss: 0.01173 | val_0_rmse: 0.10441 | val_1_rmse: 0.10559 |  0:02:49s
epoch 113| loss: 0.01152 | val_0_rmse: 0.10202 | val_1_rmse: 0.1044  |  0:02:51s
epoch 114| loss: 0.01156 | val_0_rmse: 0.10169 | val_1_rmse: 0.10356 |  0:02:52s
epoch 115| loss: 0.01143 | val_0_rmse: 0.10163 | val_1_rmse: 0.10255 |  0:02:54s
epoch 116| loss: 0.01175 | val_0_rmse: 0.10334 | val_1_rmse: 0.10516 |  0:02:55s
epoch 117| loss: 0.01206 | val_0_rmse: 0.10463 | val_1_rmse: 0.10532 |  0:02:57s
epoch 118| loss: 0.01238 | val_0_rmse: 0.10747 | val_1_rmse: 0.10795 |  0:02:58s
epoch 119| loss: 0.01216 | val_0_rmse: 0.10363 | val_1_rmse: 0.10376 |  0:03:00s
epoch 120| loss: 0.01176 | val_0_rmse: 0.10201 | val_1_rmse: 0.1021  |  0:03:01s
epoch 121| loss: 0.01169 | val_0_rmse: 0.1043  | val_1_rmse: 0.10476 |  0:03:02s
epoch 122| loss: 0.01185 | val_0_rmse: 0.10252 | val_1_rmse: 0.1039  |  0:03:04s
epoch 123| loss: 0.01171 | val_0_rmse: 0.10606 | val_1_rmse: 0.10755 |  0:03:05s
epoch 124| loss: 0.01216 | val_0_rmse: 0.10364 | val_1_rmse: 0.10452 |  0:03:07s
epoch 125| loss: 0.01151 | val_0_rmse: 0.10815 | val_1_rmse: 0.1099  |  0:03:08s
epoch 126| loss: 0.01181 | val_0_rmse: 0.10732 | val_1_rmse: 0.10763 |  0:03:10s
epoch 127| loss: 0.01178 | val_0_rmse: 0.10277 | val_1_rmse: 0.10331 |  0:03:11s
epoch 128| loss: 0.01142 | val_0_rmse: 0.10075 | val_1_rmse: 0.10137 |  0:03:13s
epoch 129| loss: 0.01165 | val_0_rmse: 0.10208 | val_1_rmse: 0.10317 |  0:03:14s
epoch 130| loss: 0.0116  | val_0_rmse: 0.10127 | val_1_rmse: 0.10254 |  0:03:15s
epoch 131| loss: 0.01185 | val_0_rmse: 0.10826 | val_1_rmse: 0.10929 |  0:03:17s
epoch 132| loss: 0.01182 | val_0_rmse: 0.10164 | val_1_rmse: 0.10346 |  0:03:18s
epoch 133| loss: 0.01165 | val_0_rmse: 0.10188 | val_1_rmse: 0.10352 |  0:03:20s
epoch 134| loss: 0.01138 | val_0_rmse: 0.09999 | val_1_rmse: 0.10151 |  0:03:21s
epoch 135| loss: 0.01136 | val_0_rmse: 0.10254 | val_1_rmse: 0.10417 |  0:03:23s
epoch 136| loss: 0.01158 | val_0_rmse: 0.10533 | val_1_rmse: 0.1052  |  0:03:24s
epoch 137| loss: 0.01176 | val_0_rmse: 0.10254 | val_1_rmse: 0.10484 |  0:03:26s
epoch 138| loss: 0.01178 | val_0_rmse: 0.10265 | val_1_rmse: 0.10503 |  0:03:27s
epoch 139| loss: 0.01162 | val_0_rmse: 0.10283 | val_1_rmse: 0.10294 |  0:03:29s
epoch 140| loss: 0.01146 | val_0_rmse: 0.10166 | val_1_rmse: 0.10284 |  0:03:30s
epoch 141| loss: 0.01135 | val_0_rmse: 0.10012 | val_1_rmse: 0.10199 |  0:03:32s
epoch 142| loss: 0.01143 | val_0_rmse: 0.10294 | val_1_rmse: 0.10392 |  0:03:33s
epoch 143| loss: 0.01214 | val_0_rmse: 0.10449 | val_1_rmse: 0.10501 |  0:03:34s
epoch 144| loss: 0.01157 | val_0_rmse: 0.10017 | val_1_rmse: 0.10179 |  0:03:36s
epoch 145| loss: 0.01156 | val_0_rmse: 0.10294 | val_1_rmse: 0.10475 |  0:03:37s
epoch 146| loss: 0.01146 | val_0_rmse: 0.10217 | val_1_rmse: 0.10438 |  0:03:39s
epoch 147| loss: 0.01143 | val_0_rmse: 0.10242 | val_1_rmse: 0.10441 |  0:03:40s
epoch 148| loss: 0.01133 | val_0_rmse: 0.1022  | val_1_rmse: 0.10206 |  0:03:42s
epoch 149| loss: 0.01131 | val_0_rmse: 0.10185 | val_1_rmse: 0.10399 |  0:03:43s
Stop training because you reached max_epochs = 150 with best_epoch = 128 and best_val_1_rmse = 0.10137
Best weights from best epoch are automatically used!
ended training at: 04:18:31
Feature importance:
[('Area', 0.2101519714544154), ('Baths', 0.1354838514407222), ('Beds', 0.04880703966754643), ('Latitude', 0.2188817751907808), ('Longitude', 0.36377987130205147), ('Month', 0.02289549094448372), ('Year', 0.0)]
Mean squared error is of 6336751248.197517
Mean absolute error:54739.8306315939
MAPE:0.1790355305250655
R2 score:0.7142349694663679
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:18:32
epoch 0  | loss: 0.14107 | val_0_rmse: 0.19945 | val_1_rmse: 0.19715 |  0:00:01s
epoch 1  | loss: 0.02234 | val_0_rmse: 0.18788 | val_1_rmse: 0.18572 |  0:00:02s
epoch 2  | loss: 0.01858 | val_0_rmse: 0.18405 | val_1_rmse: 0.18179 |  0:00:04s
epoch 3  | loss: 0.01696 | val_0_rmse: 0.1887  | val_1_rmse: 0.18643 |  0:00:05s
epoch 4  | loss: 0.01569 | val_0_rmse: 0.19409 | val_1_rmse: 0.1918  |  0:00:07s
epoch 5  | loss: 0.01549 | val_0_rmse: 0.18478 | val_1_rmse: 0.18202 |  0:00:08s
epoch 6  | loss: 0.01583 | val_0_rmse: 0.18753 | val_1_rmse: 0.18476 |  0:00:10s
epoch 7  | loss: 0.01542 | val_0_rmse: 0.18732 | val_1_rmse: 0.18479 |  0:00:11s
epoch 8  | loss: 0.01521 | val_0_rmse: 0.16712 | val_1_rmse: 0.16411 |  0:00:13s
epoch 9  | loss: 0.01434 | val_0_rmse: 0.173   | val_1_rmse: 0.17014 |  0:00:14s
epoch 10 | loss: 0.01461 | val_0_rmse: 0.17404 | val_1_rmse: 0.1711  |  0:00:16s
epoch 11 | loss: 0.01511 | val_0_rmse: 0.15612 | val_1_rmse: 0.15263 |  0:00:17s
epoch 12 | loss: 0.01451 | val_0_rmse: 0.14561 | val_1_rmse: 0.14273 |  0:00:19s
epoch 13 | loss: 0.01463 | val_0_rmse: 0.13999 | val_1_rmse: 0.13655 |  0:00:20s
epoch 14 | loss: 0.01482 | val_0_rmse: 0.14091 | val_1_rmse: 0.13773 |  0:00:21s
epoch 15 | loss: 0.01392 | val_0_rmse: 0.1343  | val_1_rmse: 0.13089 |  0:00:23s
epoch 16 | loss: 0.01385 | val_0_rmse: 0.13064 | val_1_rmse: 0.12688 |  0:00:24s
epoch 17 | loss: 0.01372 | val_0_rmse: 0.12986 | val_1_rmse: 0.12621 |  0:00:26s
epoch 18 | loss: 0.01406 | val_0_rmse: 0.13377 | val_1_rmse: 0.12931 |  0:00:27s
epoch 19 | loss: 0.01492 | val_0_rmse: 0.12283 | val_1_rmse: 0.11868 |  0:00:29s
epoch 20 | loss: 0.014   | val_0_rmse: 0.12144 | val_1_rmse: 0.11785 |  0:00:30s
epoch 21 | loss: 0.01369 | val_0_rmse: 0.11723 | val_1_rmse: 0.11388 |  0:00:32s
epoch 22 | loss: 0.01379 | val_0_rmse: 0.12215 | val_1_rmse: 0.11918 |  0:00:33s
epoch 23 | loss: 0.01378 | val_0_rmse: 0.11454 | val_1_rmse: 0.11103 |  0:00:35s
epoch 24 | loss: 0.01373 | val_0_rmse: 0.1152  | val_1_rmse: 0.11161 |  0:00:36s
epoch 25 | loss: 0.01353 | val_0_rmse: 0.11265 | val_1_rmse: 0.10954 |  0:00:38s
epoch 26 | loss: 0.01338 | val_0_rmse: 0.11126 | val_1_rmse: 0.10829 |  0:00:39s
epoch 27 | loss: 0.01356 | val_0_rmse: 0.11142 | val_1_rmse: 0.10838 |  0:00:41s
epoch 28 | loss: 0.01368 | val_0_rmse: 0.11249 | val_1_rmse: 0.10876 |  0:00:42s
epoch 29 | loss: 0.01377 | val_0_rmse: 0.1134  | val_1_rmse: 0.1095  |  0:00:44s
epoch 30 | loss: 0.01335 | val_0_rmse: 0.11014 | val_1_rmse: 0.10662 |  0:00:45s
epoch 31 | loss: 0.01311 | val_0_rmse: 0.11049 | val_1_rmse: 0.10681 |  0:00:47s
epoch 32 | loss: 0.01314 | val_0_rmse: 0.11517 | val_1_rmse: 0.11207 |  0:00:48s
epoch 33 | loss: 0.01356 | val_0_rmse: 0.12104 | val_1_rmse: 0.11823 |  0:00:49s
epoch 34 | loss: 0.01407 | val_0_rmse: 0.11159 | val_1_rmse: 0.10785 |  0:00:51s
epoch 35 | loss: 0.01365 | val_0_rmse: 0.10978 | val_1_rmse: 0.1063  |  0:00:52s
epoch 36 | loss: 0.01313 | val_0_rmse: 0.10945 | val_1_rmse: 0.10612 |  0:00:54s
epoch 37 | loss: 0.0132  | val_0_rmse: 0.10993 | val_1_rmse: 0.10727 |  0:00:55s
epoch 38 | loss: 0.01297 | val_0_rmse: 0.10767 | val_1_rmse: 0.10445 |  0:00:57s
epoch 39 | loss: 0.01317 | val_0_rmse: 0.11129 | val_1_rmse: 0.10779 |  0:00:59s
epoch 40 | loss: 0.0133  | val_0_rmse: 0.11275 | val_1_rmse: 0.11028 |  0:01:00s
epoch 41 | loss: 0.01295 | val_0_rmse: 0.10754 | val_1_rmse: 0.1041  |  0:01:02s
epoch 42 | loss: 0.01275 | val_0_rmse: 0.10735 | val_1_rmse: 0.10412 |  0:01:03s
epoch 43 | loss: 0.01362 | val_0_rmse: 0.10907 | val_1_rmse: 0.10592 |  0:01:05s
epoch 44 | loss: 0.01351 | val_0_rmse: 0.10772 | val_1_rmse: 0.10471 |  0:01:06s
epoch 45 | loss: 0.01275 | val_0_rmse: 0.10681 | val_1_rmse: 0.10388 |  0:01:08s
epoch 46 | loss: 0.01271 | val_0_rmse: 0.10612 | val_1_rmse: 0.10269 |  0:01:09s
epoch 47 | loss: 0.01251 | val_0_rmse: 0.10988 | val_1_rmse: 0.10661 |  0:01:11s
epoch 48 | loss: 0.01271 | val_0_rmse: 0.10486 | val_1_rmse: 0.10195 |  0:01:13s
epoch 49 | loss: 0.01253 | val_0_rmse: 0.11121 | val_1_rmse: 0.1082  |  0:01:14s
epoch 50 | loss: 0.01277 | val_0_rmse: 0.10715 | val_1_rmse: 0.10365 |  0:01:16s
epoch 51 | loss: 0.01243 | val_0_rmse: 0.10911 | val_1_rmse: 0.10593 |  0:01:17s
epoch 52 | loss: 0.01253 | val_0_rmse: 0.10486 | val_1_rmse: 0.10186 |  0:01:19s
epoch 53 | loss: 0.01212 | val_0_rmse: 0.10604 | val_1_rmse: 0.10206 |  0:01:20s
epoch 54 | loss: 0.01199 | val_0_rmse: 0.10617 | val_1_rmse: 0.10375 |  0:01:22s
epoch 55 | loss: 0.01216 | val_0_rmse: 0.10918 | val_1_rmse: 0.10756 |  0:01:23s
epoch 56 | loss: 0.01236 | val_0_rmse: 0.10587 | val_1_rmse: 0.10368 |  0:01:24s
epoch 57 | loss: 0.01228 | val_0_rmse: 0.10679 | val_1_rmse: 0.10398 |  0:01:26s
epoch 58 | loss: 0.01242 | val_0_rmse: 0.105   | val_1_rmse: 0.10212 |  0:01:27s
epoch 59 | loss: 0.01204 | val_0_rmse: 0.10703 | val_1_rmse: 0.10459 |  0:01:29s
epoch 60 | loss: 0.01224 | val_0_rmse: 0.10726 | val_1_rmse: 0.10427 |  0:01:30s
epoch 61 | loss: 0.01244 | val_0_rmse: 0.10502 | val_1_rmse: 0.10325 |  0:01:32s
epoch 62 | loss: 0.01202 | val_0_rmse: 0.11176 | val_1_rmse: 0.11003 |  0:01:33s
epoch 63 | loss: 0.01251 | val_0_rmse: 0.10499 | val_1_rmse: 0.10273 |  0:01:35s
epoch 64 | loss: 0.01207 | val_0_rmse: 0.10396 | val_1_rmse: 0.10119 |  0:01:36s
epoch 65 | loss: 0.01191 | val_0_rmse: 0.10478 | val_1_rmse: 0.10182 |  0:01:38s
epoch 66 | loss: 0.01248 | val_0_rmse: 0.10446 | val_1_rmse: 0.10219 |  0:01:39s
epoch 67 | loss: 0.01219 | val_0_rmse: 0.10638 | val_1_rmse: 0.10412 |  0:01:41s
epoch 68 | loss: 0.01223 | val_0_rmse: 0.11139 | val_1_rmse: 0.10999 |  0:01:42s
epoch 69 | loss: 0.01274 | val_0_rmse: 0.11637 | val_1_rmse: 0.11424 |  0:01:44s
epoch 70 | loss: 0.01202 | val_0_rmse: 0.10469 | val_1_rmse: 0.1025  |  0:01:45s
epoch 71 | loss: 0.01217 | val_0_rmse: 0.10477 | val_1_rmse: 0.1024  |  0:01:47s
epoch 72 | loss: 0.0121  | val_0_rmse: 0.10864 | val_1_rmse: 0.10672 |  0:01:48s
epoch 73 | loss: 0.01262 | val_0_rmse: 0.10485 | val_1_rmse: 0.10236 |  0:01:50s
epoch 74 | loss: 0.01205 | val_0_rmse: 0.10391 | val_1_rmse: 0.10171 |  0:01:51s
epoch 75 | loss: 0.01188 | val_0_rmse: 0.10488 | val_1_rmse: 0.10198 |  0:01:53s
epoch 76 | loss: 0.01192 | val_0_rmse: 0.10358 | val_1_rmse: 0.10085 |  0:01:54s
epoch 77 | loss: 0.01181 | val_0_rmse: 0.10502 | val_1_rmse: 0.10318 |  0:01:55s
epoch 78 | loss: 0.01182 | val_0_rmse: 0.10649 | val_1_rmse: 0.10428 |  0:01:57s
epoch 79 | loss: 0.01228 | val_0_rmse: 0.10615 | val_1_rmse: 0.1038  |  0:01:58s
epoch 80 | loss: 0.01198 | val_0_rmse: 0.10843 | val_1_rmse: 0.10599 |  0:02:00s
epoch 81 | loss: 0.012   | val_0_rmse: 0.10492 | val_1_rmse: 0.10149 |  0:02:01s
epoch 82 | loss: 0.0123  | val_0_rmse: 0.10542 | val_1_rmse: 0.10278 |  0:02:03s
epoch 83 | loss: 0.01193 | val_0_rmse: 0.10364 | val_1_rmse: 0.10075 |  0:02:04s
epoch 84 | loss: 0.01191 | val_0_rmse: 0.10615 | val_1_rmse: 0.10468 |  0:02:06s
epoch 85 | loss: 0.01236 | val_0_rmse: 0.11022 | val_1_rmse: 0.10804 |  0:02:07s
epoch 86 | loss: 0.01238 | val_0_rmse: 0.11729 | val_1_rmse: 0.1156  |  0:02:09s
epoch 87 | loss: 0.013   | val_0_rmse: 0.10597 | val_1_rmse: 0.10286 |  0:02:10s
epoch 88 | loss: 0.01205 | val_0_rmse: 0.10422 | val_1_rmse: 0.10149 |  0:02:12s
epoch 89 | loss: 0.01186 | val_0_rmse: 0.10383 | val_1_rmse: 0.10036 |  0:02:13s
epoch 90 | loss: 0.01177 | val_0_rmse: 0.10538 | val_1_rmse: 0.10241 |  0:02:15s
epoch 91 | loss: 0.01181 | val_0_rmse: 0.10549 | val_1_rmse: 0.10225 |  0:02:16s
epoch 92 | loss: 0.01181 | val_0_rmse: 0.10692 | val_1_rmse: 0.10477 |  0:02:18s
epoch 93 | loss: 0.01201 | val_0_rmse: 0.10504 | val_1_rmse: 0.10295 |  0:02:19s
epoch 94 | loss: 0.01196 | val_0_rmse: 0.10547 | val_1_rmse: 0.10329 |  0:02:20s
epoch 95 | loss: 0.01226 | val_0_rmse: 0.11124 | val_1_rmse: 0.10947 |  0:02:22s
epoch 96 | loss: 0.01231 | val_0_rmse: 0.10859 | val_1_rmse: 0.10673 |  0:02:23s
epoch 97 | loss: 0.01257 | val_0_rmse: 0.11011 | val_1_rmse: 0.10786 |  0:02:25s
epoch 98 | loss: 0.01266 | val_0_rmse: 0.10492 | val_1_rmse: 0.10167 |  0:02:26s
epoch 99 | loss: 0.01203 | val_0_rmse: 0.10469 | val_1_rmse: 0.10326 |  0:02:28s
epoch 100| loss: 0.01176 | val_0_rmse: 0.104   | val_1_rmse: 0.10114 |  0:02:29s
epoch 101| loss: 0.0118  | val_0_rmse: 0.10675 | val_1_rmse: 0.10474 |  0:02:31s
epoch 102| loss: 0.01201 | val_0_rmse: 0.10337 | val_1_rmse: 0.10048 |  0:02:32s
epoch 103| loss: 0.01207 | val_0_rmse: 0.10787 | val_1_rmse: 0.10624 |  0:02:34s
epoch 104| loss: 0.01204 | val_0_rmse: 0.1031  | val_1_rmse: 0.10083 |  0:02:35s
epoch 105| loss: 0.01197 | val_0_rmse: 0.10508 | val_1_rmse: 0.1024  |  0:02:36s
epoch 106| loss: 0.01192 | val_0_rmse: 0.10416 | val_1_rmse: 0.10251 |  0:02:38s
epoch 107| loss: 0.01198 | val_0_rmse: 0.10652 | val_1_rmse: 0.10469 |  0:02:39s
epoch 108| loss: 0.01185 | val_0_rmse: 0.10439 | val_1_rmse: 0.10259 |  0:02:41s
epoch 109| loss: 0.0122  | val_0_rmse: 0.10786 | val_1_rmse: 0.10592 |  0:02:42s
epoch 110| loss: 0.01191 | val_0_rmse: 0.10959 | val_1_rmse: 0.10778 |  0:02:44s
epoch 111| loss: 0.01201 | val_0_rmse: 0.10384 | val_1_rmse: 0.10147 |  0:02:45s
epoch 112| loss: 0.01202 | val_0_rmse: 0.10677 | val_1_rmse: 0.10406 |  0:02:47s
epoch 113| loss: 0.01245 | val_0_rmse: 0.10935 | val_1_rmse: 0.10692 |  0:02:48s
epoch 114| loss: 0.01257 | val_0_rmse: 0.10701 | val_1_rmse: 0.10422 |  0:02:50s
epoch 115| loss: 0.01193 | val_0_rmse: 0.10331 | val_1_rmse: 0.10062 |  0:02:51s
epoch 116| loss: 0.01187 | val_0_rmse: 0.10353 | val_1_rmse: 0.10078 |  0:02:52s
epoch 117| loss: 0.01203 | val_0_rmse: 0.11029 | val_1_rmse: 0.10849 |  0:02:54s
epoch 118| loss: 0.01205 | val_0_rmse: 0.10325 | val_1_rmse: 0.10088 |  0:02:55s
epoch 119| loss: 0.01178 | val_0_rmse: 0.10993 | val_1_rmse: 0.10837 |  0:02:57s

Early stopping occured at epoch 119 with best_epoch = 89 and best_val_1_rmse = 0.10036
Best weights from best epoch are automatically used!
ended training at: 04:21:30
Feature importance:
[('Area', 0.24135661301682101), ('Baths', 0.04060296244670809), ('Beds', 0.0012442750946940846), ('Latitude', 0.27194943239682845), ('Longitude', 0.37287166291268237), ('Month', 0.036345966884567586), ('Year', 0.03562908724769842)]
Mean squared error is of 6902136298.213394
Mean absolute error:58237.28154327625
MAPE:0.19724199337291573
R2 score:0.68780464194884
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:21:30
epoch 0  | loss: 0.13301 | val_0_rmse: 0.17812 | val_1_rmse: 0.18095 |  0:00:01s
epoch 1  | loss: 0.02632 | val_0_rmse: 0.17562 | val_1_rmse: 0.17845 |  0:00:02s
epoch 2  | loss: 0.0218  | val_0_rmse: 0.17768 | val_1_rmse: 0.1809  |  0:00:04s
epoch 3  | loss: 0.01876 | val_0_rmse: 0.17633 | val_1_rmse: 0.17895 |  0:00:05s
epoch 4  | loss: 0.01747 | val_0_rmse: 0.18202 | val_1_rmse: 0.18429 |  0:00:07s
epoch 5  | loss: 0.01608 | val_0_rmse: 0.17438 | val_1_rmse: 0.17703 |  0:00:08s
epoch 6  | loss: 0.01587 | val_0_rmse: 0.18883 | val_1_rmse: 0.19093 |  0:00:10s
epoch 7  | loss: 0.01553 | val_0_rmse: 0.18891 | val_1_rmse: 0.19083 |  0:00:11s
epoch 8  | loss: 0.01509 | val_0_rmse: 0.17677 | val_1_rmse: 0.17844 |  0:00:13s
epoch 9  | loss: 0.01481 | val_0_rmse: 0.16009 | val_1_rmse: 0.16168 |  0:00:14s
epoch 10 | loss: 0.01484 | val_0_rmse: 0.16242 | val_1_rmse: 0.16312 |  0:00:16s
epoch 11 | loss: 0.01441 | val_0_rmse: 0.15799 | val_1_rmse: 0.15912 |  0:00:17s
epoch 12 | loss: 0.01429 | val_0_rmse: 0.15116 | val_1_rmse: 0.15246 |  0:00:19s
epoch 13 | loss: 0.01399 | val_0_rmse: 0.1471  | val_1_rmse: 0.14798 |  0:00:20s
epoch 14 | loss: 0.01406 | val_0_rmse: 0.14105 | val_1_rmse: 0.1421  |  0:00:22s
epoch 15 | loss: 0.01432 | val_0_rmse: 0.13396 | val_1_rmse: 0.13587 |  0:00:23s
epoch 16 | loss: 0.01406 | val_0_rmse: 0.13148 | val_1_rmse: 0.1342  |  0:00:24s
epoch 17 | loss: 0.01471 | val_0_rmse: 0.13478 | val_1_rmse: 0.13623 |  0:00:26s
epoch 18 | loss: 0.01381 | val_0_rmse: 0.12823 | val_1_rmse: 0.13016 |  0:00:27s
epoch 19 | loss: 0.01376 | val_0_rmse: 0.12586 | val_1_rmse: 0.12941 |  0:00:29s
epoch 20 | loss: 0.01402 | val_0_rmse: 0.11942 | val_1_rmse: 0.1223  |  0:00:30s
epoch 21 | loss: 0.01393 | val_0_rmse: 0.11994 | val_1_rmse: 0.12231 |  0:00:32s
epoch 22 | loss: 0.01381 | val_0_rmse: 0.11452 | val_1_rmse: 0.11718 |  0:00:33s
epoch 23 | loss: 0.01349 | val_0_rmse: 0.11646 | val_1_rmse: 0.11913 |  0:00:35s
epoch 24 | loss: 0.01389 | val_0_rmse: 0.11638 | val_1_rmse: 0.11864 |  0:00:36s
epoch 25 | loss: 0.01369 | val_0_rmse: 0.12121 | val_1_rmse: 0.12315 |  0:00:38s
epoch 26 | loss: 0.01387 | val_0_rmse: 0.11225 | val_1_rmse: 0.11412 |  0:00:39s
epoch 27 | loss: 0.01347 | val_0_rmse: 0.12205 | val_1_rmse: 0.12362 |  0:00:41s
epoch 28 | loss: 0.01322 | val_0_rmse: 0.11118 | val_1_rmse: 0.11265 |  0:00:42s
epoch 29 | loss: 0.01324 | val_0_rmse: 0.11096 | val_1_rmse: 0.11307 |  0:00:44s
epoch 30 | loss: 0.01309 | val_0_rmse: 0.11243 | val_1_rmse: 0.11374 |  0:00:45s
epoch 31 | loss: 0.01303 | val_0_rmse: 0.10905 | val_1_rmse: 0.11085 |  0:00:47s
epoch 32 | loss: 0.01318 | val_0_rmse: 0.11365 | val_1_rmse: 0.1163  |  0:00:48s
epoch 33 | loss: 0.0133  | val_0_rmse: 0.10969 | val_1_rmse: 0.11141 |  0:00:49s
epoch 34 | loss: 0.01354 | val_0_rmse: 0.11063 | val_1_rmse: 0.11328 |  0:00:51s
epoch 35 | loss: 0.01288 | val_0_rmse: 0.1108  | val_1_rmse: 0.11302 |  0:00:52s
epoch 36 | loss: 0.01322 | val_0_rmse: 0.11465 | val_1_rmse: 0.11632 |  0:00:54s
epoch 37 | loss: 0.01306 | val_0_rmse: 0.10722 | val_1_rmse: 0.10979 |  0:00:55s
epoch 38 | loss: 0.01257 | val_0_rmse: 0.10734 | val_1_rmse: 0.10985 |  0:00:57s
epoch 39 | loss: 0.01318 | val_0_rmse: 0.1087  | val_1_rmse: 0.11183 |  0:00:58s
epoch 40 | loss: 0.01282 | val_0_rmse: 0.109   | val_1_rmse: 0.11209 |  0:01:00s
epoch 41 | loss: 0.01291 | val_0_rmse: 0.10862 | val_1_rmse: 0.1116  |  0:01:01s
epoch 42 | loss: 0.01266 | val_0_rmse: 0.10662 | val_1_rmse: 0.10917 |  0:01:03s
epoch 43 | loss: 0.01295 | val_0_rmse: 0.10971 | val_1_rmse: 0.11279 |  0:01:04s
epoch 44 | loss: 0.0127  | val_0_rmse: 0.1061  | val_1_rmse: 0.10853 |  0:01:06s
epoch 45 | loss: 0.01287 | val_0_rmse: 0.10773 | val_1_rmse: 0.10954 |  0:01:07s
epoch 46 | loss: 0.01267 | val_0_rmse: 0.10683 | val_1_rmse: 0.1089  |  0:01:09s
epoch 47 | loss: 0.01248 | val_0_rmse: 0.10657 | val_1_rmse: 0.10897 |  0:01:10s
epoch 48 | loss: 0.01224 | val_0_rmse: 0.10823 | val_1_rmse: 0.11054 |  0:01:11s
epoch 49 | loss: 0.01271 | val_0_rmse: 0.1066  | val_1_rmse: 0.10933 |  0:01:13s
epoch 50 | loss: 0.01241 | val_0_rmse: 0.10591 | val_1_rmse: 0.10793 |  0:01:14s
epoch 51 | loss: 0.01253 | val_0_rmse: 0.12002 | val_1_rmse: 0.12216 |  0:01:16s
epoch 52 | loss: 0.01277 | val_0_rmse: 0.10528 | val_1_rmse: 0.10754 |  0:01:17s
epoch 53 | loss: 0.01243 | val_0_rmse: 0.10912 | val_1_rmse: 0.11083 |  0:01:19s
epoch 54 | loss: 0.0124  | val_0_rmse: 0.10551 | val_1_rmse: 0.10811 |  0:01:20s
epoch 55 | loss: 0.0123  | val_0_rmse: 0.10892 | val_1_rmse: 0.11067 |  0:01:22s
epoch 56 | loss: 0.01261 | val_0_rmse: 0.10432 | val_1_rmse: 0.10705 |  0:01:23s
epoch 57 | loss: 0.01243 | val_0_rmse: 0.11052 | val_1_rmse: 0.1132  |  0:01:25s
epoch 58 | loss: 0.01272 | val_0_rmse: 0.1059  | val_1_rmse: 0.10841 |  0:01:26s
epoch 59 | loss: 0.01238 | val_0_rmse: 0.10688 | val_1_rmse: 0.10975 |  0:01:28s
epoch 60 | loss: 0.01225 | val_0_rmse: 0.1077  | val_1_rmse: 0.11036 |  0:01:29s
epoch 61 | loss: 0.01225 | val_0_rmse: 0.10527 | val_1_rmse: 0.10739 |  0:01:30s
epoch 62 | loss: 0.01232 | val_0_rmse: 0.10536 | val_1_rmse: 0.1079  |  0:01:32s
epoch 63 | loss: 0.01233 | val_0_rmse: 0.10575 | val_1_rmse: 0.10844 |  0:01:33s
epoch 64 | loss: 0.0121  | val_0_rmse: 0.10666 | val_1_rmse: 0.10887 |  0:01:35s
epoch 65 | loss: 0.01222 | val_0_rmse: 0.11565 | val_1_rmse: 0.11758 |  0:01:36s
epoch 66 | loss: 0.01268 | val_0_rmse: 0.11003 | val_1_rmse: 0.1127  |  0:01:38s
epoch 67 | loss: 0.01326 | val_0_rmse: 0.11558 | val_1_rmse: 0.11719 |  0:01:39s
epoch 68 | loss: 0.01295 | val_0_rmse: 0.10604 | val_1_rmse: 0.10899 |  0:01:41s
epoch 69 | loss: 0.01232 | val_0_rmse: 0.10462 | val_1_rmse: 0.10683 |  0:01:42s
epoch 70 | loss: 0.01199 | val_0_rmse: 0.10498 | val_1_rmse: 0.10733 |  0:01:44s
epoch 71 | loss: 0.01233 | val_0_rmse: 0.10707 | val_1_rmse: 0.10869 |  0:01:45s
epoch 72 | loss: 0.01202 | val_0_rmse: 0.10528 | val_1_rmse: 0.10824 |  0:01:47s
epoch 73 | loss: 0.01202 | val_0_rmse: 0.10599 | val_1_rmse: 0.10852 |  0:01:48s
epoch 74 | loss: 0.01207 | val_0_rmse: 0.11534 | val_1_rmse: 0.11775 |  0:01:49s
epoch 75 | loss: 0.01276 | val_0_rmse: 0.10856 | val_1_rmse: 0.11043 |  0:01:51s
epoch 76 | loss: 0.01222 | val_0_rmse: 0.10685 | val_1_rmse: 0.10877 |  0:01:52s
epoch 77 | loss: 0.01242 | val_0_rmse: 0.10449 | val_1_rmse: 0.10625 |  0:01:54s
epoch 78 | loss: 0.01238 | val_0_rmse: 0.10464 | val_1_rmse: 0.1069  |  0:01:55s
epoch 79 | loss: 0.01241 | val_0_rmse: 0.10498 | val_1_rmse: 0.10714 |  0:01:57s
epoch 80 | loss: 0.01196 | val_0_rmse: 0.10461 | val_1_rmse: 0.10687 |  0:01:58s
epoch 81 | loss: 0.0118  | val_0_rmse: 0.10308 | val_1_rmse: 0.10549 |  0:02:00s
epoch 82 | loss: 0.01183 | val_0_rmse: 0.11793 | val_1_rmse: 0.11926 |  0:02:01s
epoch 83 | loss: 0.01225 | val_0_rmse: 0.10623 | val_1_rmse: 0.10927 |  0:02:03s
epoch 84 | loss: 0.0119  | val_0_rmse: 0.10525 | val_1_rmse: 0.10745 |  0:02:04s
epoch 85 | loss: 0.01175 | val_0_rmse: 0.10671 | val_1_rmse: 0.10818 |  0:02:06s
epoch 86 | loss: 0.01208 | val_0_rmse: 0.10465 | val_1_rmse: 0.10663 |  0:02:07s
epoch 87 | loss: 0.01174 | val_0_rmse: 0.10482 | val_1_rmse: 0.10708 |  0:02:08s
epoch 88 | loss: 0.0119  | val_0_rmse: 0.10364 | val_1_rmse: 0.10549 |  0:02:10s
epoch 89 | loss: 0.01191 | val_0_rmse: 0.1053  | val_1_rmse: 0.10732 |  0:02:11s
epoch 90 | loss: 0.01179 | val_0_rmse: 0.10475 | val_1_rmse: 0.10758 |  0:02:13s
epoch 91 | loss: 0.01197 | val_0_rmse: 0.1061  | val_1_rmse: 0.10767 |  0:02:14s
epoch 92 | loss: 0.01225 | val_0_rmse: 0.10508 | val_1_rmse: 0.10767 |  0:02:16s
epoch 93 | loss: 0.01217 | val_0_rmse: 0.11218 | val_1_rmse: 0.11408 |  0:02:17s
epoch 94 | loss: 0.01261 | val_0_rmse: 0.10566 | val_1_rmse: 0.10747 |  0:02:19s
epoch 95 | loss: 0.01208 | val_0_rmse: 0.10981 | val_1_rmse: 0.11235 |  0:02:20s
epoch 96 | loss: 0.01228 | val_0_rmse: 0.10397 | val_1_rmse: 0.10672 |  0:02:21s
epoch 97 | loss: 0.01171 | val_0_rmse: 0.10557 | val_1_rmse: 0.10857 |  0:02:23s
epoch 98 | loss: 0.01178 | val_0_rmse: 0.10354 | val_1_rmse: 0.106   |  0:02:24s
epoch 99 | loss: 0.01166 | val_0_rmse: 0.10404 | val_1_rmse: 0.10715 |  0:02:26s
epoch 100| loss: 0.01184 | val_0_rmse: 0.10296 | val_1_rmse: 0.10572 |  0:02:27s
epoch 101| loss: 0.01166 | val_0_rmse: 0.10646 | val_1_rmse: 0.10927 |  0:02:29s
epoch 102| loss: 0.01187 | val_0_rmse: 0.10807 | val_1_rmse: 0.11086 |  0:02:30s
epoch 103| loss: 0.01288 | val_0_rmse: 0.10981 | val_1_rmse: 0.11189 |  0:02:32s
epoch 104| loss: 0.01222 | val_0_rmse: 0.1044  | val_1_rmse: 0.10687 |  0:02:33s
epoch 105| loss: 0.01225 | val_0_rmse: 0.10399 | val_1_rmse: 0.10633 |  0:02:35s
epoch 106| loss: 0.0121  | val_0_rmse: 0.10654 | val_1_rmse: 0.10849 |  0:02:36s
epoch 107| loss: 0.01187 | val_0_rmse: 0.11003 | val_1_rmse: 0.112   |  0:02:38s
epoch 108| loss: 0.01207 | val_0_rmse: 0.11105 | val_1_rmse: 0.11396 |  0:02:39s
epoch 109| loss: 0.01283 | val_0_rmse: 0.10656 | val_1_rmse: 0.1095  |  0:02:40s
epoch 110| loss: 0.01201 | val_0_rmse: 0.10381 | val_1_rmse: 0.1057  |  0:02:42s
epoch 111| loss: 0.01167 | val_0_rmse: 0.1031  | val_1_rmse: 0.10616 |  0:02:43s

Early stopping occured at epoch 111 with best_epoch = 81 and best_val_1_rmse = 0.10549
Best weights from best epoch are automatically used!
ended training at: 04:24:14
Feature importance:
[('Area', 0.23179481198623367), ('Baths', 0.0), ('Beds', 0.0002593777892296755), ('Latitude', 0.17929756209585268), ('Longitude', 0.3543720732169196), ('Month', 0.018357926638276036), ('Year', 0.21591824827348835)]
Mean squared error is of 6685374756.385131
Mean absolute error:56781.799024775304
MAPE:0.18975451111190428
R2 score:0.7041632558001114
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:24:14
epoch 0  | loss: 0.13477 | val_0_rmse: 0.19252 | val_1_rmse: 0.19516 |  0:00:01s
epoch 1  | loss: 0.02671 | val_0_rmse: 0.18443 | val_1_rmse: 0.18604 |  0:00:03s
epoch 2  | loss: 0.02153 | val_0_rmse: 0.18025 | val_1_rmse: 0.18213 |  0:00:04s
epoch 3  | loss: 0.01874 | val_0_rmse: 0.17997 | val_1_rmse: 0.18377 |  0:00:06s
epoch 4  | loss: 0.01738 | val_0_rmse: 0.18408 | val_1_rmse: 0.18601 |  0:00:07s
epoch 5  | loss: 0.01649 | val_0_rmse: 0.17378 | val_1_rmse: 0.17543 |  0:00:09s
epoch 6  | loss: 0.01598 | val_0_rmse: 0.17078 | val_1_rmse: 0.17273 |  0:00:10s
epoch 7  | loss: 0.01519 | val_0_rmse: 0.17445 | val_1_rmse: 0.176   |  0:00:12s
epoch 8  | loss: 0.01512 | val_0_rmse: 0.16556 | val_1_rmse: 0.16615 |  0:00:13s
epoch 9  | loss: 0.01548 | val_0_rmse: 0.17032 | val_1_rmse: 0.17088 |  0:00:15s
epoch 10 | loss: 0.01551 | val_0_rmse: 0.1546  | val_1_rmse: 0.15576 |  0:00:16s
epoch 11 | loss: 0.01456 | val_0_rmse: 0.17082 | val_1_rmse: 0.17092 |  0:00:18s
epoch 12 | loss: 0.01501 | val_0_rmse: 0.15032 | val_1_rmse: 0.15099 |  0:00:19s
epoch 13 | loss: 0.01433 | val_0_rmse: 0.15167 | val_1_rmse: 0.15178 |  0:00:21s
epoch 14 | loss: 0.01431 | val_0_rmse: 0.13719 | val_1_rmse: 0.13778 |  0:00:22s
epoch 15 | loss: 0.01395 | val_0_rmse: 0.1467  | val_1_rmse: 0.14652 |  0:00:24s
epoch 16 | loss: 0.01388 | val_0_rmse: 0.12695 | val_1_rmse: 0.12769 |  0:00:25s
epoch 17 | loss: 0.01381 | val_0_rmse: 0.14087 | val_1_rmse: 0.14055 |  0:00:26s
epoch 18 | loss: 0.01388 | val_0_rmse: 0.12172 | val_1_rmse: 0.1225  |  0:00:28s
epoch 19 | loss: 0.01387 | val_0_rmse: 0.12086 | val_1_rmse: 0.12164 |  0:00:29s
epoch 20 | loss: 0.01389 | val_0_rmse: 0.11615 | val_1_rmse: 0.11707 |  0:00:31s
epoch 21 | loss: 0.01327 | val_0_rmse: 0.11645 | val_1_rmse: 0.1176  |  0:00:32s
epoch 22 | loss: 0.01327 | val_0_rmse: 0.11286 | val_1_rmse: 0.11406 |  0:00:34s
epoch 23 | loss: 0.01316 | val_0_rmse: 0.11066 | val_1_rmse: 0.11208 |  0:00:35s
epoch 24 | loss: 0.01292 | val_0_rmse: 0.1152  | val_1_rmse: 0.11607 |  0:00:37s
epoch 25 | loss: 0.01387 | val_0_rmse: 0.1114  | val_1_rmse: 0.11318 |  0:00:38s
epoch 26 | loss: 0.01351 | val_0_rmse: 0.11029 | val_1_rmse: 0.11232 |  0:00:40s
epoch 27 | loss: 0.01263 | val_0_rmse: 0.10867 | val_1_rmse: 0.11077 |  0:00:41s
epoch 28 | loss: 0.01285 | val_0_rmse: 0.11067 | val_1_rmse: 0.11291 |  0:00:43s
epoch 29 | loss: 0.01307 | val_0_rmse: 0.11182 | val_1_rmse: 0.11351 |  0:00:44s
epoch 30 | loss: 0.01307 | val_0_rmse: 0.11319 | val_1_rmse: 0.11566 |  0:00:46s
epoch 31 | loss: 0.01294 | val_0_rmse: 0.10932 | val_1_rmse: 0.1115  |  0:00:47s
epoch 32 | loss: 0.01297 | val_0_rmse: 0.10713 | val_1_rmse: 0.10924 |  0:00:48s
epoch 33 | loss: 0.01315 | val_0_rmse: 0.1092  | val_1_rmse: 0.1119  |  0:00:50s
epoch 34 | loss: 0.01285 | val_0_rmse: 0.10534 | val_1_rmse: 0.10793 |  0:00:51s
epoch 35 | loss: 0.0126  | val_0_rmse: 0.11064 | val_1_rmse: 0.11309 |  0:00:53s
epoch 36 | loss: 0.01282 | val_0_rmse: 0.10634 | val_1_rmse: 0.10898 |  0:00:54s
epoch 37 | loss: 0.01262 | val_0_rmse: 0.10828 | val_1_rmse: 0.11157 |  0:00:56s
epoch 38 | loss: 0.01304 | val_0_rmse: 0.10717 | val_1_rmse: 0.11053 |  0:00:57s
epoch 39 | loss: 0.01276 | val_0_rmse: 0.10728 | val_1_rmse: 0.11038 |  0:00:59s
epoch 40 | loss: 0.01244 | val_0_rmse: 0.10569 | val_1_rmse: 0.10893 |  0:01:00s
epoch 41 | loss: 0.01263 | val_0_rmse: 0.10618 | val_1_rmse: 0.10865 |  0:01:02s
epoch 42 | loss: 0.01236 | val_0_rmse: 0.10599 | val_1_rmse: 0.10795 |  0:01:03s
epoch 43 | loss: 0.01253 | val_0_rmse: 0.10574 | val_1_rmse: 0.10863 |  0:01:05s
epoch 44 | loss: 0.01278 | val_0_rmse: 0.10486 | val_1_rmse: 0.10842 |  0:01:06s
epoch 45 | loss: 0.01285 | val_0_rmse: 0.10548 | val_1_rmse: 0.10834 |  0:01:08s
epoch 46 | loss: 0.01282 | val_0_rmse: 0.1079  | val_1_rmse: 0.11148 |  0:01:09s
epoch 47 | loss: 0.01257 | val_0_rmse: 0.11887 | val_1_rmse: 0.12214 |  0:01:10s
epoch 48 | loss: 0.01266 | val_0_rmse: 0.10696 | val_1_rmse: 0.10957 |  0:01:12s
epoch 49 | loss: 0.01283 | val_0_rmse: 0.1068  | val_1_rmse: 0.11009 |  0:01:13s
epoch 50 | loss: 0.01281 | val_0_rmse: 0.10596 | val_1_rmse: 0.10908 |  0:01:15s
epoch 51 | loss: 0.0125  | val_0_rmse: 0.10432 | val_1_rmse: 0.10752 |  0:01:16s
epoch 52 | loss: 0.01211 | val_0_rmse: 0.10818 | val_1_rmse: 0.11138 |  0:01:18s
epoch 53 | loss: 0.01231 | val_0_rmse: 0.10887 | val_1_rmse: 0.11212 |  0:01:19s
epoch 54 | loss: 0.01229 | val_0_rmse: 0.10645 | val_1_rmse: 0.10897 |  0:01:21s
epoch 55 | loss: 0.01223 | val_0_rmse: 0.10426 | val_1_rmse: 0.10641 |  0:01:22s
epoch 56 | loss: 0.01242 | val_0_rmse: 0.10596 | val_1_rmse: 0.10807 |  0:01:24s
epoch 57 | loss: 0.01276 | val_0_rmse: 0.10915 | val_1_rmse: 0.11159 |  0:01:25s
epoch 58 | loss: 0.01287 | val_0_rmse: 0.11424 | val_1_rmse: 0.11744 |  0:01:27s
epoch 59 | loss: 0.0135  | val_0_rmse: 0.10934 | val_1_rmse: 0.11132 |  0:01:28s
epoch 60 | loss: 0.01278 | val_0_rmse: 0.10749 | val_1_rmse: 0.1095  |  0:01:30s
epoch 61 | loss: 0.01278 | val_0_rmse: 0.10683 | val_1_rmse: 0.10885 |  0:01:31s
epoch 62 | loss: 0.0123  | val_0_rmse: 0.11349 | val_1_rmse: 0.11637 |  0:01:32s
epoch 63 | loss: 0.01227 | val_0_rmse: 0.10594 | val_1_rmse: 0.10874 |  0:01:34s
epoch 64 | loss: 0.01227 | val_0_rmse: 0.10517 | val_1_rmse: 0.10757 |  0:01:35s
epoch 65 | loss: 0.01237 | val_0_rmse: 0.11299 | val_1_rmse: 0.11488 |  0:01:37s
epoch 66 | loss: 0.01277 | val_0_rmse: 0.10468 | val_1_rmse: 0.10654 |  0:01:38s
epoch 67 | loss: 0.01225 | val_0_rmse: 0.10339 | val_1_rmse: 0.10546 |  0:01:40s
epoch 68 | loss: 0.0123  | val_0_rmse: 0.10421 | val_1_rmse: 0.10647 |  0:01:41s
epoch 69 | loss: 0.01232 | val_0_rmse: 0.10658 | val_1_rmse: 0.10871 |  0:01:43s
epoch 70 | loss: 0.01222 | val_0_rmse: 0.10254 | val_1_rmse: 0.10475 |  0:01:44s
epoch 71 | loss: 0.01183 | val_0_rmse: 0.10296 | val_1_rmse: 0.10578 |  0:01:46s
epoch 72 | loss: 0.01181 | val_0_rmse: 0.10338 | val_1_rmse: 0.10612 |  0:01:47s
epoch 73 | loss: 0.01189 | val_0_rmse: 0.10281 | val_1_rmse: 0.1053  |  0:01:49s
epoch 74 | loss: 0.01211 | val_0_rmse: 0.10799 | val_1_rmse: 0.11115 |  0:01:50s
epoch 75 | loss: 0.01249 | val_0_rmse: 0.10644 | val_1_rmse: 0.10925 |  0:01:51s
epoch 76 | loss: 0.0126  | val_0_rmse: 0.10627 | val_1_rmse: 0.10888 |  0:01:53s
epoch 77 | loss: 0.01235 | val_0_rmse: 0.10503 | val_1_rmse: 0.10711 |  0:01:54s
epoch 78 | loss: 0.012   | val_0_rmse: 0.10407 | val_1_rmse: 0.10696 |  0:01:56s
epoch 79 | loss: 0.01243 | val_0_rmse: 0.12789 | val_1_rmse: 0.12945 |  0:01:57s
epoch 80 | loss: 0.01285 | val_0_rmse: 0.10704 | val_1_rmse: 0.10945 |  0:01:59s
epoch 81 | loss: 0.01192 | val_0_rmse: 0.10363 | val_1_rmse: 0.10603 |  0:02:00s
epoch 82 | loss: 0.01206 | val_0_rmse: 0.10323 | val_1_rmse: 0.10651 |  0:02:02s
epoch 83 | loss: 0.01193 | val_0_rmse: 0.10403 | val_1_rmse: 0.1066  |  0:02:03s
epoch 84 | loss: 0.01155 | val_0_rmse: 0.10296 | val_1_rmse: 0.10553 |  0:02:05s
epoch 85 | loss: 0.01196 | val_0_rmse: 0.10856 | val_1_rmse: 0.11034 |  0:02:06s
epoch 86 | loss: 0.0117  | val_0_rmse: 0.10302 | val_1_rmse: 0.10464 |  0:02:08s
epoch 87 | loss: 0.01211 | val_0_rmse: 0.10673 | val_1_rmse: 0.10883 |  0:02:09s
epoch 88 | loss: 0.01198 | val_0_rmse: 0.1069  | val_1_rmse: 0.10912 |  0:02:11s
epoch 89 | loss: 0.01271 | val_0_rmse: 0.10387 | val_1_rmse: 0.10601 |  0:02:12s
epoch 90 | loss: 0.01197 | val_0_rmse: 0.11381 | val_1_rmse: 0.11729 |  0:02:14s
epoch 91 | loss: 0.01261 | val_0_rmse: 0.10381 | val_1_rmse: 0.10721 |  0:02:15s
epoch 92 | loss: 0.01248 | val_0_rmse: 0.1065  | val_1_rmse: 0.10824 |  0:02:16s
epoch 93 | loss: 0.0121  | val_0_rmse: 0.10992 | val_1_rmse: 0.11256 |  0:02:18s
epoch 94 | loss: 0.01325 | val_0_rmse: 0.11228 | val_1_rmse: 0.11315 |  0:02:19s
epoch 95 | loss: 0.01289 | val_0_rmse: 0.1144  | val_1_rmse: 0.11767 |  0:02:21s
epoch 96 | loss: 0.01251 | val_0_rmse: 0.11521 | val_1_rmse: 0.11623 |  0:02:22s
epoch 97 | loss: 0.01216 | val_0_rmse: 0.11173 | val_1_rmse: 0.11319 |  0:02:24s
epoch 98 | loss: 0.01216 | val_0_rmse: 0.10432 | val_1_rmse: 0.10601 |  0:02:25s
epoch 99 | loss: 0.01203 | val_0_rmse: 0.10383 | val_1_rmse: 0.10557 |  0:02:27s
epoch 100| loss: 0.01206 | val_0_rmse: 0.10317 | val_1_rmse: 0.10484 |  0:02:28s
epoch 101| loss: 0.01169 | val_0_rmse: 0.10296 | val_1_rmse: 0.10501 |  0:02:30s
epoch 102| loss: 0.01164 | val_0_rmse: 0.10579 | val_1_rmse: 0.10707 |  0:02:31s
epoch 103| loss: 0.01201 | val_0_rmse: 0.10573 | val_1_rmse: 0.10775 |  0:02:33s
epoch 104| loss: 0.01259 | val_0_rmse: 0.10617 | val_1_rmse: 0.10756 |  0:02:34s
epoch 105| loss: 0.01213 | val_0_rmse: 0.10346 | val_1_rmse: 0.10549 |  0:02:35s
epoch 106| loss: 0.01174 | val_0_rmse: 0.10438 | val_1_rmse: 0.10613 |  0:02:37s
epoch 107| loss: 0.01199 | val_0_rmse: 0.10732 | val_1_rmse: 0.10924 |  0:02:38s
epoch 108| loss: 0.01182 | val_0_rmse: 0.10477 | val_1_rmse: 0.10615 |  0:02:40s
epoch 109| loss: 0.01167 | val_0_rmse: 0.10316 | val_1_rmse: 0.1054  |  0:02:41s
epoch 110| loss: 0.01176 | val_0_rmse: 0.10238 | val_1_rmse: 0.10425 |  0:02:43s
epoch 111| loss: 0.01178 | val_0_rmse: 0.10642 | val_1_rmse: 0.10802 |  0:02:44s
epoch 112| loss: 0.01197 | val_0_rmse: 0.10552 | val_1_rmse: 0.108   |  0:02:46s
epoch 113| loss: 0.01188 | val_0_rmse: 0.10498 | val_1_rmse: 0.10747 |  0:02:47s
epoch 114| loss: 0.01184 | val_0_rmse: 0.10668 | val_1_rmse: 0.10885 |  0:02:49s
epoch 115| loss: 0.01175 | val_0_rmse: 0.10251 | val_1_rmse: 0.10447 |  0:02:50s
epoch 116| loss: 0.01186 | val_0_rmse: 0.10531 | val_1_rmse: 0.10785 |  0:02:52s
epoch 117| loss: 0.01173 | val_0_rmse: 0.10379 | val_1_rmse: 0.10513 |  0:02:53s
epoch 118| loss: 0.01172 | val_0_rmse: 0.10847 | val_1_rmse: 0.10939 |  0:02:55s
epoch 119| loss: 0.0118  | val_0_rmse: 0.1031  | val_1_rmse: 0.10501 |  0:02:56s
epoch 120| loss: 0.01165 | val_0_rmse: 0.10311 | val_1_rmse: 0.10545 |  0:02:57s
epoch 121| loss: 0.01167 | val_0_rmse: 0.10503 | val_1_rmse: 0.1078  |  0:02:59s
epoch 122| loss: 0.01152 | val_0_rmse: 0.10271 | val_1_rmse: 0.10435 |  0:03:00s
epoch 123| loss: 0.01193 | val_0_rmse: 0.1109  | val_1_rmse: 0.11382 |  0:03:02s
epoch 124| loss: 0.01195 | val_0_rmse: 0.10572 | val_1_rmse: 0.10702 |  0:03:03s
epoch 125| loss: 0.01216 | val_0_rmse: 0.10487 | val_1_rmse: 0.10667 |  0:03:05s
epoch 126| loss: 0.01176 | val_0_rmse: 0.10742 | val_1_rmse: 0.1083  |  0:03:06s
epoch 127| loss: 0.01154 | val_0_rmse: 0.10256 | val_1_rmse: 0.10404 |  0:03:08s
epoch 128| loss: 0.01142 | val_0_rmse: 0.10382 | val_1_rmse: 0.10505 |  0:03:09s
epoch 129| loss: 0.01181 | val_0_rmse: 0.10225 | val_1_rmse: 0.10382 |  0:03:11s
epoch 130| loss: 0.01153 | val_0_rmse: 0.10273 | val_1_rmse: 0.10417 |  0:03:12s
epoch 131| loss: 0.01181 | val_0_rmse: 0.10563 | val_1_rmse: 0.10826 |  0:03:14s
epoch 132| loss: 0.01169 | val_0_rmse: 0.10706 | val_1_rmse: 0.10991 |  0:03:15s
epoch 133| loss: 0.01172 | val_0_rmse: 0.10175 | val_1_rmse: 0.10367 |  0:03:17s
epoch 134| loss: 0.01159 | val_0_rmse: 0.10214 | val_1_rmse: 0.10374 |  0:03:18s
epoch 135| loss: 0.01155 | val_0_rmse: 0.10294 | val_1_rmse: 0.10494 |  0:03:19s
epoch 136| loss: 0.01232 | val_0_rmse: 0.10914 | val_1_rmse: 0.11162 |  0:03:21s
epoch 137| loss: 0.01198 | val_0_rmse: 0.10497 | val_1_rmse: 0.10739 |  0:03:22s
epoch 138| loss: 0.01145 | val_0_rmse: 0.10341 | val_1_rmse: 0.10469 |  0:03:24s
epoch 139| loss: 0.01178 | val_0_rmse: 0.12229 | val_1_rmse: 0.12529 |  0:03:25s
epoch 140| loss: 0.01203 | val_0_rmse: 0.10434 | val_1_rmse: 0.10599 |  0:03:27s
epoch 141| loss: 0.0117  | val_0_rmse: 0.10934 | val_1_rmse: 0.11109 |  0:03:28s
epoch 142| loss: 0.01144 | val_0_rmse: 0.10567 | val_1_rmse: 0.10873 |  0:03:30s
epoch 143| loss: 0.01167 | val_0_rmse: 0.10391 | val_1_rmse: 0.10564 |  0:03:31s
epoch 144| loss: 0.01249 | val_0_rmse: 0.11325 | val_1_rmse: 0.11558 |  0:03:33s
epoch 145| loss: 0.0122  | val_0_rmse: 0.10878 | val_1_rmse: 0.1089  |  0:03:34s
epoch 146| loss: 0.01222 | val_0_rmse: 0.10558 | val_1_rmse: 0.10687 |  0:03:36s
epoch 147| loss: 0.01196 | val_0_rmse: 0.10223 | val_1_rmse: 0.10341 |  0:03:37s
epoch 148| loss: 0.01179 | val_0_rmse: 0.1038  | val_1_rmse: 0.10513 |  0:03:39s
epoch 149| loss: 0.01193 | val_0_rmse: 0.10874 | val_1_rmse: 0.10884 |  0:03:40s
Stop training because you reached max_epochs = 150 with best_epoch = 147 and best_val_1_rmse = 0.10341
Best weights from best epoch are automatically used!
ended training at: 04:27:55
Feature importance:
[('Area', 0.35323386137804386), ('Baths', 0.05787385006294192), ('Beds', 2.2914301468336207e-05), ('Latitude', 0.25731306032727036), ('Longitude', 0.13584317309830776), ('Month', 0.0), ('Year', 0.19571314083196775)]
Mean squared error is of 6808170775.698
Mean absolute error:56836.780627200074
MAPE:0.18592527108752965
R2 score:0.7007127265714244
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:27:56
epoch 0  | loss: 0.14373 | val_0_rmse: 0.18935 | val_1_rmse: 0.19386 |  0:00:01s
epoch 1  | loss: 0.02638 | val_0_rmse: 0.17995 | val_1_rmse: 0.18429 |  0:00:02s
epoch 2  | loss: 0.02191 | val_0_rmse: 0.18574 | val_1_rmse: 0.18966 |  0:00:04s
epoch 3  | loss: 0.01861 | val_0_rmse: 0.19781 | val_1_rmse: 0.20024 |  0:00:05s
epoch 4  | loss: 0.0173  | val_0_rmse: 0.18571 | val_1_rmse: 0.18891 |  0:00:07s
epoch 5  | loss: 0.01653 | val_0_rmse: 0.19271 | val_1_rmse: 0.19516 |  0:00:08s
epoch 6  | loss: 0.01614 | val_0_rmse: 0.18958 | val_1_rmse: 0.19146 |  0:00:10s
epoch 7  | loss: 0.01575 | val_0_rmse: 0.19562 | val_1_rmse: 0.19637 |  0:00:11s
epoch 8  | loss: 0.0154  | val_0_rmse: 0.17381 | val_1_rmse: 0.17542 |  0:00:13s
epoch 9  | loss: 0.01463 | val_0_rmse: 0.17909 | val_1_rmse: 0.18039 |  0:00:14s
epoch 10 | loss: 0.01458 | val_0_rmse: 0.16411 | val_1_rmse: 0.16618 |  0:00:16s
epoch 11 | loss: 0.01491 | val_0_rmse: 0.16699 | val_1_rmse: 0.16896 |  0:00:17s
epoch 12 | loss: 0.01488 | val_0_rmse: 0.16512 | val_1_rmse: 0.1664  |  0:00:18s
epoch 13 | loss: 0.01445 | val_0_rmse: 0.17408 | val_1_rmse: 0.17482 |  0:00:20s
epoch 14 | loss: 0.01423 | val_0_rmse: 0.15041 | val_1_rmse: 0.15222 |  0:00:21s
epoch 15 | loss: 0.01402 | val_0_rmse: 0.13568 | val_1_rmse: 0.13917 |  0:00:23s
epoch 16 | loss: 0.01419 | val_0_rmse: 0.13386 | val_1_rmse: 0.13539 |  0:00:24s
epoch 17 | loss: 0.0137  | val_0_rmse: 0.15143 | val_1_rmse: 0.15282 |  0:00:26s
epoch 18 | loss: 0.01377 | val_0_rmse: 0.1259  | val_1_rmse: 0.12819 |  0:00:27s
epoch 19 | loss: 0.0136  | val_0_rmse: 0.12141 | val_1_rmse: 0.12399 |  0:00:29s
epoch 20 | loss: 0.01424 | val_0_rmse: 0.12316 | val_1_rmse: 0.12586 |  0:00:30s
epoch 21 | loss: 0.01414 | val_0_rmse: 0.11697 | val_1_rmse: 0.12041 |  0:00:32s
epoch 22 | loss: 0.0135  | val_0_rmse: 0.11391 | val_1_rmse: 0.11702 |  0:00:33s
epoch 23 | loss: 0.01326 | val_0_rmse: 0.11116 | val_1_rmse: 0.11465 |  0:00:35s
epoch 24 | loss: 0.0135  | val_0_rmse: 0.11033 | val_1_rmse: 0.11319 |  0:00:36s
epoch 25 | loss: 0.01328 | val_0_rmse: 0.11032 | val_1_rmse: 0.1135  |  0:00:38s
epoch 26 | loss: 0.01291 | val_0_rmse: 0.11309 | val_1_rmse: 0.11629 |  0:00:39s
epoch 27 | loss: 0.0133  | val_0_rmse: 0.10826 | val_1_rmse: 0.11168 |  0:00:41s
epoch 28 | loss: 0.0129  | val_0_rmse: 0.10864 | val_1_rmse: 0.11141 |  0:00:42s
epoch 29 | loss: 0.01281 | val_0_rmse: 0.10783 | val_1_rmse: 0.11087 |  0:00:43s
epoch 30 | loss: 0.01288 | val_0_rmse: 0.1067  | val_1_rmse: 0.10961 |  0:00:45s
epoch 31 | loss: 0.01261 | val_0_rmse: 0.10588 | val_1_rmse: 0.1083  |  0:00:46s
epoch 32 | loss: 0.01266 | val_0_rmse: 0.10971 | val_1_rmse: 0.1124  |  0:00:48s
epoch 33 | loss: 0.01314 | val_0_rmse: 0.10824 | val_1_rmse: 0.11125 |  0:00:49s
epoch 34 | loss: 0.01297 | val_0_rmse: 0.11059 | val_1_rmse: 0.11279 |  0:00:51s
epoch 35 | loss: 0.0132  | val_0_rmse: 0.10836 | val_1_rmse: 0.11155 |  0:00:52s
epoch 36 | loss: 0.01303 | val_0_rmse: 0.1113  | val_1_rmse: 0.11462 |  0:00:54s
epoch 37 | loss: 0.01292 | val_0_rmse: 0.10984 | val_1_rmse: 0.11298 |  0:00:55s
epoch 38 | loss: 0.0125  | val_0_rmse: 0.10567 | val_1_rmse: 0.10833 |  0:00:57s
epoch 39 | loss: 0.0124  | val_0_rmse: 0.10569 | val_1_rmse: 0.10799 |  0:00:58s
epoch 40 | loss: 0.01234 | val_0_rmse: 0.10714 | val_1_rmse: 0.1094  |  0:00:59s
epoch 41 | loss: 0.01296 | val_0_rmse: 0.10972 | val_1_rmse: 0.11143 |  0:01:01s
epoch 42 | loss: 0.01273 | val_0_rmse: 0.10941 | val_1_rmse: 0.11245 |  0:01:02s
epoch 43 | loss: 0.01258 | val_0_rmse: 0.10664 | val_1_rmse: 0.10967 |  0:01:04s
epoch 44 | loss: 0.01252 | val_0_rmse: 0.10971 | val_1_rmse: 0.11237 |  0:01:05s
epoch 45 | loss: 0.01237 | val_0_rmse: 0.10623 | val_1_rmse: 0.10764 |  0:01:07s
epoch 46 | loss: 0.01245 | val_0_rmse: 0.1076  | val_1_rmse: 0.11029 |  0:01:08s
epoch 47 | loss: 0.01238 | val_0_rmse: 0.10569 | val_1_rmse: 0.10796 |  0:01:10s
epoch 48 | loss: 0.01267 | val_0_rmse: 0.1053  | val_1_rmse: 0.10815 |  0:01:11s
epoch 49 | loss: 0.01226 | val_0_rmse: 0.10851 | val_1_rmse: 0.11148 |  0:01:13s
epoch 50 | loss: 0.01307 | val_0_rmse: 0.1125  | val_1_rmse: 0.11426 |  0:01:14s
epoch 51 | loss: 0.013   | val_0_rmse: 0.11857 | val_1_rmse: 0.12155 |  0:01:16s
epoch 52 | loss: 0.01294 | val_0_rmse: 0.10688 | val_1_rmse: 0.10966 |  0:01:17s
epoch 53 | loss: 0.01261 | val_0_rmse: 0.10911 | val_1_rmse: 0.11128 |  0:01:18s
epoch 54 | loss: 0.01258 | val_0_rmse: 0.10951 | val_1_rmse: 0.11259 |  0:01:20s
epoch 55 | loss: 0.01309 | val_0_rmse: 0.1069  | val_1_rmse: 0.10968 |  0:01:21s
epoch 56 | loss: 0.01272 | val_0_rmse: 0.10735 | val_1_rmse: 0.11071 |  0:01:23s
epoch 57 | loss: 0.0129  | val_0_rmse: 0.1147  | val_1_rmse: 0.11769 |  0:01:24s
epoch 58 | loss: 0.01368 | val_0_rmse: 0.10812 | val_1_rmse: 0.11067 |  0:01:26s
epoch 59 | loss: 0.01271 | val_0_rmse: 0.10777 | val_1_rmse: 0.11142 |  0:01:27s
epoch 60 | loss: 0.0122  | val_0_rmse: 0.10672 | val_1_rmse: 0.11004 |  0:01:29s
epoch 61 | loss: 0.01261 | val_0_rmse: 0.10831 | val_1_rmse: 0.11088 |  0:01:30s
epoch 62 | loss: 0.01294 | val_0_rmse: 0.10695 | val_1_rmse: 0.1104  |  0:01:32s
epoch 63 | loss: 0.01244 | val_0_rmse: 0.11144 | val_1_rmse: 0.11481 |  0:01:33s
epoch 64 | loss: 0.01243 | val_0_rmse: 0.10643 | val_1_rmse: 0.10926 |  0:01:35s
epoch 65 | loss: 0.01242 | val_0_rmse: 0.10679 | val_1_rmse: 0.1101  |  0:01:36s
epoch 66 | loss: 0.01229 | val_0_rmse: 0.10739 | val_1_rmse: 0.1103  |  0:01:38s
epoch 67 | loss: 0.01256 | val_0_rmse: 0.1053  | val_1_rmse: 0.10818 |  0:01:39s
epoch 68 | loss: 0.01244 | val_0_rmse: 0.10692 | val_1_rmse: 0.11053 |  0:01:41s
epoch 69 | loss: 0.01248 | val_0_rmse: 0.10897 | val_1_rmse: 0.11193 |  0:01:42s
epoch 70 | loss: 0.01285 | val_0_rmse: 0.11188 | val_1_rmse: 0.11491 |  0:01:43s
epoch 71 | loss: 0.01246 | val_0_rmse: 0.1099  | val_1_rmse: 0.11327 |  0:01:45s
epoch 72 | loss: 0.01284 | val_0_rmse: 0.11156 | val_1_rmse: 0.11551 |  0:01:46s
epoch 73 | loss: 0.01315 | val_0_rmse: 0.10622 | val_1_rmse: 0.10935 |  0:01:48s
epoch 74 | loss: 0.01249 | val_0_rmse: 0.10524 | val_1_rmse: 0.1084  |  0:01:49s
epoch 75 | loss: 0.01229 | val_0_rmse: 0.10462 | val_1_rmse: 0.10757 |  0:01:51s
epoch 76 | loss: 0.0126  | val_0_rmse: 0.10494 | val_1_rmse: 0.10717 |  0:01:52s
epoch 77 | loss: 0.01267 | val_0_rmse: 0.10391 | val_1_rmse: 0.10724 |  0:01:54s
epoch 78 | loss: 0.01205 | val_0_rmse: 0.10721 | val_1_rmse: 0.10997 |  0:01:55s
epoch 79 | loss: 0.01204 | val_0_rmse: 0.10484 | val_1_rmse: 0.10742 |  0:01:57s
epoch 80 | loss: 0.01198 | val_0_rmse: 0.10402 | val_1_rmse: 0.10738 |  0:01:58s
epoch 81 | loss: 0.0121  | val_0_rmse: 0.10539 | val_1_rmse: 0.10832 |  0:01:59s
epoch 82 | loss: 0.01217 | val_0_rmse: 0.10615 | val_1_rmse: 0.10921 |  0:02:01s
epoch 83 | loss: 0.01194 | val_0_rmse: 0.1039  | val_1_rmse: 0.10694 |  0:02:02s
epoch 84 | loss: 0.01204 | val_0_rmse: 0.10615 | val_1_rmse: 0.10897 |  0:02:04s
epoch 85 | loss: 0.01218 | val_0_rmse: 0.10458 | val_1_rmse: 0.10766 |  0:02:05s
epoch 86 | loss: 0.01207 | val_0_rmse: 0.10275 | val_1_rmse: 0.10645 |  0:02:07s
epoch 87 | loss: 0.01203 | val_0_rmse: 0.10528 | val_1_rmse: 0.10778 |  0:02:08s
epoch 88 | loss: 0.01243 | val_0_rmse: 0.10426 | val_1_rmse: 0.1077  |  0:02:10s
epoch 89 | loss: 0.01229 | val_0_rmse: 0.11177 | val_1_rmse: 0.11464 |  0:02:11s
epoch 90 | loss: 0.0123  | val_0_rmse: 0.10559 | val_1_rmse: 0.10784 |  0:02:13s
epoch 91 | loss: 0.01216 | val_0_rmse: 0.10806 | val_1_rmse: 0.1117  |  0:02:14s
epoch 92 | loss: 0.01234 | val_0_rmse: 0.1077  | val_1_rmse: 0.11017 |  0:02:16s
epoch 93 | loss: 0.01223 | val_0_rmse: 0.10371 | val_1_rmse: 0.10722 |  0:02:17s
epoch 94 | loss: 0.0118  | val_0_rmse: 0.10547 | val_1_rmse: 0.10857 |  0:02:18s
epoch 95 | loss: 0.01202 | val_0_rmse: 0.10391 | val_1_rmse: 0.1077  |  0:02:20s
epoch 96 | loss: 0.01204 | val_0_rmse: 0.12275 | val_1_rmse: 0.12566 |  0:02:21s
epoch 97 | loss: 0.01346 | val_0_rmse: 0.11669 | val_1_rmse: 0.11961 |  0:02:23s
epoch 98 | loss: 0.01285 | val_0_rmse: 0.10701 | val_1_rmse: 0.11067 |  0:02:24s
epoch 99 | loss: 0.01251 | val_0_rmse: 0.11702 | val_1_rmse: 0.12091 |  0:02:26s
epoch 100| loss: 0.01369 | val_0_rmse: 0.10991 | val_1_rmse: 0.11352 |  0:02:27s
epoch 101| loss: 0.01334 | val_0_rmse: 0.10773 | val_1_rmse: 0.11124 |  0:02:29s
epoch 102| loss: 0.0125  | val_0_rmse: 0.10734 | val_1_rmse: 0.10959 |  0:02:30s
epoch 103| loss: 0.01286 | val_0_rmse: 0.10796 | val_1_rmse: 0.11049 |  0:02:32s
epoch 104| loss: 0.01284 | val_0_rmse: 0.10651 | val_1_rmse: 0.10969 |  0:02:33s
epoch 105| loss: 0.01309 | val_0_rmse: 0.11259 | val_1_rmse: 0.11471 |  0:02:35s
epoch 106| loss: 0.01275 | val_0_rmse: 0.10941 | val_1_rmse: 0.11134 |  0:02:36s
epoch 107| loss: 0.01237 | val_0_rmse: 0.10932 | val_1_rmse: 0.1113  |  0:02:37s
epoch 108| loss: 0.01258 | val_0_rmse: 0.1063  | val_1_rmse: 0.1099  |  0:02:39s
epoch 109| loss: 0.01228 | val_0_rmse: 0.10573 | val_1_rmse: 0.10897 |  0:02:40s
epoch 110| loss: 0.01253 | val_0_rmse: 0.10702 | val_1_rmse: 0.10901 |  0:02:42s
epoch 111| loss: 0.01307 | val_0_rmse: 0.10615 | val_1_rmse: 0.10913 |  0:02:43s
epoch 112| loss: 0.01258 | val_0_rmse: 0.11025 | val_1_rmse: 0.11296 |  0:02:45s
epoch 113| loss: 0.01244 | val_0_rmse: 0.1088  | val_1_rmse: 0.11074 |  0:02:46s
epoch 114| loss: 0.01225 | val_0_rmse: 0.10568 | val_1_rmse: 0.10783 |  0:02:48s
epoch 115| loss: 0.0123  | val_0_rmse: 0.10554 | val_1_rmse: 0.10778 |  0:02:49s
epoch 116| loss: 0.01213 | val_0_rmse: 0.10507 | val_1_rmse: 0.10727 |  0:02:51s

Early stopping occured at epoch 116 with best_epoch = 86 and best_val_1_rmse = 0.10645
Best weights from best epoch are automatically used!
ended training at: 04:30:47
Feature importance:
[('Area', 0.3396156140122472), ('Baths', 6.912298480762161e-05), ('Beds', 0.1814910915684148), ('Latitude', 0.16578193071728303), ('Longitude', 0.15937708701822015), ('Month', 0.10397711232125913), ('Year', 0.04968804137776802)]
Mean squared error is of 6823124905.455184
Mean absolute error:57316.585859793464
MAPE:0.18792573160682638
R2 score:0.6982085101617163
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:30:47
epoch 0  | loss: 0.05275 | val_0_rmse: 0.20184 | val_1_rmse: 0.20173 |  0:00:05s
epoch 1  | loss: 0.02235 | val_0_rmse: 0.18949 | val_1_rmse: 0.18946 |  0:00:11s
epoch 2  | loss: 0.02174 | val_0_rmse: 0.18412 | val_1_rmse: 0.18389 |  0:00:16s
epoch 3  | loss: 0.02154 | val_0_rmse: 0.15017 | val_1_rmse: 0.15008 |  0:00:22s
epoch 4  | loss: 0.02043 | val_0_rmse: 0.15163 | val_1_rmse: 0.15141 |  0:00:27s
epoch 5  | loss: 0.01927 | val_0_rmse: 0.15301 | val_1_rmse: 0.15317 |  0:00:33s
epoch 6  | loss: 0.01927 | val_0_rmse: 0.15016 | val_1_rmse: 0.14966 |  0:00:38s
epoch 7  | loss: 0.01814 | val_0_rmse: 0.13526 | val_1_rmse: 0.13531 |  0:00:44s
epoch 8  | loss: 0.01748 | val_0_rmse: 0.13425 | val_1_rmse: 0.13397 |  0:00:49s
epoch 9  | loss: 0.01722 | val_0_rmse: 0.12801 | val_1_rmse: 0.12818 |  0:00:55s
epoch 10 | loss: 0.01738 | val_0_rmse: 0.13164 | val_1_rmse: 0.13191 |  0:01:00s
epoch 11 | loss: 0.01703 | val_0_rmse: 0.12781 | val_1_rmse: 0.1283  |  0:01:06s
epoch 12 | loss: 0.01679 | val_0_rmse: 0.12514 | val_1_rmse: 0.12584 |  0:01:11s
epoch 13 | loss: 0.01653 | val_0_rmse: 0.12583 | val_1_rmse: 0.12642 |  0:01:17s
epoch 14 | loss: 0.01647 | val_0_rmse: 0.12768 | val_1_rmse: 0.12827 |  0:01:22s
epoch 15 | loss: 0.01653 | val_0_rmse: 0.13431 | val_1_rmse: 0.13434 |  0:01:28s
epoch 16 | loss: 0.01625 | val_0_rmse: 0.12512 | val_1_rmse: 0.1255  |  0:01:34s
epoch 17 | loss: 0.01651 | val_0_rmse: 0.13197 | val_1_rmse: 0.13285 |  0:01:39s
epoch 18 | loss: 0.0165  | val_0_rmse: 0.13125 | val_1_rmse: 0.1319  |  0:01:45s
epoch 19 | loss: 0.01656 | val_0_rmse: 0.12355 | val_1_rmse: 0.12392 |  0:01:50s
epoch 20 | loss: 0.01612 | val_0_rmse: 0.12931 | val_1_rmse: 0.12944 |  0:01:56s
epoch 21 | loss: 0.0165  | val_0_rmse: 0.17073 | val_1_rmse: 0.1706  |  0:02:01s
epoch 22 | loss: 0.01646 | val_0_rmse: 0.1365  | val_1_rmse: 0.13728 |  0:02:07s
epoch 23 | loss: 0.01599 | val_0_rmse: 0.13627 | val_1_rmse: 0.13636 |  0:02:12s
epoch 24 | loss: 0.0159  | val_0_rmse: 0.12861 | val_1_rmse: 0.1293  |  0:02:18s
epoch 25 | loss: 0.01602 | val_0_rmse: 0.14082 | val_1_rmse: 0.14115 |  0:02:23s
epoch 26 | loss: 0.01604 | val_0_rmse: 0.12399 | val_1_rmse: 0.12422 |  0:02:29s
epoch 27 | loss: 0.01586 | val_0_rmse: 0.12225 | val_1_rmse: 0.12265 |  0:02:34s
epoch 28 | loss: 0.01588 | val_0_rmse: 0.12713 | val_1_rmse: 0.12721 |  0:02:40s
epoch 29 | loss: 0.01591 | val_0_rmse: 0.12382 | val_1_rmse: 0.12399 |  0:02:45s
epoch 30 | loss: 0.01587 | val_0_rmse: 0.14003 | val_1_rmse: 0.14027 |  0:02:51s
epoch 31 | loss: 0.01594 | val_0_rmse: 0.12846 | val_1_rmse: 0.12868 |  0:02:56s
epoch 32 | loss: 0.01572 | val_0_rmse: 0.12303 | val_1_rmse: 0.12325 |  0:03:02s
epoch 33 | loss: 0.01565 | val_0_rmse: 0.12215 | val_1_rmse: 0.12223 |  0:03:07s
epoch 34 | loss: 0.01566 | val_0_rmse: 0.12584 | val_1_rmse: 0.12619 |  0:03:13s
epoch 35 | loss: 0.01562 | val_0_rmse: 0.12361 | val_1_rmse: 0.12397 |  0:03:19s
epoch 36 | loss: 0.01556 | val_0_rmse: 0.12791 | val_1_rmse: 0.12806 |  0:03:24s
epoch 37 | loss: 0.01574 | val_0_rmse: 0.12927 | val_1_rmse: 0.12933 |  0:03:30s
epoch 38 | loss: 0.01567 | val_0_rmse: 0.12294 | val_1_rmse: 0.12329 |  0:03:35s
epoch 39 | loss: 0.0157  | val_0_rmse: 0.12232 | val_1_rmse: 0.12249 |  0:03:41s
epoch 40 | loss: 0.01561 | val_0_rmse: 0.12066 | val_1_rmse: 0.12115 |  0:03:46s
epoch 41 | loss: 0.01547 | val_0_rmse: 0.13147 | val_1_rmse: 0.13188 |  0:03:52s
epoch 42 | loss: 0.01546 | val_0_rmse: 0.12213 | val_1_rmse: 0.12241 |  0:03:57s
epoch 43 | loss: 0.01593 | val_0_rmse: 0.14313 | val_1_rmse: 0.14365 |  0:04:03s
epoch 44 | loss: 0.01593 | val_0_rmse: 0.12581 | val_1_rmse: 0.12606 |  0:04:08s
epoch 45 | loss: 0.01578 | val_0_rmse: 0.12529 | val_1_rmse: 0.12553 |  0:04:14s
epoch 46 | loss: 0.01543 | val_0_rmse: 0.12194 | val_1_rmse: 0.12192 |  0:04:19s
epoch 47 | loss: 0.01566 | val_0_rmse: 0.13343 | val_1_rmse: 0.13332 |  0:04:25s
epoch 48 | loss: 0.01525 | val_0_rmse: 0.15899 | val_1_rmse: 0.15885 |  0:04:30s
epoch 49 | loss: 0.0155  | val_0_rmse: 0.13864 | val_1_rmse: 0.13889 |  0:04:36s
epoch 50 | loss: 0.01552 | val_0_rmse: 0.12503 | val_1_rmse: 0.1256  |  0:04:41s
epoch 51 | loss: 0.01552 | val_0_rmse: 0.12216 | val_1_rmse: 0.12235 |  0:04:47s
epoch 52 | loss: 0.01547 | val_0_rmse: 0.12805 | val_1_rmse: 0.12851 |  0:04:52s
epoch 53 | loss: 0.01557 | val_0_rmse: 0.13493 | val_1_rmse: 0.13506 |  0:04:58s
epoch 54 | loss: 0.01554 | val_0_rmse: 0.13172 | val_1_rmse: 0.13252 |  0:05:03s
epoch 55 | loss: 0.01551 | val_0_rmse: 0.16558 | val_1_rmse: 0.16566 |  0:05:09s
epoch 56 | loss: 0.0154  | val_0_rmse: 0.1228  | val_1_rmse: 0.12325 |  0:05:14s
epoch 57 | loss: 0.01544 | val_0_rmse: 0.12507 | val_1_rmse: 0.1256  |  0:05:20s
epoch 58 | loss: 0.01535 | val_0_rmse: 0.12137 | val_1_rmse: 0.12143 |  0:05:25s
epoch 59 | loss: 0.01538 | val_0_rmse: 0.12199 | val_1_rmse: 0.1225  |  0:05:31s
epoch 60 | loss: 0.01562 | val_0_rmse: 0.12575 | val_1_rmse: 0.12657 |  0:05:37s
epoch 61 | loss: 0.01627 | val_0_rmse: 0.12439 | val_1_rmse: 0.12499 |  0:05:42s
epoch 62 | loss: 0.01545 | val_0_rmse: 0.12236 | val_1_rmse: 0.1226  |  0:05:48s
epoch 63 | loss: 0.01539 | val_0_rmse: 0.12208 | val_1_rmse: 0.12207 |  0:05:53s
epoch 64 | loss: 0.01528 | val_0_rmse: 0.12042 | val_1_rmse: 0.12041 |  0:05:59s
epoch 65 | loss: 0.0153  | val_0_rmse: 0.13976 | val_1_rmse: 0.14011 |  0:06:04s
epoch 66 | loss: 0.01532 | val_0_rmse: 0.13071 | val_1_rmse: 0.13135 |  0:06:10s
epoch 67 | loss: 0.01524 | val_0_rmse: 0.13317 | val_1_rmse: 0.13353 |  0:06:15s
epoch 68 | loss: 0.01556 | val_0_rmse: 0.12533 | val_1_rmse: 0.126   |  0:06:21s
epoch 69 | loss: 0.01547 | val_0_rmse: 0.1254  | val_1_rmse: 0.12613 |  0:06:26s
epoch 70 | loss: 0.01563 | val_0_rmse: 0.12033 | val_1_rmse: 0.12058 |  0:06:32s
epoch 71 | loss: 0.01563 | val_0_rmse: 0.12762 | val_1_rmse: 0.12798 |  0:06:37s
epoch 72 | loss: 0.01546 | val_0_rmse: 0.12308 | val_1_rmse: 0.12337 |  0:06:43s
epoch 73 | loss: 0.0156  | val_0_rmse: 0.12156 | val_1_rmse: 0.12219 |  0:06:48s
epoch 74 | loss: 0.01526 | val_0_rmse: 0.12189 | val_1_rmse: 0.12226 |  0:06:54s
epoch 75 | loss: 0.01538 | val_0_rmse: 0.12245 | val_1_rmse: 0.12286 |  0:06:59s
epoch 76 | loss: 0.01523 | val_0_rmse: 0.13264 | val_1_rmse: 0.1332  |  0:07:05s
epoch 77 | loss: 0.01556 | val_0_rmse: 0.12041 | val_1_rmse: 0.12067 |  0:07:10s
epoch 78 | loss: 0.01549 | val_0_rmse: 0.15787 | val_1_rmse: 0.1573  |  0:07:16s
epoch 79 | loss: 0.01565 | val_0_rmse: 0.12097 | val_1_rmse: 0.12144 |  0:07:21s
epoch 80 | loss: 0.01529 | val_0_rmse: 0.13028 | val_1_rmse: 0.13104 |  0:07:27s
epoch 81 | loss: 0.01503 | val_0_rmse: 0.1212  | val_1_rmse: 0.1217  |  0:07:32s
epoch 82 | loss: 0.01498 | val_0_rmse: 0.12936 | val_1_rmse: 0.12934 |  0:07:38s
epoch 83 | loss: 0.01518 | val_0_rmse: 0.11827 | val_1_rmse: 0.11862 |  0:07:43s
epoch 84 | loss: 0.01523 | val_0_rmse: 0.12567 | val_1_rmse: 0.12649 |  0:07:49s
epoch 85 | loss: 0.01506 | val_0_rmse: 0.12288 | val_1_rmse: 0.12336 |  0:07:54s
epoch 86 | loss: 0.01514 | val_0_rmse: 0.12518 | val_1_rmse: 0.12569 |  0:08:00s
epoch 87 | loss: 0.01512 | val_0_rmse: 0.12068 | val_1_rmse: 0.12105 |  0:08:05s
epoch 88 | loss: 0.01504 | val_0_rmse: 0.12127 | val_1_rmse: 0.12171 |  0:08:11s
epoch 89 | loss: 0.01506 | val_0_rmse: 0.12369 | val_1_rmse: 0.12433 |  0:08:16s
epoch 90 | loss: 0.01514 | val_0_rmse: 0.12149 | val_1_rmse: 0.12175 |  0:08:22s
epoch 91 | loss: 0.01492 | val_0_rmse: 0.12362 | val_1_rmse: 0.12413 |  0:08:28s
epoch 92 | loss: 0.01497 | val_0_rmse: 0.1238  | val_1_rmse: 0.12393 |  0:08:34s
epoch 93 | loss: 0.01535 | val_0_rmse: 0.12678 | val_1_rmse: 0.12694 |  0:08:40s
epoch 94 | loss: 0.01525 | val_0_rmse: 0.12904 | val_1_rmse: 0.12942 |  0:08:45s
epoch 95 | loss: 0.01516 | val_0_rmse: 0.12813 | val_1_rmse: 0.12843 |  0:08:51s
epoch 96 | loss: 0.01518 | val_0_rmse: 0.13122 | val_1_rmse: 0.1311  |  0:08:56s
epoch 97 | loss: 0.0149  | val_0_rmse: 0.12452 | val_1_rmse: 0.12535 |  0:09:02s
epoch 98 | loss: 0.01518 | val_0_rmse: 0.13025 | val_1_rmse: 0.13037 |  0:09:07s
epoch 99 | loss: 0.01521 | val_0_rmse: 0.12159 | val_1_rmse: 0.12164 |  0:09:13s
epoch 100| loss: 0.01496 | val_0_rmse: 0.13936 | val_1_rmse: 0.13967 |  0:09:18s
epoch 101| loss: 0.01483 | val_0_rmse: 0.12723 | val_1_rmse: 0.12811 |  0:09:24s
epoch 102| loss: 0.01489 | val_0_rmse: 0.13725 | val_1_rmse: 0.13769 |  0:09:29s
epoch 103| loss: 0.01489 | val_0_rmse: 0.12215 | val_1_rmse: 0.12241 |  0:09:35s
epoch 104| loss: 0.01511 | val_0_rmse: 0.1232  | val_1_rmse: 0.12355 |  0:09:40s
epoch 105| loss: 0.01496 | val_0_rmse: 0.12281 | val_1_rmse: 0.12289 |  0:09:46s
epoch 106| loss: 0.01506 | val_0_rmse: 0.11952 | val_1_rmse: 0.11972 |  0:09:51s
epoch 107| loss: 0.01524 | val_0_rmse: 0.12221 | val_1_rmse: 0.1225  |  0:09:57s
epoch 108| loss: 0.01511 | val_0_rmse: 0.12418 | val_1_rmse: 0.1248  |  0:10:02s
epoch 109| loss: 0.01536 | val_0_rmse: 0.12891 | val_1_rmse: 0.12925 |  0:10:08s
epoch 110| loss: 0.01546 | val_0_rmse: 0.1203  | val_1_rmse: 0.12088 |  0:10:13s
epoch 111| loss: 0.01502 | val_0_rmse: 0.14163 | val_1_rmse: 0.14189 |  0:10:19s
epoch 112| loss: 0.01503 | val_0_rmse: 0.1259  | val_1_rmse: 0.12647 |  0:10:24s
epoch 113| loss: 0.01494 | val_0_rmse: 0.12324 | val_1_rmse: 0.12387 |  0:10:30s

Early stopping occured at epoch 113 with best_epoch = 83 and best_val_1_rmse = 0.11862
Best weights from best epoch are automatically used!
ended training at: 04:41:19
Feature importance:
[('Area', 0.35683202403860526), ('Baths', 0.13819455051953985), ('Beds', 0.0), ('Latitude', 0.22200816077997979), ('Longitude', 0.24680734753457656), ('Month', 0.03615791712729854), ('Year', 0.0)]
Mean squared error is of 2425718323.2385945
Mean absolute error:35783.60441158596
MAPE:0.3589473958710744
R2 score:0.6456906668879367
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:41:21
epoch 0  | loss: 0.05543 | val_0_rmse: 0.21064 | val_1_rmse: 0.21041 |  0:00:05s
epoch 1  | loss: 0.02272 | val_0_rmse: 0.23544 | val_1_rmse: 0.2347  |  0:00:11s
epoch 2  | loss: 0.0223  | val_0_rmse: 0.21553 | val_1_rmse: 0.21447 |  0:00:16s
epoch 3  | loss: 0.02097 | val_0_rmse: 0.1901  | val_1_rmse: 0.18967 |  0:00:22s
epoch 4  | loss: 0.02017 | val_0_rmse: 0.1704  | val_1_rmse: 0.16973 |  0:00:27s
epoch 5  | loss: 0.01952 | val_0_rmse: 0.16015 | val_1_rmse: 0.15961 |  0:00:33s
epoch 6  | loss: 0.01849 | val_0_rmse: 0.13593 | val_1_rmse: 0.13656 |  0:00:38s
epoch 7  | loss: 0.01779 | val_0_rmse: 0.13238 | val_1_rmse: 0.1332  |  0:00:44s
epoch 8  | loss: 0.01664 | val_0_rmse: 0.12846 | val_1_rmse: 0.12925 |  0:00:49s
epoch 9  | loss: 0.01646 | val_0_rmse: 0.12419 | val_1_rmse: 0.12451 |  0:00:55s
epoch 10 | loss: 0.0161  | val_0_rmse: 0.12908 | val_1_rmse: 0.12979 |  0:01:00s
epoch 11 | loss: 0.01614 | val_0_rmse: 0.12601 | val_1_rmse: 0.12632 |  0:01:06s
epoch 12 | loss: 0.01596 | val_0_rmse: 0.12846 | val_1_rmse: 0.12918 |  0:01:11s
epoch 13 | loss: 0.01603 | val_0_rmse: 0.12485 | val_1_rmse: 0.12538 |  0:01:17s
epoch 14 | loss: 0.01588 | val_0_rmse: 0.1224  | val_1_rmse: 0.12268 |  0:01:23s
epoch 15 | loss: 0.01574 | val_0_rmse: 0.13674 | val_1_rmse: 0.13651 |  0:01:28s
epoch 16 | loss: 0.01586 | val_0_rmse: 0.12824 | val_1_rmse: 0.1286  |  0:01:34s
epoch 17 | loss: 0.01585 | val_0_rmse: 0.12432 | val_1_rmse: 0.12443 |  0:01:39s
epoch 18 | loss: 0.01569 | val_0_rmse: 0.12777 | val_1_rmse: 0.12835 |  0:01:45s
epoch 19 | loss: 0.01566 | val_0_rmse: 0.1566  | val_1_rmse: 0.15733 |  0:01:50s
epoch 20 | loss: 0.0158  | val_0_rmse: 0.15642 | val_1_rmse: 0.15744 |  0:01:56s
epoch 21 | loss: 0.01565 | val_0_rmse: 0.14066 | val_1_rmse: 0.14145 |  0:02:01s
epoch 22 | loss: 0.01545 | val_0_rmse: 0.13012 | val_1_rmse: 0.13076 |  0:02:07s
epoch 23 | loss: 0.01538 | val_0_rmse: 0.12597 | val_1_rmse: 0.12643 |  0:02:12s
epoch 24 | loss: 0.01588 | val_0_rmse: 0.1284  | val_1_rmse: 0.12885 |  0:02:18s
epoch 25 | loss: 0.01575 | val_0_rmse: 0.12387 | val_1_rmse: 0.12418 |  0:02:23s
epoch 26 | loss: 0.01605 | val_0_rmse: 0.14245 | val_1_rmse: 0.14288 |  0:02:29s
epoch 27 | loss: 0.01594 | val_0_rmse: 0.1227  | val_1_rmse: 0.12303 |  0:02:35s
epoch 28 | loss: 0.01588 | val_0_rmse: 0.12173 | val_1_rmse: 0.12195 |  0:02:40s
epoch 29 | loss: 0.01576 | val_0_rmse: 0.12495 | val_1_rmse: 0.125   |  0:02:46s
epoch 30 | loss: 0.01583 | val_0_rmse: 0.12688 | val_1_rmse: 0.12736 |  0:02:51s
epoch 31 | loss: 0.01571 | val_0_rmse: 0.12212 | val_1_rmse: 0.12243 |  0:02:57s
epoch 32 | loss: 0.0153  | val_0_rmse: 0.12911 | val_1_rmse: 0.12948 |  0:03:02s
epoch 33 | loss: 0.01557 | val_0_rmse: 0.15233 | val_1_rmse: 0.15283 |  0:03:08s
epoch 34 | loss: 0.01561 | val_0_rmse: 0.12807 | val_1_rmse: 0.12834 |  0:03:13s
epoch 35 | loss: 0.01549 | val_0_rmse: 0.12338 | val_1_rmse: 0.12373 |  0:03:19s
epoch 36 | loss: 0.0153  | val_0_rmse: 0.1409  | val_1_rmse: 0.14146 |  0:03:24s
epoch 37 | loss: 0.01503 | val_0_rmse: 0.14391 | val_1_rmse: 0.14387 |  0:03:30s
epoch 38 | loss: 0.01501 | val_0_rmse: 0.12748 | val_1_rmse: 0.12809 |  0:03:35s
epoch 39 | loss: 0.01527 | val_0_rmse: 0.11937 | val_1_rmse: 0.11926 |  0:03:41s
epoch 40 | loss: 0.01508 | val_0_rmse: 0.15247 | val_1_rmse: 0.15287 |  0:03:46s
epoch 41 | loss: 0.01499 | val_0_rmse: 0.12191 | val_1_rmse: 0.12226 |  0:03:52s
epoch 42 | loss: 0.01519 | val_0_rmse: 0.13024 | val_1_rmse: 0.13076 |  0:03:58s
epoch 43 | loss: 0.01528 | val_0_rmse: 0.12593 | val_1_rmse: 0.12605 |  0:04:03s
epoch 44 | loss: 0.01555 | val_0_rmse: 0.12309 | val_1_rmse: 0.12353 |  0:04:09s
epoch 45 | loss: 0.01543 | val_0_rmse: 0.12147 | val_1_rmse: 0.12196 |  0:04:14s
epoch 46 | loss: 0.01539 | val_0_rmse: 0.13467 | val_1_rmse: 0.13514 |  0:04:20s
epoch 47 | loss: 0.01537 | val_0_rmse: 0.13575 | val_1_rmse: 0.13645 |  0:04:26s
epoch 48 | loss: 0.0151  | val_0_rmse: 0.12482 | val_1_rmse: 0.12501 |  0:04:31s
epoch 49 | loss: 0.01518 | val_0_rmse: 0.12293 | val_1_rmse: 0.12332 |  0:04:37s
epoch 50 | loss: 0.01508 | val_0_rmse: 0.12026 | val_1_rmse: 0.12051 |  0:04:42s
epoch 51 | loss: 0.01511 | val_0_rmse: 0.14155 | val_1_rmse: 0.14193 |  0:04:48s
epoch 52 | loss: 0.01529 | val_0_rmse: 0.14594 | val_1_rmse: 0.14559 |  0:04:53s
epoch 53 | loss: 0.0153  | val_0_rmse: 0.13427 | val_1_rmse: 0.13516 |  0:04:59s
epoch 54 | loss: 0.01524 | val_0_rmse: 0.14273 | val_1_rmse: 0.14334 |  0:05:04s
epoch 55 | loss: 0.0149  | val_0_rmse: 0.14886 | val_1_rmse: 0.14944 |  0:05:10s
epoch 56 | loss: 0.01519 | val_0_rmse: 0.12817 | val_1_rmse: 0.1285  |  0:05:15s
epoch 57 | loss: 0.0151  | val_0_rmse: 0.1241  | val_1_rmse: 0.12462 |  0:05:21s
epoch 58 | loss: 0.01525 | val_0_rmse: 0.15514 | val_1_rmse: 0.15599 |  0:05:26s
epoch 59 | loss: 0.01516 | val_0_rmse: 0.12141 | val_1_rmse: 0.12186 |  0:05:32s
epoch 60 | loss: 0.01509 | val_0_rmse: 0.13258 | val_1_rmse: 0.1327  |  0:05:37s
epoch 61 | loss: 0.01522 | val_0_rmse: 0.11977 | val_1_rmse: 0.12016 |  0:05:43s
epoch 62 | loss: 0.015   | val_0_rmse: 0.13737 | val_1_rmse: 0.13822 |  0:05:48s
epoch 63 | loss: 0.01503 | val_0_rmse: 0.15445 | val_1_rmse: 0.15425 |  0:05:54s
epoch 64 | loss: 0.01512 | val_0_rmse: 0.13043 | val_1_rmse: 0.13094 |  0:05:59s
epoch 65 | loss: 0.01507 | val_0_rmse: 0.12466 | val_1_rmse: 0.12501 |  0:06:05s
epoch 66 | loss: 0.01515 | val_0_rmse: 0.12177 | val_1_rmse: 0.12237 |  0:06:11s
epoch 67 | loss: 0.01474 | val_0_rmse: 0.12512 | val_1_rmse: 0.12583 |  0:06:16s
epoch 68 | loss: 0.01478 | val_0_rmse: 0.15922 | val_1_rmse: 0.15969 |  0:06:22s
epoch 69 | loss: 0.01505 | val_0_rmse: 0.17887 | val_1_rmse: 0.17799 |  0:06:27s

Early stopping occured at epoch 69 with best_epoch = 39 and best_val_1_rmse = 0.11926
Best weights from best epoch are automatically used!
ended training at: 04:47:50
Feature importance:
[('Area', 0.392450449282324), ('Baths', 0.21240169622422475), ('Beds', 3.8562445429788195e-05), ('Latitude', 0.1283992433415936), ('Longitude', 0.23829761539733718), ('Month', 0.010421653158074494), ('Year', 0.0179907801510162)]
Mean squared error is of 2436379141.723077
Mean absolute error:35840.46962760814
MAPE:0.3526117426119269
R2 score:0.642981000361173
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:47:51
epoch 0  | loss: 0.054   | val_0_rmse: 0.18252 | val_1_rmse: 0.18334 |  0:00:05s
epoch 1  | loss: 0.02354 | val_0_rmse: 0.16921 | val_1_rmse: 0.16989 |  0:00:11s
epoch 2  | loss: 0.02323 | val_0_rmse: 0.16325 | val_1_rmse: 0.16385 |  0:00:16s
epoch 3  | loss: 0.02306 | val_0_rmse: 0.1562  | val_1_rmse: 0.15739 |  0:00:22s
epoch 4  | loss: 0.0223  | val_0_rmse: 0.14978 | val_1_rmse: 0.15123 |  0:00:27s
epoch 5  | loss: 0.02221 | val_0_rmse: 0.14535 | val_1_rmse: 0.14647 |  0:00:33s
epoch 6  | loss: 0.02052 | val_0_rmse: 0.14257 | val_1_rmse: 0.14375 |  0:00:39s
epoch 7  | loss: 0.01983 | val_0_rmse: 0.14776 | val_1_rmse: 0.14904 |  0:00:44s
epoch 8  | loss: 0.01958 | val_0_rmse: 0.1431  | val_1_rmse: 0.144   |  0:00:50s
epoch 9  | loss: 0.01897 | val_0_rmse: 0.13424 | val_1_rmse: 0.13557 |  0:00:55s
epoch 10 | loss: 0.0189  | val_0_rmse: 0.13488 | val_1_rmse: 0.13609 |  0:01:01s
epoch 11 | loss: 0.0188  | val_0_rmse: 0.14167 | val_1_rmse: 0.14299 |  0:01:06s
epoch 12 | loss: 0.01861 | val_0_rmse: 0.13572 | val_1_rmse: 0.13669 |  0:01:12s
epoch 13 | loss: 0.01855 | val_0_rmse: 0.13106 | val_1_rmse: 0.13219 |  0:01:17s
epoch 14 | loss: 0.01824 | val_0_rmse: 0.14156 | val_1_rmse: 0.14257 |  0:01:23s
epoch 15 | loss: 0.01838 | val_0_rmse: 0.1307  | val_1_rmse: 0.13238 |  0:01:28s
epoch 16 | loss: 0.01795 | val_0_rmse: 0.13447 | val_1_rmse: 0.13605 |  0:01:34s
epoch 17 | loss: 0.01789 | val_0_rmse: 0.13163 | val_1_rmse: 0.13324 |  0:01:40s
epoch 18 | loss: 0.01777 | val_0_rmse: 0.14692 | val_1_rmse: 0.1475  |  0:01:45s
epoch 19 | loss: 0.01762 | val_0_rmse: 0.12914 | val_1_rmse: 0.13044 |  0:01:51s
epoch 20 | loss: 0.01757 | val_0_rmse: 0.13283 | val_1_rmse: 0.134   |  0:01:57s
epoch 21 | loss: 0.01812 | val_0_rmse: 0.14023 | val_1_rmse: 0.14164 |  0:02:02s
epoch 22 | loss: 0.01776 | val_0_rmse: 0.14004 | val_1_rmse: 0.14173 |  0:02:08s
epoch 23 | loss: 0.01748 | val_0_rmse: 0.12976 | val_1_rmse: 0.13142 |  0:02:13s
epoch 24 | loss: 0.0176  | val_0_rmse: 0.14471 | val_1_rmse: 0.14705 |  0:02:19s
epoch 25 | loss: 0.01768 | val_0_rmse: 0.13131 | val_1_rmse: 0.13268 |  0:02:24s
epoch 26 | loss: 0.01742 | val_0_rmse: 0.12863 | val_1_rmse: 0.13021 |  0:02:30s
epoch 27 | loss: 0.01749 | val_0_rmse: 0.13813 | val_1_rmse: 0.1394  |  0:02:35s
epoch 28 | loss: 0.01762 | val_0_rmse: 0.13394 | val_1_rmse: 0.13557 |  0:02:41s
epoch 29 | loss: 0.01735 | val_0_rmse: 0.12843 | val_1_rmse: 0.13002 |  0:02:46s
epoch 30 | loss: 0.01724 | val_0_rmse: 0.1372  | val_1_rmse: 0.13901 |  0:02:52s
epoch 31 | loss: 0.01797 | val_0_rmse: 0.1368  | val_1_rmse: 0.13785 |  0:02:58s
epoch 32 | loss: 0.01765 | val_0_rmse: 0.13608 | val_1_rmse: 0.13754 |  0:03:03s
epoch 33 | loss: 0.01715 | val_0_rmse: 0.12895 | val_1_rmse: 0.13062 |  0:03:09s
epoch 34 | loss: 0.01685 | val_0_rmse: 0.1311  | val_1_rmse: 0.13307 |  0:03:14s
epoch 35 | loss: 0.01722 | val_0_rmse: 0.13626 | val_1_rmse: 0.13804 |  0:03:20s
epoch 36 | loss: 0.01832 | val_0_rmse: 0.18877 | val_1_rmse: 0.18882 |  0:03:25s
epoch 37 | loss: 0.01891 | val_0_rmse: 0.13243 | val_1_rmse: 0.13392 |  0:03:31s
epoch 38 | loss: 0.01788 | val_0_rmse: 0.13785 | val_1_rmse: 0.13875 |  0:03:37s
epoch 39 | loss: 0.01819 | val_0_rmse: 0.1434  | val_1_rmse: 0.14529 |  0:03:42s
epoch 40 | loss: 0.0175  | val_0_rmse: 0.13781 | val_1_rmse: 0.13983 |  0:03:48s
epoch 41 | loss: 0.0177  | val_0_rmse: 0.13987 | val_1_rmse: 0.14149 |  0:03:53s
epoch 42 | loss: 0.0177  | val_0_rmse: 0.13691 | val_1_rmse: 0.1389  |  0:03:59s
epoch 43 | loss: 0.01732 | val_0_rmse: 0.13123 | val_1_rmse: 0.13282 |  0:04:04s
epoch 44 | loss: 0.01749 | val_0_rmse: 0.13967 | val_1_rmse: 0.14139 |  0:04:10s
epoch 45 | loss: 0.01732 | val_0_rmse: 0.13002 | val_1_rmse: 0.13189 |  0:04:15s
epoch 46 | loss: 0.01725 | val_0_rmse: 0.16259 | val_1_rmse: 0.16505 |  0:04:21s
epoch 47 | loss: 0.01736 | val_0_rmse: 0.13255 | val_1_rmse: 0.13443 |  0:04:26s
epoch 48 | loss: 0.01702 | val_0_rmse: 0.13785 | val_1_rmse: 0.13969 |  0:04:32s
epoch 49 | loss: 0.01716 | val_0_rmse: 0.13483 | val_1_rmse: 0.13645 |  0:04:37s
epoch 50 | loss: 0.01709 | val_0_rmse: 0.13196 | val_1_rmse: 0.13348 |  0:04:43s
epoch 51 | loss: 0.01706 | val_0_rmse: 0.13328 | val_1_rmse: 0.13503 |  0:04:49s
epoch 52 | loss: 0.01707 | val_0_rmse: 0.1291  | val_1_rmse: 0.13091 |  0:04:54s
epoch 53 | loss: 0.01698 | val_0_rmse: 0.1288  | val_1_rmse: 0.13077 |  0:05:00s
epoch 54 | loss: 0.0169  | val_0_rmse: 0.13089 | val_1_rmse: 0.13256 |  0:05:05s
epoch 55 | loss: 0.0171  | val_0_rmse: 0.12828 | val_1_rmse: 0.13    |  0:05:11s
epoch 56 | loss: 0.017   | val_0_rmse: 0.13176 | val_1_rmse: 0.13318 |  0:05:16s
epoch 57 | loss: 0.01695 | val_0_rmse: 0.1274  | val_1_rmse: 0.12932 |  0:05:22s
epoch 58 | loss: 0.01685 | val_0_rmse: 0.13365 | val_1_rmse: 0.13509 |  0:05:27s
epoch 59 | loss: 0.01677 | val_0_rmse: 0.12854 | val_1_rmse: 0.13019 |  0:05:33s
epoch 60 | loss: 0.0168  | val_0_rmse: 0.14085 | val_1_rmse: 0.14173 |  0:05:38s
epoch 61 | loss: 0.0169  | val_0_rmse: 0.13129 | val_1_rmse: 0.13307 |  0:05:44s
epoch 62 | loss: 0.01668 | val_0_rmse: 0.13338 | val_1_rmse: 0.13504 |  0:05:49s
epoch 63 | loss: 0.01665 | val_0_rmse: 0.13836 | val_1_rmse: 0.14019 |  0:05:55s
epoch 64 | loss: 0.0169  | val_0_rmse: 0.14701 | val_1_rmse: 0.14937 |  0:06:00s
epoch 65 | loss: 0.01682 | val_0_rmse: 0.12965 | val_1_rmse: 0.13172 |  0:06:06s
epoch 66 | loss: 0.01667 | val_0_rmse: 0.13251 | val_1_rmse: 0.13467 |  0:06:12s
epoch 67 | loss: 0.01708 | val_0_rmse: 0.12869 | val_1_rmse: 0.13059 |  0:06:17s
epoch 68 | loss: 0.01659 | val_0_rmse: 0.1446  | val_1_rmse: 0.14683 |  0:06:23s
epoch 69 | loss: 0.01664 | val_0_rmse: 0.12763 | val_1_rmse: 0.12913 |  0:06:28s
epoch 70 | loss: 0.01668 | val_0_rmse: 0.12757 | val_1_rmse: 0.12917 |  0:06:34s
epoch 71 | loss: 0.01687 | val_0_rmse: 0.12985 | val_1_rmse: 0.13201 |  0:06:39s
epoch 72 | loss: 0.01676 | val_0_rmse: 0.14207 | val_1_rmse: 0.14468 |  0:06:45s
epoch 73 | loss: 0.01683 | val_0_rmse: 0.12679 | val_1_rmse: 0.12854 |  0:06:51s
epoch 74 | loss: 0.01673 | val_0_rmse: 0.14529 | val_1_rmse: 0.14731 |  0:06:56s
epoch 75 | loss: 0.01691 | val_0_rmse: 0.13672 | val_1_rmse: 0.13782 |  0:07:02s
epoch 76 | loss: 0.01658 | val_0_rmse: 0.13748 | val_1_rmse: 0.13929 |  0:07:07s
epoch 77 | loss: 0.01704 | val_0_rmse: 0.13548 | val_1_rmse: 0.1369  |  0:07:13s
epoch 78 | loss: 0.01738 | val_0_rmse: 0.13775 | val_1_rmse: 0.13936 |  0:07:18s
epoch 79 | loss: 0.01745 | val_0_rmse: 0.13731 | val_1_rmse: 0.13922 |  0:07:24s
epoch 80 | loss: 0.0174  | val_0_rmse: 0.12897 | val_1_rmse: 0.13074 |  0:07:29s
epoch 81 | loss: 0.01621 | val_0_rmse: 0.12777 | val_1_rmse: 0.12965 |  0:07:35s
epoch 82 | loss: 0.01601 | val_0_rmse: 0.12923 | val_1_rmse: 0.13069 |  0:07:40s
epoch 83 | loss: 0.01624 | val_0_rmse: 0.12335 | val_1_rmse: 0.12509 |  0:07:46s
epoch 84 | loss: 0.01571 | val_0_rmse: 0.12495 | val_1_rmse: 0.12664 |  0:07:52s
epoch 85 | loss: 0.01567 | val_0_rmse: 0.13692 | val_1_rmse: 0.13789 |  0:07:57s
epoch 86 | loss: 0.01552 | val_0_rmse: 0.15394 | val_1_rmse: 0.15584 |  0:08:03s
epoch 87 | loss: 0.01569 | val_0_rmse: 0.12571 | val_1_rmse: 0.12772 |  0:08:08s
epoch 88 | loss: 0.01569 | val_0_rmse: 0.13007 | val_1_rmse: 0.13185 |  0:08:14s
epoch 89 | loss: 0.01547 | val_0_rmse: 0.1257  | val_1_rmse: 0.12772 |  0:08:19s
epoch 90 | loss: 0.01556 | val_0_rmse: 0.12809 | val_1_rmse: 0.13019 |  0:08:25s
epoch 91 | loss: 0.01555 | val_0_rmse: 0.12793 | val_1_rmse: 0.12964 |  0:08:30s
epoch 92 | loss: 0.01554 | val_0_rmse: 0.12538 | val_1_rmse: 0.12734 |  0:08:36s
epoch 93 | loss: 0.01536 | val_0_rmse: 0.12136 | val_1_rmse: 0.12335 |  0:08:41s
epoch 94 | loss: 0.01537 | val_0_rmse: 0.12659 | val_1_rmse: 0.12862 |  0:08:47s
epoch 95 | loss: 0.01624 | val_0_rmse: 0.16409 | val_1_rmse: 0.1649  |  0:08:52s
epoch 96 | loss: 0.01939 | val_0_rmse: 0.14923 | val_1_rmse: 0.1504  |  0:08:58s
epoch 97 | loss: 0.01758 | val_0_rmse: 0.12835 | val_1_rmse: 0.13048 |  0:09:03s
epoch 98 | loss: 0.01644 | val_0_rmse: 0.12594 | val_1_rmse: 0.12795 |  0:09:09s
epoch 99 | loss: 0.01618 | val_0_rmse: 0.13141 | val_1_rmse: 0.13334 |  0:09:14s
epoch 100| loss: 0.01596 | val_0_rmse: 0.12632 | val_1_rmse: 0.1284  |  0:09:20s
epoch 101| loss: 0.01578 | val_0_rmse: 0.13149 | val_1_rmse: 0.13328 |  0:09:25s
epoch 102| loss: 0.01562 | val_0_rmse: 0.12832 | val_1_rmse: 0.13046 |  0:09:31s
epoch 103| loss: 0.0155  | val_0_rmse: 0.12438 | val_1_rmse: 0.12658 |  0:09:37s
epoch 104| loss: 0.01556 | val_0_rmse: 0.12201 | val_1_rmse: 0.12415 |  0:09:42s
epoch 105| loss: 0.0154  | val_0_rmse: 0.14713 | val_1_rmse: 0.14912 |  0:09:48s
epoch 106| loss: 0.01547 | val_0_rmse: 0.12291 | val_1_rmse: 0.12469 |  0:09:53s
epoch 107| loss: 0.01536 | val_0_rmse: 0.1299  | val_1_rmse: 0.13155 |  0:09:59s
epoch 108| loss: 0.01532 | val_0_rmse: 0.1251  | val_1_rmse: 0.12719 |  0:10:04s
epoch 109| loss: 0.01539 | val_0_rmse: 0.12407 | val_1_rmse: 0.12601 |  0:10:10s
epoch 110| loss: 0.01546 | val_0_rmse: 0.12918 | val_1_rmse: 0.13099 |  0:10:15s
epoch 111| loss: 0.01562 | val_0_rmse: 0.12191 | val_1_rmse: 0.12398 |  0:10:21s
epoch 112| loss: 0.01527 | val_0_rmse: 0.12457 | val_1_rmse: 0.12665 |  0:10:26s
epoch 113| loss: 0.01548 | val_0_rmse: 0.15462 | val_1_rmse: 0.15642 |  0:10:32s
epoch 114| loss: 0.01547 | val_0_rmse: 0.13337 | val_1_rmse: 0.13481 |  0:10:37s
epoch 115| loss: 0.01531 | val_0_rmse: 0.14575 | val_1_rmse: 0.14733 |  0:10:43s
epoch 116| loss: 0.01534 | val_0_rmse: 0.12387 | val_1_rmse: 0.1257  |  0:10:48s
epoch 117| loss: 0.01553 | val_0_rmse: 0.12856 | val_1_rmse: 0.1303  |  0:10:54s
epoch 118| loss: 0.01529 | val_0_rmse: 0.27902 | val_1_rmse: 0.28163 |  0:10:59s
epoch 119| loss: 0.02036 | val_0_rmse: 0.14322 | val_1_rmse: 0.14444 |  0:11:05s
epoch 120| loss: 0.01761 | val_0_rmse: 0.14358 | val_1_rmse: 0.14407 |  0:11:10s
epoch 121| loss: 0.017   | val_0_rmse: 0.12958 | val_1_rmse: 0.13106 |  0:11:16s
epoch 122| loss: 0.01669 | val_0_rmse: 0.13657 | val_1_rmse: 0.138   |  0:11:22s
epoch 123| loss: 0.01653 | val_0_rmse: 0.12568 | val_1_rmse: 0.12722 |  0:11:27s

Early stopping occured at epoch 123 with best_epoch = 93 and best_val_1_rmse = 0.12335
Best weights from best epoch are automatically used!
ended training at: 04:59:20
Feature importance:
[('Area', 0.24990391589928534), ('Baths', 0.11018574410395521), ('Beds', 0.0), ('Latitude', 0.2664392757947979), ('Longitude', 0.32255349752302204), ('Month', 0.028107199238891715), ('Year', 0.02281036744004777)]
Mean squared error is of 2503722540.732629
Mean absolute error:36160.40012644873
MAPE:0.3509689187482358
R2 score:0.6329842057225293
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:59:21
epoch 0  | loss: 0.06001 | val_0_rmse: 0.18223 | val_1_rmse: 0.18395 |  0:00:05s
epoch 1  | loss: 0.02306 | val_0_rmse: 0.17727 | val_1_rmse: 0.17916 |  0:00:11s
epoch 2  | loss: 0.02236 | val_0_rmse: 0.167   | val_1_rmse: 0.16873 |  0:00:16s
epoch 3  | loss: 0.02172 | val_0_rmse: 0.15775 | val_1_rmse: 0.15938 |  0:00:22s
epoch 4  | loss: 0.02129 | val_0_rmse: 0.15931 | val_1_rmse: 0.16079 |  0:00:28s
epoch 5  | loss: 0.02056 | val_0_rmse: 0.1497  | val_1_rmse: 0.15114 |  0:00:33s
epoch 6  | loss: 0.0197  | val_0_rmse: 0.13917 | val_1_rmse: 0.14062 |  0:00:39s
epoch 7  | loss: 0.01838 | val_0_rmse: 0.13326 | val_1_rmse: 0.1345  |  0:00:44s
epoch 8  | loss: 0.01756 | val_0_rmse: 0.13188 | val_1_rmse: 0.13263 |  0:00:50s
epoch 9  | loss: 0.01634 | val_0_rmse: 0.1302  | val_1_rmse: 0.1309  |  0:00:55s
epoch 10 | loss: 0.01628 | val_0_rmse: 0.13723 | val_1_rmse: 0.13816 |  0:01:01s
epoch 11 | loss: 0.01603 | val_0_rmse: 0.13382 | val_1_rmse: 0.13457 |  0:01:06s
epoch 12 | loss: 0.01637 | val_0_rmse: 0.14267 | val_1_rmse: 0.14324 |  0:01:12s
epoch 13 | loss: 0.01572 | val_0_rmse: 0.1248  | val_1_rmse: 0.12516 |  0:01:17s
epoch 14 | loss: 0.01534 | val_0_rmse: 0.12219 | val_1_rmse: 0.12307 |  0:01:23s
epoch 15 | loss: 0.01536 | val_0_rmse: 0.12643 | val_1_rmse: 0.12713 |  0:01:29s
epoch 16 | loss: 0.01543 | val_0_rmse: 0.12223 | val_1_rmse: 0.12314 |  0:01:34s
epoch 17 | loss: 0.01514 | val_0_rmse: 0.12459 | val_1_rmse: 0.12547 |  0:01:40s
epoch 18 | loss: 0.01526 | val_0_rmse: 0.12606 | val_1_rmse: 0.12676 |  0:01:45s
epoch 19 | loss: 0.01496 | val_0_rmse: 0.13145 | val_1_rmse: 0.13238 |  0:01:51s
epoch 20 | loss: 0.01497 | val_0_rmse: 0.12729 | val_1_rmse: 0.12826 |  0:01:56s
epoch 21 | loss: 0.01468 | val_0_rmse: 0.18889 | val_1_rmse: 0.18894 |  0:02:02s
epoch 22 | loss: 0.01489 | val_0_rmse: 0.17478 | val_1_rmse: 0.17528 |  0:02:07s
epoch 23 | loss: 0.01488 | val_0_rmse: 0.11944 | val_1_rmse: 0.12068 |  0:02:13s
epoch 24 | loss: 0.01517 | val_0_rmse: 0.13496 | val_1_rmse: 0.13506 |  0:02:18s
epoch 25 | loss: 0.01514 | val_0_rmse: 0.1366  | val_1_rmse: 0.13707 |  0:02:24s
epoch 26 | loss: 0.01506 | val_0_rmse: 0.11771 | val_1_rmse: 0.11881 |  0:02:30s
epoch 27 | loss: 0.01491 | val_0_rmse: 0.13628 | val_1_rmse: 0.13703 |  0:02:35s
epoch 28 | loss: 0.01511 | val_0_rmse: 0.13775 | val_1_rmse: 0.13837 |  0:02:41s
epoch 29 | loss: 0.01455 | val_0_rmse: 0.11623 | val_1_rmse: 0.11736 |  0:02:46s
epoch 30 | loss: 0.0147  | val_0_rmse: 0.12802 | val_1_rmse: 0.12931 |  0:02:52s
epoch 31 | loss: 0.01494 | val_0_rmse: 0.12473 | val_1_rmse: 0.12531 |  0:02:57s
epoch 32 | loss: 0.0146  | val_0_rmse: 0.1694  | val_1_rmse: 0.16968 |  0:03:03s
epoch 33 | loss: 0.01466 | val_0_rmse: 0.13334 | val_1_rmse: 0.1344  |  0:03:08s
epoch 34 | loss: 0.01479 | val_0_rmse: 0.11807 | val_1_rmse: 0.11935 |  0:03:14s
epoch 35 | loss: 0.01462 | val_0_rmse: 0.12101 | val_1_rmse: 0.12251 |  0:03:19s
epoch 36 | loss: 0.01451 | val_0_rmse: 0.11731 | val_1_rmse: 0.11837 |  0:03:25s
epoch 37 | loss: 0.01474 | val_0_rmse: 0.12183 | val_1_rmse: 0.12292 |  0:03:30s
epoch 38 | loss: 0.01476 | val_0_rmse: 0.12565 | val_1_rmse: 0.12657 |  0:03:36s
epoch 39 | loss: 0.01463 | val_0_rmse: 0.14274 | val_1_rmse: 0.14347 |  0:03:41s
epoch 40 | loss: 0.01487 | val_0_rmse: 0.14027 | val_1_rmse: 0.14078 |  0:03:47s
epoch 41 | loss: 0.01462 | val_0_rmse: 0.13333 | val_1_rmse: 0.13354 |  0:03:52s
epoch 42 | loss: 0.01452 | val_0_rmse: 0.12808 | val_1_rmse: 0.12889 |  0:03:58s
epoch 43 | loss: 0.01434 | val_0_rmse: 0.11628 | val_1_rmse: 0.11793 |  0:04:04s
epoch 44 | loss: 0.01456 | val_0_rmse: 0.14673 | val_1_rmse: 0.14739 |  0:04:09s
epoch 45 | loss: 0.01467 | val_0_rmse: 0.20809 | val_1_rmse: 0.20842 |  0:04:15s
epoch 46 | loss: 0.01447 | val_0_rmse: 0.13277 | val_1_rmse: 0.13348 |  0:04:20s
epoch 47 | loss: 0.01448 | val_0_rmse: 0.11768 | val_1_rmse: 0.1189  |  0:04:26s
epoch 48 | loss: 0.01441 | val_0_rmse: 0.13016 | val_1_rmse: 0.13182 |  0:04:31s
epoch 49 | loss: 0.01494 | val_0_rmse: 0.11728 | val_1_rmse: 0.11849 |  0:04:37s
epoch 50 | loss: 0.01461 | val_0_rmse: 0.12743 | val_1_rmse: 0.12811 |  0:04:42s
epoch 51 | loss: 0.01453 | val_0_rmse: 0.15075 | val_1_rmse: 0.15133 |  0:04:48s
epoch 52 | loss: 0.0146  | val_0_rmse: 0.14491 | val_1_rmse: 0.14517 |  0:04:53s
epoch 53 | loss: 0.01465 | val_0_rmse: 0.14177 | val_1_rmse: 0.14247 |  0:04:59s
epoch 54 | loss: 0.01508 | val_0_rmse: 0.15954 | val_1_rmse: 0.16007 |  0:05:04s
epoch 55 | loss: 0.01581 | val_0_rmse: 0.13705 | val_1_rmse: 0.13756 |  0:05:10s
epoch 56 | loss: 0.01564 | val_0_rmse: 0.13471 | val_1_rmse: 0.1352  |  0:05:16s
epoch 57 | loss: 0.01487 | val_0_rmse: 0.14589 | val_1_rmse: 0.14615 |  0:05:21s
epoch 58 | loss: 0.01469 | val_0_rmse: 0.1961  | val_1_rmse: 0.19587 |  0:05:27s
epoch 59 | loss: 0.01508 | val_0_rmse: 0.14648 | val_1_rmse: 0.14662 |  0:05:32s

Early stopping occured at epoch 59 with best_epoch = 29 and best_val_1_rmse = 0.11736
Best weights from best epoch are automatically used!
ended training at: 05:04:55
Feature importance:
[('Area', 0.10770188421612595), ('Baths', 0.17427797805900397), ('Beds', 0.0957479073541005), ('Latitude', 0.32175646436132954), ('Longitude', 0.26985012245060397), ('Month', 0.02024284123546842), ('Year', 0.010422802323367694)]
Mean squared error is of 2351362434.3901024
Mean absolute error:35093.424080466706
MAPE:0.33375888239822027
R2 score:0.6616876870494177
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:04:56
epoch 0  | loss: 0.05717 | val_0_rmse: 0.20987 | val_1_rmse: 0.2087  |  0:00:05s
epoch 1  | loss: 0.02331 | val_0_rmse: 0.20051 | val_1_rmse: 0.19947 |  0:00:11s
epoch 2  | loss: 0.02268 | val_0_rmse: 0.18388 | val_1_rmse: 0.1827  |  0:00:16s
epoch 3  | loss: 0.02229 | val_0_rmse: 0.18005 | val_1_rmse: 0.17885 |  0:00:22s
epoch 4  | loss: 0.02191 | val_0_rmse: 0.15641 | val_1_rmse: 0.15514 |  0:00:27s
epoch 5  | loss: 0.02066 | val_0_rmse: 0.14789 | val_1_rmse: 0.14717 |  0:00:33s
epoch 6  | loss: 0.01971 | val_0_rmse: 0.14121 | val_1_rmse: 0.14094 |  0:00:38s
epoch 7  | loss: 0.01903 | val_0_rmse: 0.13488 | val_1_rmse: 0.13452 |  0:00:44s
epoch 8  | loss: 0.01893 | val_0_rmse: 0.13648 | val_1_rmse: 0.13504 |  0:00:49s
epoch 9  | loss: 0.01811 | val_0_rmse: 0.12863 | val_1_rmse: 0.12768 |  0:00:55s
epoch 10 | loss: 0.0173  | val_0_rmse: 0.13284 | val_1_rmse: 0.13188 |  0:01:00s
epoch 11 | loss: 0.01692 | val_0_rmse: 0.13184 | val_1_rmse: 0.13205 |  0:01:06s
epoch 12 | loss: 0.01673 | val_0_rmse: 0.12893 | val_1_rmse: 0.12811 |  0:01:12s
epoch 13 | loss: 0.01675 | val_0_rmse: 0.12899 | val_1_rmse: 0.1286  |  0:01:17s
epoch 14 | loss: 0.01661 | val_0_rmse: 0.13431 | val_1_rmse: 0.13312 |  0:01:23s
epoch 15 | loss: 0.01666 | val_0_rmse: 0.12999 | val_1_rmse: 0.12888 |  0:01:28s
epoch 16 | loss: 0.01679 | val_0_rmse: 0.14201 | val_1_rmse: 0.14055 |  0:01:34s
epoch 17 | loss: 0.01635 | val_0_rmse: 0.12561 | val_1_rmse: 0.12496 |  0:01:39s
epoch 18 | loss: 0.01632 | val_0_rmse: 0.125   | val_1_rmse: 0.12419 |  0:01:45s
epoch 19 | loss: 0.01627 | val_0_rmse: 0.12934 | val_1_rmse: 0.1288  |  0:01:50s
epoch 20 | loss: 0.01665 | val_0_rmse: 0.13564 | val_1_rmse: 0.13607 |  0:01:56s
epoch 21 | loss: 0.01611 | val_0_rmse: 0.12506 | val_1_rmse: 0.12451 |  0:02:02s
epoch 22 | loss: 0.01607 | val_0_rmse: 0.12786 | val_1_rmse: 0.12792 |  0:02:07s
epoch 23 | loss: 0.01622 | val_0_rmse: 0.12191 | val_1_rmse: 0.12163 |  0:02:13s
epoch 24 | loss: 0.0155  | val_0_rmse: 0.12462 | val_1_rmse: 0.1242  |  0:02:18s
epoch 25 | loss: 0.0154  | val_0_rmse: 0.12322 | val_1_rmse: 0.12251 |  0:02:24s
epoch 26 | loss: 0.01521 | val_0_rmse: 0.12037 | val_1_rmse: 0.12016 |  0:02:29s
epoch 27 | loss: 0.01517 | val_0_rmse: 0.12891 | val_1_rmse: 0.12871 |  0:02:35s
epoch 28 | loss: 0.01517 | val_0_rmse: 0.12675 | val_1_rmse: 0.12557 |  0:02:40s
epoch 29 | loss: 0.01549 | val_0_rmse: 0.13408 | val_1_rmse: 0.13421 |  0:02:46s
epoch 30 | loss: 0.01503 | val_0_rmse: 0.1275  | val_1_rmse: 0.12629 |  0:02:51s
epoch 31 | loss: 0.01517 | val_0_rmse: 0.13741 | val_1_rmse: 0.13585 |  0:02:57s
epoch 32 | loss: 0.01535 | val_0_rmse: 0.12539 | val_1_rmse: 0.12493 |  0:03:03s
epoch 33 | loss: 0.01507 | val_0_rmse: 0.12164 | val_1_rmse: 0.12091 |  0:03:08s
epoch 34 | loss: 0.01515 | val_0_rmse: 0.12344 | val_1_rmse: 0.12267 |  0:03:14s
epoch 35 | loss: 0.01525 | val_0_rmse: 0.11984 | val_1_rmse: 0.11958 |  0:03:19s
epoch 36 | loss: 0.01511 | val_0_rmse: 0.12668 | val_1_rmse: 0.12613 |  0:03:25s
epoch 37 | loss: 0.01513 | val_0_rmse: 0.12911 | val_1_rmse: 0.12799 |  0:03:30s
epoch 38 | loss: 0.01528 | val_0_rmse: 0.12437 | val_1_rmse: 0.12373 |  0:03:36s
epoch 39 | loss: 0.015   | val_0_rmse: 0.12512 | val_1_rmse: 0.12377 |  0:03:41s
epoch 40 | loss: 0.01514 | val_0_rmse: 0.12216 | val_1_rmse: 0.12114 |  0:03:47s
epoch 41 | loss: 0.01695 | val_0_rmse: 0.13626 | val_1_rmse: 0.13589 |  0:03:52s
epoch 42 | loss: 0.01678 | val_0_rmse: 0.13075 | val_1_rmse: 0.13004 |  0:03:58s
epoch 43 | loss: 0.01657 | val_0_rmse: 0.13148 | val_1_rmse: 0.13097 |  0:04:03s
epoch 44 | loss: 0.01641 | val_0_rmse: 0.12743 | val_1_rmse: 0.12668 |  0:04:09s
epoch 45 | loss: 0.01649 | val_0_rmse: 0.136   | val_1_rmse: 0.13448 |  0:04:15s
epoch 46 | loss: 0.01669 | val_0_rmse: 0.13107 | val_1_rmse: 0.13087 |  0:04:20s
epoch 47 | loss: 0.01653 | val_0_rmse: 0.13782 | val_1_rmse: 0.13768 |  0:04:26s
epoch 48 | loss: 0.01662 | val_0_rmse: 0.14631 | val_1_rmse: 0.14639 |  0:04:31s
epoch 49 | loss: 0.01646 | val_0_rmse: 0.12493 | val_1_rmse: 0.12488 |  0:04:37s
epoch 50 | loss: 0.01628 | val_0_rmse: 0.14988 | val_1_rmse: 0.14737 |  0:04:42s
epoch 51 | loss: 0.01638 | val_0_rmse: 0.13588 | val_1_rmse: 0.13584 |  0:04:48s
epoch 52 | loss: 0.01599 | val_0_rmse: 0.12772 | val_1_rmse: 0.12712 |  0:04:53s
epoch 53 | loss: 0.01601 | val_0_rmse: 0.14829 | val_1_rmse: 0.14702 |  0:04:59s
epoch 54 | loss: 0.01599 | val_0_rmse: 0.1233  | val_1_rmse: 0.12307 |  0:05:05s
epoch 55 | loss: 0.01631 | val_0_rmse: 0.12714 | val_1_rmse: 0.12631 |  0:05:10s
epoch 56 | loss: 0.01587 | val_0_rmse: 0.14558 | val_1_rmse: 0.14449 |  0:05:16s
epoch 57 | loss: 0.01614 | val_0_rmse: 0.14078 | val_1_rmse: 0.14043 |  0:05:21s
epoch 58 | loss: 0.01607 | val_0_rmse: 0.1317  | val_1_rmse: 0.13103 |  0:05:27s
epoch 59 | loss: 0.01586 | val_0_rmse: 0.13426 | val_1_rmse: 0.13412 |  0:05:32s
epoch 60 | loss: 0.01576 | val_0_rmse: 0.13383 | val_1_rmse: 0.1333  |  0:05:38s
epoch 61 | loss: 0.01558 | val_0_rmse: 0.13368 | val_1_rmse: 0.13354 |  0:05:43s
epoch 62 | loss: 0.01551 | val_0_rmse: 0.12762 | val_1_rmse: 0.12668 |  0:05:49s
epoch 63 | loss: 0.01538 | val_0_rmse: 0.12465 | val_1_rmse: 0.12453 |  0:05:55s
epoch 64 | loss: 0.01559 | val_0_rmse: 0.1326  | val_1_rmse: 0.13142 |  0:06:00s
epoch 65 | loss: 0.01528 | val_0_rmse: 0.13206 | val_1_rmse: 0.13191 |  0:06:06s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.11958
Best weights from best epoch are automatically used!
ended training at: 05:11:04
Feature importance:
[('Area', 0.4646090384010209), ('Baths', 0.05066813476016149), ('Beds', 0.0369943043426455), ('Latitude', 0.12302188537077582), ('Longitude', 0.29475905945050723), ('Month', 0.02994234461230233), ('Year', 5.233062586721171e-06)]
Mean squared error is of 2376521695.156972
Mean absolute error:34822.931017056195
MAPE:0.3217835954503597
R2 score:0.6528372566802654
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:11:04
epoch 0  | loss: 0.10752 | val_0_rmse: 0.19017 | val_1_rmse: 0.19082 |  0:00:02s
epoch 1  | loss: 0.02024 | val_0_rmse: 0.17826 | val_1_rmse: 0.17948 |  0:00:04s
epoch 2  | loss: 0.01804 | val_0_rmse: 0.17512 | val_1_rmse: 0.17665 |  0:00:06s
epoch 3  | loss: 0.01753 | val_0_rmse: 0.17193 | val_1_rmse: 0.17342 |  0:00:08s
epoch 4  | loss: 0.01692 | val_0_rmse: 0.16552 | val_1_rmse: 0.16725 |  0:00:10s
epoch 5  | loss: 0.01629 | val_0_rmse: 0.16032 | val_1_rmse: 0.16194 |  0:00:12s
epoch 6  | loss: 0.01581 | val_0_rmse: 0.15449 | val_1_rmse: 0.15608 |  0:00:14s
epoch 7  | loss: 0.01561 | val_0_rmse: 0.14945 | val_1_rmse: 0.15121 |  0:00:16s
epoch 8  | loss: 0.01573 | val_0_rmse: 0.14578 | val_1_rmse: 0.14705 |  0:00:18s
epoch 9  | loss: 0.01566 | val_0_rmse: 0.13877 | val_1_rmse: 0.14062 |  0:00:20s
epoch 10 | loss: 0.01529 | val_0_rmse: 0.14349 | val_1_rmse: 0.1448  |  0:00:22s
epoch 11 | loss: 0.01541 | val_0_rmse: 0.13066 | val_1_rmse: 0.13323 |  0:00:24s
epoch 12 | loss: 0.01582 | val_0_rmse: 0.12778 | val_1_rmse: 0.13057 |  0:00:26s
epoch 13 | loss: 0.01515 | val_0_rmse: 0.1243  | val_1_rmse: 0.12665 |  0:00:28s
epoch 14 | loss: 0.01465 | val_0_rmse: 0.12309 | val_1_rmse: 0.12521 |  0:00:30s
epoch 15 | loss: 0.01476 | val_0_rmse: 0.11965 | val_1_rmse: 0.1222  |  0:00:32s
epoch 16 | loss: 0.01475 | val_0_rmse: 0.12074 | val_1_rmse: 0.12359 |  0:00:34s
epoch 17 | loss: 0.01466 | val_0_rmse: 0.13021 | val_1_rmse: 0.13305 |  0:00:36s
epoch 18 | loss: 0.01471 | val_0_rmse: 0.11777 | val_1_rmse: 0.12064 |  0:00:38s
epoch 19 | loss: 0.01466 | val_0_rmse: 0.11722 | val_1_rmse: 0.12012 |  0:00:40s
epoch 20 | loss: 0.01436 | val_0_rmse: 0.11745 | val_1_rmse: 0.12041 |  0:00:42s
epoch 21 | loss: 0.01422 | val_0_rmse: 0.11742 | val_1_rmse: 0.11981 |  0:00:44s
epoch 22 | loss: 0.01433 | val_0_rmse: 0.11737 | val_1_rmse: 0.11971 |  0:00:46s
epoch 23 | loss: 0.01429 | val_0_rmse: 0.11495 | val_1_rmse: 0.11733 |  0:00:48s
epoch 24 | loss: 0.01411 | val_0_rmse: 0.11845 | val_1_rmse: 0.12082 |  0:00:50s
epoch 25 | loss: 0.01409 | val_0_rmse: 0.11755 | val_1_rmse: 0.12026 |  0:00:52s
epoch 26 | loss: 0.0139  | val_0_rmse: 0.11331 | val_1_rmse: 0.11643 |  0:00:54s
epoch 27 | loss: 0.01375 | val_0_rmse: 0.11266 | val_1_rmse: 0.11589 |  0:00:56s
epoch 28 | loss: 0.0136  | val_0_rmse: 0.11174 | val_1_rmse: 0.11404 |  0:00:58s
epoch 29 | loss: 0.01387 | val_0_rmse: 0.11024 | val_1_rmse: 0.11312 |  0:01:00s
epoch 30 | loss: 0.01384 | val_0_rmse: 0.11154 | val_1_rmse: 0.11461 |  0:01:02s
epoch 31 | loss: 0.01367 | val_0_rmse: 0.1151  | val_1_rmse: 0.11772 |  0:01:04s
epoch 32 | loss: 0.01364 | val_0_rmse: 0.11312 | val_1_rmse: 0.1158  |  0:01:06s
epoch 33 | loss: 0.01375 | val_0_rmse: 0.11383 | val_1_rmse: 0.11574 |  0:01:08s
epoch 34 | loss: 0.01353 | val_0_rmse: 0.11381 | val_1_rmse: 0.11585 |  0:01:10s
epoch 35 | loss: 0.01338 | val_0_rmse: 0.10956 | val_1_rmse: 0.11267 |  0:01:12s
epoch 36 | loss: 0.0137  | val_0_rmse: 0.11108 | val_1_rmse: 0.11397 |  0:01:14s
epoch 37 | loss: 0.01358 | val_0_rmse: 0.10993 | val_1_rmse: 0.11281 |  0:01:16s
epoch 38 | loss: 0.01366 | val_0_rmse: 0.11178 | val_1_rmse: 0.11478 |  0:01:18s
epoch 39 | loss: 0.01346 | val_0_rmse: 0.10984 | val_1_rmse: 0.11157 |  0:01:20s
epoch 40 | loss: 0.01304 | val_0_rmse: 0.11446 | val_1_rmse: 0.11716 |  0:01:22s
epoch 41 | loss: 0.01343 | val_0_rmse: 0.11228 | val_1_rmse: 0.11479 |  0:01:24s
epoch 42 | loss: 0.01307 | val_0_rmse: 0.10999 | val_1_rmse: 0.11309 |  0:01:26s
epoch 43 | loss: 0.01335 | val_0_rmse: 0.11183 | val_1_rmse: 0.11343 |  0:01:28s
epoch 44 | loss: 0.01309 | val_0_rmse: 0.10932 | val_1_rmse: 0.11193 |  0:01:30s
epoch 45 | loss: 0.01278 | val_0_rmse: 0.11427 | val_1_rmse: 0.11658 |  0:01:32s
epoch 46 | loss: 0.013   | val_0_rmse: 0.11169 | val_1_rmse: 0.11376 |  0:01:34s
epoch 47 | loss: 0.01282 | val_0_rmse: 0.10996 | val_1_rmse: 0.11272 |  0:01:36s
epoch 48 | loss: 0.01275 | val_0_rmse: 0.10796 | val_1_rmse: 0.10994 |  0:01:38s
epoch 49 | loss: 0.01268 | val_0_rmse: 0.10848 | val_1_rmse: 0.11073 |  0:01:40s
epoch 50 | loss: 0.01291 | val_0_rmse: 0.10967 | val_1_rmse: 0.1127  |  0:01:42s
epoch 51 | loss: 0.01313 | val_0_rmse: 0.10801 | val_1_rmse: 0.11011 |  0:01:44s
epoch 52 | loss: 0.0127  | val_0_rmse: 0.11216 | val_1_rmse: 0.11398 |  0:01:46s
epoch 53 | loss: 0.01294 | val_0_rmse: 0.112   | val_1_rmse: 0.11387 |  0:01:48s
epoch 54 | loss: 0.01262 | val_0_rmse: 0.10951 | val_1_rmse: 0.11148 |  0:01:50s
epoch 55 | loss: 0.01243 | val_0_rmse: 0.11978 | val_1_rmse: 0.12265 |  0:01:52s
epoch 56 | loss: 0.01249 | val_0_rmse: 0.10793 | val_1_rmse: 0.10991 |  0:01:54s
epoch 57 | loss: 0.01268 | val_0_rmse: 0.10787 | val_1_rmse: 0.10993 |  0:01:56s
epoch 58 | loss: 0.01288 | val_0_rmse: 0.10883 | val_1_rmse: 0.11123 |  0:01:58s
epoch 59 | loss: 0.01248 | val_0_rmse: 0.11169 | val_1_rmse: 0.11411 |  0:02:00s
epoch 60 | loss: 0.01259 | val_0_rmse: 0.11336 | val_1_rmse: 0.1156  |  0:02:02s
epoch 61 | loss: 0.01289 | val_0_rmse: 0.10993 | val_1_rmse: 0.11209 |  0:02:04s
epoch 62 | loss: 0.01284 | val_0_rmse: 0.11105 | val_1_rmse: 0.11359 |  0:02:06s
epoch 63 | loss: 0.01296 | val_0_rmse: 0.10689 | val_1_rmse: 0.10927 |  0:02:08s
epoch 64 | loss: 0.01279 | val_0_rmse: 0.10847 | val_1_rmse: 0.11043 |  0:02:10s
epoch 65 | loss: 0.0127  | val_0_rmse: 0.10941 | val_1_rmse: 0.11155 |  0:02:13s
epoch 66 | loss: 0.01267 | val_0_rmse: 0.11247 | val_1_rmse: 0.11405 |  0:02:15s
epoch 67 | loss: 0.01236 | val_0_rmse: 0.10874 | val_1_rmse: 0.11043 |  0:02:17s
epoch 68 | loss: 0.01244 | val_0_rmse: 0.12253 | val_1_rmse: 0.12453 |  0:02:19s
epoch 69 | loss: 0.01256 | val_0_rmse: 0.11039 | val_1_rmse: 0.11251 |  0:02:21s
epoch 70 | loss: 0.01253 | val_0_rmse: 0.11061 | val_1_rmse: 0.11259 |  0:02:23s
epoch 71 | loss: 0.01253 | val_0_rmse: 0.10708 | val_1_rmse: 0.10854 |  0:02:25s
epoch 72 | loss: 0.01263 | val_0_rmse: 0.11297 | val_1_rmse: 0.11434 |  0:02:27s
epoch 73 | loss: 0.01285 | val_0_rmse: 0.11242 | val_1_rmse: 0.11453 |  0:02:29s
epoch 74 | loss: 0.01281 | val_0_rmse: 0.10955 | val_1_rmse: 0.11181 |  0:02:31s
epoch 75 | loss: 0.01287 | val_0_rmse: 0.11195 | val_1_rmse: 0.11313 |  0:02:33s
epoch 76 | loss: 0.01242 | val_0_rmse: 0.1066  | val_1_rmse: 0.10878 |  0:02:35s
epoch 77 | loss: 0.0123  | val_0_rmse: 0.10689 | val_1_rmse: 0.10919 |  0:02:37s
epoch 78 | loss: 0.01284 | val_0_rmse: 0.11265 | val_1_rmse: 0.11474 |  0:02:39s
epoch 79 | loss: 0.01249 | val_0_rmse: 0.1082  | val_1_rmse: 0.11064 |  0:02:41s
epoch 80 | loss: 0.01225 | val_0_rmse: 0.11541 | val_1_rmse: 0.1186  |  0:02:43s
epoch 81 | loss: 0.013   | val_0_rmse: 0.11007 | val_1_rmse: 0.11145 |  0:02:45s
epoch 82 | loss: 0.01244 | val_0_rmse: 0.10968 | val_1_rmse: 0.11166 |  0:02:47s
epoch 83 | loss: 0.01223 | val_0_rmse: 0.10822 | val_1_rmse: 0.11015 |  0:02:49s
epoch 84 | loss: 0.01219 | val_0_rmse: 0.10774 | val_1_rmse: 0.10935 |  0:02:51s
epoch 85 | loss: 0.01237 | val_0_rmse: 0.11057 | val_1_rmse: 0.11369 |  0:02:53s
epoch 86 | loss: 0.01222 | val_0_rmse: 0.10747 | val_1_rmse: 0.10875 |  0:02:55s
epoch 87 | loss: 0.01213 | val_0_rmse: 0.10699 | val_1_rmse: 0.10875 |  0:02:57s
epoch 88 | loss: 0.01259 | val_0_rmse: 0.10612 | val_1_rmse: 0.10762 |  0:02:59s
epoch 89 | loss: 0.01249 | val_0_rmse: 0.11466 | val_1_rmse: 0.11568 |  0:03:01s
epoch 90 | loss: 0.0123  | val_0_rmse: 0.11274 | val_1_rmse: 0.11498 |  0:03:03s
epoch 91 | loss: 0.01228 | val_0_rmse: 0.10663 | val_1_rmse: 0.10845 |  0:03:05s
epoch 92 | loss: 0.01243 | val_0_rmse: 0.11119 | val_1_rmse: 0.11377 |  0:03:07s
epoch 93 | loss: 0.01253 | val_0_rmse: 0.10675 | val_1_rmse: 0.10823 |  0:03:09s
epoch 94 | loss: 0.01228 | val_0_rmse: 0.10721 | val_1_rmse: 0.10988 |  0:03:11s
epoch 95 | loss: 0.0121  | val_0_rmse: 0.11295 | val_1_rmse: 0.11539 |  0:03:13s
epoch 96 | loss: 0.012   | val_0_rmse: 0.10824 | val_1_rmse: 0.11072 |  0:03:15s
epoch 97 | loss: 0.01235 | val_0_rmse: 0.10895 | val_1_rmse: 0.11084 |  0:03:17s
epoch 98 | loss: 0.01239 | val_0_rmse: 0.12027 | val_1_rmse: 0.12295 |  0:03:19s
epoch 99 | loss: 0.0123  | val_0_rmse: 0.10671 | val_1_rmse: 0.10838 |  0:03:21s
epoch 100| loss: 0.01218 | val_0_rmse: 0.10832 | val_1_rmse: 0.11092 |  0:03:23s
epoch 101| loss: 0.0123  | val_0_rmse: 0.11275 | val_1_rmse: 0.11601 |  0:03:25s
epoch 102| loss: 0.01232 | val_0_rmse: 0.10544 | val_1_rmse: 0.10735 |  0:03:27s
epoch 103| loss: 0.01212 | val_0_rmse: 0.10757 | val_1_rmse: 0.10875 |  0:03:29s
epoch 104| loss: 0.01201 | val_0_rmse: 0.10997 | val_1_rmse: 0.11233 |  0:03:31s
epoch 105| loss: 0.01187 | val_0_rmse: 0.10806 | val_1_rmse: 0.10968 |  0:03:33s
epoch 106| loss: 0.01249 | val_0_rmse: 0.10686 | val_1_rmse: 0.10831 |  0:03:35s
epoch 107| loss: 0.01237 | val_0_rmse: 0.10835 | val_1_rmse: 0.11015 |  0:03:37s
epoch 108| loss: 0.01221 | val_0_rmse: 0.10789 | val_1_rmse: 0.1088  |  0:03:39s
epoch 109| loss: 0.01218 | val_0_rmse: 0.10799 | val_1_rmse: 0.11009 |  0:03:41s
epoch 110| loss: 0.01234 | val_0_rmse: 0.10698 | val_1_rmse: 0.10843 |  0:03:43s
epoch 111| loss: 0.01219 | val_0_rmse: 0.11147 | val_1_rmse: 0.11199 |  0:03:45s
epoch 112| loss: 0.01223 | val_0_rmse: 0.10681 | val_1_rmse: 0.1081  |  0:03:47s
epoch 113| loss: 0.01232 | val_0_rmse: 0.10606 | val_1_rmse: 0.10717 |  0:03:49s
epoch 114| loss: 0.01211 | val_0_rmse: 0.10695 | val_1_rmse: 0.10881 |  0:03:51s
epoch 115| loss: 0.01214 | val_0_rmse: 0.10824 | val_1_rmse: 0.11028 |  0:03:53s
epoch 116| loss: 0.01237 | val_0_rmse: 0.11429 | val_1_rmse: 0.11654 |  0:03:55s
epoch 117| loss: 0.01194 | val_0_rmse: 0.11482 | val_1_rmse: 0.11689 |  0:03:57s
epoch 118| loss: 0.01205 | val_0_rmse: 0.10657 | val_1_rmse: 0.10844 |  0:03:59s
epoch 119| loss: 0.01188 | val_0_rmse: 0.1155  | val_1_rmse: 0.11801 |  0:04:01s
epoch 120| loss: 0.01183 | val_0_rmse: 0.10945 | val_1_rmse: 0.11162 |  0:04:03s
epoch 121| loss: 0.01182 | val_0_rmse: 0.10634 | val_1_rmse: 0.10844 |  0:04:05s
epoch 122| loss: 0.01199 | val_0_rmse: 0.11311 | val_1_rmse: 0.11355 |  0:04:07s
epoch 123| loss: 0.01223 | val_0_rmse: 0.11776 | val_1_rmse: 0.12086 |  0:04:09s
epoch 124| loss: 0.01204 | val_0_rmse: 0.10561 | val_1_rmse: 0.10759 |  0:04:11s
epoch 125| loss: 0.01208 | val_0_rmse: 0.11316 | val_1_rmse: 0.11483 |  0:04:13s
epoch 126| loss: 0.0119  | val_0_rmse: 0.10809 | val_1_rmse: 0.10989 |  0:04:15s
epoch 127| loss: 0.01195 | val_0_rmse: 0.10687 | val_1_rmse: 0.10941 |  0:04:17s
epoch 128| loss: 0.01195 | val_0_rmse: 0.11428 | val_1_rmse: 0.1146  |  0:04:19s
epoch 129| loss: 0.01218 | val_0_rmse: 0.1061  | val_1_rmse: 0.10759 |  0:04:21s
epoch 130| loss: 0.01197 | val_0_rmse: 0.10581 | val_1_rmse: 0.10766 |  0:04:23s
epoch 131| loss: 0.0119  | val_0_rmse: 0.10629 | val_1_rmse: 0.10824 |  0:04:25s
epoch 132| loss: 0.01193 | val_0_rmse: 0.10926 | val_1_rmse: 0.11184 |  0:04:27s
epoch 133| loss: 0.01209 | val_0_rmse: 0.1132  | val_1_rmse: 0.11417 |  0:04:29s
epoch 134| loss: 0.01198 | val_0_rmse: 0.12423 | val_1_rmse: 0.12682 |  0:04:31s
epoch 135| loss: 0.0131  | val_0_rmse: 0.11767 | val_1_rmse: 0.11949 |  0:04:33s
epoch 136| loss: 0.01292 | val_0_rmse: 0.11124 | val_1_rmse: 0.11177 |  0:04:35s
epoch 137| loss: 0.0124  | val_0_rmse: 0.10848 | val_1_rmse: 0.10995 |  0:04:37s
epoch 138| loss: 0.01235 | val_0_rmse: 0.10822 | val_1_rmse: 0.10999 |  0:04:39s
epoch 139| loss: 0.01206 | val_0_rmse: 0.10703 | val_1_rmse: 0.10843 |  0:04:41s
epoch 140| loss: 0.01236 | val_0_rmse: 0.10889 | val_1_rmse: 0.10918 |  0:04:43s
epoch 141| loss: 0.01224 | val_0_rmse: 0.11569 | val_1_rmse: 0.11589 |  0:04:45s
epoch 142| loss: 0.0124  | val_0_rmse: 0.10819 | val_1_rmse: 0.10996 |  0:04:47s
epoch 143| loss: 0.01245 | val_0_rmse: 0.13866 | val_1_rmse: 0.13787 |  0:04:49s

Early stopping occured at epoch 143 with best_epoch = 113 and best_val_1_rmse = 0.10717
Best weights from best epoch are automatically used!
ended training at: 05:15:55
Feature importance:
[('Area', 0.2921777544025266), ('Baths', 0.2457300521450002), ('Beds', 0.1048944544219912), ('Latitude', 0.18241277054011004), ('Longitude', 0.08078838861159797), ('Month', 0.0795519456933805), ('Year', 0.014444634185393466)]
Mean squared error is of 1002243578.0207335
Mean absolute error:21681.50157040201
MAPE:0.26337599373614995
R2 score:0.7503544289561701
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:15:56
epoch 0  | loss: 0.10811 | val_0_rmse: 0.1967  | val_1_rmse: 0.19351 |  0:00:02s
epoch 1  | loss: 0.01921 | val_0_rmse: 0.19771 | val_1_rmse: 0.19455 |  0:00:04s
epoch 2  | loss: 0.01717 | val_0_rmse: 0.19475 | val_1_rmse: 0.19144 |  0:00:06s
epoch 3  | loss: 0.01653 | val_0_rmse: 0.18258 | val_1_rmse: 0.17949 |  0:00:08s
epoch 4  | loss: 0.01655 | val_0_rmse: 0.17653 | val_1_rmse: 0.17362 |  0:00:10s
epoch 5  | loss: 0.01586 | val_0_rmse: 0.17058 | val_1_rmse: 0.16804 |  0:00:12s
epoch 6  | loss: 0.01554 | val_0_rmse: 0.15787 | val_1_rmse: 0.15526 |  0:00:14s
epoch 7  | loss: 0.0152  | val_0_rmse: 0.15073 | val_1_rmse: 0.14835 |  0:00:16s
epoch 8  | loss: 0.01539 | val_0_rmse: 0.1455  | val_1_rmse: 0.14352 |  0:00:18s
epoch 9  | loss: 0.01489 | val_0_rmse: 0.13676 | val_1_rmse: 0.13482 |  0:00:20s
epoch 10 | loss: 0.0149  | val_0_rmse: 0.13333 | val_1_rmse: 0.13212 |  0:00:22s
epoch 11 | loss: 0.01593 | val_0_rmse: 0.12755 | val_1_rmse: 0.12639 |  0:00:24s
epoch 12 | loss: 0.01491 | val_0_rmse: 0.12469 | val_1_rmse: 0.1236  |  0:00:26s
epoch 13 | loss: 0.01489 | val_0_rmse: 0.12407 | val_1_rmse: 0.12288 |  0:00:28s
epoch 14 | loss: 0.01511 | val_0_rmse: 0.1225  | val_1_rmse: 0.12094 |  0:00:30s
epoch 15 | loss: 0.01516 | val_0_rmse: 0.11927 | val_1_rmse: 0.11858 |  0:00:32s
epoch 16 | loss: 0.01461 | val_0_rmse: 0.11865 | val_1_rmse: 0.11798 |  0:00:34s
epoch 17 | loss: 0.01502 | val_0_rmse: 0.11822 | val_1_rmse: 0.11732 |  0:00:36s
epoch 18 | loss: 0.01444 | val_0_rmse: 0.11607 | val_1_rmse: 0.1153  |  0:00:38s
epoch 19 | loss: 0.01423 | val_0_rmse: 0.11571 | val_1_rmse: 0.11472 |  0:00:40s
epoch 20 | loss: 0.01416 | val_0_rmse: 0.11756 | val_1_rmse: 0.11712 |  0:00:42s
epoch 21 | loss: 0.01442 | val_0_rmse: 0.11452 | val_1_rmse: 0.11431 |  0:00:44s
epoch 22 | loss: 0.01442 | val_0_rmse: 0.11676 | val_1_rmse: 0.11678 |  0:00:46s
epoch 23 | loss: 0.01427 | val_0_rmse: 0.11598 | val_1_rmse: 0.11481 |  0:00:48s
epoch 24 | loss: 0.01433 | val_0_rmse: 0.11807 | val_1_rmse: 0.1174  |  0:00:50s
epoch 25 | loss: 0.01415 | val_0_rmse: 0.11536 | val_1_rmse: 0.11465 |  0:00:52s
epoch 26 | loss: 0.01413 | val_0_rmse: 0.12117 | val_1_rmse: 0.12055 |  0:00:54s
epoch 27 | loss: 0.01414 | val_0_rmse: 0.11575 | val_1_rmse: 0.11486 |  0:00:56s
epoch 28 | loss: 0.01404 | val_0_rmse: 0.11735 | val_1_rmse: 0.11718 |  0:00:58s
epoch 29 | loss: 0.01401 | val_0_rmse: 0.11912 | val_1_rmse: 0.1189  |  0:01:00s
epoch 30 | loss: 0.01476 | val_0_rmse: 0.12072 | val_1_rmse: 0.12092 |  0:01:02s
epoch 31 | loss: 0.01455 | val_0_rmse: 0.11908 | val_1_rmse: 0.11811 |  0:01:04s
epoch 32 | loss: 0.01415 | val_0_rmse: 0.11767 | val_1_rmse: 0.11765 |  0:01:06s
epoch 33 | loss: 0.01397 | val_0_rmse: 0.114   | val_1_rmse: 0.11375 |  0:01:08s
epoch 34 | loss: 0.01403 | val_0_rmse: 0.11682 | val_1_rmse: 0.11686 |  0:01:10s
epoch 35 | loss: 0.01408 | val_0_rmse: 0.11347 | val_1_rmse: 0.11256 |  0:01:12s
epoch 36 | loss: 0.01397 | val_0_rmse: 0.11427 | val_1_rmse: 0.11365 |  0:01:14s
epoch 37 | loss: 0.014   | val_0_rmse: 0.11481 | val_1_rmse: 0.11396 |  0:01:16s
epoch 38 | loss: 0.01365 | val_0_rmse: 0.11381 | val_1_rmse: 0.11343 |  0:01:18s
epoch 39 | loss: 0.01375 | val_0_rmse: 0.11266 | val_1_rmse: 0.11241 |  0:01:20s
epoch 40 | loss: 0.01368 | val_0_rmse: 0.1177  | val_1_rmse: 0.11773 |  0:01:22s
epoch 41 | loss: 0.01397 | val_0_rmse: 0.11323 | val_1_rmse: 0.11231 |  0:01:24s
epoch 42 | loss: 0.0137  | val_0_rmse: 0.11398 | val_1_rmse: 0.11336 |  0:01:26s
epoch 43 | loss: 0.01365 | val_0_rmse: 0.11216 | val_1_rmse: 0.1113  |  0:01:28s
epoch 44 | loss: 0.01399 | val_0_rmse: 0.1132  | val_1_rmse: 0.1122  |  0:01:30s
epoch 45 | loss: 0.01377 | val_0_rmse: 0.11203 | val_1_rmse: 0.11129 |  0:01:32s
epoch 46 | loss: 0.01374 | val_0_rmse: 0.11245 | val_1_rmse: 0.11199 |  0:01:34s
epoch 47 | loss: 0.01341 | val_0_rmse: 0.11242 | val_1_rmse: 0.11195 |  0:01:36s
epoch 48 | loss: 0.01345 | val_0_rmse: 0.11404 | val_1_rmse: 0.11341 |  0:01:38s
epoch 49 | loss: 0.01405 | val_0_rmse: 0.1147  | val_1_rmse: 0.11463 |  0:01:40s
epoch 50 | loss: 0.01351 | val_0_rmse: 0.11165 | val_1_rmse: 0.11149 |  0:01:42s
epoch 51 | loss: 0.01348 | val_0_rmse: 0.11403 | val_1_rmse: 0.11353 |  0:01:44s
epoch 52 | loss: 0.01357 | val_0_rmse: 0.11306 | val_1_rmse: 0.1121  |  0:01:46s
epoch 53 | loss: 0.01361 | val_0_rmse: 0.11425 | val_1_rmse: 0.11312 |  0:01:48s
epoch 54 | loss: 0.01372 | val_0_rmse: 0.11382 | val_1_rmse: 0.11377 |  0:01:50s
epoch 55 | loss: 0.01336 | val_0_rmse: 0.11348 | val_1_rmse: 0.11347 |  0:01:52s
epoch 56 | loss: 0.01363 | val_0_rmse: 0.11607 | val_1_rmse: 0.11536 |  0:01:54s
epoch 57 | loss: 0.01353 | val_0_rmse: 0.1171  | val_1_rmse: 0.1174  |  0:01:56s
epoch 58 | loss: 0.01347 | val_0_rmse: 0.11119 | val_1_rmse: 0.11058 |  0:01:58s
epoch 59 | loss: 0.01317 | val_0_rmse: 0.11125 | val_1_rmse: 0.11096 |  0:02:00s
epoch 60 | loss: 0.0135  | val_0_rmse: 0.11212 | val_1_rmse: 0.11137 |  0:02:02s
epoch 61 | loss: 0.01364 | val_0_rmse: 0.11399 | val_1_rmse: 0.11263 |  0:02:04s
epoch 62 | loss: 0.01361 | val_0_rmse: 0.11132 | val_1_rmse: 0.11088 |  0:02:06s
epoch 63 | loss: 0.01331 | val_0_rmse: 0.11174 | val_1_rmse: 0.11099 |  0:02:08s
epoch 64 | loss: 0.01374 | val_0_rmse: 0.11613 | val_1_rmse: 0.11725 |  0:02:10s
epoch 65 | loss: 0.01394 | val_0_rmse: 0.11333 | val_1_rmse: 0.11178 |  0:02:12s
epoch 66 | loss: 0.01358 | val_0_rmse: 0.11322 | val_1_rmse: 0.11262 |  0:02:14s
epoch 67 | loss: 0.01408 | val_0_rmse: 0.11632 | val_1_rmse: 0.1158  |  0:02:16s
epoch 68 | loss: 0.01496 | val_0_rmse: 0.11486 | val_1_rmse: 0.11322 |  0:02:18s
epoch 69 | loss: 0.01377 | val_0_rmse: 0.11725 | val_1_rmse: 0.11665 |  0:02:20s
epoch 70 | loss: 0.01373 | val_0_rmse: 0.11235 | val_1_rmse: 0.11115 |  0:02:22s
epoch 71 | loss: 0.01342 | val_0_rmse: 0.12208 | val_1_rmse: 0.1207  |  0:02:24s
epoch 72 | loss: 0.01343 | val_0_rmse: 0.11292 | val_1_rmse: 0.11187 |  0:02:26s
epoch 73 | loss: 0.01312 | val_0_rmse: 0.11368 | val_1_rmse: 0.11365 |  0:02:28s
epoch 74 | loss: 0.01327 | val_0_rmse: 0.11547 | val_1_rmse: 0.11439 |  0:02:30s
epoch 75 | loss: 0.01319 | val_0_rmse: 0.11115 | val_1_rmse: 0.11077 |  0:02:32s
epoch 76 | loss: 0.01352 | val_0_rmse: 0.11588 | val_1_rmse: 0.11468 |  0:02:34s
epoch 77 | loss: 0.01347 | val_0_rmse: 0.11285 | val_1_rmse: 0.11199 |  0:02:36s
epoch 78 | loss: 0.01364 | val_0_rmse: 0.12402 | val_1_rmse: 0.12341 |  0:02:38s
epoch 79 | loss: 0.01283 | val_0_rmse: 0.10961 | val_1_rmse: 0.10935 |  0:02:40s
epoch 80 | loss: 0.01282 | val_0_rmse: 0.11131 | val_1_rmse: 0.1102  |  0:02:42s
epoch 81 | loss: 0.01259 | val_0_rmse: 0.12281 | val_1_rmse: 0.12304 |  0:02:44s
epoch 82 | loss: 0.01262 | val_0_rmse: 0.10775 | val_1_rmse: 0.1074  |  0:02:46s
epoch 83 | loss: 0.01243 | val_0_rmse: 0.10749 | val_1_rmse: 0.10691 |  0:02:48s
epoch 84 | loss: 0.01251 | val_0_rmse: 0.11275 | val_1_rmse: 0.11243 |  0:02:50s
epoch 85 | loss: 0.01229 | val_0_rmse: 0.11118 | val_1_rmse: 0.11089 |  0:02:52s
epoch 86 | loss: 0.01264 | val_0_rmse: 0.11176 | val_1_rmse: 0.1125  |  0:02:54s
epoch 87 | loss: 0.01286 | val_0_rmse: 0.11048 | val_1_rmse: 0.11147 |  0:02:56s
epoch 88 | loss: 0.01277 | val_0_rmse: 0.11346 | val_1_rmse: 0.11314 |  0:02:58s
epoch 89 | loss: 0.0128  | val_0_rmse: 0.12819 | val_1_rmse: 0.12763 |  0:03:00s
epoch 90 | loss: 0.01275 | val_0_rmse: 0.1142  | val_1_rmse: 0.11524 |  0:03:02s
epoch 91 | loss: 0.01238 | val_0_rmse: 0.10844 | val_1_rmse: 0.10872 |  0:03:04s
epoch 92 | loss: 0.01229 | val_0_rmse: 0.10792 | val_1_rmse: 0.10798 |  0:03:06s
epoch 93 | loss: 0.01279 | val_0_rmse: 0.11185 | val_1_rmse: 0.1115  |  0:03:08s
epoch 94 | loss: 0.01268 | val_0_rmse: 0.10851 | val_1_rmse: 0.10829 |  0:03:10s
epoch 95 | loss: 0.01242 | val_0_rmse: 0.11492 | val_1_rmse: 0.1149  |  0:03:13s
epoch 96 | loss: 0.0124  | val_0_rmse: 0.11211 | val_1_rmse: 0.11253 |  0:03:15s
epoch 97 | loss: 0.01233 | val_0_rmse: 0.12593 | val_1_rmse: 0.12507 |  0:03:17s
epoch 98 | loss: 0.01246 | val_0_rmse: 0.11152 | val_1_rmse: 0.11146 |  0:03:19s
epoch 99 | loss: 0.01248 | val_0_rmse: 0.11478 | val_1_rmse: 0.11443 |  0:03:21s
epoch 100| loss: 0.01216 | val_0_rmse: 0.11492 | val_1_rmse: 0.11411 |  0:03:23s
epoch 101| loss: 0.0123  | val_0_rmse: 0.11075 | val_1_rmse: 0.11138 |  0:03:25s
epoch 102| loss: 0.01244 | val_0_rmse: 0.10943 | val_1_rmse: 0.10803 |  0:03:27s
epoch 103| loss: 0.01237 | val_0_rmse: 0.11833 | val_1_rmse: 0.11741 |  0:03:29s
epoch 104| loss: 0.01273 | val_0_rmse: 0.11122 | val_1_rmse: 0.1119  |  0:03:31s
epoch 105| loss: 0.01213 | val_0_rmse: 0.10741 | val_1_rmse: 0.1077  |  0:03:33s
epoch 106| loss: 0.01207 | val_0_rmse: 0.10727 | val_1_rmse: 0.10778 |  0:03:35s
epoch 107| loss: 0.01221 | val_0_rmse: 0.10685 | val_1_rmse: 0.10632 |  0:03:37s
epoch 108| loss: 0.01224 | val_0_rmse: 0.10965 | val_1_rmse: 0.10897 |  0:03:39s
epoch 109| loss: 0.01237 | val_0_rmse: 0.1073  | val_1_rmse: 0.10664 |  0:03:41s
epoch 110| loss: 0.01209 | val_0_rmse: 0.1081  | val_1_rmse: 0.10798 |  0:03:43s
epoch 111| loss: 0.01204 | val_0_rmse: 0.11534 | val_1_rmse: 0.11514 |  0:03:45s
epoch 112| loss: 0.01224 | val_0_rmse: 0.10739 | val_1_rmse: 0.10696 |  0:03:47s
epoch 113| loss: 0.01217 | val_0_rmse: 0.11008 | val_1_rmse: 0.1099  |  0:03:49s
epoch 114| loss: 0.01209 | val_0_rmse: 0.11183 | val_1_rmse: 0.11117 |  0:03:51s
epoch 115| loss: 0.01216 | val_0_rmse: 0.11721 | val_1_rmse: 0.11746 |  0:03:53s
epoch 116| loss: 0.01218 | val_0_rmse: 0.10898 | val_1_rmse: 0.10883 |  0:03:55s
epoch 117| loss: 0.012   | val_0_rmse: 0.10964 | val_1_rmse: 0.11042 |  0:03:57s
epoch 118| loss: 0.01188 | val_0_rmse: 0.10859 | val_1_rmse: 0.10851 |  0:03:59s
epoch 119| loss: 0.01206 | val_0_rmse: 0.10899 | val_1_rmse: 0.10934 |  0:04:01s
epoch 120| loss: 0.01231 | val_0_rmse: 0.1106  | val_1_rmse: 0.10982 |  0:04:03s
epoch 121| loss: 0.01201 | val_0_rmse: 0.10795 | val_1_rmse: 0.10758 |  0:04:05s
epoch 122| loss: 0.01189 | val_0_rmse: 0.12405 | val_1_rmse: 0.12407 |  0:04:07s
epoch 123| loss: 0.01191 | val_0_rmse: 0.11238 | val_1_rmse: 0.11281 |  0:04:09s
epoch 124| loss: 0.01203 | val_0_rmse: 0.10724 | val_1_rmse: 0.10695 |  0:04:11s
epoch 125| loss: 0.01211 | val_0_rmse: 0.10778 | val_1_rmse: 0.10795 |  0:04:13s
epoch 126| loss: 0.01204 | val_0_rmse: 0.12895 | val_1_rmse: 0.12975 |  0:04:15s
epoch 127| loss: 0.01277 | val_0_rmse: 0.11248 | val_1_rmse: 0.1114  |  0:04:17s
epoch 128| loss: 0.01265 | val_0_rmse: 0.11716 | val_1_rmse: 0.11688 |  0:04:19s
epoch 129| loss: 0.01199 | val_0_rmse: 0.10682 | val_1_rmse: 0.10673 |  0:04:21s
epoch 130| loss: 0.01238 | val_0_rmse: 0.10779 | val_1_rmse: 0.10773 |  0:04:23s
epoch 131| loss: 0.01247 | val_0_rmse: 0.10919 | val_1_rmse: 0.10809 |  0:04:25s
epoch 132| loss: 0.01238 | val_0_rmse: 0.10773 | val_1_rmse: 0.10795 |  0:04:27s
epoch 133| loss: 0.01194 | val_0_rmse: 0.10747 | val_1_rmse: 0.10733 |  0:04:29s
epoch 134| loss: 0.01206 | val_0_rmse: 0.11889 | val_1_rmse: 0.11831 |  0:04:31s
epoch 135| loss: 0.01266 | val_0_rmse: 0.11415 | val_1_rmse: 0.11447 |  0:04:33s
epoch 136| loss: 0.01291 | val_0_rmse: 0.10949 | val_1_rmse: 0.10901 |  0:04:35s
epoch 137| loss: 0.01223 | val_0_rmse: 0.10962 | val_1_rmse: 0.10979 |  0:04:37s

Early stopping occured at epoch 137 with best_epoch = 107 and best_val_1_rmse = 0.10632
Best weights from best epoch are automatically used!
ended training at: 05:20:33
Feature importance:
[('Area', 0.3384254440396186), ('Baths', 0.20188619761013743), ('Beds', 0.009717735171419786), ('Latitude', 0.17928083796830607), ('Longitude', 0.16433995828238146), ('Month', 0.10634982692813666), ('Year', 0.0)]
Mean squared error is of 994323711.7485343
Mean absolute error:21655.38330289637
MAPE:0.275455275481284
R2 score:0.7497061128484254
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:20:34
epoch 0  | loss: 0.10367 | val_0_rmse: 0.24199 | val_1_rmse: 0.24467 |  0:00:02s
epoch 1  | loss: 0.0188  | val_0_rmse: 0.2557  | val_1_rmse: 0.25837 |  0:00:04s
epoch 2  | loss: 0.01737 | val_0_rmse: 0.23205 | val_1_rmse: 0.23556 |  0:00:06s
epoch 3  | loss: 0.01684 | val_0_rmse: 0.19841 | val_1_rmse: 0.2019  |  0:00:08s
epoch 4  | loss: 0.01617 | val_0_rmse: 0.17849 | val_1_rmse: 0.18184 |  0:00:10s
epoch 5  | loss: 0.01584 | val_0_rmse: 0.16315 | val_1_rmse: 0.16664 |  0:00:12s
epoch 6  | loss: 0.0161  | val_0_rmse: 0.15938 | val_1_rmse: 0.16337 |  0:00:14s
epoch 7  | loss: 0.01561 | val_0_rmse: 0.15448 | val_1_rmse: 0.15808 |  0:00:16s
epoch 8  | loss: 0.01585 | val_0_rmse: 0.14586 | val_1_rmse: 0.15002 |  0:00:18s
epoch 9  | loss: 0.01518 | val_0_rmse: 0.14245 | val_1_rmse: 0.14669 |  0:00:20s
epoch 10 | loss: 0.01499 | val_0_rmse: 0.13552 | val_1_rmse: 0.13888 |  0:00:22s
epoch 11 | loss: 0.01489 | val_0_rmse: 0.13325 | val_1_rmse: 0.13701 |  0:00:24s
epoch 12 | loss: 0.01478 | val_0_rmse: 0.12501 | val_1_rmse: 0.1289  |  0:00:26s
epoch 13 | loss: 0.01473 | val_0_rmse: 0.12095 | val_1_rmse: 0.1248  |  0:00:28s
epoch 14 | loss: 0.01519 | val_0_rmse: 0.12558 | val_1_rmse: 0.12933 |  0:00:30s
epoch 15 | loss: 0.01479 | val_0_rmse: 0.12021 | val_1_rmse: 0.12378 |  0:00:32s
epoch 16 | loss: 0.01476 | val_0_rmse: 0.12155 | val_1_rmse: 0.12561 |  0:00:34s
epoch 17 | loss: 0.01472 | val_0_rmse: 0.12069 | val_1_rmse: 0.12484 |  0:00:36s
epoch 18 | loss: 0.01449 | val_0_rmse: 0.1162  | val_1_rmse: 0.121   |  0:00:38s
epoch 19 | loss: 0.01423 | val_0_rmse: 0.11775 | val_1_rmse: 0.12134 |  0:00:40s
epoch 20 | loss: 0.01452 | val_0_rmse: 0.11792 | val_1_rmse: 0.12172 |  0:00:42s
epoch 21 | loss: 0.01426 | val_0_rmse: 0.11602 | val_1_rmse: 0.12039 |  0:00:44s
epoch 22 | loss: 0.01433 | val_0_rmse: 0.1199  | val_1_rmse: 0.1239  |  0:00:46s
epoch 23 | loss: 0.01412 | val_0_rmse: 0.11943 | val_1_rmse: 0.12349 |  0:00:48s
epoch 24 | loss: 0.01413 | val_0_rmse: 0.11386 | val_1_rmse: 0.11826 |  0:00:50s
epoch 25 | loss: 0.01396 | val_0_rmse: 0.11589 | val_1_rmse: 0.11958 |  0:00:52s
epoch 26 | loss: 0.01392 | val_0_rmse: 0.11323 | val_1_rmse: 0.11756 |  0:00:54s
epoch 27 | loss: 0.01374 | val_0_rmse: 0.11329 | val_1_rmse: 0.11761 |  0:00:56s
epoch 28 | loss: 0.01363 | val_0_rmse: 0.11593 | val_1_rmse: 0.11953 |  0:00:58s
epoch 29 | loss: 0.01348 | val_0_rmse: 0.11302 | val_1_rmse: 0.11757 |  0:01:00s
epoch 30 | loss: 0.01349 | val_0_rmse: 0.11285 | val_1_rmse: 0.11733 |  0:01:02s
epoch 31 | loss: 0.01368 | val_0_rmse: 0.11408 | val_1_rmse: 0.1192  |  0:01:04s
epoch 32 | loss: 0.0138  | val_0_rmse: 0.11881 | val_1_rmse: 0.12221 |  0:01:06s
epoch 33 | loss: 0.01431 | val_0_rmse: 0.11371 | val_1_rmse: 0.11816 |  0:01:08s
epoch 34 | loss: 0.01348 | val_0_rmse: 0.11217 | val_1_rmse: 0.11701 |  0:01:10s
epoch 35 | loss: 0.01343 | val_0_rmse: 0.11868 | val_1_rmse: 0.12317 |  0:01:12s
epoch 36 | loss: 0.01338 | val_0_rmse: 0.11222 | val_1_rmse: 0.1167  |  0:01:14s
epoch 37 | loss: 0.01328 | val_0_rmse: 0.11197 | val_1_rmse: 0.11561 |  0:01:16s
epoch 38 | loss: 0.01328 | val_0_rmse: 0.11223 | val_1_rmse: 0.1167  |  0:01:18s
epoch 39 | loss: 0.01331 | val_0_rmse: 0.11273 | val_1_rmse: 0.11681 |  0:01:20s
epoch 40 | loss: 0.01332 | val_0_rmse: 0.11302 | val_1_rmse: 0.1178  |  0:01:22s
epoch 41 | loss: 0.01328 | val_0_rmse: 0.11615 | val_1_rmse: 0.12055 |  0:01:24s
epoch 42 | loss: 0.01326 | val_0_rmse: 0.11203 | val_1_rmse: 0.11625 |  0:01:26s
epoch 43 | loss: 0.01309 | val_0_rmse: 0.11248 | val_1_rmse: 0.11734 |  0:01:28s
epoch 44 | loss: 0.01308 | val_0_rmse: 0.11072 | val_1_rmse: 0.11567 |  0:01:30s
epoch 45 | loss: 0.01309 | val_0_rmse: 0.11082 | val_1_rmse: 0.1159  |  0:01:32s
epoch 46 | loss: 0.01336 | val_0_rmse: 0.11703 | val_1_rmse: 0.12142 |  0:01:34s
epoch 47 | loss: 0.0135  | val_0_rmse: 0.11026 | val_1_rmse: 0.11496 |  0:01:36s
epoch 48 | loss: 0.01322 | val_0_rmse: 0.11461 | val_1_rmse: 0.11911 |  0:01:38s
epoch 49 | loss: 0.01297 | val_0_rmse: 0.11263 | val_1_rmse: 0.11713 |  0:01:40s
epoch 50 | loss: 0.01303 | val_0_rmse: 0.11181 | val_1_rmse: 0.11586 |  0:01:42s
epoch 51 | loss: 0.01313 | val_0_rmse: 0.11069 | val_1_rmse: 0.11572 |  0:01:44s
epoch 52 | loss: 0.01331 | val_0_rmse: 0.11237 | val_1_rmse: 0.11739 |  0:01:46s
epoch 53 | loss: 0.01359 | val_0_rmse: 0.11362 | val_1_rmse: 0.11767 |  0:01:48s
epoch 54 | loss: 0.01313 | val_0_rmse: 0.11204 | val_1_rmse: 0.1158  |  0:01:50s
epoch 55 | loss: 0.01294 | val_0_rmse: 0.1182  | val_1_rmse: 0.12304 |  0:01:52s
epoch 56 | loss: 0.01302 | val_0_rmse: 0.11076 | val_1_rmse: 0.11537 |  0:01:54s
epoch 57 | loss: 0.01306 | val_0_rmse: 0.10953 | val_1_rmse: 0.11436 |  0:01:56s
epoch 58 | loss: 0.01291 | val_0_rmse: 0.1139  | val_1_rmse: 0.11767 |  0:01:58s
epoch 59 | loss: 0.01334 | val_0_rmse: 0.11106 | val_1_rmse: 0.11568 |  0:02:00s
epoch 60 | loss: 0.01317 | val_0_rmse: 0.11269 | val_1_rmse: 0.11683 |  0:02:02s
epoch 61 | loss: 0.01356 | val_0_rmse: 0.11572 | val_1_rmse: 0.11986 |  0:02:04s
epoch 62 | loss: 0.01402 | val_0_rmse: 0.11672 | val_1_rmse: 0.11988 |  0:02:06s
epoch 63 | loss: 0.0142  | val_0_rmse: 0.12919 | val_1_rmse: 0.13234 |  0:02:08s
epoch 64 | loss: 0.01461 | val_0_rmse: 0.11403 | val_1_rmse: 0.11812 |  0:02:10s
epoch 65 | loss: 0.01454 | val_0_rmse: 0.11835 | val_1_rmse: 0.12146 |  0:02:12s
epoch 66 | loss: 0.01426 | val_0_rmse: 0.11482 | val_1_rmse: 0.11885 |  0:02:14s
epoch 67 | loss: 0.01391 | val_0_rmse: 0.11683 | val_1_rmse: 0.1193  |  0:02:16s
epoch 68 | loss: 0.01372 | val_0_rmse: 0.11353 | val_1_rmse: 0.11736 |  0:02:18s
epoch 69 | loss: 0.01344 | val_0_rmse: 0.1137  | val_1_rmse: 0.11848 |  0:02:20s
epoch 70 | loss: 0.01339 | val_0_rmse: 0.11606 | val_1_rmse: 0.12068 |  0:02:22s
epoch 71 | loss: 0.01324 | val_0_rmse: 0.11398 | val_1_rmse: 0.11838 |  0:02:24s
epoch 72 | loss: 0.01329 | val_0_rmse: 0.12175 | val_1_rmse: 0.12653 |  0:02:26s
epoch 73 | loss: 0.01359 | val_0_rmse: 0.1198  | val_1_rmse: 0.12342 |  0:02:28s
epoch 74 | loss: 0.01346 | val_0_rmse: 0.11251 | val_1_rmse: 0.11579 |  0:02:30s
epoch 75 | loss: 0.01316 | val_0_rmse: 0.11056 | val_1_rmse: 0.11466 |  0:02:32s
epoch 76 | loss: 0.01288 | val_0_rmse: 0.10957 | val_1_rmse: 0.11504 |  0:02:34s
epoch 77 | loss: 0.0128  | val_0_rmse: 0.11097 | val_1_rmse: 0.11569 |  0:02:36s
epoch 78 | loss: 0.01295 | val_0_rmse: 0.11158 | val_1_rmse: 0.11605 |  0:02:38s
epoch 79 | loss: 0.01297 | val_0_rmse: 0.10996 | val_1_rmse: 0.11383 |  0:02:40s
epoch 80 | loss: 0.01266 | val_0_rmse: 0.10971 | val_1_rmse: 0.11486 |  0:02:42s
epoch 81 | loss: 0.01261 | val_0_rmse: 0.11206 | val_1_rmse: 0.11698 |  0:02:44s
epoch 82 | loss: 0.01267 | val_0_rmse: 0.11124 | val_1_rmse: 0.11522 |  0:02:46s
epoch 83 | loss: 0.01298 | val_0_rmse: 0.11063 | val_1_rmse: 0.1148  |  0:02:48s
epoch 84 | loss: 0.01297 | val_0_rmse: 0.11135 | val_1_rmse: 0.11585 |  0:02:50s
epoch 85 | loss: 0.0129  | val_0_rmse: 0.1113  | val_1_rmse: 0.11494 |  0:02:52s
epoch 86 | loss: 0.0128  | val_0_rmse: 0.10879 | val_1_rmse: 0.11347 |  0:02:54s
epoch 87 | loss: 0.01242 | val_0_rmse: 0.10853 | val_1_rmse: 0.11209 |  0:02:56s
epoch 88 | loss: 0.01244 | val_0_rmse: 0.10735 | val_1_rmse: 0.11125 |  0:02:58s
epoch 89 | loss: 0.01211 | val_0_rmse: 0.10847 | val_1_rmse: 0.11164 |  0:03:00s
epoch 90 | loss: 0.01242 | val_0_rmse: 0.11205 | val_1_rmse: 0.11588 |  0:03:02s
epoch 91 | loss: 0.01235 | val_0_rmse: 0.10693 | val_1_rmse: 0.11093 |  0:03:04s
epoch 92 | loss: 0.01317 | val_0_rmse: 0.13952 | val_1_rmse: 0.14242 |  0:03:06s
epoch 93 | loss: 0.0152  | val_0_rmse: 0.12329 | val_1_rmse: 0.12613 |  0:03:08s
epoch 94 | loss: 0.01454 | val_0_rmse: 0.11777 | val_1_rmse: 0.12067 |  0:03:10s
epoch 95 | loss: 0.01386 | val_0_rmse: 0.11413 | val_1_rmse: 0.11804 |  0:03:12s
epoch 96 | loss: 0.01357 | val_0_rmse: 0.11498 | val_1_rmse: 0.11759 |  0:03:14s
epoch 97 | loss: 0.01336 | val_0_rmse: 0.11689 | val_1_rmse: 0.11972 |  0:03:16s
epoch 98 | loss: 0.01291 | val_0_rmse: 0.11313 | val_1_rmse: 0.11645 |  0:03:18s
epoch 99 | loss: 0.01373 | val_0_rmse: 0.12069 | val_1_rmse: 0.12477 |  0:03:20s
epoch 100| loss: 0.01364 | val_0_rmse: 0.12202 | val_1_rmse: 0.1253  |  0:03:22s
epoch 101| loss: 0.01438 | val_0_rmse: 0.1155  | val_1_rmse: 0.11902 |  0:03:24s
epoch 102| loss: 0.01257 | val_0_rmse: 0.11445 | val_1_rmse: 0.11847 |  0:03:26s
epoch 103| loss: 0.01255 | val_0_rmse: 0.11043 | val_1_rmse: 0.11421 |  0:03:28s
epoch 104| loss: 0.01264 | val_0_rmse: 0.11559 | val_1_rmse: 0.11971 |  0:03:30s
epoch 105| loss: 0.01266 | val_0_rmse: 0.10861 | val_1_rmse: 0.11107 |  0:03:32s
epoch 106| loss: 0.01235 | val_0_rmse: 0.10831 | val_1_rmse: 0.1108  |  0:03:34s
epoch 107| loss: 0.01218 | val_0_rmse: 0.10858 | val_1_rmse: 0.1124  |  0:03:36s
epoch 108| loss: 0.01201 | val_0_rmse: 0.10964 | val_1_rmse: 0.11229 |  0:03:38s
epoch 109| loss: 0.01222 | val_0_rmse: 0.10892 | val_1_rmse: 0.11294 |  0:03:40s
epoch 110| loss: 0.01217 | val_0_rmse: 0.1124  | val_1_rmse: 0.11594 |  0:03:42s
epoch 111| loss: 0.01201 | val_0_rmse: 0.10707 | val_1_rmse: 0.11075 |  0:03:44s
epoch 112| loss: 0.01189 | val_0_rmse: 0.10996 | val_1_rmse: 0.11283 |  0:03:46s
epoch 113| loss: 0.01197 | val_0_rmse: 0.10742 | val_1_rmse: 0.11019 |  0:03:48s
epoch 114| loss: 0.01184 | val_0_rmse: 0.10809 | val_1_rmse: 0.11123 |  0:03:50s
epoch 115| loss: 0.01186 | val_0_rmse: 0.1085  | val_1_rmse: 0.11083 |  0:03:52s
epoch 116| loss: 0.01173 | val_0_rmse: 0.10882 | val_1_rmse: 0.1132  |  0:03:54s
epoch 117| loss: 0.01208 | val_0_rmse: 0.10702 | val_1_rmse: 0.11053 |  0:03:56s
epoch 118| loss: 0.01181 | val_0_rmse: 0.10718 | val_1_rmse: 0.11023 |  0:03:58s
epoch 119| loss: 0.01179 | val_0_rmse: 0.10688 | val_1_rmse: 0.10923 |  0:04:00s
epoch 120| loss: 0.01185 | val_0_rmse: 0.10911 | val_1_rmse: 0.11202 |  0:04:02s
epoch 121| loss: 0.01193 | val_0_rmse: 0.10564 | val_1_rmse: 0.10855 |  0:04:04s
epoch 122| loss: 0.01178 | val_0_rmse: 0.10646 | val_1_rmse: 0.10968 |  0:04:06s
epoch 123| loss: 0.01171 | val_0_rmse: 0.11589 | val_1_rmse: 0.11878 |  0:04:08s
epoch 124| loss: 0.01192 | val_0_rmse: 0.10966 | val_1_rmse: 0.11327 |  0:04:10s
epoch 125| loss: 0.01186 | val_0_rmse: 0.10589 | val_1_rmse: 0.10848 |  0:04:12s
epoch 126| loss: 0.0117  | val_0_rmse: 0.10913 | val_1_rmse: 0.1126  |  0:04:14s
epoch 127| loss: 0.01165 | val_0_rmse: 0.10581 | val_1_rmse: 0.10892 |  0:04:16s
epoch 128| loss: 0.01158 | val_0_rmse: 0.10897 | val_1_rmse: 0.11233 |  0:04:18s
epoch 129| loss: 0.01145 | val_0_rmse: 0.1061  | val_1_rmse: 0.1098  |  0:04:20s
epoch 130| loss: 0.0115  | val_0_rmse: 0.10805 | val_1_rmse: 0.11049 |  0:04:22s
epoch 131| loss: 0.01148 | val_0_rmse: 0.10428 | val_1_rmse: 0.10726 |  0:04:24s
epoch 132| loss: 0.01151 | val_0_rmse: 0.10554 | val_1_rmse: 0.10825 |  0:04:27s
epoch 133| loss: 0.01142 | val_0_rmse: 0.10874 | val_1_rmse: 0.11102 |  0:04:28s
epoch 134| loss: 0.01158 | val_0_rmse: 0.10883 | val_1_rmse: 0.11199 |  0:04:31s
epoch 135| loss: 0.0119  | val_0_rmse: 0.10595 | val_1_rmse: 0.10885 |  0:04:32s
epoch 136| loss: 0.01149 | val_0_rmse: 0.10866 | val_1_rmse: 0.11204 |  0:04:34s
epoch 137| loss: 0.01127 | val_0_rmse: 0.10549 | val_1_rmse: 0.10818 |  0:04:37s
epoch 138| loss: 0.01138 | val_0_rmse: 0.10681 | val_1_rmse: 0.10965 |  0:04:38s
epoch 139| loss: 0.01126 | val_0_rmse: 0.10352 | val_1_rmse: 0.10675 |  0:04:40s
epoch 140| loss: 0.01157 | val_0_rmse: 0.10648 | val_1_rmse: 0.10945 |  0:04:42s
epoch 141| loss: 0.01138 | val_0_rmse: 0.11004 | val_1_rmse: 0.1138  |  0:04:44s
epoch 142| loss: 0.01158 | val_0_rmse: 0.11085 | val_1_rmse: 0.11377 |  0:04:47s
epoch 143| loss: 0.01122 | val_0_rmse: 0.10614 | val_1_rmse: 0.10977 |  0:04:48s
epoch 144| loss: 0.01141 | val_0_rmse: 0.10559 | val_1_rmse: 0.10877 |  0:04:50s
epoch 145| loss: 0.01144 | val_0_rmse: 0.1031  | val_1_rmse: 0.10643 |  0:04:53s
epoch 146| loss: 0.01151 | val_0_rmse: 0.10607 | val_1_rmse: 0.10902 |  0:04:55s
epoch 147| loss: 0.01148 | val_0_rmse: 0.11009 | val_1_rmse: 0.11436 |  0:04:57s
epoch 148| loss: 0.01131 | val_0_rmse: 0.10716 | val_1_rmse: 0.11043 |  0:04:59s
epoch 149| loss: 0.01143 | val_0_rmse: 0.10951 | val_1_rmse: 0.11301 |  0:05:01s
Stop training because you reached max_epochs = 150 with best_epoch = 145 and best_val_1_rmse = 0.10643
Best weights from best epoch are automatically used!
ended training at: 05:25:35
Feature importance:
[('Area', 0.5577229791784376), ('Baths', 0.12032053786980951), ('Beds', 2.4462253929937625e-07), ('Latitude', 0.0), ('Longitude', 0.3147959706046229), ('Month', 0.007160267724590666), ('Year', 0.0)]
Mean squared error is of 1012276345.8884774
Mean absolute error:21555.336527488907
MAPE:0.2604421452378169
R2 score:0.7481277534668713
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:25:36
epoch 0  | loss: 0.10579 | val_0_rmse: 0.21419 | val_1_rmse: 0.21472 |  0:00:01s
epoch 1  | loss: 0.02032 | val_0_rmse: 0.22189 | val_1_rmse: 0.2232  |  0:00:04s
epoch 2  | loss: 0.01871 | val_0_rmse: 0.20465 | val_1_rmse: 0.20577 |  0:00:06s
epoch 3  | loss: 0.01686 | val_0_rmse: 0.18428 | val_1_rmse: 0.18618 |  0:00:08s
epoch 4  | loss: 0.01614 | val_0_rmse: 0.16839 | val_1_rmse: 0.17103 |  0:00:10s
epoch 5  | loss: 0.01558 | val_0_rmse: 0.15992 | val_1_rmse: 0.16367 |  0:00:12s
epoch 6  | loss: 0.0159  | val_0_rmse: 0.15761 | val_1_rmse: 0.16045 |  0:00:14s
epoch 7  | loss: 0.0154  | val_0_rmse: 0.14945 | val_1_rmse: 0.15265 |  0:00:16s
epoch 8  | loss: 0.01524 | val_0_rmse: 0.14241 | val_1_rmse: 0.14591 |  0:00:18s
epoch 9  | loss: 0.01519 | val_0_rmse: 0.13719 | val_1_rmse: 0.14059 |  0:00:20s
epoch 10 | loss: 0.01498 | val_0_rmse: 0.13338 | val_1_rmse: 0.13609 |  0:00:22s
epoch 11 | loss: 0.01467 | val_0_rmse: 0.12991 | val_1_rmse: 0.13327 |  0:00:24s
epoch 12 | loss: 0.01507 | val_0_rmse: 0.12302 | val_1_rmse: 0.12639 |  0:00:26s
epoch 13 | loss: 0.01449 | val_0_rmse: 0.12263 | val_1_rmse: 0.12691 |  0:00:28s
epoch 14 | loss: 0.01489 | val_0_rmse: 0.12806 | val_1_rmse: 0.13139 |  0:00:30s
epoch 15 | loss: 0.01484 | val_0_rmse: 0.12002 | val_1_rmse: 0.12422 |  0:00:32s
epoch 16 | loss: 0.01435 | val_0_rmse: 0.1177  | val_1_rmse: 0.12207 |  0:00:34s
epoch 17 | loss: 0.0144  | val_0_rmse: 0.11998 | val_1_rmse: 0.12275 |  0:00:36s
epoch 18 | loss: 0.01405 | val_0_rmse: 0.11772 | val_1_rmse: 0.12012 |  0:00:38s
epoch 19 | loss: 0.01399 | val_0_rmse: 0.12637 | val_1_rmse: 0.13106 |  0:00:40s
epoch 20 | loss: 0.01424 | val_0_rmse: 0.11508 | val_1_rmse: 0.11804 |  0:00:42s
epoch 21 | loss: 0.01429 | val_0_rmse: 0.11951 | val_1_rmse: 0.12308 |  0:00:44s
epoch 22 | loss: 0.0148  | val_0_rmse: 0.1169  | val_1_rmse: 0.12043 |  0:00:46s
epoch 23 | loss: 0.01409 | val_0_rmse: 0.11888 | val_1_rmse: 0.12236 |  0:00:48s
epoch 24 | loss: 0.0135  | val_0_rmse: 0.11241 | val_1_rmse: 0.11469 |  0:00:50s
epoch 25 | loss: 0.01343 | val_0_rmse: 0.11129 | val_1_rmse: 0.11363 |  0:00:52s
epoch 26 | loss: 0.01331 | val_0_rmse: 0.11174 | val_1_rmse: 0.11458 |  0:00:54s
epoch 27 | loss: 0.0139  | val_0_rmse: 0.11938 | val_1_rmse: 0.12313 |  0:00:56s
epoch 28 | loss: 0.01379 | val_0_rmse: 0.11406 | val_1_rmse: 0.11721 |  0:00:58s
epoch 29 | loss: 0.01369 | val_0_rmse: 0.11366 | val_1_rmse: 0.1154  |  0:01:00s
epoch 30 | loss: 0.0134  | val_0_rmse: 0.11046 | val_1_rmse: 0.11309 |  0:01:02s
epoch 31 | loss: 0.01356 | val_0_rmse: 0.11094 | val_1_rmse: 0.11378 |  0:01:04s
epoch 32 | loss: 0.01334 | val_0_rmse: 0.11133 | val_1_rmse: 0.11399 |  0:01:06s
epoch 33 | loss: 0.01359 | val_0_rmse: 0.11104 | val_1_rmse: 0.1139  |  0:01:08s
epoch 34 | loss: 0.01301 | val_0_rmse: 0.11033 | val_1_rmse: 0.11296 |  0:01:10s
epoch 35 | loss: 0.01367 | val_0_rmse: 0.11211 | val_1_rmse: 0.11493 |  0:01:12s
epoch 36 | loss: 0.01305 | val_0_rmse: 0.11126 | val_1_rmse: 0.11372 |  0:01:14s
epoch 37 | loss: 0.01288 | val_0_rmse: 0.11766 | val_1_rmse: 0.12066 |  0:01:16s
epoch 38 | loss: 0.01306 | val_0_rmse: 0.111   | val_1_rmse: 0.1131  |  0:01:18s
epoch 39 | loss: 0.01298 | val_0_rmse: 0.11766 | val_1_rmse: 0.12042 |  0:01:20s
epoch 40 | loss: 0.01309 | val_0_rmse: 0.11016 | val_1_rmse: 0.11278 |  0:01:22s
epoch 41 | loss: 0.01314 | val_0_rmse: 0.11002 | val_1_rmse: 0.11261 |  0:01:24s
epoch 42 | loss: 0.0128  | val_0_rmse: 0.11039 | val_1_rmse: 0.11335 |  0:01:26s
epoch 43 | loss: 0.01312 | val_0_rmse: 0.11173 | val_1_rmse: 0.1145  |  0:01:28s
epoch 44 | loss: 0.01323 | val_0_rmse: 0.11606 | val_1_rmse: 0.12039 |  0:01:30s
epoch 45 | loss: 0.01352 | val_0_rmse: 0.11067 | val_1_rmse: 0.11321 |  0:01:32s
epoch 46 | loss: 0.01286 | val_0_rmse: 0.11614 | val_1_rmse: 0.11928 |  0:01:34s
epoch 47 | loss: 0.01307 | val_0_rmse: 0.11007 | val_1_rmse: 0.11294 |  0:01:36s
epoch 48 | loss: 0.01299 | val_0_rmse: 0.11096 | val_1_rmse: 0.11373 |  0:01:38s
epoch 49 | loss: 0.01288 | val_0_rmse: 0.11289 | val_1_rmse: 0.11502 |  0:01:40s
epoch 50 | loss: 0.01299 | val_0_rmse: 0.11157 | val_1_rmse: 0.11435 |  0:01:42s
epoch 51 | loss: 0.01302 | val_0_rmse: 0.10971 | val_1_rmse: 0.11245 |  0:01:44s
epoch 52 | loss: 0.01278 | val_0_rmse: 0.11173 | val_1_rmse: 0.11452 |  0:01:46s
epoch 53 | loss: 0.01251 | val_0_rmse: 0.10886 | val_1_rmse: 0.11147 |  0:01:48s
epoch 54 | loss: 0.01261 | val_0_rmse: 0.10788 | val_1_rmse: 0.11101 |  0:01:50s
epoch 55 | loss: 0.01233 | val_0_rmse: 0.11471 | val_1_rmse: 0.11734 |  0:01:52s
epoch 56 | loss: 0.01241 | val_0_rmse: 0.10935 | val_1_rmse: 0.1118  |  0:01:54s
epoch 57 | loss: 0.0124  | val_0_rmse: 0.10787 | val_1_rmse: 0.11028 |  0:01:56s
epoch 58 | loss: 0.01213 | val_0_rmse: 0.10913 | val_1_rmse: 0.11192 |  0:01:58s
epoch 59 | loss: 0.01224 | val_0_rmse: 0.1093  | val_1_rmse: 0.11217 |  0:02:00s
epoch 60 | loss: 0.01217 | val_0_rmse: 0.11211 | val_1_rmse: 0.11542 |  0:02:02s
epoch 61 | loss: 0.01231 | val_0_rmse: 0.11213 | val_1_rmse: 0.11524 |  0:02:04s
epoch 62 | loss: 0.01232 | val_0_rmse: 0.10905 | val_1_rmse: 0.11192 |  0:02:06s
epoch 63 | loss: 0.01247 | val_0_rmse: 0.13025 | val_1_rmse: 0.13277 |  0:02:08s
epoch 64 | loss: 0.01269 | val_0_rmse: 0.10766 | val_1_rmse: 0.11026 |  0:02:10s
epoch 65 | loss: 0.01225 | val_0_rmse: 0.10703 | val_1_rmse: 0.10897 |  0:02:12s
epoch 66 | loss: 0.01241 | val_0_rmse: 0.11061 | val_1_rmse: 0.11305 |  0:02:14s
epoch 67 | loss: 0.01245 | val_0_rmse: 0.11118 | val_1_rmse: 0.11408 |  0:02:16s
epoch 68 | loss: 0.01321 | val_0_rmse: 0.12086 | val_1_rmse: 0.12354 |  0:02:18s
epoch 69 | loss: 0.01322 | val_0_rmse: 0.11141 | val_1_rmse: 0.11539 |  0:02:20s
epoch 70 | loss: 0.01265 | val_0_rmse: 0.11104 | val_1_rmse: 0.11412 |  0:02:22s
epoch 71 | loss: 0.0126  | val_0_rmse: 0.10846 | val_1_rmse: 0.11114 |  0:02:24s
epoch 72 | loss: 0.01238 | val_0_rmse: 0.11958 | val_1_rmse: 0.1226  |  0:02:26s
epoch 73 | loss: 0.01199 | val_0_rmse: 0.13475 | val_1_rmse: 0.13803 |  0:02:28s
epoch 74 | loss: 0.01223 | val_0_rmse: 0.11186 | val_1_rmse: 0.11481 |  0:02:30s
epoch 75 | loss: 0.0119  | val_0_rmse: 0.10532 | val_1_rmse: 0.10787 |  0:02:32s
epoch 76 | loss: 0.01195 | val_0_rmse: 0.11506 | val_1_rmse: 0.11745 |  0:02:34s
epoch 77 | loss: 0.01207 | val_0_rmse: 0.15033 | val_1_rmse: 0.15258 |  0:02:36s
epoch 78 | loss: 0.01245 | val_0_rmse: 0.11879 | val_1_rmse: 0.12137 |  0:02:38s
epoch 79 | loss: 0.01257 | val_0_rmse: 0.1107  | val_1_rmse: 0.11239 |  0:02:40s
epoch 80 | loss: 0.01248 | val_0_rmse: 0.10952 | val_1_rmse: 0.11209 |  0:02:42s
epoch 81 | loss: 0.01242 | val_0_rmse: 0.1119  | val_1_rmse: 0.11528 |  0:02:44s
epoch 82 | loss: 0.01259 | val_0_rmse: 0.10756 | val_1_rmse: 0.11032 |  0:02:46s
epoch 83 | loss: 0.01222 | val_0_rmse: 0.10785 | val_1_rmse: 0.11015 |  0:02:48s
epoch 84 | loss: 0.01195 | val_0_rmse: 0.10987 | val_1_rmse: 0.11237 |  0:02:50s
epoch 85 | loss: 0.01174 | val_0_rmse: 0.11441 | val_1_rmse: 0.1169  |  0:02:52s
epoch 86 | loss: 0.01166 | val_0_rmse: 0.10472 | val_1_rmse: 0.10765 |  0:02:54s
epoch 87 | loss: 0.01182 | val_0_rmse: 0.10832 | val_1_rmse: 0.1109  |  0:02:56s
epoch 88 | loss: 0.01228 | val_0_rmse: 0.12698 | val_1_rmse: 0.13061 |  0:02:58s
epoch 89 | loss: 0.01226 | val_0_rmse: 0.11962 | val_1_rmse: 0.12168 |  0:03:00s
epoch 90 | loss: 0.01191 | val_0_rmse: 0.10897 | val_1_rmse: 0.11157 |  0:03:02s
epoch 91 | loss: 0.01241 | val_0_rmse: 0.11168 | val_1_rmse: 0.11475 |  0:03:04s
epoch 92 | loss: 0.01204 | val_0_rmse: 0.11134 | val_1_rmse: 0.11299 |  0:03:06s
epoch 93 | loss: 0.01207 | val_0_rmse: 0.11294 | val_1_rmse: 0.11583 |  0:03:08s
epoch 94 | loss: 0.01234 | val_0_rmse: 0.10779 | val_1_rmse: 0.11019 |  0:03:10s
epoch 95 | loss: 0.01254 | val_0_rmse: 0.10919 | val_1_rmse: 0.11206 |  0:03:12s
epoch 96 | loss: 0.01204 | val_0_rmse: 0.11077 | val_1_rmse: 0.11322 |  0:03:14s
epoch 97 | loss: 0.0119  | val_0_rmse: 0.11559 | val_1_rmse: 0.11758 |  0:03:16s
epoch 98 | loss: 0.01182 | val_0_rmse: 0.10837 | val_1_rmse: 0.11081 |  0:03:18s
epoch 99 | loss: 0.01172 | val_0_rmse: 0.11135 | val_1_rmse: 0.11421 |  0:03:21s
epoch 100| loss: 0.01191 | val_0_rmse: 0.10515 | val_1_rmse: 0.108   |  0:03:22s
epoch 101| loss: 0.01202 | val_0_rmse: 0.11059 | val_1_rmse: 0.11346 |  0:03:24s
epoch 102| loss: 0.01294 | val_0_rmse: 0.14334 | val_1_rmse: 0.14517 |  0:03:27s
epoch 103| loss: 0.01383 | val_0_rmse: 0.11349 | val_1_rmse: 0.11808 |  0:03:29s
epoch 104| loss: 0.01307 | val_0_rmse: 0.11978 | val_1_rmse: 0.12268 |  0:03:31s
epoch 105| loss: 0.01253 | val_0_rmse: 0.10973 | val_1_rmse: 0.11261 |  0:03:33s
epoch 106| loss: 0.01219 | val_0_rmse: 0.10716 | val_1_rmse: 0.1096  |  0:03:35s
epoch 107| loss: 0.01202 | val_0_rmse: 0.10987 | val_1_rmse: 0.11274 |  0:03:37s
epoch 108| loss: 0.01207 | val_0_rmse: 0.10767 | val_1_rmse: 0.11064 |  0:03:39s
epoch 109| loss: 0.01221 | val_0_rmse: 0.10993 | val_1_rmse: 0.11288 |  0:03:41s
epoch 110| loss: 0.01216 | val_0_rmse: 0.11731 | val_1_rmse: 0.11988 |  0:03:43s
epoch 111| loss: 0.01564 | val_0_rmse: 0.14455 | val_1_rmse: 0.14478 |  0:03:45s
epoch 112| loss: 0.0158  | val_0_rmse: 0.12051 | val_1_rmse: 0.12118 |  0:03:47s
epoch 113| loss: 0.01449 | val_0_rmse: 0.11695 | val_1_rmse: 0.11905 |  0:03:49s
epoch 114| loss: 0.01384 | val_0_rmse: 0.12201 | val_1_rmse: 0.1234  |  0:03:51s
epoch 115| loss: 0.01397 | val_0_rmse: 0.11601 | val_1_rmse: 0.11797 |  0:03:53s
epoch 116| loss: 0.01349 | val_0_rmse: 0.112   | val_1_rmse: 0.11287 |  0:03:55s

Early stopping occured at epoch 116 with best_epoch = 86 and best_val_1_rmse = 0.10765
Best weights from best epoch are automatically used!
ended training at: 05:29:32
Feature importance:
[('Area', 0.3658281766015288), ('Baths', 0.029093341306016006), ('Beds', 0.0968361889246944), ('Latitude', 0.2736897168391831), ('Longitude', 0.21569243682992878), ('Month', 0.017051388346122836), ('Year', 0.001808751152526139)]
Mean squared error is of 981748046.160666
Mean absolute error:21586.42524776586
MAPE:0.2697995080162671
R2 score:0.7488701582786952
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:29:32
epoch 0  | loss: 0.11299 | val_0_rmse: 0.20127 | val_1_rmse: 0.19977 |  0:00:02s
epoch 1  | loss: 0.02191 | val_0_rmse: 0.20234 | val_1_rmse: 0.20092 |  0:00:04s
epoch 2  | loss: 0.01909 | val_0_rmse: 0.21081 | val_1_rmse: 0.20935 |  0:00:06s
epoch 3  | loss: 0.01706 | val_0_rmse: 0.2042  | val_1_rmse: 0.20267 |  0:00:08s
epoch 4  | loss: 0.01639 | val_0_rmse: 0.20608 | val_1_rmse: 0.20458 |  0:00:10s
epoch 5  | loss: 0.01614 | val_0_rmse: 0.17364 | val_1_rmse: 0.17238 |  0:00:12s
epoch 6  | loss: 0.01566 | val_0_rmse: 0.16199 | val_1_rmse: 0.161   |  0:00:14s
epoch 7  | loss: 0.01529 | val_0_rmse: 0.15116 | val_1_rmse: 0.15022 |  0:00:16s
epoch 8  | loss: 0.01538 | val_0_rmse: 0.14543 | val_1_rmse: 0.1453  |  0:00:18s
epoch 9  | loss: 0.01543 | val_0_rmse: 0.13558 | val_1_rmse: 0.13548 |  0:00:20s
epoch 10 | loss: 0.01496 | val_0_rmse: 0.13367 | val_1_rmse: 0.1339  |  0:00:22s
epoch 11 | loss: 0.01497 | val_0_rmse: 0.13381 | val_1_rmse: 0.13384 |  0:00:24s
epoch 12 | loss: 0.01464 | val_0_rmse: 0.12908 | val_1_rmse: 0.12919 |  0:00:26s
epoch 13 | loss: 0.01447 | val_0_rmse: 0.12405 | val_1_rmse: 0.12406 |  0:00:28s
epoch 14 | loss: 0.01431 | val_0_rmse: 0.12097 | val_1_rmse: 0.12186 |  0:00:30s
epoch 15 | loss: 0.01408 | val_0_rmse: 0.12364 | val_1_rmse: 0.12471 |  0:00:32s
epoch 16 | loss: 0.01423 | val_0_rmse: 0.12615 | val_1_rmse: 0.12711 |  0:00:34s
epoch 17 | loss: 0.01413 | val_0_rmse: 0.12219 | val_1_rmse: 0.12355 |  0:00:36s
epoch 18 | loss: 0.0139  | val_0_rmse: 0.11534 | val_1_rmse: 0.11706 |  0:00:38s
epoch 19 | loss: 0.01387 | val_0_rmse: 0.11226 | val_1_rmse: 0.11382 |  0:00:40s
epoch 20 | loss: 0.01464 | val_0_rmse: 0.11395 | val_1_rmse: 0.11558 |  0:00:42s
epoch 21 | loss: 0.01397 | val_0_rmse: 0.11756 | val_1_rmse: 0.11921 |  0:00:44s
epoch 22 | loss: 0.01375 | val_0_rmse: 0.11315 | val_1_rmse: 0.11542 |  0:00:46s
epoch 23 | loss: 0.01368 | val_0_rmse: 0.11211 | val_1_rmse: 0.11385 |  0:00:48s
epoch 24 | loss: 0.01399 | val_0_rmse: 0.12667 | val_1_rmse: 0.12867 |  0:00:50s
epoch 25 | loss: 0.01374 | val_0_rmse: 0.11119 | val_1_rmse: 0.1135  |  0:00:52s
epoch 26 | loss: 0.01341 | val_0_rmse: 0.11006 | val_1_rmse: 0.11254 |  0:00:54s
epoch 27 | loss: 0.01335 | val_0_rmse: 0.10987 | val_1_rmse: 0.11239 |  0:00:56s
epoch 28 | loss: 0.01322 | val_0_rmse: 0.10996 | val_1_rmse: 0.11277 |  0:00:58s
epoch 29 | loss: 0.01326 | val_0_rmse: 0.11067 | val_1_rmse: 0.11366 |  0:01:00s
epoch 30 | loss: 0.01307 | val_0_rmse: 0.10938 | val_1_rmse: 0.11183 |  0:01:02s
epoch 31 | loss: 0.01296 | val_0_rmse: 0.11413 | val_1_rmse: 0.11655 |  0:01:04s
epoch 32 | loss: 0.0129  | val_0_rmse: 0.10827 | val_1_rmse: 0.11119 |  0:01:06s
epoch 33 | loss: 0.01286 | val_0_rmse: 0.10992 | val_1_rmse: 0.11181 |  0:01:08s
epoch 34 | loss: 0.01319 | val_0_rmse: 0.11223 | val_1_rmse: 0.11456 |  0:01:10s
epoch 35 | loss: 0.0129  | val_0_rmse: 0.10777 | val_1_rmse: 0.11042 |  0:01:12s
epoch 36 | loss: 0.01294 | val_0_rmse: 0.11316 | val_1_rmse: 0.115   |  0:01:14s
epoch 37 | loss: 0.01275 | val_0_rmse: 0.10929 | val_1_rmse: 0.11205 |  0:01:16s
epoch 38 | loss: 0.01258 | val_0_rmse: 0.10865 | val_1_rmse: 0.11162 |  0:01:18s
epoch 39 | loss: 0.01289 | val_0_rmse: 0.10911 | val_1_rmse: 0.11212 |  0:01:20s
epoch 40 | loss: 0.01284 | val_0_rmse: 0.10832 | val_1_rmse: 0.11114 |  0:01:22s
epoch 41 | loss: 0.01284 | val_0_rmse: 0.11096 | val_1_rmse: 0.11344 |  0:01:24s
epoch 42 | loss: 0.01255 | val_0_rmse: 0.1076  | val_1_rmse: 0.11    |  0:01:26s
epoch 43 | loss: 0.01246 | val_0_rmse: 0.11196 | val_1_rmse: 0.11471 |  0:01:28s
epoch 44 | loss: 0.01263 | val_0_rmse: 0.11143 | val_1_rmse: 0.11481 |  0:01:30s
epoch 45 | loss: 0.0124  | val_0_rmse: 0.10986 | val_1_rmse: 0.11283 |  0:01:32s
epoch 46 | loss: 0.01272 | val_0_rmse: 0.11171 | val_1_rmse: 0.11405 |  0:01:34s
epoch 47 | loss: 0.0124  | val_0_rmse: 0.10927 | val_1_rmse: 0.11216 |  0:01:36s
epoch 48 | loss: 0.01248 | val_0_rmse: 0.11251 | val_1_rmse: 0.11563 |  0:01:38s
epoch 49 | loss: 0.01255 | val_0_rmse: 0.10776 | val_1_rmse: 0.11065 |  0:01:40s
epoch 50 | loss: 0.01243 | val_0_rmse: 0.11377 | val_1_rmse: 0.11635 |  0:01:42s
epoch 51 | loss: 0.01266 | val_0_rmse: 0.11402 | val_1_rmse: 0.11641 |  0:01:44s
epoch 52 | loss: 0.01251 | val_0_rmse: 0.1094  | val_1_rmse: 0.112   |  0:01:46s
epoch 53 | loss: 0.01236 | val_0_rmse: 0.11382 | val_1_rmse: 0.11627 |  0:01:48s
epoch 54 | loss: 0.0125  | val_0_rmse: 0.1144  | val_1_rmse: 0.11729 |  0:01:50s
epoch 55 | loss: 0.01235 | val_0_rmse: 0.10761 | val_1_rmse: 0.10975 |  0:01:52s
epoch 56 | loss: 0.01218 | val_0_rmse: 0.1206  | val_1_rmse: 0.12406 |  0:01:54s
epoch 57 | loss: 0.01262 | val_0_rmse: 0.11393 | val_1_rmse: 0.11706 |  0:01:56s
epoch 58 | loss: 0.01243 | val_0_rmse: 0.10647 | val_1_rmse: 0.10963 |  0:01:58s
epoch 59 | loss: 0.01248 | val_0_rmse: 0.12483 | val_1_rmse: 0.12839 |  0:02:00s
epoch 60 | loss: 0.01239 | val_0_rmse: 0.10815 | val_1_rmse: 0.11164 |  0:02:02s
epoch 61 | loss: 0.01227 | val_0_rmse: 0.11135 | val_1_rmse: 0.11425 |  0:02:04s
epoch 62 | loss: 0.01263 | val_0_rmse: 0.1088  | val_1_rmse: 0.11255 |  0:02:06s
epoch 63 | loss: 0.01261 | val_0_rmse: 0.10781 | val_1_rmse: 0.11126 |  0:02:08s
epoch 64 | loss: 0.01235 | val_0_rmse: 0.10676 | val_1_rmse: 0.10973 |  0:02:10s
epoch 65 | loss: 0.01222 | val_0_rmse: 0.11175 | val_1_rmse: 0.11485 |  0:02:12s
epoch 66 | loss: 0.01266 | val_0_rmse: 0.11032 | val_1_rmse: 0.11337 |  0:02:14s
epoch 67 | loss: 0.01228 | val_0_rmse: 0.11255 | val_1_rmse: 0.11515 |  0:02:16s
epoch 68 | loss: 0.01223 | val_0_rmse: 0.10789 | val_1_rmse: 0.11108 |  0:02:18s
epoch 69 | loss: 0.01228 | val_0_rmse: 0.11547 | val_1_rmse: 0.11866 |  0:02:20s
epoch 70 | loss: 0.01227 | val_0_rmse: 0.10706 | val_1_rmse: 0.11024 |  0:02:22s
epoch 71 | loss: 0.01269 | val_0_rmse: 0.10769 | val_1_rmse: 0.1112  |  0:02:24s
epoch 72 | loss: 0.0125  | val_0_rmse: 0.11038 | val_1_rmse: 0.11216 |  0:02:26s
epoch 73 | loss: 0.0123  | val_0_rmse: 0.11264 | val_1_rmse: 0.11551 |  0:02:28s
epoch 74 | loss: 0.01219 | val_0_rmse: 0.1101  | val_1_rmse: 0.11302 |  0:02:30s
epoch 75 | loss: 0.01272 | val_0_rmse: 0.10746 | val_1_rmse: 0.11058 |  0:02:32s
epoch 76 | loss: 0.0123  | val_0_rmse: 0.10661 | val_1_rmse: 0.10949 |  0:02:34s
epoch 77 | loss: 0.01223 | val_0_rmse: 0.11013 | val_1_rmse: 0.11281 |  0:02:36s
epoch 78 | loss: 0.01268 | val_0_rmse: 0.10899 | val_1_rmse: 0.11197 |  0:02:38s
epoch 79 | loss: 0.01227 | val_0_rmse: 0.10742 | val_1_rmse: 0.11077 |  0:02:40s
epoch 80 | loss: 0.0124  | val_0_rmse: 0.11667 | val_1_rmse: 0.11917 |  0:02:42s
epoch 81 | loss: 0.01223 | val_0_rmse: 0.12255 | val_1_rmse: 0.12418 |  0:02:44s
epoch 82 | loss: 0.0124  | val_0_rmse: 0.10727 | val_1_rmse: 0.1104  |  0:02:46s
epoch 83 | loss: 0.01218 | val_0_rmse: 0.11179 | val_1_rmse: 0.11475 |  0:02:48s
epoch 84 | loss: 0.0121  | val_0_rmse: 0.10833 | val_1_rmse: 0.1115  |  0:02:50s
epoch 85 | loss: 0.01217 | val_0_rmse: 0.10914 | val_1_rmse: 0.11193 |  0:02:52s
epoch 86 | loss: 0.01194 | val_0_rmse: 0.10566 | val_1_rmse: 0.10873 |  0:02:54s
epoch 87 | loss: 0.01204 | val_0_rmse: 0.12407 | val_1_rmse: 0.12652 |  0:02:56s
epoch 88 | loss: 0.01213 | val_0_rmse: 0.11305 | val_1_rmse: 0.11531 |  0:02:58s
epoch 89 | loss: 0.01184 | val_0_rmse: 0.10629 | val_1_rmse: 0.10956 |  0:03:00s
epoch 90 | loss: 0.01208 | val_0_rmse: 0.10837 | val_1_rmse: 0.11084 |  0:03:02s
epoch 91 | loss: 0.01233 | val_0_rmse: 0.10901 | val_1_rmse: 0.11118 |  0:03:04s
epoch 92 | loss: 0.0125  | val_0_rmse: 0.11754 | val_1_rmse: 0.12003 |  0:03:06s
epoch 93 | loss: 0.01201 | val_0_rmse: 0.10688 | val_1_rmse: 0.10971 |  0:03:08s
epoch 94 | loss: 0.01193 | val_0_rmse: 0.10975 | val_1_rmse: 0.11232 |  0:03:10s
epoch 95 | loss: 0.0119  | val_0_rmse: 0.10876 | val_1_rmse: 0.11133 |  0:03:13s
epoch 96 | loss: 0.01189 | val_0_rmse: 0.11472 | val_1_rmse: 0.11733 |  0:03:14s
epoch 97 | loss: 0.01256 | val_0_rmse: 0.109   | val_1_rmse: 0.11187 |  0:03:17s
epoch 98 | loss: 0.01194 | val_0_rmse: 0.10738 | val_1_rmse: 0.10977 |  0:03:19s
epoch 99 | loss: 0.01173 | val_0_rmse: 0.10914 | val_1_rmse: 0.112   |  0:03:20s
epoch 100| loss: 0.01224 | val_0_rmse: 0.10646 | val_1_rmse: 0.10893 |  0:03:23s
epoch 101| loss: 0.01203 | val_0_rmse: 0.1163  | val_1_rmse: 0.11951 |  0:03:25s
epoch 102| loss: 0.01171 | val_0_rmse: 0.10921 | val_1_rmse: 0.11235 |  0:03:27s
epoch 103| loss: 0.01189 | val_0_rmse: 0.11038 | val_1_rmse: 0.11308 |  0:03:28s
epoch 104| loss: 0.01212 | val_0_rmse: 0.11413 | val_1_rmse: 0.1171  |  0:03:31s
epoch 105| loss: 0.0119  | val_0_rmse: 0.10675 | val_1_rmse: 0.11005 |  0:03:33s
epoch 106| loss: 0.01261 | val_0_rmse: 0.12487 | val_1_rmse: 0.12627 |  0:03:35s
epoch 107| loss: 0.0129  | val_0_rmse: 0.12673 | val_1_rmse: 0.12951 |  0:03:37s
epoch 108| loss: 0.01227 | val_0_rmse: 0.1255  | val_1_rmse: 0.12675 |  0:03:39s
epoch 109| loss: 0.0121  | val_0_rmse: 0.11189 | val_1_rmse: 0.11436 |  0:03:41s
epoch 110| loss: 0.01234 | val_0_rmse: 0.11063 | val_1_rmse: 0.1126  |  0:03:43s
epoch 111| loss: 0.01217 | val_0_rmse: 0.1147  | val_1_rmse: 0.11742 |  0:03:45s
epoch 112| loss: 0.01224 | val_0_rmse: 0.11127 | val_1_rmse: 0.11257 |  0:03:47s
epoch 113| loss: 0.01205 | val_0_rmse: 0.1068  | val_1_rmse: 0.10955 |  0:03:49s
epoch 114| loss: 0.01188 | val_0_rmse: 0.10673 | val_1_rmse: 0.10908 |  0:03:51s
epoch 115| loss: 0.01159 | val_0_rmse: 0.11559 | val_1_rmse: 0.118   |  0:03:53s
epoch 116| loss: 0.01166 | val_0_rmse: 0.10823 | val_1_rmse: 0.11054 |  0:03:55s

Early stopping occured at epoch 116 with best_epoch = 86 and best_val_1_rmse = 0.10873
Best weights from best epoch are automatically used!
ended training at: 05:33:28
Feature importance:
[('Area', 0.3221234595970338), ('Baths', 0.1611043867310909), ('Beds', 0.014422498110844707), ('Latitude', 0.055581374642503574), ('Longitude', 0.3615509769469129), ('Month', 0.004357986187830559), ('Year', 0.08085931778378357)]
Mean squared error is of 988134233.0389853
Mean absolute error:21780.91052544957
MAPE:0.2712733477654716
R2 score:0.752342215185804
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:33:28
epoch 0  | loss: 0.06802 | val_0_rmse: 0.19834 | val_1_rmse: 0.19803 |  0:00:03s
epoch 1  | loss: 0.01834 | val_0_rmse: 0.18685 | val_1_rmse: 0.1855  |  0:00:06s
epoch 2  | loss: 0.01644 | val_0_rmse: 0.18702 | val_1_rmse: 0.18546 |  0:00:09s
epoch 3  | loss: 0.01481 | val_0_rmse: 0.18307 | val_1_rmse: 0.18171 |  0:00:13s
epoch 4  | loss: 0.01392 | val_0_rmse: 0.17232 | val_1_rmse: 0.17062 |  0:00:16s
epoch 5  | loss: 0.01306 | val_0_rmse: 0.16006 | val_1_rmse: 0.15897 |  0:00:19s
epoch 6  | loss: 0.01256 | val_0_rmse: 0.14655 | val_1_rmse: 0.14598 |  0:00:22s
epoch 7  | loss: 0.01267 | val_0_rmse: 0.13753 | val_1_rmse: 0.13757 |  0:00:26s
epoch 8  | loss: 0.01257 | val_0_rmse: 0.11722 | val_1_rmse: 0.11741 |  0:00:29s
epoch 9  | loss: 0.01167 | val_0_rmse: 0.10939 | val_1_rmse: 0.10977 |  0:00:32s
epoch 10 | loss: 0.01141 | val_0_rmse: 0.10742 | val_1_rmse: 0.10773 |  0:00:35s
epoch 11 | loss: 0.01153 | val_0_rmse: 0.10117 | val_1_rmse: 0.10112 |  0:00:39s
epoch 12 | loss: 0.01118 | val_0_rmse: 0.10016 | val_1_rmse: 0.09944 |  0:00:42s
epoch 13 | loss: 0.01119 | val_0_rmse: 0.09927 | val_1_rmse: 0.09937 |  0:00:45s
epoch 14 | loss: 0.01108 | val_0_rmse: 0.09869 | val_1_rmse: 0.0986  |  0:00:48s
epoch 15 | loss: 0.01129 | val_0_rmse: 0.09841 | val_1_rmse: 0.09864 |  0:00:52s
epoch 16 | loss: 0.01117 | val_0_rmse: 0.09772 | val_1_rmse: 0.09813 |  0:00:55s
epoch 17 | loss: 0.01133 | val_0_rmse: 0.09945 | val_1_rmse: 0.10048 |  0:00:58s
epoch 18 | loss: 0.01101 | val_0_rmse: 0.09926 | val_1_rmse: 0.10028 |  0:01:01s
epoch 19 | loss: 0.01078 | val_0_rmse: 0.09912 | val_1_rmse: 0.09961 |  0:01:04s
epoch 20 | loss: 0.01036 | val_0_rmse: 0.09473 | val_1_rmse: 0.09561 |  0:01:08s
epoch 21 | loss: 0.01034 | val_0_rmse: 0.09363 | val_1_rmse: 0.0942  |  0:01:11s
epoch 22 | loss: 0.01028 | val_0_rmse: 0.09366 | val_1_rmse: 0.09453 |  0:01:14s
epoch 23 | loss: 0.0103  | val_0_rmse: 0.09658 | val_1_rmse: 0.09732 |  0:01:17s
epoch 24 | loss: 0.01054 | val_0_rmse: 0.10077 | val_1_rmse: 0.1015  |  0:01:21s
epoch 25 | loss: 0.01051 | val_0_rmse: 0.0932  | val_1_rmse: 0.09385 |  0:01:24s
epoch 26 | loss: 0.0101  | val_0_rmse: 0.09415 | val_1_rmse: 0.09454 |  0:01:27s
epoch 27 | loss: 0.01011 | val_0_rmse: 0.09338 | val_1_rmse: 0.0946  |  0:01:30s
epoch 28 | loss: 0.01031 | val_0_rmse: 0.09303 | val_1_rmse: 0.09384 |  0:01:33s
epoch 29 | loss: 0.01006 | val_0_rmse: 0.09247 | val_1_rmse: 0.09344 |  0:01:37s
epoch 30 | loss: 0.01071 | val_0_rmse: 0.09544 | val_1_rmse: 0.09654 |  0:01:40s
epoch 31 | loss: 0.01016 | val_0_rmse: 0.09287 | val_1_rmse: 0.0939  |  0:01:43s
epoch 32 | loss: 0.01006 | val_0_rmse: 0.09644 | val_1_rmse: 0.0971  |  0:01:46s
epoch 33 | loss: 0.00993 | val_0_rmse: 0.0932  | val_1_rmse: 0.09394 |  0:01:50s
epoch 34 | loss: 0.00994 | val_0_rmse: 0.0917  | val_1_rmse: 0.0929  |  0:01:53s
epoch 35 | loss: 0.01006 | val_0_rmse: 0.09452 | val_1_rmse: 0.09508 |  0:01:56s
epoch 36 | loss: 0.00986 | val_0_rmse: 0.09396 | val_1_rmse: 0.09454 |  0:01:59s
epoch 37 | loss: 0.00996 | val_0_rmse: 0.09643 | val_1_rmse: 0.0973  |  0:02:02s
epoch 38 | loss: 0.00996 | val_0_rmse: 0.09447 | val_1_rmse: 0.09586 |  0:02:06s
epoch 39 | loss: 0.01    | val_0_rmse: 0.09287 | val_1_rmse: 0.09437 |  0:02:09s
epoch 40 | loss: 0.00999 | val_0_rmse: 0.09199 | val_1_rmse: 0.09277 |  0:02:12s
epoch 41 | loss: 0.00997 | val_0_rmse: 0.09591 | val_1_rmse: 0.09699 |  0:02:15s
epoch 42 | loss: 0.00987 | val_0_rmse: 0.09702 | val_1_rmse: 0.09736 |  0:02:19s
epoch 43 | loss: 0.01011 | val_0_rmse: 0.0946  | val_1_rmse: 0.09497 |  0:02:22s
epoch 44 | loss: 0.00997 | val_0_rmse: 0.09171 | val_1_rmse: 0.09322 |  0:02:25s
epoch 45 | loss: 0.00965 | val_0_rmse: 0.09097 | val_1_rmse: 0.09197 |  0:02:28s
epoch 46 | loss: 0.00984 | val_0_rmse: 0.09276 | val_1_rmse: 0.09326 |  0:02:32s
epoch 47 | loss: 0.00986 | val_0_rmse: 0.09208 | val_1_rmse: 0.09278 |  0:02:35s
epoch 48 | loss: 0.00964 | val_0_rmse: 0.09431 | val_1_rmse: 0.09515 |  0:02:38s
epoch 49 | loss: 0.00989 | val_0_rmse: 0.09075 | val_1_rmse: 0.09187 |  0:02:41s
epoch 50 | loss: 0.00964 | val_0_rmse: 0.09261 | val_1_rmse: 0.09356 |  0:02:44s
epoch 51 | loss: 0.00998 | val_0_rmse: 0.09506 | val_1_rmse: 0.09563 |  0:02:48s
epoch 52 | loss: 0.0099  | val_0_rmse: 0.09535 | val_1_rmse: 0.0964  |  0:02:51s
epoch 53 | loss: 0.00984 | val_0_rmse: 0.09191 | val_1_rmse: 0.09308 |  0:02:54s
epoch 54 | loss: 0.0095  | val_0_rmse: 0.09101 | val_1_rmse: 0.0924  |  0:02:57s
epoch 55 | loss: 0.00949 | val_0_rmse: 0.09077 | val_1_rmse: 0.09207 |  0:03:01s
epoch 56 | loss: 0.00972 | val_0_rmse: 0.0923  | val_1_rmse: 0.09379 |  0:03:04s
epoch 57 | loss: 0.00983 | val_0_rmse: 0.0909  | val_1_rmse: 0.09272 |  0:03:07s
epoch 58 | loss: 0.00977 | val_0_rmse: 0.0936  | val_1_rmse: 0.09499 |  0:03:10s
epoch 59 | loss: 0.0097  | val_0_rmse: 0.09172 | val_1_rmse: 0.09301 |  0:03:14s
epoch 60 | loss: 0.0094  | val_0_rmse: 0.0911  | val_1_rmse: 0.09237 |  0:03:17s
epoch 61 | loss: 0.00963 | val_0_rmse: 0.0935  | val_1_rmse: 0.09442 |  0:03:20s
epoch 62 | loss: 0.01007 | val_0_rmse: 0.09169 | val_1_rmse: 0.09282 |  0:03:23s
epoch 63 | loss: 0.00994 | val_0_rmse: 0.09077 | val_1_rmse: 0.0916  |  0:03:27s
epoch 64 | loss: 0.00945 | val_0_rmse: 0.09116 | val_1_rmse: 0.09193 |  0:03:30s
epoch 65 | loss: 0.00937 | val_0_rmse: 0.09093 | val_1_rmse: 0.09177 |  0:03:33s
epoch 66 | loss: 0.00968 | val_0_rmse: 0.09101 | val_1_rmse: 0.09206 |  0:03:36s
epoch 67 | loss: 0.00953 | val_0_rmse: 0.09718 | val_1_rmse: 0.09767 |  0:03:39s
epoch 68 | loss: 0.00961 | val_0_rmse: 0.0928  | val_1_rmse: 0.09413 |  0:03:43s
epoch 69 | loss: 0.00962 | val_0_rmse: 0.08966 | val_1_rmse: 0.09144 |  0:03:46s
epoch 70 | loss: 0.00949 | val_0_rmse: 0.09023 | val_1_rmse: 0.09153 |  0:03:49s
epoch 71 | loss: 0.00944 | val_0_rmse: 0.09096 | val_1_rmse: 0.09147 |  0:03:52s
epoch 72 | loss: 0.0094  | val_0_rmse: 0.09114 | val_1_rmse: 0.09241 |  0:03:56s
epoch 73 | loss: 0.00935 | val_0_rmse: 0.09172 | val_1_rmse: 0.09297 |  0:03:59s
epoch 74 | loss: 0.00931 | val_0_rmse: 0.09232 | val_1_rmse: 0.09319 |  0:04:02s
epoch 75 | loss: 0.00961 | val_0_rmse: 0.09047 | val_1_rmse: 0.09147 |  0:04:05s
epoch 76 | loss: 0.00944 | val_0_rmse: 0.09139 | val_1_rmse: 0.09268 |  0:04:08s
epoch 77 | loss: 0.00943 | val_0_rmse: 0.08922 | val_1_rmse: 0.09105 |  0:04:12s
epoch 78 | loss: 0.00927 | val_0_rmse: 0.0917  | val_1_rmse: 0.09375 |  0:04:15s
epoch 79 | loss: 0.00964 | val_0_rmse: 0.09154 | val_1_rmse: 0.09366 |  0:04:18s
epoch 80 | loss: 0.00958 | val_0_rmse: 0.09074 | val_1_rmse: 0.09224 |  0:04:21s
epoch 81 | loss: 0.00932 | val_0_rmse: 0.09441 | val_1_rmse: 0.0956  |  0:04:25s
epoch 82 | loss: 0.00967 | val_0_rmse: 0.09175 | val_1_rmse: 0.09288 |  0:04:28s
epoch 83 | loss: 0.00952 | val_0_rmse: 0.09104 | val_1_rmse: 0.0927  |  0:04:31s
epoch 84 | loss: 0.00954 | val_0_rmse: 0.09285 | val_1_rmse: 0.09456 |  0:04:34s
epoch 85 | loss: 0.00946 | val_0_rmse: 0.08957 | val_1_rmse: 0.09113 |  0:04:38s
epoch 86 | loss: 0.00954 | val_0_rmse: 0.09125 | val_1_rmse: 0.09246 |  0:04:41s
epoch 87 | loss: 0.00925 | val_0_rmse: 0.09302 | val_1_rmse: 0.09522 |  0:04:44s
epoch 88 | loss: 0.00935 | val_0_rmse: 0.0899  | val_1_rmse: 0.09128 |  0:04:47s
epoch 89 | loss: 0.00952 | val_0_rmse: 0.08876 | val_1_rmse: 0.09042 |  0:04:50s
epoch 90 | loss: 0.00927 | val_0_rmse: 0.09017 | val_1_rmse: 0.0914  |  0:04:54s
epoch 91 | loss: 0.00917 | val_0_rmse: 0.09092 | val_1_rmse: 0.09193 |  0:04:57s
epoch 92 | loss: 0.00919 | val_0_rmse: 0.09012 | val_1_rmse: 0.09144 |  0:05:00s
epoch 93 | loss: 0.0094  | val_0_rmse: 0.08895 | val_1_rmse: 0.0904  |  0:05:03s
epoch 94 | loss: 0.00911 | val_0_rmse: 0.09063 | val_1_rmse: 0.09249 |  0:05:07s
epoch 95 | loss: 0.00935 | val_0_rmse: 0.0933  | val_1_rmse: 0.09524 |  0:05:10s
epoch 96 | loss: 0.00932 | val_0_rmse: 0.09147 | val_1_rmse: 0.0925  |  0:05:13s
epoch 97 | loss: 0.00924 | val_0_rmse: 0.08944 | val_1_rmse: 0.09126 |  0:05:16s
epoch 98 | loss: 0.00914 | val_0_rmse: 0.09039 | val_1_rmse: 0.09155 |  0:05:20s
epoch 99 | loss: 0.00922 | val_0_rmse: 0.08879 | val_1_rmse: 0.09036 |  0:05:23s
epoch 100| loss: 0.00929 | val_0_rmse: 0.08834 | val_1_rmse: 0.09012 |  0:05:26s
epoch 101| loss: 0.00913 | val_0_rmse: 0.08959 | val_1_rmse: 0.09193 |  0:05:29s
epoch 102| loss: 0.00905 | val_0_rmse: 0.08832 | val_1_rmse: 0.09053 |  0:05:33s
epoch 103| loss: 0.00915 | val_0_rmse: 0.08987 | val_1_rmse: 0.09207 |  0:05:36s
epoch 104| loss: 0.00908 | val_0_rmse: 0.09143 | val_1_rmse: 0.09273 |  0:05:39s
epoch 105| loss: 0.00906 | val_0_rmse: 0.0878  | val_1_rmse: 0.09014 |  0:05:42s
epoch 106| loss: 0.00898 | val_0_rmse: 0.08959 | val_1_rmse: 0.09156 |  0:05:46s
epoch 107| loss: 0.0091  | val_0_rmse: 0.08878 | val_1_rmse: 0.09103 |  0:05:49s
epoch 108| loss: 0.0091  | val_0_rmse: 0.09091 | val_1_rmse: 0.09234 |  0:05:52s
epoch 109| loss: 0.0094  | val_0_rmse: 0.09293 | val_1_rmse: 0.09442 |  0:05:55s
epoch 110| loss: 0.00907 | val_0_rmse: 0.08975 | val_1_rmse: 0.09134 |  0:05:58s
epoch 111| loss: 0.00921 | val_0_rmse: 0.08982 | val_1_rmse: 0.09108 |  0:06:02s
epoch 112| loss: 0.00938 | val_0_rmse: 0.08925 | val_1_rmse: 0.09087 |  0:06:05s
epoch 113| loss: 0.00918 | val_0_rmse: 0.08852 | val_1_rmse: 0.09075 |  0:06:08s
epoch 114| loss: 0.00907 | val_0_rmse: 0.09163 | val_1_rmse: 0.09301 |  0:06:11s
epoch 115| loss: 0.00908 | val_0_rmse: 0.09236 | val_1_rmse: 0.09411 |  0:06:15s
epoch 116| loss: 0.00916 | val_0_rmse: 0.09077 | val_1_rmse: 0.0928  |  0:06:18s
epoch 117| loss: 0.00936 | val_0_rmse: 0.08933 | val_1_rmse: 0.09098 |  0:06:21s
epoch 118| loss: 0.00904 | val_0_rmse: 0.08994 | val_1_rmse: 0.09154 |  0:06:24s
epoch 119| loss: 0.00883 | val_0_rmse: 0.08815 | val_1_rmse: 0.09046 |  0:06:27s
epoch 120| loss: 0.00892 | val_0_rmse: 0.08867 | val_1_rmse: 0.09052 |  0:06:31s
epoch 121| loss: 0.00888 | val_0_rmse: 0.08837 | val_1_rmse: 0.09061 |  0:06:34s
epoch 122| loss: 0.00896 | val_0_rmse: 0.08852 | val_1_rmse: 0.09018 |  0:06:37s
epoch 123| loss: 0.00934 | val_0_rmse: 0.08857 | val_1_rmse: 0.09062 |  0:06:40s
epoch 124| loss: 0.0091  | val_0_rmse: 0.08877 | val_1_rmse: 0.09098 |  0:06:44s
epoch 125| loss: 0.00889 | val_0_rmse: 0.09009 | val_1_rmse: 0.09253 |  0:06:47s
epoch 126| loss: 0.00907 | val_0_rmse: 0.08969 | val_1_rmse: 0.09186 |  0:06:50s
epoch 127| loss: 0.00882 | val_0_rmse: 0.08719 | val_1_rmse: 0.08949 |  0:06:53s
epoch 128| loss: 0.009   | val_0_rmse: 0.08842 | val_1_rmse: 0.09072 |  0:06:56s
epoch 129| loss: 0.00892 | val_0_rmse: 0.08874 | val_1_rmse: 0.09144 |  0:07:00s
epoch 130| loss: 0.00898 | val_0_rmse: 0.08707 | val_1_rmse: 0.08957 |  0:07:03s
epoch 131| loss: 0.00913 | val_0_rmse: 0.08915 | val_1_rmse: 0.09056 |  0:07:06s
epoch 132| loss: 0.00886 | val_0_rmse: 0.08837 | val_1_rmse: 0.09056 |  0:07:09s
epoch 133| loss: 0.00895 | val_0_rmse: 0.08982 | val_1_rmse: 0.09209 |  0:07:13s
epoch 134| loss: 0.00903 | val_0_rmse: 0.08996 | val_1_rmse: 0.0931  |  0:07:16s
epoch 135| loss: 0.00891 | val_0_rmse: 0.08877 | val_1_rmse: 0.09107 |  0:07:19s
epoch 136| loss: 0.00917 | val_0_rmse: 0.08662 | val_1_rmse: 0.08886 |  0:07:22s
epoch 137| loss: 0.00889 | val_0_rmse: 0.08836 | val_1_rmse: 0.09064 |  0:07:26s
epoch 138| loss: 0.00882 | val_0_rmse: 0.09115 | val_1_rmse: 0.0935  |  0:07:29s
epoch 139| loss: 0.009   | val_0_rmse: 0.08803 | val_1_rmse: 0.08985 |  0:07:32s
epoch 140| loss: 0.00899 | val_0_rmse: 0.08916 | val_1_rmse: 0.09173 |  0:07:35s
epoch 141| loss: 0.00888 | val_0_rmse: 0.09044 | val_1_rmse: 0.09314 |  0:07:39s
epoch 142| loss: 0.00907 | val_0_rmse: 0.08878 | val_1_rmse: 0.09072 |  0:07:42s
epoch 143| loss: 0.00893 | val_0_rmse: 0.09133 | val_1_rmse: 0.09368 |  0:07:45s
epoch 144| loss: 0.0089  | val_0_rmse: 0.08809 | val_1_rmse: 0.09001 |  0:07:48s
epoch 145| loss: 0.00881 | val_0_rmse: 0.08745 | val_1_rmse: 0.08975 |  0:07:51s
epoch 146| loss: 0.00883 | val_0_rmse: 0.08854 | val_1_rmse: 0.09059 |  0:07:55s
epoch 147| loss: 0.00898 | val_0_rmse: 0.08978 | val_1_rmse: 0.09208 |  0:07:58s
epoch 148| loss: 0.00895 | val_0_rmse: 0.08674 | val_1_rmse: 0.08984 |  0:08:01s
epoch 149| loss: 0.00887 | val_0_rmse: 0.08961 | val_1_rmse: 0.09243 |  0:08:04s
Stop training because you reached max_epochs = 150 with best_epoch = 136 and best_val_1_rmse = 0.08886
Best weights from best epoch are automatically used!
ended training at: 05:41:34
Feature importance:
[('Area', 0.0), ('Baths', 0.32960261893193515), ('Beds', 0.13966011624322272), ('Latitude', 0.08571664166550177), ('Longitude', 0.1443796763783069), ('Month', 0.0), ('Year', 0.3006409467810334)]
Mean squared error is of 10312171474.57286
Mean absolute error:71157.06953211509
MAPE:0.29812646497207873
R2 score:0.822321282038688
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:41:35
epoch 0  | loss: 0.06727 | val_0_rmse: 0.18902 | val_1_rmse: 0.19031 |  0:00:03s
epoch 1  | loss: 0.0195  | val_0_rmse: 0.18888 | val_1_rmse: 0.19019 |  0:00:06s
epoch 2  | loss: 0.0165  | val_0_rmse: 0.1801  | val_1_rmse: 0.18121 |  0:00:09s
epoch 3  | loss: 0.01525 | val_0_rmse: 0.18056 | val_1_rmse: 0.18093 |  0:00:13s
epoch 4  | loss: 0.01458 | val_0_rmse: 0.15981 | val_1_rmse: 0.1598  |  0:00:16s
epoch 5  | loss: 0.01383 | val_0_rmse: 0.15593 | val_1_rmse: 0.15559 |  0:00:19s
epoch 6  | loss: 0.01324 | val_0_rmse: 0.14193 | val_1_rmse: 0.14186 |  0:00:22s
epoch 7  | loss: 0.01228 | val_0_rmse: 0.12433 | val_1_rmse: 0.12343 |  0:00:25s
epoch 8  | loss: 0.01201 | val_0_rmse: 0.1124  | val_1_rmse: 0.11141 |  0:00:29s
epoch 9  | loss: 0.01161 | val_0_rmse: 0.10894 | val_1_rmse: 0.10837 |  0:00:32s
epoch 10 | loss: 0.01161 | val_0_rmse: 0.10474 | val_1_rmse: 0.10496 |  0:00:35s
epoch 11 | loss: 0.01132 | val_0_rmse: 0.10332 | val_1_rmse: 0.10341 |  0:00:38s
epoch 12 | loss: 0.011   | val_0_rmse: 0.09702 | val_1_rmse: 0.09729 |  0:00:42s
epoch 13 | loss: 0.01089 | val_0_rmse: 0.09608 | val_1_rmse: 0.09623 |  0:00:45s
epoch 14 | loss: 0.01096 | val_0_rmse: 0.09798 | val_1_rmse: 0.0979  |  0:00:48s
epoch 15 | loss: 0.01096 | val_0_rmse: 0.09682 | val_1_rmse: 0.09713 |  0:00:51s
epoch 16 | loss: 0.01076 | val_0_rmse: 0.09866 | val_1_rmse: 0.09914 |  0:00:55s
epoch 17 | loss: 0.01065 | val_0_rmse: 0.09476 | val_1_rmse: 0.09506 |  0:00:58s
epoch 18 | loss: 0.01056 | val_0_rmse: 0.0946  | val_1_rmse: 0.0947  |  0:01:01s
epoch 19 | loss: 0.01069 | val_0_rmse: 0.09926 | val_1_rmse: 0.09855 |  0:01:04s
epoch 20 | loss: 0.01076 | val_0_rmse: 0.09837 | val_1_rmse: 0.09824 |  0:01:08s
epoch 21 | loss: 0.01056 | val_0_rmse: 0.09449 | val_1_rmse: 0.09447 |  0:01:11s
epoch 22 | loss: 0.01046 | val_0_rmse: 0.10348 | val_1_rmse: 0.10339 |  0:01:14s
epoch 23 | loss: 0.01044 | val_0_rmse: 0.09579 | val_1_rmse: 0.0962  |  0:01:17s
epoch 24 | loss: 0.01028 | val_0_rmse: 0.09376 | val_1_rmse: 0.09377 |  0:01:20s
epoch 25 | loss: 0.01041 | val_0_rmse: 0.09563 | val_1_rmse: 0.09597 |  0:01:24s
epoch 26 | loss: 0.01024 | val_0_rmse: 0.0946  | val_1_rmse: 0.09508 |  0:01:27s
epoch 27 | loss: 0.01045 | val_0_rmse: 0.09747 | val_1_rmse: 0.09803 |  0:01:30s
epoch 28 | loss: 0.01051 | val_0_rmse: 0.09566 | val_1_rmse: 0.0958  |  0:01:33s
epoch 29 | loss: 0.01028 | val_0_rmse: 0.09543 | val_1_rmse: 0.09555 |  0:01:37s
epoch 30 | loss: 0.01006 | val_0_rmse: 0.0969  | val_1_rmse: 0.09699 |  0:01:40s
epoch 31 | loss: 0.01004 | val_0_rmse: 0.09627 | val_1_rmse: 0.09672 |  0:01:43s
epoch 32 | loss: 0.0103  | val_0_rmse: 0.09333 | val_1_rmse: 0.09325 |  0:01:46s
epoch 33 | loss: 0.00997 | val_0_rmse: 0.09397 | val_1_rmse: 0.09368 |  0:01:49s
epoch 34 | loss: 0.00997 | val_0_rmse: 0.09477 | val_1_rmse: 0.09479 |  0:01:53s
epoch 35 | loss: 0.00998 | val_0_rmse: 0.09964 | val_1_rmse: 0.10028 |  0:01:56s
epoch 36 | loss: 0.01018 | val_0_rmse: 0.09338 | val_1_rmse: 0.09347 |  0:01:59s
epoch 37 | loss: 0.01008 | val_0_rmse: 0.09549 | val_1_rmse: 0.09567 |  0:02:02s
epoch 38 | loss: 0.01007 | val_0_rmse: 0.09608 | val_1_rmse: 0.09589 |  0:02:06s
epoch 39 | loss: 0.01009 | val_0_rmse: 0.09662 | val_1_rmse: 0.09633 |  0:02:09s
epoch 40 | loss: 0.01002 | val_0_rmse: 0.09086 | val_1_rmse: 0.09116 |  0:02:12s
epoch 41 | loss: 0.00982 | val_0_rmse: 0.09118 | val_1_rmse: 0.09137 |  0:02:15s
epoch 42 | loss: 0.00962 | val_0_rmse: 0.10012 | val_1_rmse: 0.1004  |  0:02:19s
epoch 43 | loss: 0.00967 | val_0_rmse: 0.09357 | val_1_rmse: 0.09342 |  0:02:22s
epoch 44 | loss: 0.01041 | val_0_rmse: 0.09434 | val_1_rmse: 0.09426 |  0:02:25s
epoch 45 | loss: 0.01029 | val_0_rmse: 0.09754 | val_1_rmse: 0.09761 |  0:02:28s
epoch 46 | loss: 0.00969 | val_0_rmse: 0.09525 | val_1_rmse: 0.09508 |  0:02:31s
epoch 47 | loss: 0.00992 | val_0_rmse: 0.09358 | val_1_rmse: 0.09374 |  0:02:35s
epoch 48 | loss: 0.00969 | val_0_rmse: 0.09121 | val_1_rmse: 0.09092 |  0:02:38s
epoch 49 | loss: 0.00965 | val_0_rmse: 0.09589 | val_1_rmse: 0.09617 |  0:02:41s
epoch 50 | loss: 0.00963 | val_0_rmse: 0.09173 | val_1_rmse: 0.09218 |  0:02:44s
epoch 51 | loss: 0.00985 | val_0_rmse: 0.08996 | val_1_rmse: 0.09044 |  0:02:48s
epoch 52 | loss: 0.00975 | val_0_rmse: 0.09197 | val_1_rmse: 0.09221 |  0:02:51s
epoch 53 | loss: 0.00944 | val_0_rmse: 0.09109 | val_1_rmse: 0.09114 |  0:02:54s
epoch 54 | loss: 0.00973 | val_0_rmse: 0.08965 | val_1_rmse: 0.0899  |  0:02:58s
epoch 55 | loss: 0.00955 | val_0_rmse: 0.09158 | val_1_rmse: 0.0916  |  0:03:01s
epoch 56 | loss: 0.00993 | val_0_rmse: 0.09404 | val_1_rmse: 0.09425 |  0:03:04s
epoch 57 | loss: 0.00963 | val_0_rmse: 0.09169 | val_1_rmse: 0.09187 |  0:03:07s
epoch 58 | loss: 0.00992 | val_0_rmse: 0.09586 | val_1_rmse: 0.0964  |  0:03:10s
epoch 59 | loss: 0.00957 | val_0_rmse: 0.09479 | val_1_rmse: 0.0954  |  0:03:14s
epoch 60 | loss: 0.00949 | val_0_rmse: 0.09228 | val_1_rmse: 0.09278 |  0:03:17s
epoch 61 | loss: 0.01007 | val_0_rmse: 0.0919  | val_1_rmse: 0.09195 |  0:03:20s
epoch 62 | loss: 0.00965 | val_0_rmse: 0.09242 | val_1_rmse: 0.09273 |  0:03:23s
epoch 63 | loss: 0.00943 | val_0_rmse: 0.09113 | val_1_rmse: 0.09117 |  0:03:27s
epoch 64 | loss: 0.00922 | val_0_rmse: 0.08943 | val_1_rmse: 0.08971 |  0:03:30s
epoch 65 | loss: 0.00942 | val_0_rmse: 0.09453 | val_1_rmse: 0.09472 |  0:03:33s
epoch 66 | loss: 0.00951 | val_0_rmse: 0.09256 | val_1_rmse: 0.09246 |  0:03:36s
epoch 67 | loss: 0.00942 | val_0_rmse: 0.09054 | val_1_rmse: 0.09073 |  0:03:40s
epoch 68 | loss: 0.00984 | val_0_rmse: 0.09115 | val_1_rmse: 0.09078 |  0:03:43s
epoch 69 | loss: 0.0094  | val_0_rmse: 0.08986 | val_1_rmse: 0.08996 |  0:03:46s
epoch 70 | loss: 0.00954 | val_0_rmse: 0.09147 | val_1_rmse: 0.09154 |  0:03:49s
epoch 71 | loss: 0.00942 | val_0_rmse: 0.08945 | val_1_rmse: 0.08951 |  0:03:53s
epoch 72 | loss: 0.00982 | val_0_rmse: 0.0939  | val_1_rmse: 0.09422 |  0:03:56s
epoch 73 | loss: 0.00937 | val_0_rmse: 0.09266 | val_1_rmse: 0.09268 |  0:03:59s
epoch 74 | loss: 0.00945 | val_0_rmse: 0.09077 | val_1_rmse: 0.09101 |  0:04:02s
epoch 75 | loss: 0.00949 | val_0_rmse: 0.08881 | val_1_rmse: 0.08891 |  0:04:05s
epoch 76 | loss: 0.00955 | val_0_rmse: 0.09281 | val_1_rmse: 0.09366 |  0:04:09s
epoch 77 | loss: 0.01003 | val_0_rmse: 0.09133 | val_1_rmse: 0.0915  |  0:04:12s
epoch 78 | loss: 0.00932 | val_0_rmse: 0.09633 | val_1_rmse: 0.09573 |  0:04:15s
epoch 79 | loss: 0.00962 | val_0_rmse: 0.09142 | val_1_rmse: 0.09142 |  0:04:18s
epoch 80 | loss: 0.00998 | val_0_rmse: 0.09458 | val_1_rmse: 0.0942  |  0:04:22s
epoch 81 | loss: 0.00967 | val_0_rmse: 0.10442 | val_1_rmse: 0.10383 |  0:04:25s
epoch 82 | loss: 0.00944 | val_0_rmse: 0.08912 | val_1_rmse: 0.08945 |  0:04:28s
epoch 83 | loss: 0.00916 | val_0_rmse: 0.08881 | val_1_rmse: 0.08914 |  0:04:31s
epoch 84 | loss: 0.00932 | val_0_rmse: 0.08949 | val_1_rmse: 0.08946 |  0:04:35s
epoch 85 | loss: 0.00933 | val_0_rmse: 0.10053 | val_1_rmse: 0.10063 |  0:04:38s
epoch 86 | loss: 0.00949 | val_0_rmse: 0.09317 | val_1_rmse: 0.09379 |  0:04:41s
epoch 87 | loss: 0.00928 | val_0_rmse: 0.0956  | val_1_rmse: 0.09558 |  0:04:44s
epoch 88 | loss: 0.00971 | val_0_rmse: 0.08918 | val_1_rmse: 0.08897 |  0:04:47s
epoch 89 | loss: 0.00947 | val_0_rmse: 0.09476 | val_1_rmse: 0.09468 |  0:04:51s
epoch 90 | loss: 0.00935 | val_0_rmse: 0.08903 | val_1_rmse: 0.08931 |  0:04:54s
epoch 91 | loss: 0.00903 | val_0_rmse: 0.08927 | val_1_rmse: 0.08963 |  0:04:57s
epoch 92 | loss: 0.00888 | val_0_rmse: 0.09001 | val_1_rmse: 0.09051 |  0:05:00s
epoch 93 | loss: 0.00887 | val_0_rmse: 0.0901  | val_1_rmse: 0.09083 |  0:05:04s
epoch 94 | loss: 0.00891 | val_0_rmse: 0.08839 | val_1_rmse: 0.08855 |  0:05:07s
epoch 95 | loss: 0.00877 | val_0_rmse: 0.08866 | val_1_rmse: 0.08925 |  0:05:10s
epoch 96 | loss: 0.00947 | val_0_rmse: 0.09159 | val_1_rmse: 0.09205 |  0:05:13s
epoch 97 | loss: 0.00889 | val_0_rmse: 0.08966 | val_1_rmse: 0.09015 |  0:05:17s
epoch 98 | loss: 0.00911 | val_0_rmse: 0.09138 | val_1_rmse: 0.09175 |  0:05:20s
epoch 99 | loss: 0.00903 | val_0_rmse: 0.09088 | val_1_rmse: 0.09102 |  0:05:23s
epoch 100| loss: 0.00911 | val_0_rmse: 0.09203 | val_1_rmse: 0.09297 |  0:05:26s
epoch 101| loss: 0.00887 | val_0_rmse: 0.08963 | val_1_rmse: 0.09034 |  0:05:30s
epoch 102| loss: 0.00894 | val_0_rmse: 0.08853 | val_1_rmse: 0.08954 |  0:05:33s
epoch 103| loss: 0.00893 | val_0_rmse: 0.0885  | val_1_rmse: 0.08912 |  0:05:36s
epoch 104| loss: 0.00904 | val_0_rmse: 0.09203 | val_1_rmse: 0.09271 |  0:05:39s
epoch 105| loss: 0.00895 | val_0_rmse: 0.08728 | val_1_rmse: 0.08832 |  0:05:42s
epoch 106| loss: 0.00873 | val_0_rmse: 0.0886  | val_1_rmse: 0.08954 |  0:05:46s
epoch 107| loss: 0.00928 | val_0_rmse: 0.09539 | val_1_rmse: 0.0964  |  0:05:49s
epoch 108| loss: 0.01058 | val_0_rmse: 0.09646 | val_1_rmse: 0.09579 |  0:05:52s
epoch 109| loss: 0.00998 | val_0_rmse: 0.09409 | val_1_rmse: 0.09378 |  0:05:55s
epoch 110| loss: 0.00937 | val_0_rmse: 0.0941  | val_1_rmse: 0.09404 |  0:05:59s
epoch 111| loss: 0.00955 | val_0_rmse: 0.09381 | val_1_rmse: 0.09361 |  0:06:02s
epoch 112| loss: 0.00992 | val_0_rmse: 0.09533 | val_1_rmse: 0.09528 |  0:06:05s
epoch 113| loss: 0.00969 | val_0_rmse: 0.09198 | val_1_rmse: 0.09124 |  0:06:08s
epoch 114| loss: 0.00941 | val_0_rmse: 0.08976 | val_1_rmse: 0.08994 |  0:06:12s
epoch 115| loss: 0.00944 | val_0_rmse: 0.09332 | val_1_rmse: 0.09333 |  0:06:15s
epoch 116| loss: 0.00918 | val_0_rmse: 0.08916 | val_1_rmse: 0.08879 |  0:06:18s
epoch 117| loss: 0.00969 | val_0_rmse: 0.12632 | val_1_rmse: 0.12572 |  0:06:21s
epoch 118| loss: 0.01612 | val_0_rmse: 0.13257 | val_1_rmse: 0.13098 |  0:06:25s
epoch 119| loss: 0.0145  | val_0_rmse: 0.11498 | val_1_rmse: 0.11393 |  0:06:28s
epoch 120| loss: 0.01328 | val_0_rmse: 0.11227 | val_1_rmse: 0.11149 |  0:06:31s
epoch 121| loss: 0.01314 | val_0_rmse: 0.10935 | val_1_rmse: 0.1076  |  0:06:34s
epoch 122| loss: 0.01288 | val_0_rmse: 0.10628 | val_1_rmse: 0.10587 |  0:06:37s
epoch 123| loss: 0.01202 | val_0_rmse: 0.0999  | val_1_rmse: 0.09902 |  0:06:41s
epoch 124| loss: 0.01115 | val_0_rmse: 0.1034  | val_1_rmse: 0.10303 |  0:06:44s
epoch 125| loss: 0.01053 | val_0_rmse: 0.09941 | val_1_rmse: 0.09848 |  0:06:47s
epoch 126| loss: 0.01044 | val_0_rmse: 0.09935 | val_1_rmse: 0.09848 |  0:06:50s
epoch 127| loss: 0.01027 | val_0_rmse: 0.09651 | val_1_rmse: 0.09563 |  0:06:54s
epoch 128| loss: 0.01037 | val_0_rmse: 0.09953 | val_1_rmse: 0.09945 |  0:06:57s
epoch 129| loss: 0.01023 | val_0_rmse: 0.09418 | val_1_rmse: 0.09414 |  0:07:00s
epoch 130| loss: 0.00982 | val_0_rmse: 0.0924  | val_1_rmse: 0.09197 |  0:07:03s
epoch 131| loss: 0.00989 | val_0_rmse: 0.09244 | val_1_rmse: 0.09213 |  0:07:07s
epoch 132| loss: 0.01323 | val_0_rmse: 0.10745 | val_1_rmse: 0.10731 |  0:07:10s
epoch 133| loss: 0.01124 | val_0_rmse: 0.10431 | val_1_rmse: 0.10417 |  0:07:13s
epoch 134| loss: 0.01085 | val_0_rmse: 0.10371 | val_1_rmse: 0.1028  |  0:07:16s
epoch 135| loss: 0.01163 | val_0_rmse: 0.10882 | val_1_rmse: 0.10848 |  0:07:19s

Early stopping occured at epoch 135 with best_epoch = 105 and best_val_1_rmse = 0.08832
Best weights from best epoch are automatically used!
ended training at: 05:48:56
Feature importance:
[('Area', 0.14411877245628846), ('Baths', 0.10152422768929284), ('Beds', 0.05973338471408632), ('Latitude', 0.15268747258895982), ('Longitude', 0.22101840708471812), ('Month', 0.0), ('Year', 0.3209177354666544)]
Mean squared error is of 10566579725.119156
Mean absolute error:71707.57466589844
MAPE:0.3095695721353348
R2 score:0.8181954074162183
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:48:56
epoch 0  | loss: 0.06422 | val_0_rmse: 0.19775 | val_1_rmse: 0.19598 |  0:00:03s
epoch 1  | loss: 0.01667 | val_0_rmse: 0.19442 | val_1_rmse: 0.19361 |  0:00:06s
epoch 2  | loss: 0.01479 | val_0_rmse: 0.1842  | val_1_rmse: 0.18319 |  0:00:09s
epoch 3  | loss: 0.01357 | val_0_rmse: 0.17901 | val_1_rmse: 0.17828 |  0:00:12s
epoch 4  | loss: 0.01303 | val_0_rmse: 0.17024 | val_1_rmse: 0.16925 |  0:00:16s
epoch 5  | loss: 0.01228 | val_0_rmse: 0.16233 | val_1_rmse: 0.16141 |  0:00:19s
epoch 6  | loss: 0.01199 | val_0_rmse: 0.1464  | val_1_rmse: 0.14539 |  0:00:22s
epoch 7  | loss: 0.01165 | val_0_rmse: 0.12887 | val_1_rmse: 0.12809 |  0:00:25s
epoch 8  | loss: 0.01164 | val_0_rmse: 0.117   | val_1_rmse: 0.11595 |  0:00:29s
epoch 9  | loss: 0.01118 | val_0_rmse: 0.10666 | val_1_rmse: 0.10532 |  0:00:32s
epoch 10 | loss: 0.01104 | val_0_rmse: 0.09987 | val_1_rmse: 0.09893 |  0:00:35s
epoch 11 | loss: 0.01082 | val_0_rmse: 0.10281 | val_1_rmse: 0.10137 |  0:00:38s
epoch 12 | loss: 0.01088 | val_0_rmse: 0.09769 | val_1_rmse: 0.09656 |  0:00:42s
epoch 13 | loss: 0.0109  | val_0_rmse: 0.09547 | val_1_rmse: 0.09422 |  0:00:45s
epoch 14 | loss: 0.01055 | val_0_rmse: 0.09757 | val_1_rmse: 0.09599 |  0:00:48s
epoch 15 | loss: 0.01058 | val_0_rmse: 0.09663 | val_1_rmse: 0.09536 |  0:00:51s
epoch 16 | loss: 0.01045 | val_0_rmse: 0.09737 | val_1_rmse: 0.09586 |  0:00:55s
epoch 17 | loss: 0.01049 | val_0_rmse: 0.09868 | val_1_rmse: 0.09793 |  0:00:58s
epoch 18 | loss: 0.01077 | val_0_rmse: 0.09674 | val_1_rmse: 0.09549 |  0:01:01s
epoch 19 | loss: 0.01031 | val_0_rmse: 0.09439 | val_1_rmse: 0.09277 |  0:01:04s
epoch 20 | loss: 0.01042 | val_0_rmse: 0.09403 | val_1_rmse: 0.09226 |  0:01:08s
epoch 21 | loss: 0.01036 | val_0_rmse: 0.09997 | val_1_rmse: 0.09826 |  0:01:11s
epoch 22 | loss: 0.01067 | val_0_rmse: 0.09448 | val_1_rmse: 0.09327 |  0:01:14s
epoch 23 | loss: 0.01027 | val_0_rmse: 0.09294 | val_1_rmse: 0.09138 |  0:01:17s
epoch 24 | loss: 0.01062 | val_0_rmse: 0.09474 | val_1_rmse: 0.0934  |  0:01:21s
epoch 25 | loss: 0.01012 | val_0_rmse: 0.09274 | val_1_rmse: 0.09156 |  0:01:24s
epoch 26 | loss: 0.01007 | val_0_rmse: 0.09473 | val_1_rmse: 0.09342 |  0:01:27s
epoch 27 | loss: 0.00998 | val_0_rmse: 0.09207 | val_1_rmse: 0.09065 |  0:01:30s
epoch 28 | loss: 0.00993 | val_0_rmse: 0.09459 | val_1_rmse: 0.0933  |  0:01:33s
epoch 29 | loss: 0.01036 | val_0_rmse: 0.09251 | val_1_rmse: 0.09118 |  0:01:37s
epoch 30 | loss: 0.01029 | val_0_rmse: 0.09298 | val_1_rmse: 0.09152 |  0:01:40s
epoch 31 | loss: 0.00993 | val_0_rmse: 0.09273 | val_1_rmse: 0.09185 |  0:01:43s
epoch 32 | loss: 0.01    | val_0_rmse: 0.0925  | val_1_rmse: 0.09108 |  0:01:46s
epoch 33 | loss: 0.01012 | val_0_rmse: 0.09224 | val_1_rmse: 0.09099 |  0:01:50s
epoch 34 | loss: 0.00992 | val_0_rmse: 0.0913  | val_1_rmse: 0.09011 |  0:01:53s
epoch 35 | loss: 0.00975 | val_0_rmse: 0.09223 | val_1_rmse: 0.09099 |  0:01:56s
epoch 36 | loss: 0.00989 | val_0_rmse: 0.09053 | val_1_rmse: 0.08968 |  0:01:59s
epoch 37 | loss: 0.00972 | val_0_rmse: 0.09337 | val_1_rmse: 0.09263 |  0:02:03s
epoch 38 | loss: 0.0098  | val_0_rmse: 0.09286 | val_1_rmse: 0.09182 |  0:02:06s
epoch 39 | loss: 0.01009 | val_0_rmse: 0.09262 | val_1_rmse: 0.09176 |  0:02:09s
epoch 40 | loss: 0.00988 | val_0_rmse: 0.09232 | val_1_rmse: 0.09108 |  0:02:12s
epoch 41 | loss: 0.01013 | val_0_rmse: 0.09288 | val_1_rmse: 0.0912  |  0:02:16s
epoch 42 | loss: 0.00966 | val_0_rmse: 0.09341 | val_1_rmse: 0.09274 |  0:02:19s
epoch 43 | loss: 0.00969 | val_0_rmse: 0.09458 | val_1_rmse: 0.09401 |  0:02:22s
epoch 44 | loss: 0.00967 | val_0_rmse: 0.09414 | val_1_rmse: 0.09346 |  0:02:25s
epoch 45 | loss: 0.00948 | val_0_rmse: 0.09254 | val_1_rmse: 0.09179 |  0:02:29s
epoch 46 | loss: 0.00971 | val_0_rmse: 0.09381 | val_1_rmse: 0.09268 |  0:02:32s
epoch 47 | loss: 0.00956 | val_0_rmse: 0.09149 | val_1_rmse: 0.09075 |  0:02:35s
epoch 48 | loss: 0.00953 | val_0_rmse: 0.08995 | val_1_rmse: 0.08948 |  0:02:38s
epoch 49 | loss: 0.00982 | val_0_rmse: 0.09261 | val_1_rmse: 0.09155 |  0:02:42s
epoch 50 | loss: 0.00976 | val_0_rmse: 0.08994 | val_1_rmse: 0.08956 |  0:02:45s
epoch 51 | loss: 0.00992 | val_0_rmse: 0.09558 | val_1_rmse: 0.09441 |  0:02:48s
epoch 52 | loss: 0.00966 | val_0_rmse: 0.09422 | val_1_rmse: 0.09362 |  0:02:51s
epoch 53 | loss: 0.0097  | val_0_rmse: 0.09281 | val_1_rmse: 0.09189 |  0:02:54s
epoch 54 | loss: 0.00989 | val_0_rmse: 0.09264 | val_1_rmse: 0.09213 |  0:02:58s
epoch 55 | loss: 0.00974 | val_0_rmse: 0.09059 | val_1_rmse: 0.09001 |  0:03:01s
epoch 56 | loss: 0.00956 | val_0_rmse: 0.09185 | val_1_rmse: 0.0905  |  0:03:04s
epoch 57 | loss: 0.00941 | val_0_rmse: 0.0906  | val_1_rmse: 0.08993 |  0:03:07s
epoch 58 | loss: 0.00938 | val_0_rmse: 0.09007 | val_1_rmse: 0.08905 |  0:03:11s
epoch 59 | loss: 0.00948 | val_0_rmse: 0.09096 | val_1_rmse: 0.08991 |  0:03:14s
epoch 60 | loss: 0.00987 | val_0_rmse: 0.09432 | val_1_rmse: 0.09351 |  0:03:17s
epoch 61 | loss: 0.00956 | val_0_rmse: 0.0901  | val_1_rmse: 0.08916 |  0:03:20s
epoch 62 | loss: 0.00954 | val_0_rmse: 0.10154 | val_1_rmse: 0.10063 |  0:03:23s
epoch 63 | loss: 0.00952 | val_0_rmse: 0.09121 | val_1_rmse: 0.09079 |  0:03:27s
epoch 64 | loss: 0.00942 | val_0_rmse: 0.09211 | val_1_rmse: 0.0905  |  0:03:30s
epoch 65 | loss: 0.00936 | val_0_rmse: 0.09028 | val_1_rmse: 0.08944 |  0:03:33s
epoch 66 | loss: 0.00948 | val_0_rmse: 0.09131 | val_1_rmse: 0.08989 |  0:03:36s
epoch 67 | loss: 0.00929 | val_0_rmse: 0.09005 | val_1_rmse: 0.08926 |  0:03:40s
epoch 68 | loss: 0.00922 | val_0_rmse: 0.09073 | val_1_rmse: 0.08985 |  0:03:43s
epoch 69 | loss: 0.00925 | val_0_rmse: 0.08985 | val_1_rmse: 0.08927 |  0:03:46s
epoch 70 | loss: 0.00931 | val_0_rmse: 0.09144 | val_1_rmse: 0.09058 |  0:03:49s
epoch 71 | loss: 0.00935 | val_0_rmse: 0.08918 | val_1_rmse: 0.08827 |  0:03:53s
epoch 72 | loss: 0.00951 | val_0_rmse: 0.09705 | val_1_rmse: 0.09646 |  0:03:56s
epoch 73 | loss: 0.00933 | val_0_rmse: 0.08981 | val_1_rmse: 0.08861 |  0:03:59s
epoch 74 | loss: 0.00918 | val_0_rmse: 0.09215 | val_1_rmse: 0.0912  |  0:04:02s
epoch 75 | loss: 0.00943 | val_0_rmse: 0.09804 | val_1_rmse: 0.09622 |  0:04:05s
epoch 76 | loss: 0.00991 | val_0_rmse: 0.09115 | val_1_rmse: 0.09049 |  0:04:09s
epoch 77 | loss: 0.00955 | val_0_rmse: 0.09152 | val_1_rmse: 0.09075 |  0:04:12s
epoch 78 | loss: 0.00942 | val_0_rmse: 0.0923  | val_1_rmse: 0.09085 |  0:04:15s
epoch 79 | loss: 0.00926 | val_0_rmse: 0.09282 | val_1_rmse: 0.09144 |  0:04:18s
epoch 80 | loss: 0.00917 | val_0_rmse: 0.089   | val_1_rmse: 0.08857 |  0:04:22s
epoch 81 | loss: 0.00919 | val_0_rmse: 0.08902 | val_1_rmse: 0.08854 |  0:04:25s
epoch 82 | loss: 0.00916 | val_0_rmse: 0.08911 | val_1_rmse: 0.08822 |  0:04:28s
epoch 83 | loss: 0.00934 | val_0_rmse: 0.09228 | val_1_rmse: 0.09161 |  0:04:31s
epoch 84 | loss: 0.01015 | val_0_rmse: 0.09242 | val_1_rmse: 0.09098 |  0:04:35s
epoch 85 | loss: 0.00945 | val_0_rmse: 0.09165 | val_1_rmse: 0.09027 |  0:04:38s
epoch 86 | loss: 0.00935 | val_0_rmse: 0.08932 | val_1_rmse: 0.08826 |  0:04:41s
epoch 87 | loss: 0.00973 | val_0_rmse: 0.09528 | val_1_rmse: 0.09446 |  0:04:44s
epoch 88 | loss: 0.00933 | val_0_rmse: 0.08863 | val_1_rmse: 0.08785 |  0:04:48s
epoch 89 | loss: 0.01013 | val_0_rmse: 0.09574 | val_1_rmse: 0.09435 |  0:04:51s
epoch 90 | loss: 0.00941 | val_0_rmse: 0.09272 | val_1_rmse: 0.09199 |  0:04:54s
epoch 91 | loss: 0.00932 | val_0_rmse: 0.08992 | val_1_rmse: 0.08968 |  0:04:57s
epoch 92 | loss: 0.00913 | val_0_rmse: 0.09022 | val_1_rmse: 0.08957 |  0:05:00s
epoch 93 | loss: 0.00936 | val_0_rmse: 0.09136 | val_1_rmse: 0.09063 |  0:05:04s
epoch 94 | loss: 0.00937 | val_0_rmse: 0.09215 | val_1_rmse: 0.09248 |  0:05:07s
epoch 95 | loss: 0.00944 | val_0_rmse: 0.0889  | val_1_rmse: 0.08834 |  0:05:10s
epoch 96 | loss: 0.00904 | val_0_rmse: 0.08915 | val_1_rmse: 0.0885  |  0:05:13s
epoch 97 | loss: 0.00895 | val_0_rmse: 0.08932 | val_1_rmse: 0.0887  |  0:05:17s
epoch 98 | loss: 0.00898 | val_0_rmse: 0.09005 | val_1_rmse: 0.08952 |  0:05:20s
epoch 99 | loss: 0.00905 | val_0_rmse: 0.08792 | val_1_rmse: 0.0879  |  0:05:23s
epoch 100| loss: 0.00891 | val_0_rmse: 0.08841 | val_1_rmse: 0.08787 |  0:05:26s
epoch 101| loss: 0.00925 | val_0_rmse: 0.08977 | val_1_rmse: 0.08885 |  0:05:30s
epoch 102| loss: 0.00892 | val_0_rmse: 0.08803 | val_1_rmse: 0.08781 |  0:05:33s
epoch 103| loss: 0.00908 | val_0_rmse: 0.08845 | val_1_rmse: 0.08741 |  0:05:36s
epoch 104| loss: 0.00918 | val_0_rmse: 0.08879 | val_1_rmse: 0.08848 |  0:05:39s
epoch 105| loss: 0.00902 | val_0_rmse: 0.08971 | val_1_rmse: 0.08915 |  0:05:42s
epoch 106| loss: 0.00906 | val_0_rmse: 0.08885 | val_1_rmse: 0.08812 |  0:05:46s
epoch 107| loss: 0.00887 | val_0_rmse: 0.08778 | val_1_rmse: 0.08795 |  0:05:49s
epoch 108| loss: 0.00918 | val_0_rmse: 0.09152 | val_1_rmse: 0.09151 |  0:05:52s
epoch 109| loss: 0.00932 | val_0_rmse: 0.08961 | val_1_rmse: 0.0889  |  0:05:55s
epoch 110| loss: 0.00914 | val_0_rmse: 0.0899  | val_1_rmse: 0.08926 |  0:05:59s
epoch 111| loss: 0.00961 | val_0_rmse: 0.09236 | val_1_rmse: 0.0919  |  0:06:02s
epoch 112| loss: 0.00956 | val_0_rmse: 0.08975 | val_1_rmse: 0.08881 |  0:06:05s
epoch 113| loss: 0.00912 | val_0_rmse: 0.0906  | val_1_rmse: 0.09012 |  0:06:08s
epoch 114| loss: 0.00913 | val_0_rmse: 0.08903 | val_1_rmse: 0.08867 |  0:06:12s
epoch 115| loss: 0.00899 | val_0_rmse: 0.08778 | val_1_rmse: 0.08725 |  0:06:15s
epoch 116| loss: 0.0088  | val_0_rmse: 0.08835 | val_1_rmse: 0.08794 |  0:06:18s
epoch 117| loss: 0.00866 | val_0_rmse: 0.08903 | val_1_rmse: 0.08886 |  0:06:21s
epoch 118| loss: 0.00877 | val_0_rmse: 0.08756 | val_1_rmse: 0.08731 |  0:06:24s
epoch 119| loss: 0.00885 | val_0_rmse: 0.08743 | val_1_rmse: 0.08712 |  0:06:28s
epoch 120| loss: 0.00899 | val_0_rmse: 0.0874  | val_1_rmse: 0.0869  |  0:06:31s
epoch 121| loss: 0.00928 | val_0_rmse: 0.09346 | val_1_rmse: 0.09265 |  0:06:34s
epoch 122| loss: 0.01015 | val_0_rmse: 0.09435 | val_1_rmse: 0.09323 |  0:06:37s
epoch 123| loss: 0.01002 | val_0_rmse: 0.09509 | val_1_rmse: 0.09439 |  0:06:41s
epoch 124| loss: 0.00929 | val_0_rmse: 0.09318 | val_1_rmse: 0.09248 |  0:06:44s
epoch 125| loss: 0.00899 | val_0_rmse: 0.08796 | val_1_rmse: 0.08697 |  0:06:47s
epoch 126| loss: 0.00906 | val_0_rmse: 0.08825 | val_1_rmse: 0.08734 |  0:06:50s
epoch 127| loss: 0.00904 | val_0_rmse: 0.08983 | val_1_rmse: 0.0895  |  0:06:54s
epoch 128| loss: 0.00909 | val_0_rmse: 0.09214 | val_1_rmse: 0.09137 |  0:06:57s
epoch 129| loss: 0.00903 | val_0_rmse: 0.09121 | val_1_rmse: 0.09097 |  0:07:00s
epoch 130| loss: 0.00954 | val_0_rmse: 0.09361 | val_1_rmse: 0.09256 |  0:07:03s
epoch 131| loss: 0.00927 | val_0_rmse: 0.08832 | val_1_rmse: 0.08752 |  0:07:06s
epoch 132| loss: 0.0091  | val_0_rmse: 0.08816 | val_1_rmse: 0.08735 |  0:07:10s
epoch 133| loss: 0.00897 | val_0_rmse: 0.0896  | val_1_rmse: 0.08938 |  0:07:13s
epoch 134| loss: 0.00934 | val_0_rmse: 0.09036 | val_1_rmse: 0.08953 |  0:07:16s
epoch 135| loss: 0.00979 | val_0_rmse: 0.0943  | val_1_rmse: 0.09364 |  0:07:19s
epoch 136| loss: 0.0091  | val_0_rmse: 0.09008 | val_1_rmse: 0.0894  |  0:07:23s
epoch 137| loss: 0.00912 | val_0_rmse: 0.08871 | val_1_rmse: 0.0879  |  0:07:26s
epoch 138| loss: 0.00917 | val_0_rmse: 0.08931 | val_1_rmse: 0.08844 |  0:07:29s
epoch 139| loss: 0.00889 | val_0_rmse: 0.08927 | val_1_rmse: 0.08919 |  0:07:32s
epoch 140| loss: 0.00882 | val_0_rmse: 0.08984 | val_1_rmse: 0.08903 |  0:07:36s
epoch 141| loss: 0.00918 | val_0_rmse: 0.09052 | val_1_rmse: 0.08978 |  0:07:39s
epoch 142| loss: 0.0089  | val_0_rmse: 0.08883 | val_1_rmse: 0.08798 |  0:07:42s
epoch 143| loss: 0.0088  | val_0_rmse: 0.09223 | val_1_rmse: 0.0919  |  0:07:45s
epoch 144| loss: 0.0087  | val_0_rmse: 0.0898  | val_1_rmse: 0.08925 |  0:07:48s
epoch 145| loss: 0.00876 | val_0_rmse: 0.08945 | val_1_rmse: 0.08879 |  0:07:52s
epoch 146| loss: 0.00882 | val_0_rmse: 0.08816 | val_1_rmse: 0.08788 |  0:07:55s
epoch 147| loss: 0.0089  | val_0_rmse: 0.08898 | val_1_rmse: 0.08765 |  0:07:58s
epoch 148| loss: 0.00899 | val_0_rmse: 0.08813 | val_1_rmse: 0.08771 |  0:08:01s
epoch 149| loss: 0.00897 | val_0_rmse: 0.09023 | val_1_rmse: 0.08985 |  0:08:05s
Stop training because you reached max_epochs = 150 with best_epoch = 120 and best_val_1_rmse = 0.0869
Best weights from best epoch are automatically used!
ended training at: 05:57:02
Feature importance:
[('Area', 3.313493988151405e-05), ('Baths', 0.19771647713525842), ('Beds', 0.11297685294891872), ('Latitude', 0.12939396215068033), ('Longitude', 0.16737120734564548), ('Month', 0.0994738369779295), ('Year', 0.29303452850168604)]
Mean squared error is of 10596453444.590862
Mean absolute error:70825.5117983091
MAPE:0.29603552653194765
R2 score:0.8169741822550375
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:57:03
epoch 0  | loss: 0.06613 | val_0_rmse: 0.20643 | val_1_rmse: 0.20639 |  0:00:03s
epoch 1  | loss: 0.01619 | val_0_rmse: 0.18804 | val_1_rmse: 0.1876  |  0:00:06s
epoch 2  | loss: 0.01418 | val_0_rmse: 0.1818  | val_1_rmse: 0.1817  |  0:00:09s
epoch 3  | loss: 0.01311 | val_0_rmse: 0.18002 | val_1_rmse: 0.1802  |  0:00:13s
epoch 4  | loss: 0.01273 | val_0_rmse: 0.17994 | val_1_rmse: 0.18013 |  0:00:16s
epoch 5  | loss: 0.01219 | val_0_rmse: 0.15731 | val_1_rmse: 0.15767 |  0:00:19s
epoch 6  | loss: 0.01187 | val_0_rmse: 0.14729 | val_1_rmse: 0.14711 |  0:00:22s
epoch 7  | loss: 0.01166 | val_0_rmse: 0.1263  | val_1_rmse: 0.12616 |  0:00:25s
epoch 8  | loss: 0.01157 | val_0_rmse: 0.11613 | val_1_rmse: 0.11675 |  0:00:29s
epoch 9  | loss: 0.01168 | val_0_rmse: 0.11029 | val_1_rmse: 0.11133 |  0:00:32s
epoch 10 | loss: 0.01144 | val_0_rmse: 0.10278 | val_1_rmse: 0.10415 |  0:00:35s
epoch 11 | loss: 0.01122 | val_0_rmse: 0.09778 | val_1_rmse: 0.0996  |  0:00:38s
epoch 12 | loss: 0.01099 | val_0_rmse: 0.10402 | val_1_rmse: 0.10522 |  0:00:42s
epoch 13 | loss: 0.01093 | val_0_rmse: 0.09509 | val_1_rmse: 0.09736 |  0:00:45s
epoch 14 | loss: 0.01076 | val_0_rmse: 0.09599 | val_1_rmse: 0.09738 |  0:00:48s
epoch 15 | loss: 0.01096 | val_0_rmse: 0.09815 | val_1_rmse: 0.09957 |  0:00:51s
epoch 16 | loss: 0.01063 | val_0_rmse: 0.09588 | val_1_rmse: 0.09785 |  0:00:55s
epoch 17 | loss: 0.01044 | val_0_rmse: 0.09427 | val_1_rmse: 0.09717 |  0:00:58s
epoch 18 | loss: 0.01039 | val_0_rmse: 0.09506 | val_1_rmse: 0.09742 |  0:01:01s
epoch 19 | loss: 0.01043 | val_0_rmse: 0.09414 | val_1_rmse: 0.09683 |  0:01:04s
epoch 20 | loss: 0.01014 | val_0_rmse: 0.09554 | val_1_rmse: 0.09738 |  0:01:08s
epoch 21 | loss: 0.01102 | val_0_rmse: 0.09871 | val_1_rmse: 0.10016 |  0:01:11s
epoch 22 | loss: 0.01033 | val_0_rmse: 0.0937  | val_1_rmse: 0.09534 |  0:01:14s
epoch 23 | loss: 0.01005 | val_0_rmse: 0.09395 | val_1_rmse: 0.09563 |  0:01:17s
epoch 24 | loss: 0.01003 | val_0_rmse: 0.09335 | val_1_rmse: 0.09601 |  0:01:21s
epoch 25 | loss: 0.01004 | val_0_rmse: 0.09145 | val_1_rmse: 0.09424 |  0:01:24s
epoch 26 | loss: 0.0101  | val_0_rmse: 0.09249 | val_1_rmse: 0.09483 |  0:01:27s
epoch 27 | loss: 0.01003 | val_0_rmse: 0.09347 | val_1_rmse: 0.09598 |  0:01:30s
epoch 28 | loss: 0.00987 | val_0_rmse: 0.09305 | val_1_rmse: 0.09535 |  0:01:33s
epoch 29 | loss: 0.01011 | val_0_rmse: 0.0907  | val_1_rmse: 0.09308 |  0:01:37s
epoch 30 | loss: 0.00968 | val_0_rmse: 0.09211 | val_1_rmse: 0.09436 |  0:01:40s
epoch 31 | loss: 0.00977 | val_0_rmse: 0.09554 | val_1_rmse: 0.09793 |  0:01:43s
epoch 32 | loss: 0.01012 | val_0_rmse: 0.09131 | val_1_rmse: 0.09381 |  0:01:46s
epoch 33 | loss: 0.00972 | val_0_rmse: 0.092   | val_1_rmse: 0.09446 |  0:01:50s
epoch 34 | loss: 0.00979 | val_0_rmse: 0.0907  | val_1_rmse: 0.09355 |  0:01:53s
epoch 35 | loss: 0.00989 | val_0_rmse: 0.09603 | val_1_rmse: 0.0984  |  0:01:56s
epoch 36 | loss: 0.00982 | val_0_rmse: 0.09221 | val_1_rmse: 0.09473 |  0:01:59s
epoch 37 | loss: 0.00971 | val_0_rmse: 0.09196 | val_1_rmse: 0.09475 |  0:02:03s
epoch 38 | loss: 0.00969 | val_0_rmse: 0.09038 | val_1_rmse: 0.09326 |  0:02:06s
epoch 39 | loss: 0.00967 | val_0_rmse: 0.09146 | val_1_rmse: 0.09417 |  0:02:09s
epoch 40 | loss: 0.00988 | val_0_rmse: 0.09369 | val_1_rmse: 0.09607 |  0:02:12s
epoch 41 | loss: 0.00945 | val_0_rmse: 0.09172 | val_1_rmse: 0.0947  |  0:02:16s
epoch 42 | loss: 0.00975 | val_0_rmse: 0.09176 | val_1_rmse: 0.09486 |  0:02:19s
epoch 43 | loss: 0.00991 | val_0_rmse: 0.09424 | val_1_rmse: 0.09652 |  0:02:22s
epoch 44 | loss: 0.00944 | val_0_rmse: 0.09195 | val_1_rmse: 0.09535 |  0:02:25s
epoch 45 | loss: 0.00932 | val_0_rmse: 0.0902  | val_1_rmse: 0.09333 |  0:02:29s
epoch 46 | loss: 0.00957 | val_0_rmse: 0.0923  | val_1_rmse: 0.09481 |  0:02:32s
epoch 47 | loss: 0.00939 | val_0_rmse: 0.08943 | val_1_rmse: 0.09276 |  0:02:35s
epoch 48 | loss: 0.0095  | val_0_rmse: 0.09438 | val_1_rmse: 0.09659 |  0:02:38s
epoch 49 | loss: 0.01012 | val_0_rmse: 0.09615 | val_1_rmse: 0.09842 |  0:02:42s
epoch 50 | loss: 0.00966 | val_0_rmse: 0.09384 | val_1_rmse: 0.0969  |  0:02:45s
epoch 51 | loss: 0.00945 | val_0_rmse: 0.09086 | val_1_rmse: 0.09372 |  0:02:48s
epoch 52 | loss: 0.00957 | val_0_rmse: 0.0893  | val_1_rmse: 0.09245 |  0:02:51s
epoch 53 | loss: 0.00955 | val_0_rmse: 0.09175 | val_1_rmse: 0.09426 |  0:02:55s
epoch 54 | loss: 0.00963 | val_0_rmse: 0.09209 | val_1_rmse: 0.09467 |  0:02:58s
epoch 55 | loss: 0.0096  | val_0_rmse: 0.09289 | val_1_rmse: 0.09556 |  0:03:01s
epoch 56 | loss: 0.00948 | val_0_rmse: 0.09094 | val_1_rmse: 0.09338 |  0:03:04s
epoch 57 | loss: 0.00938 | val_0_rmse: 0.09228 | val_1_rmse: 0.09569 |  0:03:08s
epoch 58 | loss: 0.00954 | val_0_rmse: 0.09334 | val_1_rmse: 0.09623 |  0:03:11s
epoch 59 | loss: 0.00949 | val_0_rmse: 0.09198 | val_1_rmse: 0.09505 |  0:03:14s
epoch 60 | loss: 0.00946 | val_0_rmse: 0.09599 | val_1_rmse: 0.09828 |  0:03:17s
epoch 61 | loss: 0.00948 | val_0_rmse: 0.09041 | val_1_rmse: 0.09391 |  0:03:21s
epoch 62 | loss: 0.00939 | val_0_rmse: 0.09024 | val_1_rmse: 0.09289 |  0:03:24s
epoch 63 | loss: 0.00971 | val_0_rmse: 0.09518 | val_1_rmse: 0.09765 |  0:03:27s
epoch 64 | loss: 0.00946 | val_0_rmse: 0.09234 | val_1_rmse: 0.0952  |  0:03:30s
epoch 65 | loss: 0.00939 | val_0_rmse: 0.0913  | val_1_rmse: 0.09412 |  0:03:33s
epoch 66 | loss: 0.00958 | val_0_rmse: 0.09497 | val_1_rmse: 0.09797 |  0:03:37s
epoch 67 | loss: 0.0094  | val_0_rmse: 0.09071 | val_1_rmse: 0.09399 |  0:03:40s
epoch 68 | loss: 0.00923 | val_0_rmse: 0.08921 | val_1_rmse: 0.0922  |  0:03:43s
epoch 69 | loss: 0.00913 | val_0_rmse: 0.08925 | val_1_rmse: 0.09264 |  0:03:46s
epoch 70 | loss: 0.00932 | val_0_rmse: 0.08888 | val_1_rmse: 0.09207 |  0:03:50s
epoch 71 | loss: 0.00912 | val_0_rmse: 0.09032 | val_1_rmse: 0.09327 |  0:03:53s
epoch 72 | loss: 0.00924 | val_0_rmse: 0.0911  | val_1_rmse: 0.09429 |  0:03:56s
epoch 73 | loss: 0.00922 | val_0_rmse: 0.08969 | val_1_rmse: 0.09301 |  0:03:59s
epoch 74 | loss: 0.00944 | val_0_rmse: 0.09334 | val_1_rmse: 0.09617 |  0:04:03s
epoch 75 | loss: 0.00943 | val_0_rmse: 0.08969 | val_1_rmse: 0.09292 |  0:04:06s
epoch 76 | loss: 0.00912 | val_0_rmse: 0.09025 | val_1_rmse: 0.09329 |  0:04:09s
epoch 77 | loss: 0.00962 | val_0_rmse: 0.09356 | val_1_rmse: 0.09602 |  0:04:12s
epoch 78 | loss: 0.00966 | val_0_rmse: 0.09795 | val_1_rmse: 0.10038 |  0:04:15s
epoch 79 | loss: 0.00971 | val_0_rmse: 0.08897 | val_1_rmse: 0.09215 |  0:04:19s
epoch 80 | loss: 0.00935 | val_0_rmse: 0.09098 | val_1_rmse: 0.09396 |  0:04:22s
epoch 81 | loss: 0.00936 | val_0_rmse: 0.09218 | val_1_rmse: 0.09539 |  0:04:25s
epoch 82 | loss: 0.00928 | val_0_rmse: 0.08982 | val_1_rmse: 0.09311 |  0:04:28s
epoch 83 | loss: 0.00933 | val_0_rmse: 0.09292 | val_1_rmse: 0.0963  |  0:04:32s
epoch 84 | loss: 0.00919 | val_0_rmse: 0.0881  | val_1_rmse: 0.09137 |  0:04:35s
epoch 85 | loss: 0.00927 | val_0_rmse: 0.08881 | val_1_rmse: 0.09221 |  0:04:38s
epoch 86 | loss: 0.00931 | val_0_rmse: 0.08698 | val_1_rmse: 0.09077 |  0:04:41s
epoch 87 | loss: 0.00951 | val_0_rmse: 0.08938 | val_1_rmse: 0.09326 |  0:04:45s
epoch 88 | loss: 0.00926 | val_0_rmse: 0.08669 | val_1_rmse: 0.09015 |  0:04:48s
epoch 89 | loss: 0.00905 | val_0_rmse: 0.09038 | val_1_rmse: 0.09367 |  0:04:51s
epoch 90 | loss: 0.00907 | val_0_rmse: 0.08938 | val_1_rmse: 0.09307 |  0:04:54s
epoch 91 | loss: 0.00892 | val_0_rmse: 0.08802 | val_1_rmse: 0.0915  |  0:04:57s
epoch 92 | loss: 0.00888 | val_0_rmse: 0.08937 | val_1_rmse: 0.09305 |  0:05:01s
epoch 93 | loss: 0.00909 | val_0_rmse: 0.09265 | val_1_rmse: 0.09569 |  0:05:04s
epoch 94 | loss: 0.00909 | val_0_rmse: 0.08695 | val_1_rmse: 0.09074 |  0:05:07s
epoch 95 | loss: 0.009   | val_0_rmse: 0.08745 | val_1_rmse: 0.09131 |  0:05:10s
epoch 96 | loss: 0.00895 | val_0_rmse: 0.0905  | val_1_rmse: 0.09443 |  0:05:14s
epoch 97 | loss: 0.00927 | val_0_rmse: 0.089   | val_1_rmse: 0.09238 |  0:05:17s
epoch 98 | loss: 0.00909 | val_0_rmse: 0.08946 | val_1_rmse: 0.0929  |  0:05:20s
epoch 99 | loss: 0.00907 | val_0_rmse: 0.08966 | val_1_rmse: 0.09299 |  0:05:23s
epoch 100| loss: 0.00914 | val_0_rmse: 0.08856 | val_1_rmse: 0.09226 |  0:05:27s
epoch 101| loss: 0.00913 | val_0_rmse: 0.08912 | val_1_rmse: 0.0922  |  0:05:30s
epoch 102| loss: 0.0092  | val_0_rmse: 0.09242 | val_1_rmse: 0.09561 |  0:05:33s
epoch 103| loss: 0.00916 | val_0_rmse: 0.08929 | val_1_rmse: 0.09213 |  0:05:36s
epoch 104| loss: 0.00916 | val_0_rmse: 0.08835 | val_1_rmse: 0.09221 |  0:05:40s
epoch 105| loss: 0.00925 | val_0_rmse: 0.08798 | val_1_rmse: 0.09181 |  0:05:43s
epoch 106| loss: 0.00919 | val_0_rmse: 0.08928 | val_1_rmse: 0.09222 |  0:05:46s
epoch 107| loss: 0.00896 | val_0_rmse: 0.08742 | val_1_rmse: 0.09124 |  0:05:49s
epoch 108| loss: 0.00919 | val_0_rmse: 0.08981 | val_1_rmse: 0.09323 |  0:05:52s
epoch 109| loss: 0.00916 | val_0_rmse: 0.09932 | val_1_rmse: 0.10171 |  0:05:56s
epoch 110| loss: 0.00959 | val_0_rmse: 0.09024 | val_1_rmse: 0.09378 |  0:05:59s
epoch 111| loss: 0.00905 | val_0_rmse: 0.08736 | val_1_rmse: 0.09121 |  0:06:02s
epoch 112| loss: 0.00885 | val_0_rmse: 0.08718 | val_1_rmse: 0.09083 |  0:06:05s
epoch 113| loss: 0.0089  | val_0_rmse: 0.09053 | val_1_rmse: 0.09356 |  0:06:09s
epoch 114| loss: 0.00914 | val_0_rmse: 0.09166 | val_1_rmse: 0.09446 |  0:06:12s
epoch 115| loss: 0.00896 | val_0_rmse: 0.09015 | val_1_rmse: 0.09347 |  0:06:15s
epoch 116| loss: 0.00904 | val_0_rmse: 0.09417 | val_1_rmse: 0.09669 |  0:06:18s
epoch 117| loss: 0.00928 | val_0_rmse: 0.08858 | val_1_rmse: 0.09134 |  0:06:22s
epoch 118| loss: 0.00901 | val_0_rmse: 0.09344 | val_1_rmse: 0.09706 |  0:06:25s

Early stopping occured at epoch 118 with best_epoch = 88 and best_val_1_rmse = 0.09015
Best weights from best epoch are automatically used!
ended training at: 06:03:29
Feature importance:
[('Area', 0.11315537686553469), ('Baths', 0.1875743068432323), ('Beds', 0.11603643344089748), ('Latitude', 0.14140349260500207), ('Longitude', 0.22810719952414912), ('Month', 0.0), ('Year', 0.21372319072118434)]
Mean squared error is of 10367579157.267244
Mean absolute error:70778.91179565858
MAPE:0.2923229726165497
R2 score:0.820717099849218
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:03:29
epoch 0  | loss: 0.0717  | val_0_rmse: 0.21313 | val_1_rmse: 0.21268 |  0:00:03s
epoch 1  | loss: 0.01769 | val_0_rmse: 0.19025 | val_1_rmse: 0.18962 |  0:00:06s
epoch 2  | loss: 0.01569 | val_0_rmse: 0.18028 | val_1_rmse: 0.17951 |  0:00:09s
epoch 3  | loss: 0.01404 | val_0_rmse: 0.17486 | val_1_rmse: 0.1739  |  0:00:12s
epoch 4  | loss: 0.01332 | val_0_rmse: 0.16912 | val_1_rmse: 0.169   |  0:00:16s
epoch 5  | loss: 0.01294 | val_0_rmse: 0.15277 | val_1_rmse: 0.15257 |  0:00:19s
epoch 6  | loss: 0.01229 | val_0_rmse: 0.14528 | val_1_rmse: 0.14456 |  0:00:22s
epoch 7  | loss: 0.01264 | val_0_rmse: 0.12578 | val_1_rmse: 0.12582 |  0:00:25s
epoch 8  | loss: 0.01175 | val_0_rmse: 0.12199 | val_1_rmse: 0.12219 |  0:00:29s
epoch 9  | loss: 0.0121  | val_0_rmse: 0.1136  | val_1_rmse: 0.11495 |  0:00:32s
epoch 10 | loss: 0.01166 | val_0_rmse: 0.10938 | val_1_rmse: 0.11111 |  0:00:35s
epoch 11 | loss: 0.0118  | val_0_rmse: 0.10828 | val_1_rmse: 0.10954 |  0:00:38s
epoch 12 | loss: 0.01138 | val_0_rmse: 0.10096 | val_1_rmse: 0.10312 |  0:00:42s
epoch 13 | loss: 0.01096 | val_0_rmse: 0.09584 | val_1_rmse: 0.09642 |  0:00:45s
epoch 14 | loss: 0.0111  | val_0_rmse: 0.10068 | val_1_rmse: 0.10178 |  0:00:48s
epoch 15 | loss: 0.01111 | val_0_rmse: 0.09755 | val_1_rmse: 0.09854 |  0:00:51s
epoch 16 | loss: 0.01111 | val_0_rmse: 0.09762 | val_1_rmse: 0.09856 |  0:00:54s
epoch 17 | loss: 0.01066 | val_0_rmse: 0.10371 | val_1_rmse: 0.10419 |  0:00:58s
epoch 18 | loss: 0.01114 | val_0_rmse: 0.09572 | val_1_rmse: 0.0968  |  0:01:01s
epoch 19 | loss: 0.01058 | val_0_rmse: 0.0979  | val_1_rmse: 0.09858 |  0:01:04s
epoch 20 | loss: 0.01045 | val_0_rmse: 0.09694 | val_1_rmse: 0.09791 |  0:01:07s
epoch 21 | loss: 0.0103  | val_0_rmse: 0.09405 | val_1_rmse: 0.09537 |  0:01:11s
epoch 22 | loss: 0.01046 | val_0_rmse: 0.09428 | val_1_rmse: 0.09539 |  0:01:14s
epoch 23 | loss: 0.01025 | val_0_rmse: 0.09421 | val_1_rmse: 0.09537 |  0:01:17s
epoch 24 | loss: 0.01013 | val_0_rmse: 0.09173 | val_1_rmse: 0.09275 |  0:01:20s
epoch 25 | loss: 0.01037 | val_0_rmse: 0.0944  | val_1_rmse: 0.09496 |  0:01:24s
epoch 26 | loss: 0.01053 | val_0_rmse: 0.09434 | val_1_rmse: 0.09514 |  0:01:27s
epoch 27 | loss: 0.01105 | val_0_rmse: 0.09669 | val_1_rmse: 0.09783 |  0:01:30s
epoch 28 | loss: 0.01065 | val_0_rmse: 0.09321 | val_1_rmse: 0.0947  |  0:01:33s
epoch 29 | loss: 0.0105  | val_0_rmse: 0.09309 | val_1_rmse: 0.09411 |  0:01:36s
epoch 30 | loss: 0.01012 | val_0_rmse: 0.09533 | val_1_rmse: 0.09618 |  0:01:40s
epoch 31 | loss: 0.01006 | val_0_rmse: 0.09554 | val_1_rmse: 0.09676 |  0:01:43s
epoch 32 | loss: 0.01    | val_0_rmse: 0.09184 | val_1_rmse: 0.09294 |  0:01:46s
epoch 33 | loss: 0.01026 | val_0_rmse: 0.09675 | val_1_rmse: 0.09661 |  0:01:49s
epoch 34 | loss: 0.01019 | val_0_rmse: 0.09396 | val_1_rmse: 0.09464 |  0:01:53s
epoch 35 | loss: 0.01023 | val_0_rmse: 0.09671 | val_1_rmse: 0.09813 |  0:01:56s
epoch 36 | loss: 0.00978 | val_0_rmse: 0.09564 | val_1_rmse: 0.09718 |  0:01:59s
epoch 37 | loss: 0.01009 | val_0_rmse: 0.09533 | val_1_rmse: 0.09609 |  0:02:02s
epoch 38 | loss: 0.00996 | val_0_rmse: 0.09125 | val_1_rmse: 0.09186 |  0:02:05s
epoch 39 | loss: 0.01012 | val_0_rmse: 0.09425 | val_1_rmse: 0.09525 |  0:02:09s
epoch 40 | loss: 0.01055 | val_0_rmse: 0.09378 | val_1_rmse: 0.0955  |  0:02:12s
epoch 41 | loss: 0.00992 | val_0_rmse: 0.09319 | val_1_rmse: 0.09367 |  0:02:15s
epoch 42 | loss: 0.00966 | val_0_rmse: 0.0927  | val_1_rmse: 0.0936  |  0:02:18s
epoch 43 | loss: 0.00999 | val_0_rmse: 0.09187 | val_1_rmse: 0.09242 |  0:02:22s
epoch 44 | loss: 0.00989 | val_0_rmse: 0.0917  | val_1_rmse: 0.09232 |  0:02:25s
epoch 45 | loss: 0.01011 | val_0_rmse: 0.09118 | val_1_rmse: 0.09235 |  0:02:28s
epoch 46 | loss: 0.00967 | val_0_rmse: 0.09529 | val_1_rmse: 0.09674 |  0:02:31s
epoch 47 | loss: 0.00992 | val_0_rmse: 0.09537 | val_1_rmse: 0.09555 |  0:02:35s
epoch 48 | loss: 0.00982 | val_0_rmse: 0.09402 | val_1_rmse: 0.09478 |  0:02:38s
epoch 49 | loss: 0.01004 | val_0_rmse: 0.09618 | val_1_rmse: 0.09742 |  0:02:41s
epoch 50 | loss: 0.00986 | val_0_rmse: 0.09677 | val_1_rmse: 0.09776 |  0:02:44s
epoch 51 | loss: 0.00996 | val_0_rmse: 0.09709 | val_1_rmse: 0.09828 |  0:02:48s
epoch 52 | loss: 0.00985 | val_0_rmse: 0.09453 | val_1_rmse: 0.09522 |  0:02:51s
epoch 53 | loss: 0.00976 | val_0_rmse: 0.09138 | val_1_rmse: 0.09261 |  0:02:54s
epoch 54 | loss: 0.00945 | val_0_rmse: 0.09269 | val_1_rmse: 0.09324 |  0:02:57s
epoch 55 | loss: 0.0094  | val_0_rmse: 0.09072 | val_1_rmse: 0.09143 |  0:03:00s
epoch 56 | loss: 0.00967 | val_0_rmse: 0.09416 | val_1_rmse: 0.09534 |  0:03:04s
epoch 57 | loss: 0.00991 | val_0_rmse: 0.09342 | val_1_rmse: 0.09477 |  0:03:07s
epoch 58 | loss: 0.01053 | val_0_rmse: 0.10252 | val_1_rmse: 0.10404 |  0:03:10s
epoch 59 | loss: 0.01121 | val_0_rmse: 0.10288 | val_1_rmse: 0.10279 |  0:03:13s
epoch 60 | loss: 0.01073 | val_0_rmse: 0.09892 | val_1_rmse: 0.09995 |  0:03:17s
epoch 61 | loss: 0.01024 | val_0_rmse: 0.0986  | val_1_rmse: 0.09972 |  0:03:20s
epoch 62 | loss: 0.01036 | val_0_rmse: 0.09355 | val_1_rmse: 0.09456 |  0:03:23s
epoch 63 | loss: 0.00985 | val_0_rmse: 0.09325 | val_1_rmse: 0.09338 |  0:03:26s
epoch 64 | loss: 0.00974 | val_0_rmse: 0.09056 | val_1_rmse: 0.09162 |  0:03:30s
epoch 65 | loss: 0.01005 | val_0_rmse: 0.09492 | val_1_rmse: 0.09559 |  0:03:33s
epoch 66 | loss: 0.00992 | val_0_rmse: 0.09124 | val_1_rmse: 0.09246 |  0:03:36s
epoch 67 | loss: 0.00951 | val_0_rmse: 0.09098 | val_1_rmse: 0.09157 |  0:03:39s
epoch 68 | loss: 0.00959 | val_0_rmse: 0.08924 | val_1_rmse: 0.08993 |  0:03:43s
epoch 69 | loss: 0.00956 | val_0_rmse: 0.09133 | val_1_rmse: 0.09196 |  0:03:46s
epoch 70 | loss: 0.00948 | val_0_rmse: 0.08999 | val_1_rmse: 0.09089 |  0:03:49s
epoch 71 | loss: 0.00953 | val_0_rmse: 0.09058 | val_1_rmse: 0.09138 |  0:03:52s
epoch 72 | loss: 0.00927 | val_0_rmse: 0.09268 | val_1_rmse: 0.09405 |  0:03:55s
epoch 73 | loss: 0.00974 | val_0_rmse: 0.09023 | val_1_rmse: 0.09095 |  0:03:59s
epoch 74 | loss: 0.00932 | val_0_rmse: 0.09139 | val_1_rmse: 0.09211 |  0:04:02s
epoch 75 | loss: 0.00966 | val_0_rmse: 0.09172 | val_1_rmse: 0.09272 |  0:04:05s
epoch 76 | loss: 0.00954 | val_0_rmse: 0.09674 | val_1_rmse: 0.09749 |  0:04:08s
epoch 77 | loss: 0.00936 | val_0_rmse: 0.09013 | val_1_rmse: 0.09181 |  0:04:12s
epoch 78 | loss: 0.00923 | val_0_rmse: 0.08989 | val_1_rmse: 0.09106 |  0:04:15s
epoch 79 | loss: 0.00948 | val_0_rmse: 0.09196 | val_1_rmse: 0.09306 |  0:04:18s
epoch 80 | loss: 0.00922 | val_0_rmse: 0.09138 | val_1_rmse: 0.09213 |  0:04:21s
epoch 81 | loss: 0.00921 | val_0_rmse: 0.08893 | val_1_rmse: 0.08946 |  0:04:24s
epoch 82 | loss: 0.00944 | val_0_rmse: 0.09595 | val_1_rmse: 0.09724 |  0:04:28s
epoch 83 | loss: 0.00917 | val_0_rmse: 0.0961  | val_1_rmse: 0.09756 |  0:04:31s
epoch 84 | loss: 0.00917 | val_0_rmse: 0.08761 | val_1_rmse: 0.08886 |  0:04:34s
epoch 85 | loss: 0.00921 | val_0_rmse: 0.08745 | val_1_rmse: 0.08856 |  0:04:37s
epoch 86 | loss: 0.00925 | val_0_rmse: 0.09135 | val_1_rmse: 0.0926  |  0:04:41s
epoch 87 | loss: 0.00973 | val_0_rmse: 0.09377 | val_1_rmse: 0.09506 |  0:04:44s
epoch 88 | loss: 0.00992 | val_0_rmse: 0.09463 | val_1_rmse: 0.09555 |  0:04:47s
epoch 89 | loss: 0.00957 | val_0_rmse: 0.09136 | val_1_rmse: 0.09244 |  0:04:50s
epoch 90 | loss: 0.00953 | val_0_rmse: 0.10271 | val_1_rmse: 0.10423 |  0:04:53s
epoch 91 | loss: 0.00944 | val_0_rmse: 0.0917  | val_1_rmse: 0.09282 |  0:04:57s
epoch 92 | loss: 0.00956 | val_0_rmse: 0.09373 | val_1_rmse: 0.09494 |  0:05:00s
epoch 93 | loss: 0.00957 | val_0_rmse: 0.08965 | val_1_rmse: 0.09063 |  0:05:03s
epoch 94 | loss: 0.00934 | val_0_rmse: 0.09216 | val_1_rmse: 0.09314 |  0:05:06s
epoch 95 | loss: 0.0096  | val_0_rmse: 0.09361 | val_1_rmse: 0.09517 |  0:05:10s
epoch 96 | loss: 0.00985 | val_0_rmse: 0.10402 | val_1_rmse: 0.10514 |  0:05:13s
epoch 97 | loss: 0.00944 | val_0_rmse: 0.09638 | val_1_rmse: 0.0975  |  0:05:16s
epoch 98 | loss: 0.01006 | val_0_rmse: 0.09872 | val_1_rmse: 0.09962 |  0:05:19s
epoch 99 | loss: 0.0105  | val_0_rmse: 0.09631 | val_1_rmse: 0.09664 |  0:05:23s
epoch 100| loss: 0.01029 | val_0_rmse: 0.10799 | val_1_rmse: 0.10786 |  0:05:26s
epoch 101| loss: 0.01225 | val_0_rmse: 0.11047 | val_1_rmse: 0.11075 |  0:05:29s
epoch 102| loss: 0.01166 | val_0_rmse: 0.11238 | val_1_rmse: 0.11329 |  0:05:32s
epoch 103| loss: 0.01204 | val_0_rmse: 0.1158  | val_1_rmse: 0.1156  |  0:05:36s
epoch 104| loss: 0.01357 | val_0_rmse: 0.11766 | val_1_rmse: 0.1173  |  0:05:39s
epoch 105| loss: 0.0118  | val_0_rmse: 0.10347 | val_1_rmse: 0.10428 |  0:05:42s
epoch 106| loss: 0.0116  | val_0_rmse: 0.10856 | val_1_rmse: 0.1085  |  0:05:45s
epoch 107| loss: 0.01256 | val_0_rmse: 0.10037 | val_1_rmse: 0.1012  |  0:05:49s
epoch 108| loss: 0.01185 | val_0_rmse: 0.10188 | val_1_rmse: 0.10204 |  0:05:52s
epoch 109| loss: 0.01167 | val_0_rmse: 0.10131 | val_1_rmse: 0.10101 |  0:05:55s
epoch 110| loss: 0.01129 | val_0_rmse: 0.10136 | val_1_rmse: 0.10242 |  0:05:58s
epoch 111| loss: 0.01081 | val_0_rmse: 0.09876 | val_1_rmse: 0.09917 |  0:06:02s
epoch 112| loss: 0.01089 | val_0_rmse: 0.09915 | val_1_rmse: 0.09924 |  0:06:05s
epoch 113| loss: 0.01065 | val_0_rmse: 0.09743 | val_1_rmse: 0.09864 |  0:06:08s
epoch 114| loss: 0.0107  | val_0_rmse: 0.09576 | val_1_rmse: 0.09708 |  0:06:11s
epoch 115| loss: 0.01078 | val_0_rmse: 0.10638 | val_1_rmse: 0.10613 |  0:06:14s

Early stopping occured at epoch 115 with best_epoch = 85 and best_val_1_rmse = 0.08856
Best weights from best epoch are automatically used!
ended training at: 06:09:45
Feature importance:
[('Area', 0.0675393461990803), ('Baths', 0.010498705323503943), ('Beds', 0.053748434519705575), ('Latitude', 0.16907605587755137), ('Longitude', 0.34763058915452444), ('Month', 0.040551525733793815), ('Year', 0.31095534319184054)]
Mean squared error is of 10553244664.426449
Mean absolute error:70683.00701440417
MAPE:0.2925061162593744
R2 score:0.815760241494812
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:09:46
epoch 0  | loss: 0.18662 | val_0_rmse: 0.1797  | val_1_rmse: 0.17901 |  0:00:01s
epoch 1  | loss: 0.0199  | val_0_rmse: 0.20544 | val_1_rmse: 0.20357 |  0:00:01s
epoch 2  | loss: 0.01632 | val_0_rmse: 0.20744 | val_1_rmse: 0.20568 |  0:00:02s
epoch 3  | loss: 0.01499 | val_0_rmse: 0.22352 | val_1_rmse: 0.22179 |  0:00:03s
epoch 4  | loss: 0.0136  | val_0_rmse: 0.2263  | val_1_rmse: 0.22482 |  0:00:04s
epoch 5  | loss: 0.01266 | val_0_rmse: 0.20768 | val_1_rmse: 0.20685 |  0:00:05s
epoch 6  | loss: 0.01229 | val_0_rmse: 0.19639 | val_1_rmse: 0.19588 |  0:00:06s
epoch 7  | loss: 0.01184 | val_0_rmse: 0.20497 | val_1_rmse: 0.20541 |  0:00:07s
epoch 8  | loss: 0.01179 | val_0_rmse: 0.19871 | val_1_rmse: 0.19935 |  0:00:08s
epoch 9  | loss: 0.01189 | val_0_rmse: 0.17669 | val_1_rmse: 0.177   |  0:00:09s
epoch 10 | loss: 0.01213 | val_0_rmse: 0.1608  | val_1_rmse: 0.16125 |  0:00:10s
epoch 11 | loss: 0.01282 | val_0_rmse: 0.15531 | val_1_rmse: 0.15681 |  0:00:11s
epoch 12 | loss: 0.01159 | val_0_rmse: 0.15613 | val_1_rmse: 0.15774 |  0:00:12s
epoch 13 | loss: 0.01164 | val_0_rmse: 0.16396 | val_1_rmse: 0.1661  |  0:00:13s
epoch 14 | loss: 0.01166 | val_0_rmse: 0.17429 | val_1_rmse: 0.17661 |  0:00:14s
epoch 15 | loss: 0.01096 | val_0_rmse: 0.16232 | val_1_rmse: 0.16483 |  0:00:15s
epoch 16 | loss: 0.01081 | val_0_rmse: 0.15294 | val_1_rmse: 0.15557 |  0:00:16s
epoch 17 | loss: 0.01114 | val_0_rmse: 0.15511 | val_1_rmse: 0.15804 |  0:00:17s
epoch 18 | loss: 0.01103 | val_0_rmse: 0.15112 | val_1_rmse: 0.15437 |  0:00:18s
epoch 19 | loss: 0.01099 | val_0_rmse: 0.13625 | val_1_rmse: 0.13903 |  0:00:19s
epoch 20 | loss: 0.01089 | val_0_rmse: 0.13011 | val_1_rmse: 0.13332 |  0:00:20s
epoch 21 | loss: 0.01075 | val_0_rmse: 0.13242 | val_1_rmse: 0.13534 |  0:00:21s
epoch 22 | loss: 0.01076 | val_0_rmse: 0.12596 | val_1_rmse: 0.12961 |  0:00:22s
epoch 23 | loss: 0.01108 | val_0_rmse: 0.11643 | val_1_rmse: 0.12031 |  0:00:23s
epoch 24 | loss: 0.01081 | val_0_rmse: 0.12242 | val_1_rmse: 0.12562 |  0:00:24s
epoch 25 | loss: 0.01086 | val_0_rmse: 0.11791 | val_1_rmse: 0.12139 |  0:00:25s
epoch 26 | loss: 0.01076 | val_0_rmse: 0.1074  | val_1_rmse: 0.11059 |  0:00:25s
epoch 27 | loss: 0.01086 | val_0_rmse: 0.10596 | val_1_rmse: 0.10941 |  0:00:26s
epoch 28 | loss: 0.01128 | val_0_rmse: 0.10543 | val_1_rmse: 0.10932 |  0:00:27s
epoch 29 | loss: 0.01117 | val_0_rmse: 0.10344 | val_1_rmse: 0.10703 |  0:00:28s
epoch 30 | loss: 0.01083 | val_0_rmse: 0.10435 | val_1_rmse: 0.10774 |  0:00:29s
epoch 31 | loss: 0.01095 | val_0_rmse: 0.10203 | val_1_rmse: 0.10573 |  0:00:30s
epoch 32 | loss: 0.01054 | val_0_rmse: 0.09954 | val_1_rmse: 0.10325 |  0:00:31s
epoch 33 | loss: 0.0105  | val_0_rmse: 0.10072 | val_1_rmse: 0.10383 |  0:00:32s
epoch 34 | loss: 0.01054 | val_0_rmse: 0.10144 | val_1_rmse: 0.1048  |  0:00:33s
epoch 35 | loss: 0.01065 | val_0_rmse: 0.09938 | val_1_rmse: 0.10229 |  0:00:34s
epoch 36 | loss: 0.01064 | val_0_rmse: 0.10015 | val_1_rmse: 0.10374 |  0:00:35s
epoch 37 | loss: 0.01062 | val_0_rmse: 0.10379 | val_1_rmse: 0.10655 |  0:00:36s
epoch 38 | loss: 0.01054 | val_0_rmse: 0.10156 | val_1_rmse: 0.10483 |  0:00:37s
epoch 39 | loss: 0.01028 | val_0_rmse: 0.09688 | val_1_rmse: 0.1003  |  0:00:38s
epoch 40 | loss: 0.01096 | val_0_rmse: 0.10021 | val_1_rmse: 0.1035  |  0:00:39s
epoch 41 | loss: 0.01086 | val_0_rmse: 0.09771 | val_1_rmse: 0.10144 |  0:00:40s
epoch 42 | loss: 0.01059 | val_0_rmse: 0.09904 | val_1_rmse: 0.10213 |  0:00:41s
epoch 43 | loss: 0.01055 | val_0_rmse: 0.10022 | val_1_rmse: 0.10303 |  0:00:42s
epoch 44 | loss: 0.01042 | val_0_rmse: 0.09749 | val_1_rmse: 0.10057 |  0:00:43s
epoch 45 | loss: 0.01049 | val_0_rmse: 0.09695 | val_1_rmse: 0.10034 |  0:00:44s
epoch 46 | loss: 0.01022 | val_0_rmse: 0.09665 | val_1_rmse: 0.10009 |  0:00:45s
epoch 47 | loss: 0.01022 | val_0_rmse: 0.09673 | val_1_rmse: 0.10009 |  0:00:46s
epoch 48 | loss: 0.01043 | val_0_rmse: 0.09713 | val_1_rmse: 0.10052 |  0:00:46s
epoch 49 | loss: 0.01022 | val_0_rmse: 0.09591 | val_1_rmse: 0.09974 |  0:00:47s
epoch 50 | loss: 0.01028 | val_0_rmse: 0.09871 | val_1_rmse: 0.10223 |  0:00:48s
epoch 51 | loss: 0.01027 | val_0_rmse: 0.09649 | val_1_rmse: 0.0995  |  0:00:49s
epoch 52 | loss: 0.01018 | val_0_rmse: 0.10213 | val_1_rmse: 0.10517 |  0:00:50s
epoch 53 | loss: 0.01046 | val_0_rmse: 0.09822 | val_1_rmse: 0.1025  |  0:00:51s
epoch 54 | loss: 0.01034 | val_0_rmse: 0.09657 | val_1_rmse: 0.09999 |  0:00:52s
epoch 55 | loss: 0.0102  | val_0_rmse: 0.09597 | val_1_rmse: 0.09947 |  0:00:53s
epoch 56 | loss: 0.01015 | val_0_rmse: 0.09757 | val_1_rmse: 0.10035 |  0:00:54s
epoch 57 | loss: 0.01009 | val_0_rmse: 0.09813 | val_1_rmse: 0.1015  |  0:00:55s
epoch 58 | loss: 0.0102  | val_0_rmse: 0.09911 | val_1_rmse: 0.10365 |  0:00:56s
epoch 59 | loss: 0.01025 | val_0_rmse: 0.09839 | val_1_rmse: 0.10195 |  0:00:57s
epoch 60 | loss: 0.01014 | val_0_rmse: 0.09529 | val_1_rmse: 0.09911 |  0:00:58s
epoch 61 | loss: 0.01018 | val_0_rmse: 0.09913 | val_1_rmse: 0.10176 |  0:00:59s
epoch 62 | loss: 0.01041 | val_0_rmse: 0.09648 | val_1_rmse: 0.09977 |  0:01:00s
epoch 63 | loss: 0.01027 | val_0_rmse: 0.09543 | val_1_rmse: 0.09905 |  0:01:01s
epoch 64 | loss: 0.00994 | val_0_rmse: 0.09404 | val_1_rmse: 0.09809 |  0:01:02s
epoch 65 | loss: 0.01002 | val_0_rmse: 0.09859 | val_1_rmse: 0.10185 |  0:01:03s
epoch 66 | loss: 0.01    | val_0_rmse: 0.09418 | val_1_rmse: 0.09814 |  0:01:04s
epoch 67 | loss: 0.00987 | val_0_rmse: 0.09634 | val_1_rmse: 0.10043 |  0:01:05s
epoch 68 | loss: 0.0098  | val_0_rmse: 0.09667 | val_1_rmse: 0.10077 |  0:01:06s
epoch 69 | loss: 0.01005 | val_0_rmse: 0.09361 | val_1_rmse: 0.09779 |  0:01:07s
epoch 70 | loss: 0.01009 | val_0_rmse: 0.09817 | val_1_rmse: 0.10145 |  0:01:08s
epoch 71 | loss: 0.01039 | val_0_rmse: 0.0945  | val_1_rmse: 0.09809 |  0:01:09s
epoch 72 | loss: 0.00993 | val_0_rmse: 0.09605 | val_1_rmse: 0.09948 |  0:01:10s
epoch 73 | loss: 0.00989 | val_0_rmse: 0.09679 | val_1_rmse: 0.10072 |  0:01:10s
epoch 74 | loss: 0.01013 | val_0_rmse: 0.09791 | val_1_rmse: 0.10177 |  0:01:11s
epoch 75 | loss: 0.01047 | val_0_rmse: 0.09567 | val_1_rmse: 0.09927 |  0:01:12s
epoch 76 | loss: 0.01033 | val_0_rmse: 0.09536 | val_1_rmse: 0.09973 |  0:01:13s
epoch 77 | loss: 0.01032 | val_0_rmse: 0.0954  | val_1_rmse: 0.09985 |  0:01:14s
epoch 78 | loss: 0.01009 | val_0_rmse: 0.09595 | val_1_rmse: 0.09981 |  0:01:15s
epoch 79 | loss: 0.01025 | val_0_rmse: 0.09525 | val_1_rmse: 0.09943 |  0:01:16s
epoch 80 | loss: 0.00982 | val_0_rmse: 0.09932 | val_1_rmse: 0.10371 |  0:01:17s
epoch 81 | loss: 0.01004 | val_0_rmse: 0.09779 | val_1_rmse: 0.10256 |  0:01:18s
epoch 82 | loss: 0.00975 | val_0_rmse: 0.09352 | val_1_rmse: 0.09819 |  0:01:19s
epoch 83 | loss: 0.0101  | val_0_rmse: 0.09568 | val_1_rmse: 0.09976 |  0:01:20s
epoch 84 | loss: 0.00986 | val_0_rmse: 0.09395 | val_1_rmse: 0.09729 |  0:01:21s
epoch 85 | loss: 0.00983 | val_0_rmse: 0.09635 | val_1_rmse: 0.1007  |  0:01:22s
epoch 86 | loss: 0.00958 | val_0_rmse: 0.09406 | val_1_rmse: 0.09797 |  0:01:23s
epoch 87 | loss: 0.00971 | val_0_rmse: 0.0935  | val_1_rmse: 0.09773 |  0:01:24s
epoch 88 | loss: 0.00968 | val_0_rmse: 0.0928  | val_1_rmse: 0.09686 |  0:01:25s
epoch 89 | loss: 0.00966 | val_0_rmse: 0.09371 | val_1_rmse: 0.09811 |  0:01:26s
epoch 90 | loss: 0.01006 | val_0_rmse: 0.09551 | val_1_rmse: 0.09866 |  0:01:27s
epoch 91 | loss: 0.00989 | val_0_rmse: 0.09403 | val_1_rmse: 0.09789 |  0:01:28s
epoch 92 | loss: 0.00961 | val_0_rmse: 0.09521 | val_1_rmse: 0.09957 |  0:01:29s
epoch 93 | loss: 0.00969 | val_0_rmse: 0.09364 | val_1_rmse: 0.09804 |  0:01:30s
epoch 94 | loss: 0.00976 | val_0_rmse: 0.09536 | val_1_rmse: 0.09885 |  0:01:31s
epoch 95 | loss: 0.00953 | val_0_rmse: 0.09253 | val_1_rmse: 0.09699 |  0:01:32s
epoch 96 | loss: 0.00963 | val_0_rmse: 0.09354 | val_1_rmse: 0.09833 |  0:01:32s
epoch 97 | loss: 0.00948 | val_0_rmse: 0.09361 | val_1_rmse: 0.09819 |  0:01:33s
epoch 98 | loss: 0.0097  | val_0_rmse: 0.09632 | val_1_rmse: 0.10092 |  0:01:34s
epoch 99 | loss: 0.00998 | val_0_rmse: 0.09528 | val_1_rmse: 0.10017 |  0:01:35s
epoch 100| loss: 0.00987 | val_0_rmse: 0.09508 | val_1_rmse: 0.09916 |  0:01:36s
epoch 101| loss: 0.00986 | val_0_rmse: 0.09789 | val_1_rmse: 0.10168 |  0:01:37s
epoch 102| loss: 0.00983 | val_0_rmse: 0.09583 | val_1_rmse: 0.10062 |  0:01:38s
epoch 103| loss: 0.00992 | val_0_rmse: 0.09852 | val_1_rmse: 0.1026  |  0:01:39s
epoch 104| loss: 0.01036 | val_0_rmse: 0.09837 | val_1_rmse: 0.10143 |  0:01:40s
epoch 105| loss: 0.01016 | val_0_rmse: 0.0963  | val_1_rmse: 0.09882 |  0:01:41s
epoch 106| loss: 0.00992 | val_0_rmse: 0.0971  | val_1_rmse: 0.10078 |  0:01:42s
epoch 107| loss: 0.00974 | val_0_rmse: 0.09347 | val_1_rmse: 0.09692 |  0:01:43s
epoch 108| loss: 0.00996 | val_0_rmse: 0.10038 | val_1_rmse: 0.10388 |  0:01:44s
epoch 109| loss: 0.00984 | val_0_rmse: 0.09382 | val_1_rmse: 0.09718 |  0:01:45s
epoch 110| loss: 0.00943 | val_0_rmse: 0.09325 | val_1_rmse: 0.09685 |  0:01:46s
epoch 111| loss: 0.00984 | val_0_rmse: 0.09615 | val_1_rmse: 0.10026 |  0:01:47s
epoch 112| loss: 0.00969 | val_0_rmse: 0.09557 | val_1_rmse: 0.10009 |  0:01:48s
epoch 113| loss: 0.00981 | val_0_rmse: 0.09417 | val_1_rmse: 0.09781 |  0:01:49s
epoch 114| loss: 0.00976 | val_0_rmse: 0.0943  | val_1_rmse: 0.09815 |  0:01:50s
epoch 115| loss: 0.01004 | val_0_rmse: 0.09507 | val_1_rmse: 0.09947 |  0:01:51s
epoch 116| loss: 0.00977 | val_0_rmse: 0.0982  | val_1_rmse: 0.10079 |  0:01:52s
epoch 117| loss: 0.00977 | val_0_rmse: 0.09331 | val_1_rmse: 0.09677 |  0:01:53s
epoch 118| loss: 0.00956 | val_0_rmse: 0.09394 | val_1_rmse: 0.09838 |  0:01:54s
epoch 119| loss: 0.00969 | val_0_rmse: 0.09425 | val_1_rmse: 0.09831 |  0:01:54s
epoch 120| loss: 0.00951 | val_0_rmse: 0.09505 | val_1_rmse: 0.0993  |  0:01:55s
epoch 121| loss: 0.00957 | val_0_rmse: 0.09311 | val_1_rmse: 0.0978  |  0:01:56s
epoch 122| loss: 0.00949 | val_0_rmse: 0.0932  | val_1_rmse: 0.09657 |  0:01:57s
epoch 123| loss: 0.0096  | val_0_rmse: 0.09424 | val_1_rmse: 0.09775 |  0:01:58s
epoch 124| loss: 0.00979 | val_0_rmse: 0.09544 | val_1_rmse: 0.09892 |  0:01:59s
epoch 125| loss: 0.00986 | val_0_rmse: 0.09417 | val_1_rmse: 0.09858 |  0:02:00s
epoch 126| loss: 0.00996 | val_0_rmse: 0.09365 | val_1_rmse: 0.09812 |  0:02:01s
epoch 127| loss: 0.00972 | val_0_rmse: 0.09319 | val_1_rmse: 0.09711 |  0:02:02s
epoch 128| loss: 0.00944 | val_0_rmse: 0.09356 | val_1_rmse: 0.09781 |  0:02:03s
epoch 129| loss: 0.00947 | val_0_rmse: 0.09826 | val_1_rmse: 0.10246 |  0:02:04s
epoch 130| loss: 0.00966 | val_0_rmse: 0.09321 | val_1_rmse: 0.0974  |  0:02:05s
epoch 131| loss: 0.01003 | val_0_rmse: 0.09317 | val_1_rmse: 0.09789 |  0:02:06s
epoch 132| loss: 0.00957 | val_0_rmse: 0.09504 | val_1_rmse: 0.09979 |  0:02:07s
epoch 133| loss: 0.01013 | val_0_rmse: 0.097   | val_1_rmse: 0.1007  |  0:02:08s
epoch 134| loss: 0.0102  | val_0_rmse: 0.1012  | val_1_rmse: 0.10494 |  0:02:09s
epoch 135| loss: 0.01015 | val_0_rmse: 0.09576 | val_1_rmse: 0.09936 |  0:02:10s
epoch 136| loss: 0.00971 | val_0_rmse: 0.09383 | val_1_rmse: 0.09786 |  0:02:11s
epoch 137| loss: 0.00991 | val_0_rmse: 0.09752 | val_1_rmse: 0.10161 |  0:02:12s
epoch 138| loss: 0.00952 | val_0_rmse: 0.09382 | val_1_rmse: 0.09782 |  0:02:13s
epoch 139| loss: 0.00963 | val_0_rmse: 0.09272 | val_1_rmse: 0.09734 |  0:02:14s
epoch 140| loss: 0.00953 | val_0_rmse: 0.0943  | val_1_rmse: 0.09857 |  0:02:15s
epoch 141| loss: 0.00959 | val_0_rmse: 0.09609 | val_1_rmse: 0.1003  |  0:02:16s
epoch 142| loss: 0.01003 | val_0_rmse: 0.09439 | val_1_rmse: 0.09931 |  0:02:16s
epoch 143| loss: 0.0098  | val_0_rmse: 0.09292 | val_1_rmse: 0.09719 |  0:02:17s
epoch 144| loss: 0.0097  | val_0_rmse: 0.09813 | val_1_rmse: 0.1019  |  0:02:18s
epoch 145| loss: 0.00999 | val_0_rmse: 0.09279 | val_1_rmse: 0.09631 |  0:02:19s
epoch 146| loss: 0.00972 | val_0_rmse: 0.09468 | val_1_rmse: 0.09886 |  0:02:20s
epoch 147| loss: 0.00958 | val_0_rmse: 0.09272 | val_1_rmse: 0.0973  |  0:02:21s
epoch 148| loss: 0.00939 | val_0_rmse: 0.09246 | val_1_rmse: 0.09769 |  0:02:22s
epoch 149| loss: 0.0095  | val_0_rmse: 0.09425 | val_1_rmse: 0.09834 |  0:02:23s
Stop training because you reached max_epochs = 150 with best_epoch = 145 and best_val_1_rmse = 0.09631
Best weights from best epoch are automatically used!
ended training at: 06:12:10
Feature importance:
[('Area', 0.2623000334371337), ('Baths', 0.04299274467468096), ('Beds', 0.08237310030529958), ('Latitude', 0.24720427500668218), ('Longitude', 0.24842118241503156), ('Month', 0.029181862180636557), ('Year', 0.08752680198053545)]
Mean squared error is of 6931976493.158583
Mean absolute error:60321.86282212506
MAPE:0.16403694766613203
R2 score:0.776701159305475
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:12:10
epoch 0  | loss: 0.18133 | val_0_rmse: 0.18204 | val_1_rmse: 0.18473 |  0:00:00s
epoch 1  | loss: 0.02142 | val_0_rmse: 0.19913 | val_1_rmse: 0.20156 |  0:00:01s
epoch 2  | loss: 0.01812 | val_0_rmse: 0.20449 | val_1_rmse: 0.20609 |  0:00:02s
epoch 3  | loss: 0.01582 | val_0_rmse: 0.21267 | val_1_rmse: 0.2144  |  0:00:03s
epoch 4  | loss: 0.01421 | val_0_rmse: 0.2254  | val_1_rmse: 0.22688 |  0:00:04s
epoch 5  | loss: 0.01387 | val_0_rmse: 0.21387 | val_1_rmse: 0.21537 |  0:00:05s
epoch 6  | loss: 0.01337 | val_0_rmse: 0.23773 | val_1_rmse: 0.23864 |  0:00:06s
epoch 7  | loss: 0.0131  | val_0_rmse: 0.21553 | val_1_rmse: 0.21651 |  0:00:07s
epoch 8  | loss: 0.01277 | val_0_rmse: 0.19261 | val_1_rmse: 0.19431 |  0:00:08s
epoch 9  | loss: 0.01289 | val_0_rmse: 0.19301 | val_1_rmse: 0.19463 |  0:00:09s
epoch 10 | loss: 0.01236 | val_0_rmse: 0.19503 | val_1_rmse: 0.19697 |  0:00:10s
epoch 11 | loss: 0.0119  | val_0_rmse: 0.18201 | val_1_rmse: 0.18438 |  0:00:11s
epoch 12 | loss: 0.01193 | val_0_rmse: 0.17793 | val_1_rmse: 0.18002 |  0:00:12s
epoch 13 | loss: 0.01192 | val_0_rmse: 0.17742 | val_1_rmse: 0.17863 |  0:00:13s
epoch 14 | loss: 0.0117  | val_0_rmse: 0.16251 | val_1_rmse: 0.16406 |  0:00:14s
epoch 15 | loss: 0.01156 | val_0_rmse: 0.1578  | val_1_rmse: 0.15861 |  0:00:15s
epoch 16 | loss: 0.01146 | val_0_rmse: 0.15162 | val_1_rmse: 0.15227 |  0:00:16s
epoch 17 | loss: 0.01129 | val_0_rmse: 0.13708 | val_1_rmse: 0.13812 |  0:00:17s
epoch 18 | loss: 0.01137 | val_0_rmse: 0.14789 | val_1_rmse: 0.14848 |  0:00:18s
epoch 19 | loss: 0.01125 | val_0_rmse: 0.14671 | val_1_rmse: 0.1469  |  0:00:19s
epoch 20 | loss: 0.01118 | val_0_rmse: 0.1397  | val_1_rmse: 0.14003 |  0:00:20s
epoch 21 | loss: 0.01078 | val_0_rmse: 0.12246 | val_1_rmse: 0.12294 |  0:00:21s
epoch 22 | loss: 0.01096 | val_0_rmse: 0.12038 | val_1_rmse: 0.12068 |  0:00:22s
epoch 23 | loss: 0.01087 | val_0_rmse: 0.12826 | val_1_rmse: 0.12844 |  0:00:23s
epoch 24 | loss: 0.01073 | val_0_rmse: 0.11745 | val_1_rmse: 0.11831 |  0:00:24s
epoch 25 | loss: 0.01116 | val_0_rmse: 0.1184  | val_1_rmse: 0.11868 |  0:00:24s
epoch 26 | loss: 0.01085 | val_0_rmse: 0.1122  | val_1_rmse: 0.11298 |  0:00:25s
epoch 27 | loss: 0.0106  | val_0_rmse: 0.11666 | val_1_rmse: 0.1174  |  0:00:26s
epoch 28 | loss: 0.0106  | val_0_rmse: 0.10563 | val_1_rmse: 0.10643 |  0:00:27s
epoch 29 | loss: 0.01088 | val_0_rmse: 0.10372 | val_1_rmse: 0.1047  |  0:00:28s
epoch 30 | loss: 0.01051 | val_0_rmse: 0.1012  | val_1_rmse: 0.10222 |  0:00:29s
epoch 31 | loss: 0.01061 | val_0_rmse: 0.10689 | val_1_rmse: 0.10767 |  0:00:30s
epoch 32 | loss: 0.01063 | val_0_rmse: 0.10295 | val_1_rmse: 0.10429 |  0:00:31s
epoch 33 | loss: 0.0104  | val_0_rmse: 0.10046 | val_1_rmse: 0.10231 |  0:00:32s
epoch 34 | loss: 0.01042 | val_0_rmse: 0.09945 | val_1_rmse: 0.10168 |  0:00:33s
epoch 35 | loss: 0.01054 | val_0_rmse: 0.10346 | val_1_rmse: 0.10501 |  0:00:34s
epoch 36 | loss: 0.01036 | val_0_rmse: 0.09858 | val_1_rmse: 0.10055 |  0:00:35s
epoch 37 | loss: 0.0103  | val_0_rmse: 0.09799 | val_1_rmse: 0.10003 |  0:00:36s
epoch 38 | loss: 0.01035 | val_0_rmse: 0.10006 | val_1_rmse: 0.10184 |  0:00:37s
epoch 39 | loss: 0.01039 | val_0_rmse: 0.0988  | val_1_rmse: 0.10094 |  0:00:38s
epoch 40 | loss: 0.01045 | val_0_rmse: 0.10208 | val_1_rmse: 0.10347 |  0:00:39s
epoch 41 | loss: 0.01052 | val_0_rmse: 0.09683 | val_1_rmse: 0.09918 |  0:00:40s
epoch 42 | loss: 0.01044 | val_0_rmse: 0.09692 | val_1_rmse: 0.0996  |  0:00:41s
epoch 43 | loss: 0.01064 | val_0_rmse: 0.09574 | val_1_rmse: 0.09854 |  0:00:42s
epoch 44 | loss: 0.01024 | val_0_rmse: 0.09692 | val_1_rmse: 0.09922 |  0:00:43s
epoch 45 | loss: 0.01014 | val_0_rmse: 0.096   | val_1_rmse: 0.09794 |  0:00:44s
epoch 46 | loss: 0.01005 | val_0_rmse: 0.10019 | val_1_rmse: 0.10227 |  0:00:45s
epoch 47 | loss: 0.01023 | val_0_rmse: 0.09797 | val_1_rmse: 0.10045 |  0:00:46s
epoch 48 | loss: 0.01017 | val_0_rmse: 0.09763 | val_1_rmse: 0.09985 |  0:00:47s
epoch 49 | loss: 0.01013 | val_0_rmse: 0.09711 | val_1_rmse: 0.0997  |  0:00:47s
epoch 50 | loss: 0.00996 | val_0_rmse: 0.09704 | val_1_rmse: 0.09998 |  0:00:48s
epoch 51 | loss: 0.01046 | val_0_rmse: 0.09716 | val_1_rmse: 0.09979 |  0:00:49s
epoch 52 | loss: 0.01006 | val_0_rmse: 0.09489 | val_1_rmse: 0.09747 |  0:00:50s
epoch 53 | loss: 0.01007 | val_0_rmse: 0.10031 | val_1_rmse: 0.10342 |  0:00:51s
epoch 54 | loss: 0.01032 | val_0_rmse: 0.09768 | val_1_rmse: 0.10048 |  0:00:52s
epoch 55 | loss: 0.01036 | val_0_rmse: 0.09631 | val_1_rmse: 0.09959 |  0:00:53s
epoch 56 | loss: 0.0106  | val_0_rmse: 0.09921 | val_1_rmse: 0.10271 |  0:00:54s
epoch 57 | loss: 0.01028 | val_0_rmse: 0.09611 | val_1_rmse: 0.09949 |  0:00:55s
epoch 58 | loss: 0.01007 | val_0_rmse: 0.09689 | val_1_rmse: 0.09931 |  0:00:56s
epoch 59 | loss: 0.00982 | val_0_rmse: 0.09501 | val_1_rmse: 0.0976  |  0:00:57s
epoch 60 | loss: 0.01014 | val_0_rmse: 0.09853 | val_1_rmse: 0.10205 |  0:00:58s
epoch 61 | loss: 0.00991 | val_0_rmse: 0.09417 | val_1_rmse: 0.09712 |  0:00:59s
epoch 62 | loss: 0.00992 | val_0_rmse: 0.09397 | val_1_rmse: 0.09662 |  0:01:00s
epoch 63 | loss: 0.00961 | val_0_rmse: 0.09333 | val_1_rmse: 0.09615 |  0:01:01s
epoch 64 | loss: 0.00973 | val_0_rmse: 0.09495 | val_1_rmse: 0.09712 |  0:01:02s
epoch 65 | loss: 0.00972 | val_0_rmse: 0.09738 | val_1_rmse: 0.09996 |  0:01:03s
epoch 66 | loss: 0.00975 | val_0_rmse: 0.0932  | val_1_rmse: 0.09632 |  0:01:04s
epoch 67 | loss: 0.00995 | val_0_rmse: 0.09536 | val_1_rmse: 0.09804 |  0:01:05s
epoch 68 | loss: 0.01036 | val_0_rmse: 0.09538 | val_1_rmse: 0.09788 |  0:01:06s
epoch 69 | loss: 0.00983 | val_0_rmse: 0.09475 | val_1_rmse: 0.09817 |  0:01:07s
epoch 70 | loss: 0.00976 | val_0_rmse: 0.09564 | val_1_rmse: 0.09909 |  0:01:08s
epoch 71 | loss: 0.00986 | val_0_rmse: 0.09313 | val_1_rmse: 0.09625 |  0:01:09s
epoch 72 | loss: 0.00971 | val_0_rmse: 0.09724 | val_1_rmse: 0.09928 |  0:01:09s
epoch 73 | loss: 0.00984 | val_0_rmse: 0.0931  | val_1_rmse: 0.09579 |  0:01:10s
epoch 74 | loss: 0.00965 | val_0_rmse: 0.09333 | val_1_rmse: 0.09532 |  0:01:11s
epoch 75 | loss: 0.00962 | val_0_rmse: 0.09328 | val_1_rmse: 0.09546 |  0:01:12s
epoch 76 | loss: 0.00944 | val_0_rmse: 0.0924  | val_1_rmse: 0.09508 |  0:01:13s
epoch 77 | loss: 0.00956 | val_0_rmse: 0.09396 | val_1_rmse: 0.09687 |  0:01:14s
epoch 78 | loss: 0.00942 | val_0_rmse: 0.09508 | val_1_rmse: 0.09747 |  0:01:15s
epoch 79 | loss: 0.00964 | val_0_rmse: 0.09593 | val_1_rmse: 0.09765 |  0:01:16s
epoch 80 | loss: 0.01013 | val_0_rmse: 0.09986 | val_1_rmse: 0.10212 |  0:01:17s
epoch 81 | loss: 0.00981 | val_0_rmse: 0.09371 | val_1_rmse: 0.09657 |  0:01:18s
epoch 82 | loss: 0.01024 | val_0_rmse: 0.09505 | val_1_rmse: 0.09795 |  0:01:19s
epoch 83 | loss: 0.00998 | val_0_rmse: 0.09213 | val_1_rmse: 0.09496 |  0:01:20s
epoch 84 | loss: 0.01043 | val_0_rmse: 0.09859 | val_1_rmse: 0.10094 |  0:01:21s
epoch 85 | loss: 0.01036 | val_0_rmse: 0.09674 | val_1_rmse: 0.09903 |  0:01:22s
epoch 86 | loss: 0.01016 | val_0_rmse: 0.09243 | val_1_rmse: 0.09508 |  0:01:23s
epoch 87 | loss: 0.00952 | val_0_rmse: 0.09269 | val_1_rmse: 0.09641 |  0:01:24s
epoch 88 | loss: 0.00925 | val_0_rmse: 0.09217 | val_1_rmse: 0.09462 |  0:01:25s
epoch 89 | loss: 0.00943 | val_0_rmse: 0.09404 | val_1_rmse: 0.09687 |  0:01:26s
epoch 90 | loss: 0.00932 | val_0_rmse: 0.09284 | val_1_rmse: 0.09637 |  0:01:27s
epoch 91 | loss: 0.00946 | val_0_rmse: 0.09136 | val_1_rmse: 0.09433 |  0:01:28s
epoch 92 | loss: 0.00949 | val_0_rmse: 0.09228 | val_1_rmse: 0.09552 |  0:01:29s
epoch 93 | loss: 0.00953 | val_0_rmse: 0.09283 | val_1_rmse: 0.09635 |  0:01:30s
epoch 94 | loss: 0.00945 | val_0_rmse: 0.09741 | val_1_rmse: 0.10035 |  0:01:31s
epoch 95 | loss: 0.00978 | val_0_rmse: 0.09371 | val_1_rmse: 0.09678 |  0:01:32s
epoch 96 | loss: 0.00941 | val_0_rmse: 0.0905  | val_1_rmse: 0.09373 |  0:01:32s
epoch 97 | loss: 0.00925 | val_0_rmse: 0.09244 | val_1_rmse: 0.0955  |  0:01:33s
epoch 98 | loss: 0.0094  | val_0_rmse: 0.09223 | val_1_rmse: 0.09516 |  0:01:34s
epoch 99 | loss: 0.00919 | val_0_rmse: 0.0927  | val_1_rmse: 0.09661 |  0:01:35s
epoch 100| loss: 0.00928 | val_0_rmse: 0.09119 | val_1_rmse: 0.0947  |  0:01:36s
epoch 101| loss: 0.00898 | val_0_rmse: 0.09048 | val_1_rmse: 0.09432 |  0:01:37s
epoch 102| loss: 0.00901 | val_0_rmse: 0.09086 | val_1_rmse: 0.09438 |  0:01:38s
epoch 103| loss: 0.0092  | val_0_rmse: 0.09337 | val_1_rmse: 0.09678 |  0:01:39s
epoch 104| loss: 0.00915 | val_0_rmse: 0.09583 | val_1_rmse: 0.0991  |  0:01:40s
epoch 105| loss: 0.00942 | val_0_rmse: 0.09604 | val_1_rmse: 0.09913 |  0:01:41s
epoch 106| loss: 0.00983 | val_0_rmse: 0.09243 | val_1_rmse: 0.09557 |  0:01:42s
epoch 107| loss: 0.00924 | val_0_rmse: 0.09435 | val_1_rmse: 0.09652 |  0:01:43s
epoch 108| loss: 0.00967 | val_0_rmse: 0.09205 | val_1_rmse: 0.0948  |  0:01:44s
epoch 109| loss: 0.00944 | val_0_rmse: 0.091   | val_1_rmse: 0.09382 |  0:01:45s
epoch 110| loss: 0.00914 | val_0_rmse: 0.09414 | val_1_rmse: 0.09634 |  0:01:46s
epoch 111| loss: 0.00929 | val_0_rmse: 0.09135 | val_1_rmse: 0.09418 |  0:01:47s
epoch 112| loss: 0.00909 | val_0_rmse: 0.08995 | val_1_rmse: 0.09247 |  0:01:48s
epoch 113| loss: 0.00911 | val_0_rmse: 0.0926  | val_1_rmse: 0.09545 |  0:01:49s
epoch 114| loss: 0.00895 | val_0_rmse: 0.09059 | val_1_rmse: 0.09304 |  0:01:50s
epoch 115| loss: 0.00916 | val_0_rmse: 0.0901  | val_1_rmse: 0.09268 |  0:01:51s
epoch 116| loss: 0.00903 | val_0_rmse: 0.08955 | val_1_rmse: 0.0925  |  0:01:52s
epoch 117| loss: 0.00913 | val_0_rmse: 0.0949  | val_1_rmse: 0.098   |  0:01:53s
epoch 118| loss: 0.00915 | val_0_rmse: 0.0924  | val_1_rmse: 0.095   |  0:01:53s
epoch 119| loss: 0.00893 | val_0_rmse: 0.09168 | val_1_rmse: 0.09429 |  0:01:54s
epoch 120| loss: 0.00923 | val_0_rmse: 0.09758 | val_1_rmse: 0.10022 |  0:01:55s
epoch 121| loss: 0.00968 | val_0_rmse: 0.09124 | val_1_rmse: 0.09437 |  0:01:56s
epoch 122| loss: 0.009   | val_0_rmse: 0.09044 | val_1_rmse: 0.09346 |  0:01:57s
epoch 123| loss: 0.00905 | val_0_rmse: 0.09101 | val_1_rmse: 0.09366 |  0:01:58s
epoch 124| loss: 0.00896 | val_0_rmse: 0.09083 | val_1_rmse: 0.09338 |  0:02:00s
epoch 125| loss: 0.00912 | val_0_rmse: 0.0919  | val_1_rmse: 0.09494 |  0:02:00s
epoch 126| loss: 0.00934 | val_0_rmse: 0.09118 | val_1_rmse: 0.09394 |  0:02:01s
epoch 127| loss: 0.00916 | val_0_rmse: 0.09287 | val_1_rmse: 0.09547 |  0:02:02s
epoch 128| loss: 0.00912 | val_0_rmse: 0.0913  | val_1_rmse: 0.09418 |  0:02:03s
epoch 129| loss: 0.00908 | val_0_rmse: 0.08901 | val_1_rmse: 0.09268 |  0:02:04s
epoch 130| loss: 0.00882 | val_0_rmse: 0.08994 | val_1_rmse: 0.0931  |  0:02:05s
epoch 131| loss: 0.00874 | val_0_rmse: 0.08995 | val_1_rmse: 0.09287 |  0:02:06s
epoch 132| loss: 0.00898 | val_0_rmse: 0.09224 | val_1_rmse: 0.09564 |  0:02:07s
epoch 133| loss: 0.00894 | val_0_rmse: 0.09172 | val_1_rmse: 0.09451 |  0:02:08s
epoch 134| loss: 0.00902 | val_0_rmse: 0.09012 | val_1_rmse: 0.09314 |  0:02:09s
epoch 135| loss: 0.00928 | val_0_rmse: 0.09137 | val_1_rmse: 0.09454 |  0:02:10s
epoch 136| loss: 0.009   | val_0_rmse: 0.08995 | val_1_rmse: 0.09324 |  0:02:11s
epoch 137| loss: 0.00889 | val_0_rmse: 0.09094 | val_1_rmse: 0.09398 |  0:02:12s
epoch 138| loss: 0.00911 | val_0_rmse: 0.09514 | val_1_rmse: 0.09826 |  0:02:13s
epoch 139| loss: 0.0094  | val_0_rmse: 0.09027 | val_1_rmse: 0.09288 |  0:02:14s
epoch 140| loss: 0.00901 | val_0_rmse: 0.09003 | val_1_rmse: 0.09295 |  0:02:15s
epoch 141| loss: 0.00879 | val_0_rmse: 0.09319 | val_1_rmse: 0.09616 |  0:02:16s
epoch 142| loss: 0.00918 | val_0_rmse: 0.09065 | val_1_rmse: 0.09387 |  0:02:17s

Early stopping occured at epoch 142 with best_epoch = 112 and best_val_1_rmse = 0.09247
Best weights from best epoch are automatically used!
ended training at: 06:14:28
Feature importance:
[('Area', 0.3566615671821487), ('Baths', 0.029162328560379112), ('Beds', 0.016238552920982183), ('Latitude', 0.2558765334294713), ('Longitude', 0.23521861414762005), ('Month', 0.10683431778506039), ('Year', 8.085974338269513e-06)]
Mean squared error is of 6370487029.784574
Mean absolute error:57156.10363077189
MAPE:0.15495264157028257
R2 score:0.790847744053987
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:14:28
epoch 0  | loss: 0.19562 | val_0_rmse: 0.19984 | val_1_rmse: 0.19857 |  0:00:00s
epoch 1  | loss: 0.02231 | val_0_rmse: 0.21827 | val_1_rmse: 0.21888 |  0:00:01s
epoch 2  | loss: 0.01776 | val_0_rmse: 0.21426 | val_1_rmse: 0.21458 |  0:00:03s
epoch 3  | loss: 0.01616 | val_0_rmse: 0.21089 | val_1_rmse: 0.21123 |  0:00:04s
epoch 4  | loss: 0.01551 | val_0_rmse: 0.20522 | val_1_rmse: 0.20504 |  0:00:04s
epoch 5  | loss: 0.01441 | val_0_rmse: 0.20782 | val_1_rmse: 0.2075  |  0:00:05s
epoch 6  | loss: 0.01378 | val_0_rmse: 0.20752 | val_1_rmse: 0.20685 |  0:00:06s
epoch 7  | loss: 0.01276 | val_0_rmse: 0.21098 | val_1_rmse: 0.21036 |  0:00:07s
epoch 8  | loss: 0.01232 | val_0_rmse: 0.20508 | val_1_rmse: 0.20369 |  0:00:08s
epoch 9  | loss: 0.01177 | val_0_rmse: 0.19339 | val_1_rmse: 0.19182 |  0:00:09s
epoch 10 | loss: 0.01206 | val_0_rmse: 0.18087 | val_1_rmse: 0.17927 |  0:00:10s
epoch 11 | loss: 0.01191 | val_0_rmse: 0.19286 | val_1_rmse: 0.19165 |  0:00:11s
epoch 12 | loss: 0.01154 | val_0_rmse: 0.2111  | val_1_rmse: 0.20988 |  0:00:12s
epoch 13 | loss: 0.01211 | val_0_rmse: 0.17778 | val_1_rmse: 0.1765  |  0:00:13s
epoch 14 | loss: 0.01168 | val_0_rmse: 0.15344 | val_1_rmse: 0.1521  |  0:00:14s
epoch 15 | loss: 0.01156 | val_0_rmse: 0.14399 | val_1_rmse: 0.14237 |  0:00:15s
epoch 16 | loss: 0.01144 | val_0_rmse: 0.13973 | val_1_rmse: 0.13871 |  0:00:16s
epoch 17 | loss: 0.01118 | val_0_rmse: 0.13833 | val_1_rmse: 0.13734 |  0:00:17s
epoch 18 | loss: 0.01101 | val_0_rmse: 0.13538 | val_1_rmse: 0.13477 |  0:00:18s
epoch 19 | loss: 0.01083 | val_0_rmse: 0.1424  | val_1_rmse: 0.14236 |  0:00:19s
epoch 20 | loss: 0.01106 | val_0_rmse: 0.13522 | val_1_rmse: 0.13506 |  0:00:20s
epoch 21 | loss: 0.01072 | val_0_rmse: 0.12776 | val_1_rmse: 0.12779 |  0:00:21s
epoch 22 | loss: 0.01034 | val_0_rmse: 0.13068 | val_1_rmse: 0.13123 |  0:00:22s
epoch 23 | loss: 0.01016 | val_0_rmse: 0.12766 | val_1_rmse: 0.1282  |  0:00:23s
epoch 24 | loss: 0.01004 | val_0_rmse: 0.11189 | val_1_rmse: 0.11226 |  0:00:24s
epoch 25 | loss: 0.0101  | val_0_rmse: 0.11451 | val_1_rmse: 0.1151  |  0:00:25s
epoch 26 | loss: 0.01007 | val_0_rmse: 0.11031 | val_1_rmse: 0.11073 |  0:00:26s
epoch 27 | loss: 0.01008 | val_0_rmse: 0.11995 | val_1_rmse: 0.1215  |  0:00:27s
epoch 28 | loss: 0.00993 | val_0_rmse: 0.1048  | val_1_rmse: 0.10528 |  0:00:28s
epoch 29 | loss: 0.00996 | val_0_rmse: 0.1111  | val_1_rmse: 0.11147 |  0:00:29s
epoch 30 | loss: 0.01015 | val_0_rmse: 0.10551 | val_1_rmse: 0.10629 |  0:00:30s
epoch 31 | loss: 0.01011 | val_0_rmse: 0.10789 | val_1_rmse: 0.1086  |  0:00:31s
epoch 32 | loss: 0.00974 | val_0_rmse: 0.1023  | val_1_rmse: 0.10265 |  0:00:32s
epoch 33 | loss: 0.00989 | val_0_rmse: 0.1003  | val_1_rmse: 0.10134 |  0:00:33s
epoch 34 | loss: 0.00986 | val_0_rmse: 0.10412 | val_1_rmse: 0.1048  |  0:00:34s
epoch 35 | loss: 0.00956 | val_0_rmse: 0.09628 | val_1_rmse: 0.09683 |  0:00:35s
epoch 36 | loss: 0.00964 | val_0_rmse: 0.09741 | val_1_rmse: 0.09843 |  0:00:36s
epoch 37 | loss: 0.01032 | val_0_rmse: 0.10135 | val_1_rmse: 0.10223 |  0:00:37s
epoch 38 | loss: 0.01033 | val_0_rmse: 0.09651 | val_1_rmse: 0.097   |  0:00:38s
epoch 39 | loss: 0.01023 | val_0_rmse: 0.0969  | val_1_rmse: 0.09657 |  0:00:39s
epoch 40 | loss: 0.01005 | val_0_rmse: 0.09677 | val_1_rmse: 0.09733 |  0:00:40s
epoch 41 | loss: 0.00975 | val_0_rmse: 0.09918 | val_1_rmse: 0.099   |  0:00:41s
epoch 42 | loss: 0.01005 | val_0_rmse: 0.09419 | val_1_rmse: 0.09478 |  0:00:42s
epoch 43 | loss: 0.00959 | val_0_rmse: 0.09838 | val_1_rmse: 0.09838 |  0:00:43s
epoch 44 | loss: 0.01001 | val_0_rmse: 0.0947  | val_1_rmse: 0.09534 |  0:00:44s
epoch 45 | loss: 0.00998 | val_0_rmse: 0.09713 | val_1_rmse: 0.09787 |  0:00:45s
epoch 46 | loss: 0.00983 | val_0_rmse: 0.09467 | val_1_rmse: 0.09576 |  0:00:46s
epoch 47 | loss: 0.01006 | val_0_rmse: 0.09535 | val_1_rmse: 0.09648 |  0:00:47s
epoch 48 | loss: 0.00973 | val_0_rmse: 0.09415 | val_1_rmse: 0.0952  |  0:00:48s
epoch 49 | loss: 0.00941 | val_0_rmse: 0.09545 | val_1_rmse: 0.09569 |  0:00:49s
epoch 50 | loss: 0.00971 | val_0_rmse: 0.0948  | val_1_rmse: 0.09527 |  0:00:50s
epoch 51 | loss: 0.01018 | val_0_rmse: 0.09846 | val_1_rmse: 0.09793 |  0:00:51s
epoch 52 | loss: 0.01018 | val_0_rmse: 0.09973 | val_1_rmse: 0.09907 |  0:00:52s
epoch 53 | loss: 0.00991 | val_0_rmse: 0.09479 | val_1_rmse: 0.0957  |  0:00:53s
epoch 54 | loss: 0.00966 | val_0_rmse: 0.09415 | val_1_rmse: 0.09521 |  0:00:54s
epoch 55 | loss: 0.00973 | val_0_rmse: 0.09348 | val_1_rmse: 0.09384 |  0:00:54s
epoch 56 | loss: 0.00956 | val_0_rmse: 0.09267 | val_1_rmse: 0.09273 |  0:00:55s
epoch 57 | loss: 0.00935 | val_0_rmse: 0.09503 | val_1_rmse: 0.096   |  0:00:56s
epoch 58 | loss: 0.0096  | val_0_rmse: 0.09178 | val_1_rmse: 0.09305 |  0:00:57s
epoch 59 | loss: 0.00953 | val_0_rmse: 0.09358 | val_1_rmse: 0.09391 |  0:00:58s
epoch 60 | loss: 0.00963 | val_0_rmse: 0.09293 | val_1_rmse: 0.09295 |  0:00:59s
epoch 61 | loss: 0.00931 | val_0_rmse: 0.09219 | val_1_rmse: 0.09235 |  0:01:00s
epoch 62 | loss: 0.00947 | val_0_rmse: 0.09152 | val_1_rmse: 0.09204 |  0:01:01s
epoch 63 | loss: 0.00956 | val_0_rmse: 0.09273 | val_1_rmse: 0.09389 |  0:01:02s
epoch 64 | loss: 0.00947 | val_0_rmse: 0.09813 | val_1_rmse: 0.1006  |  0:01:03s
epoch 65 | loss: 0.00976 | val_0_rmse: 0.09003 | val_1_rmse: 0.09122 |  0:01:04s
epoch 66 | loss: 0.0094  | val_0_rmse: 0.09595 | val_1_rmse: 0.09601 |  0:01:05s
epoch 67 | loss: 0.00981 | val_0_rmse: 0.10222 | val_1_rmse: 0.10192 |  0:01:06s
epoch 68 | loss: 0.01073 | val_0_rmse: 0.10728 | val_1_rmse: 0.10719 |  0:01:07s
epoch 69 | loss: 0.01023 | val_0_rmse: 0.09932 | val_1_rmse: 0.09911 |  0:01:08s
epoch 70 | loss: 0.01    | val_0_rmse: 0.09779 | val_1_rmse: 0.09918 |  0:01:09s
epoch 71 | loss: 0.01028 | val_0_rmse: 0.09421 | val_1_rmse: 0.09448 |  0:01:10s
epoch 72 | loss: 0.00993 | val_0_rmse: 0.09625 | val_1_rmse: 0.09666 |  0:01:11s
epoch 73 | loss: 0.01055 | val_0_rmse: 0.0964  | val_1_rmse: 0.09668 |  0:01:12s
epoch 74 | loss: 0.01015 | val_0_rmse: 0.0959  | val_1_rmse: 0.0961  |  0:01:13s
epoch 75 | loss: 0.00996 | val_0_rmse: 0.09448 | val_1_rmse: 0.09458 |  0:01:14s
epoch 76 | loss: 0.00965 | val_0_rmse: 0.09447 | val_1_rmse: 0.09441 |  0:01:15s
epoch 77 | loss: 0.00973 | val_0_rmse: 0.09509 | val_1_rmse: 0.09564 |  0:01:15s
epoch 78 | loss: 0.00963 | val_0_rmse: 0.09275 | val_1_rmse: 0.09299 |  0:01:16s
epoch 79 | loss: 0.0095  | val_0_rmse: 0.09547 | val_1_rmse: 0.09545 |  0:01:17s
epoch 80 | loss: 0.00959 | val_0_rmse: 0.094   | val_1_rmse: 0.09403 |  0:01:18s
epoch 81 | loss: 0.00963 | val_0_rmse: 0.09375 | val_1_rmse: 0.09354 |  0:01:19s
epoch 82 | loss: 0.00967 | val_0_rmse: 0.09291 | val_1_rmse: 0.09308 |  0:01:20s
epoch 83 | loss: 0.00948 | val_0_rmse: 0.09358 | val_1_rmse: 0.09373 |  0:01:21s
epoch 84 | loss: 0.00963 | val_0_rmse: 0.0958  | val_1_rmse: 0.09674 |  0:01:22s
epoch 85 | loss: 0.00967 | val_0_rmse: 0.09438 | val_1_rmse: 0.09425 |  0:01:23s
epoch 86 | loss: 0.00962 | val_0_rmse: 0.09206 | val_1_rmse: 0.09214 |  0:01:24s
epoch 87 | loss: 0.00957 | val_0_rmse: 0.0938  | val_1_rmse: 0.09352 |  0:01:25s
epoch 88 | loss: 0.00951 | val_0_rmse: 0.09469 | val_1_rmse: 0.09559 |  0:01:26s
epoch 89 | loss: 0.00959 | val_0_rmse: 0.09323 | val_1_rmse: 0.09366 |  0:01:27s
epoch 90 | loss: 0.00989 | val_0_rmse: 0.10105 | val_1_rmse: 0.10079 |  0:01:28s
epoch 91 | loss: 0.01012 | val_0_rmse: 0.0974  | val_1_rmse: 0.09785 |  0:01:29s
epoch 92 | loss: 0.01081 | val_0_rmse: 0.09411 | val_1_rmse: 0.09538 |  0:01:30s
epoch 93 | loss: 0.00957 | val_0_rmse: 0.09214 | val_1_rmse: 0.0933  |  0:01:31s
epoch 94 | loss: 0.00952 | val_0_rmse: 0.09447 | val_1_rmse: 0.09424 |  0:01:32s
epoch 95 | loss: 0.00983 | val_0_rmse: 0.09323 | val_1_rmse: 0.09448 |  0:01:33s

Early stopping occured at epoch 95 with best_epoch = 65 and best_val_1_rmse = 0.09122
Best weights from best epoch are automatically used!
ended training at: 06:16:02
Feature importance:
[('Area', 0.3734086500746098), ('Baths', 0.00039788849275826767), ('Beds', 0.057824367347614605), ('Latitude', 0.23016605628559914), ('Longitude', 0.20038111843117695), ('Month', 0.014394750137754068), ('Year', 0.12342716923048717)]
Mean squared error is of 6565953717.997541
Mean absolute error:59009.539216674006
MAPE:0.1576717500204557
R2 score:0.7848218794586328
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:16:02
epoch 0  | loss: 0.18735 | val_0_rmse: 0.19885 | val_1_rmse: 0.19924 |  0:00:00s
epoch 1  | loss: 0.02014 | val_0_rmse: 0.23436 | val_1_rmse: 0.23485 |  0:00:01s
epoch 2  | loss: 0.01711 | val_0_rmse: 0.24895 | val_1_rmse: 0.24951 |  0:00:02s
epoch 3  | loss: 0.01551 | val_0_rmse: 0.22987 | val_1_rmse: 0.2303  |  0:00:03s
epoch 4  | loss: 0.01527 | val_0_rmse: 0.21707 | val_1_rmse: 0.21822 |  0:00:04s
epoch 5  | loss: 0.01434 | val_0_rmse: 0.21453 | val_1_rmse: 0.21551 |  0:00:05s
epoch 6  | loss: 0.01384 | val_0_rmse: 0.21549 | val_1_rmse: 0.21755 |  0:00:06s
epoch 7  | loss: 0.01325 | val_0_rmse: 0.22315 | val_1_rmse: 0.22563 |  0:00:07s
epoch 8  | loss: 0.01267 | val_0_rmse: 0.20608 | val_1_rmse: 0.20885 |  0:00:08s
epoch 9  | loss: 0.0125  | val_0_rmse: 0.21476 | val_1_rmse: 0.21773 |  0:00:09s
epoch 10 | loss: 0.01223 | val_0_rmse: 0.2061  | val_1_rmse: 0.20904 |  0:00:10s
epoch 11 | loss: 0.01189 | val_0_rmse: 0.19336 | val_1_rmse: 0.19579 |  0:00:11s
epoch 12 | loss: 0.0123  | val_0_rmse: 0.18851 | val_1_rmse: 0.19143 |  0:00:12s
epoch 13 | loss: 0.01205 | val_0_rmse: 0.16843 | val_1_rmse: 0.1709  |  0:00:13s
epoch 14 | loss: 0.01158 | val_0_rmse: 0.16429 | val_1_rmse: 0.16703 |  0:00:14s
epoch 15 | loss: 0.01155 | val_0_rmse: 0.15732 | val_1_rmse: 0.15991 |  0:00:15s
epoch 16 | loss: 0.0115  | val_0_rmse: 0.14194 | val_1_rmse: 0.14389 |  0:00:16s
epoch 17 | loss: 0.01159 | val_0_rmse: 0.13363 | val_1_rmse: 0.13573 |  0:00:17s
epoch 18 | loss: 0.01155 | val_0_rmse: 0.14581 | val_1_rmse: 0.1479  |  0:00:18s
epoch 19 | loss: 0.0113  | val_0_rmse: 0.1308  | val_1_rmse: 0.13321 |  0:00:19s
epoch 20 | loss: 0.01137 | val_0_rmse: 0.12387 | val_1_rmse: 0.12551 |  0:00:20s
epoch 21 | loss: 0.01152 | val_0_rmse: 0.1261  | val_1_rmse: 0.12762 |  0:00:21s
epoch 22 | loss: 0.01133 | val_0_rmse: 0.11775 | val_1_rmse: 0.11966 |  0:00:22s
epoch 23 | loss: 0.01127 | val_0_rmse: 0.11317 | val_1_rmse: 0.11507 |  0:00:22s
epoch 24 | loss: 0.01184 | val_0_rmse: 0.11978 | val_1_rmse: 0.12221 |  0:00:24s
epoch 25 | loss: 0.01154 | val_0_rmse: 0.11495 | val_1_rmse: 0.11696 |  0:00:24s
epoch 26 | loss: 0.01162 | val_0_rmse: 0.11292 | val_1_rmse: 0.11507 |  0:00:25s
epoch 27 | loss: 0.01188 | val_0_rmse: 0.10675 | val_1_rmse: 0.10861 |  0:00:26s
epoch 28 | loss: 0.01144 | val_0_rmse: 0.11323 | val_1_rmse: 0.1143  |  0:00:27s
epoch 29 | loss: 0.01145 | val_0_rmse: 0.10741 | val_1_rmse: 0.10994 |  0:00:28s
epoch 30 | loss: 0.01095 | val_0_rmse: 0.10505 | val_1_rmse: 0.1071  |  0:00:29s
epoch 31 | loss: 0.01088 | val_0_rmse: 0.10597 | val_1_rmse: 0.10822 |  0:00:30s
epoch 32 | loss: 0.01094 | val_0_rmse: 0.10142 | val_1_rmse: 0.10356 |  0:00:31s
epoch 33 | loss: 0.01108 | val_0_rmse: 0.10314 | val_1_rmse: 0.10522 |  0:00:32s
epoch 34 | loss: 0.01089 | val_0_rmse: 0.1062  | val_1_rmse: 0.10804 |  0:00:33s
epoch 35 | loss: 0.0114  | val_0_rmse: 0.10447 | val_1_rmse: 0.10703 |  0:00:34s
epoch 36 | loss: 0.01164 | val_0_rmse: 0.10839 | val_1_rmse: 0.11077 |  0:00:35s
epoch 37 | loss: 0.01134 | val_0_rmse: 0.10516 | val_1_rmse: 0.10743 |  0:00:36s
epoch 38 | loss: 0.01083 | val_0_rmse: 0.10099 | val_1_rmse: 0.10365 |  0:00:37s
epoch 39 | loss: 0.01096 | val_0_rmse: 0.09875 | val_1_rmse: 0.10139 |  0:00:38s
epoch 40 | loss: 0.01068 | val_0_rmse: 0.09954 | val_1_rmse: 0.10218 |  0:00:39s
epoch 41 | loss: 0.01092 | val_0_rmse: 0.10785 | val_1_rmse: 0.11043 |  0:00:40s
epoch 42 | loss: 0.01118 | val_0_rmse: 0.10029 | val_1_rmse: 0.10309 |  0:00:41s
epoch 43 | loss: 0.01083 | val_0_rmse: 0.10341 | val_1_rmse: 0.10625 |  0:00:42s
epoch 44 | loss: 0.01075 | val_0_rmse: 0.09856 | val_1_rmse: 0.10177 |  0:00:43s
epoch 45 | loss: 0.01077 | val_0_rmse: 0.09879 | val_1_rmse: 0.10238 |  0:00:44s
epoch 46 | loss: 0.0108  | val_0_rmse: 0.09911 | val_1_rmse: 0.10231 |  0:00:45s
epoch 47 | loss: 0.01053 | val_0_rmse: 0.10091 | val_1_rmse: 0.10448 |  0:00:46s
epoch 48 | loss: 0.0108  | val_0_rmse: 0.09779 | val_1_rmse: 0.10133 |  0:00:46s
epoch 49 | loss: 0.01089 | val_0_rmse: 0.10226 | val_1_rmse: 0.10569 |  0:00:47s
epoch 50 | loss: 0.0108  | val_0_rmse: 0.10396 | val_1_rmse: 0.10823 |  0:00:48s
epoch 51 | loss: 0.01076 | val_0_rmse: 0.10203 | val_1_rmse: 0.10614 |  0:00:49s
epoch 52 | loss: 0.01066 | val_0_rmse: 0.09776 | val_1_rmse: 0.10148 |  0:00:50s
epoch 53 | loss: 0.01039 | val_0_rmse: 0.09745 | val_1_rmse: 0.10158 |  0:00:51s
epoch 54 | loss: 0.01036 | val_0_rmse: 0.09789 | val_1_rmse: 0.10242 |  0:00:52s
epoch 55 | loss: 0.01055 | val_0_rmse: 0.09692 | val_1_rmse: 0.1007  |  0:00:53s
epoch 56 | loss: 0.0104  | val_0_rmse: 0.09765 | val_1_rmse: 0.10096 |  0:00:54s
epoch 57 | loss: 0.01042 | val_0_rmse: 0.09617 | val_1_rmse: 0.1004  |  0:00:55s
epoch 58 | loss: 0.0106  | val_0_rmse: 0.09989 | val_1_rmse: 0.10387 |  0:00:56s
epoch 59 | loss: 0.0107  | val_0_rmse: 0.09607 | val_1_rmse: 0.10007 |  0:00:57s
epoch 60 | loss: 0.01021 | val_0_rmse: 0.09572 | val_1_rmse: 0.09945 |  0:00:58s
epoch 61 | loss: 0.01034 | val_0_rmse: 0.09808 | val_1_rmse: 0.10167 |  0:00:59s
epoch 62 | loss: 0.0105  | val_0_rmse: 0.0999  | val_1_rmse: 0.10416 |  0:01:00s
epoch 63 | loss: 0.01045 | val_0_rmse: 0.09581 | val_1_rmse: 0.10009 |  0:01:01s
epoch 64 | loss: 0.01027 | val_0_rmse: 0.09758 | val_1_rmse: 0.10107 |  0:01:02s
epoch 65 | loss: 0.01056 | val_0_rmse: 0.09787 | val_1_rmse: 0.10168 |  0:01:03s
epoch 66 | loss: 0.01022 | val_0_rmse: 0.09589 | val_1_rmse: 0.1004  |  0:01:04s
epoch 67 | loss: 0.01001 | val_0_rmse: 0.09548 | val_1_rmse: 0.09921 |  0:01:05s
epoch 68 | loss: 0.01039 | val_0_rmse: 0.09478 | val_1_rmse: 0.09908 |  0:01:06s
epoch 69 | loss: 0.01008 | val_0_rmse: 0.0975  | val_1_rmse: 0.10161 |  0:01:07s
epoch 70 | loss: 0.01007 | val_0_rmse: 0.0945  | val_1_rmse: 0.09843 |  0:01:08s
epoch 71 | loss: 0.00981 | val_0_rmse: 0.09383 | val_1_rmse: 0.09799 |  0:01:08s
epoch 72 | loss: 0.01007 | val_0_rmse: 0.0953  | val_1_rmse: 0.09889 |  0:01:09s
epoch 73 | loss: 0.00999 | val_0_rmse: 0.09392 | val_1_rmse: 0.09779 |  0:01:10s
epoch 74 | loss: 0.00997 | val_0_rmse: 0.09465 | val_1_rmse: 0.09834 |  0:01:11s
epoch 75 | loss: 0.00993 | val_0_rmse: 0.10511 | val_1_rmse: 0.10956 |  0:01:12s
epoch 76 | loss: 0.00993 | val_0_rmse: 0.09235 | val_1_rmse: 0.09639 |  0:01:13s
epoch 77 | loss: 0.00975 | val_0_rmse: 0.09409 | val_1_rmse: 0.09718 |  0:01:14s
epoch 78 | loss: 0.00982 | val_0_rmse: 0.09416 | val_1_rmse: 0.09827 |  0:01:15s
epoch 79 | loss: 0.00978 | val_0_rmse: 0.09553 | val_1_rmse: 0.10007 |  0:01:16s
epoch 80 | loss: 0.00976 | val_0_rmse: 0.09545 | val_1_rmse: 0.09874 |  0:01:17s
epoch 81 | loss: 0.00966 | val_0_rmse: 0.0922  | val_1_rmse: 0.09567 |  0:01:18s
epoch 82 | loss: 0.00963 | val_0_rmse: 0.09232 | val_1_rmse: 0.09564 |  0:01:19s
epoch 83 | loss: 0.00976 | val_0_rmse: 0.09205 | val_1_rmse: 0.09591 |  0:01:20s
epoch 84 | loss: 0.0097  | val_0_rmse: 0.09229 | val_1_rmse: 0.09503 |  0:01:21s
epoch 85 | loss: 0.00969 | val_0_rmse: 0.09348 | val_1_rmse: 0.09673 |  0:01:22s
epoch 86 | loss: 0.00939 | val_0_rmse: 0.09119 | val_1_rmse: 0.09463 |  0:01:23s
epoch 87 | loss: 0.00942 | val_0_rmse: 0.09306 | val_1_rmse: 0.09589 |  0:01:24s
epoch 88 | loss: 0.00938 | val_0_rmse: 0.09088 | val_1_rmse: 0.0949  |  0:01:25s
epoch 89 | loss: 0.00924 | val_0_rmse: 0.09204 | val_1_rmse: 0.09569 |  0:01:26s
epoch 90 | loss: 0.00923 | val_0_rmse: 0.09229 | val_1_rmse: 0.0957  |  0:01:27s
epoch 91 | loss: 0.00952 | val_0_rmse: 0.09066 | val_1_rmse: 0.09404 |  0:01:28s
epoch 92 | loss: 0.00937 | val_0_rmse: 0.09316 | val_1_rmse: 0.09631 |  0:01:29s
epoch 93 | loss: 0.00953 | val_0_rmse: 0.09189 | val_1_rmse: 0.09462 |  0:01:30s
epoch 94 | loss: 0.00941 | val_0_rmse: 0.09084 | val_1_rmse: 0.09438 |  0:01:31s
epoch 95 | loss: 0.00947 | val_0_rmse: 0.09466 | val_1_rmse: 0.09741 |  0:01:32s
epoch 96 | loss: 0.00957 | val_0_rmse: 0.08989 | val_1_rmse: 0.09353 |  0:01:33s
epoch 97 | loss: 0.00937 | val_0_rmse: 0.09118 | val_1_rmse: 0.09475 |  0:01:34s
epoch 98 | loss: 0.00945 | val_0_rmse: 0.09139 | val_1_rmse: 0.09464 |  0:01:34s
epoch 99 | loss: 0.0092  | val_0_rmse: 0.08976 | val_1_rmse: 0.09357 |  0:01:35s
epoch 100| loss: 0.00909 | val_0_rmse: 0.09061 | val_1_rmse: 0.09396 |  0:01:36s
epoch 101| loss: 0.00908 | val_0_rmse: 0.08961 | val_1_rmse: 0.09399 |  0:01:37s
epoch 102| loss: 0.0093  | val_0_rmse: 0.09003 | val_1_rmse: 0.09391 |  0:01:38s
epoch 103| loss: 0.00924 | val_0_rmse: 0.08842 | val_1_rmse: 0.09255 |  0:01:39s
epoch 104| loss: 0.0092  | val_0_rmse: 0.08916 | val_1_rmse: 0.09362 |  0:01:40s
epoch 105| loss: 0.00915 | val_0_rmse: 0.08929 | val_1_rmse: 0.09306 |  0:01:41s
epoch 106| loss: 0.00923 | val_0_rmse: 0.09126 | val_1_rmse: 0.09471 |  0:01:42s
epoch 107| loss: 0.00942 | val_0_rmse: 0.09149 | val_1_rmse: 0.09454 |  0:01:43s
epoch 108| loss: 0.00922 | val_0_rmse: 0.08987 | val_1_rmse: 0.09283 |  0:01:44s
epoch 109| loss: 0.00913 | val_0_rmse: 0.08989 | val_1_rmse: 0.09303 |  0:01:45s
epoch 110| loss: 0.00921 | val_0_rmse: 0.08995 | val_1_rmse: 0.09209 |  0:01:46s
epoch 111| loss: 0.00903 | val_0_rmse: 0.09237 | val_1_rmse: 0.09538 |  0:01:47s
epoch 112| loss: 0.00923 | val_0_rmse: 0.09158 | val_1_rmse: 0.09535 |  0:01:48s
epoch 113| loss: 0.0095  | val_0_rmse: 0.09548 | val_1_rmse: 0.10018 |  0:01:49s
epoch 114| loss: 0.00938 | val_0_rmse: 0.09283 | val_1_rmse: 0.09569 |  0:01:50s
epoch 115| loss: 0.0091  | val_0_rmse: 0.0904  | val_1_rmse: 0.0933  |  0:01:51s
epoch 116| loss: 0.0091  | val_0_rmse: 0.09243 | val_1_rmse: 0.09613 |  0:01:52s
epoch 117| loss: 0.00918 | val_0_rmse: 0.08891 | val_1_rmse: 0.09263 |  0:01:53s
epoch 118| loss: 0.0089  | val_0_rmse: 0.08816 | val_1_rmse: 0.09104 |  0:01:54s
epoch 119| loss: 0.00903 | val_0_rmse: 0.08955 | val_1_rmse: 0.09242 |  0:01:55s
epoch 120| loss: 0.00877 | val_0_rmse: 0.08874 | val_1_rmse: 0.09149 |  0:01:56s
epoch 121| loss: 0.00884 | val_0_rmse: 0.09122 | val_1_rmse: 0.0944  |  0:01:56s
epoch 122| loss: 0.009   | val_0_rmse: 0.09001 | val_1_rmse: 0.09307 |  0:01:57s
epoch 123| loss: 0.00895 | val_0_rmse: 0.08834 | val_1_rmse: 0.09176 |  0:01:58s
epoch 124| loss: 0.00886 | val_0_rmse: 0.08761 | val_1_rmse: 0.09105 |  0:01:59s
epoch 125| loss: 0.00896 | val_0_rmse: 0.08788 | val_1_rmse: 0.09096 |  0:02:00s
epoch 126| loss: 0.00879 | val_0_rmse: 0.08764 | val_1_rmse: 0.09083 |  0:02:01s
epoch 127| loss: 0.00867 | val_0_rmse: 0.08913 | val_1_rmse: 0.09196 |  0:02:02s
epoch 128| loss: 0.00912 | val_0_rmse: 0.08829 | val_1_rmse: 0.09213 |  0:02:03s
epoch 129| loss: 0.00909 | val_0_rmse: 0.09239 | val_1_rmse: 0.09512 |  0:02:04s
epoch 130| loss: 0.00891 | val_0_rmse: 0.09044 | val_1_rmse: 0.09418 |  0:02:05s
epoch 131| loss: 0.00873 | val_0_rmse: 0.08859 | val_1_rmse: 0.09108 |  0:02:06s
epoch 132| loss: 0.00876 | val_0_rmse: 0.09    | val_1_rmse: 0.09428 |  0:02:07s
epoch 133| loss: 0.00895 | val_0_rmse: 0.08887 | val_1_rmse: 0.09228 |  0:02:08s
epoch 134| loss: 0.00895 | val_0_rmse: 0.08861 | val_1_rmse: 0.09199 |  0:02:09s
epoch 135| loss: 0.00903 | val_0_rmse: 0.09275 | val_1_rmse: 0.09566 |  0:02:10s
epoch 136| loss: 0.009   | val_0_rmse: 0.088   | val_1_rmse: 0.09242 |  0:02:11s
epoch 137| loss: 0.00886 | val_0_rmse: 0.08821 | val_1_rmse: 0.09084 |  0:02:12s
epoch 138| loss: 0.00861 | val_0_rmse: 0.08774 | val_1_rmse: 0.0914  |  0:02:13s
epoch 139| loss: 0.00883 | val_0_rmse: 0.09035 | val_1_rmse: 0.09446 |  0:02:14s
epoch 140| loss: 0.00864 | val_0_rmse: 0.09025 | val_1_rmse: 0.09471 |  0:02:15s
epoch 141| loss: 0.00868 | val_0_rmse: 0.08788 | val_1_rmse: 0.091   |  0:02:16s
epoch 142| loss: 0.009   | val_0_rmse: 0.08878 | val_1_rmse: 0.09232 |  0:02:17s
epoch 143| loss: 0.00884 | val_0_rmse: 0.08908 | val_1_rmse: 0.09286 |  0:02:17s
epoch 144| loss: 0.00903 | val_0_rmse: 0.09025 | val_1_rmse: 0.09234 |  0:02:18s
epoch 145| loss: 0.00941 | val_0_rmse: 0.102   | val_1_rmse: 0.10402 |  0:02:19s
epoch 146| loss: 0.00903 | val_0_rmse: 0.08784 | val_1_rmse: 0.08989 |  0:02:20s
epoch 147| loss: 0.00871 | val_0_rmse: 0.08954 | val_1_rmse: 0.0928  |  0:02:21s
epoch 148| loss: 0.00866 | val_0_rmse: 0.08852 | val_1_rmse: 0.09146 |  0:02:22s
epoch 149| loss: 0.00879 | val_0_rmse: 0.08955 | val_1_rmse: 0.09207 |  0:02:23s
Stop training because you reached max_epochs = 150 with best_epoch = 146 and best_val_1_rmse = 0.08989
Best weights from best epoch are automatically used!
ended training at: 06:18:26
Feature importance:
[('Area', 0.2957308696347038), ('Baths', 0.08570800792534118), ('Beds', 0.16421281812105204), ('Latitude', 0.21681113439457142), ('Longitude', 0.10790063282996264), ('Month', 0.02377083439669958), ('Year', 0.10586570269766937)]
Mean squared error is of 6537941747.294067
Mean absolute error:58157.897173502766
MAPE:0.15424405588383197
R2 score:0.780910179460513
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:18:26
epoch 0  | loss: 0.18067 | val_0_rmse: 0.18258 | val_1_rmse: 0.17808 |  0:00:00s
epoch 1  | loss: 0.02024 | val_0_rmse: 0.21985 | val_1_rmse: 0.21477 |  0:00:01s
epoch 2  | loss: 0.01787 | val_0_rmse: 0.22052 | val_1_rmse: 0.21516 |  0:00:02s
epoch 3  | loss: 0.01615 | val_0_rmse: 0.21024 | val_1_rmse: 0.2053  |  0:00:03s
epoch 4  | loss: 0.0148  | val_0_rmse: 0.21172 | val_1_rmse: 0.20685 |  0:00:04s
epoch 5  | loss: 0.01379 | val_0_rmse: 0.20664 | val_1_rmse: 0.20194 |  0:00:05s
epoch 6  | loss: 0.01317 | val_0_rmse: 0.20905 | val_1_rmse: 0.2048  |  0:00:06s
epoch 7  | loss: 0.01257 | val_0_rmse: 0.21368 | val_1_rmse: 0.20973 |  0:00:07s
epoch 8  | loss: 0.01235 | val_0_rmse: 0.21703 | val_1_rmse: 0.21394 |  0:00:08s
epoch 9  | loss: 0.01231 | val_0_rmse: 0.19184 | val_1_rmse: 0.18887 |  0:00:09s
epoch 10 | loss: 0.01234 | val_0_rmse: 0.17414 | val_1_rmse: 0.17143 |  0:00:10s
epoch 11 | loss: 0.01227 | val_0_rmse: 0.16493 | val_1_rmse: 0.16218 |  0:00:11s
epoch 12 | loss: 0.01177 | val_0_rmse: 0.16662 | val_1_rmse: 0.16402 |  0:00:12s
epoch 13 | loss: 0.01171 | val_0_rmse: 0.169   | val_1_rmse: 0.16646 |  0:00:13s
epoch 14 | loss: 0.01179 | val_0_rmse: 0.15676 | val_1_rmse: 0.15379 |  0:00:14s
epoch 15 | loss: 0.01122 | val_0_rmse: 0.15044 | val_1_rmse: 0.14784 |  0:00:15s
epoch 16 | loss: 0.01084 | val_0_rmse: 0.14941 | val_1_rmse: 0.14692 |  0:00:16s
epoch 17 | loss: 0.01114 | val_0_rmse: 0.13721 | val_1_rmse: 0.1348  |  0:00:17s
epoch 18 | loss: 0.01129 | val_0_rmse: 0.14844 | val_1_rmse: 0.14658 |  0:00:18s
epoch 19 | loss: 0.01083 | val_0_rmse: 0.13591 | val_1_rmse: 0.13399 |  0:00:19s
epoch 20 | loss: 0.01034 | val_0_rmse: 0.13107 | val_1_rmse: 0.12952 |  0:00:20s
epoch 21 | loss: 0.01053 | val_0_rmse: 0.14697 | val_1_rmse: 0.14611 |  0:00:21s
epoch 22 | loss: 0.01064 | val_0_rmse: 0.12725 | val_1_rmse: 0.1262  |  0:00:22s
epoch 23 | loss: 0.0101  | val_0_rmse: 0.12072 | val_1_rmse: 0.11941 |  0:00:22s
epoch 24 | loss: 0.0104  | val_0_rmse: 0.1102  | val_1_rmse: 0.10891 |  0:00:23s
epoch 25 | loss: 0.01063 | val_0_rmse: 0.11745 | val_1_rmse: 0.11642 |  0:00:24s
epoch 26 | loss: 0.01041 | val_0_rmse: 0.12422 | val_1_rmse: 0.12345 |  0:00:25s
epoch 27 | loss: 0.01039 | val_0_rmse: 0.10956 | val_1_rmse: 0.10855 |  0:00:26s
epoch 28 | loss: 0.01018 | val_0_rmse: 0.109   | val_1_rmse: 0.10858 |  0:00:27s
epoch 29 | loss: 0.0101  | val_0_rmse: 0.10479 | val_1_rmse: 0.10484 |  0:00:28s
epoch 30 | loss: 0.00975 | val_0_rmse: 0.10169 | val_1_rmse: 0.10147 |  0:00:29s
epoch 31 | loss: 0.01015 | val_0_rmse: 0.10266 | val_1_rmse: 0.10291 |  0:00:30s
epoch 32 | loss: 0.00982 | val_0_rmse: 0.09996 | val_1_rmse: 0.10055 |  0:00:31s
epoch 33 | loss: 0.01023 | val_0_rmse: 0.09753 | val_1_rmse: 0.09811 |  0:00:32s
epoch 34 | loss: 0.00977 | val_0_rmse: 0.09905 | val_1_rmse: 0.09918 |  0:00:33s
epoch 35 | loss: 0.00966 | val_0_rmse: 0.09891 | val_1_rmse: 0.09935 |  0:00:34s
epoch 36 | loss: 0.00994 | val_0_rmse: 0.09678 | val_1_rmse: 0.09771 |  0:00:35s
epoch 37 | loss: 0.01007 | val_0_rmse: 0.09795 | val_1_rmse: 0.09884 |  0:00:36s
epoch 38 | loss: 0.01016 | val_0_rmse: 0.09663 | val_1_rmse: 0.09716 |  0:00:37s
epoch 39 | loss: 0.0098  | val_0_rmse: 0.0982  | val_1_rmse: 0.09962 |  0:00:38s
epoch 40 | loss: 0.01    | val_0_rmse: 0.09529 | val_1_rmse: 0.09601 |  0:00:39s
epoch 41 | loss: 0.00988 | val_0_rmse: 0.09323 | val_1_rmse: 0.09467 |  0:00:40s
epoch 42 | loss: 0.00973 | val_0_rmse: 0.10497 | val_1_rmse: 0.10667 |  0:00:41s
epoch 43 | loss: 0.01116 | val_0_rmse: 0.09483 | val_1_rmse: 0.09618 |  0:00:42s
epoch 44 | loss: 0.01033 | val_0_rmse: 0.1     | val_1_rmse: 0.10081 |  0:00:43s
epoch 45 | loss: 0.0098  | val_0_rmse: 0.0987  | val_1_rmse: 0.09959 |  0:00:44s
epoch 46 | loss: 0.01003 | val_0_rmse: 0.09337 | val_1_rmse: 0.09537 |  0:00:45s
epoch 47 | loss: 0.0094  | val_0_rmse: 0.09198 | val_1_rmse: 0.09409 |  0:00:45s
epoch 48 | loss: 0.00945 | val_0_rmse: 0.09181 | val_1_rmse: 0.09415 |  0:00:46s
epoch 49 | loss: 0.00938 | val_0_rmse: 0.09334 | val_1_rmse: 0.09483 |  0:00:47s
epoch 50 | loss: 0.00959 | val_0_rmse: 0.09551 | val_1_rmse: 0.09684 |  0:00:48s
epoch 51 | loss: 0.01004 | val_0_rmse: 0.09657 | val_1_rmse: 0.09845 |  0:00:49s
epoch 52 | loss: 0.00949 | val_0_rmse: 0.09204 | val_1_rmse: 0.09435 |  0:00:50s
epoch 53 | loss: 0.00976 | val_0_rmse: 0.09689 | val_1_rmse: 0.09897 |  0:00:51s
epoch 54 | loss: 0.00967 | val_0_rmse: 0.09208 | val_1_rmse: 0.09458 |  0:00:52s
epoch 55 | loss: 0.00918 | val_0_rmse: 0.09111 | val_1_rmse: 0.09404 |  0:00:53s
epoch 56 | loss: 0.00944 | val_0_rmse: 0.09538 | val_1_rmse: 0.09809 |  0:00:54s
epoch 57 | loss: 0.01012 | val_0_rmse: 0.09267 | val_1_rmse: 0.09527 |  0:00:55s
epoch 58 | loss: 0.00957 | val_0_rmse: 0.09139 | val_1_rmse: 0.09377 |  0:00:56s
epoch 59 | loss: 0.00948 | val_0_rmse: 0.08978 | val_1_rmse: 0.09266 |  0:00:57s
epoch 60 | loss: 0.00923 | val_0_rmse: 0.09264 | val_1_rmse: 0.09565 |  0:00:58s
epoch 61 | loss: 0.00933 | val_0_rmse: 0.09288 | val_1_rmse: 0.09583 |  0:00:59s
epoch 62 | loss: 0.0093  | val_0_rmse: 0.09127 | val_1_rmse: 0.0939  |  0:01:00s
epoch 63 | loss: 0.00918 | val_0_rmse: 0.09198 | val_1_rmse: 0.09431 |  0:01:01s
epoch 64 | loss: 0.00922 | val_0_rmse: 0.09187 | val_1_rmse: 0.09388 |  0:01:02s
epoch 65 | loss: 0.00908 | val_0_rmse: 0.09312 | val_1_rmse: 0.09571 |  0:01:03s
epoch 66 | loss: 0.00925 | val_0_rmse: 0.09263 | val_1_rmse: 0.09488 |  0:01:04s
epoch 67 | loss: 0.00951 | val_0_rmse: 0.0957  | val_1_rmse: 0.09858 |  0:01:05s
epoch 68 | loss: 0.00939 | val_0_rmse: 0.09473 | val_1_rmse: 0.09661 |  0:01:06s
epoch 69 | loss: 0.00934 | val_0_rmse: 0.09486 | val_1_rmse: 0.09705 |  0:01:07s
epoch 70 | loss: 0.00948 | val_0_rmse: 0.09238 | val_1_rmse: 0.09432 |  0:01:08s
epoch 71 | loss: 0.00939 | val_0_rmse: 0.09441 | val_1_rmse: 0.09706 |  0:01:09s
epoch 72 | loss: 0.00958 | val_0_rmse: 0.09305 | val_1_rmse: 0.09545 |  0:01:10s
epoch 73 | loss: 0.00952 | val_0_rmse: 0.09011 | val_1_rmse: 0.09229 |  0:01:10s
epoch 74 | loss: 0.00916 | val_0_rmse: 0.0913  | val_1_rmse: 0.09326 |  0:01:11s
epoch 75 | loss: 0.00922 | val_0_rmse: 0.08929 | val_1_rmse: 0.09193 |  0:01:12s
epoch 76 | loss: 0.00919 | val_0_rmse: 0.09136 | val_1_rmse: 0.09355 |  0:01:13s
epoch 77 | loss: 0.00949 | val_0_rmse: 0.09403 | val_1_rmse: 0.09611 |  0:01:14s
epoch 78 | loss: 0.00977 | val_0_rmse: 0.09399 | val_1_rmse: 0.09643 |  0:01:15s
epoch 79 | loss: 0.00938 | val_0_rmse: 0.09462 | val_1_rmse: 0.09663 |  0:01:16s
epoch 80 | loss: 0.00907 | val_0_rmse: 0.09191 | val_1_rmse: 0.09433 |  0:01:17s
epoch 81 | loss: 0.00908 | val_0_rmse: 0.09103 | val_1_rmse: 0.0934  |  0:01:18s
epoch 82 | loss: 0.00895 | val_0_rmse: 0.09029 | val_1_rmse: 0.0927  |  0:01:19s
epoch 83 | loss: 0.00912 | val_0_rmse: 0.08897 | val_1_rmse: 0.09167 |  0:01:20s
epoch 84 | loss: 0.009   | val_0_rmse: 0.08899 | val_1_rmse: 0.09199 |  0:01:21s
epoch 85 | loss: 0.00892 | val_0_rmse: 0.09275 | val_1_rmse: 0.09573 |  0:01:22s
epoch 86 | loss: 0.00933 | val_0_rmse: 0.09001 | val_1_rmse: 0.0925  |  0:01:23s
epoch 87 | loss: 0.00886 | val_0_rmse: 0.08947 | val_1_rmse: 0.09213 |  0:01:24s
epoch 88 | loss: 0.00916 | val_0_rmse: 0.08863 | val_1_rmse: 0.09134 |  0:01:25s
epoch 89 | loss: 0.00868 | val_0_rmse: 0.08958 | val_1_rmse: 0.09281 |  0:01:26s
epoch 90 | loss: 0.00914 | val_0_rmse: 0.0971  | val_1_rmse: 0.09934 |  0:01:27s
epoch 91 | loss: 0.00938 | val_0_rmse: 0.08944 | val_1_rmse: 0.09219 |  0:01:28s
epoch 92 | loss: 0.00904 | val_0_rmse: 0.08834 | val_1_rmse: 0.0906  |  0:01:29s
epoch 93 | loss: 0.00853 | val_0_rmse: 0.08768 | val_1_rmse: 0.09069 |  0:01:30s
epoch 94 | loss: 0.00857 | val_0_rmse: 0.08972 | val_1_rmse: 0.09363 |  0:01:31s
epoch 95 | loss: 0.00854 | val_0_rmse: 0.08904 | val_1_rmse: 0.09261 |  0:01:32s
epoch 96 | loss: 0.00885 | val_0_rmse: 0.09368 | val_1_rmse: 0.09676 |  0:01:33s
epoch 97 | loss: 0.00926 | val_0_rmse: 0.0934  | val_1_rmse: 0.09603 |  0:01:34s
epoch 98 | loss: 0.00925 | val_0_rmse: 0.09805 | val_1_rmse: 0.10096 |  0:01:34s
epoch 99 | loss: 0.00941 | val_0_rmse: 0.09386 | val_1_rmse: 0.09734 |  0:01:35s
epoch 100| loss: 0.0089  | val_0_rmse: 0.08783 | val_1_rmse: 0.09146 |  0:01:36s
epoch 101| loss: 0.00879 | val_0_rmse: 0.0975  | val_1_rmse: 0.10065 |  0:01:37s
epoch 102| loss: 0.00906 | val_0_rmse: 0.08846 | val_1_rmse: 0.09154 |  0:01:38s
epoch 103| loss: 0.00892 | val_0_rmse: 0.08944 | val_1_rmse: 0.09187 |  0:01:39s
epoch 104| loss: 0.00889 | val_0_rmse: 0.08983 | val_1_rmse: 0.09186 |  0:01:40s
epoch 105| loss: 0.00916 | val_0_rmse: 0.0908  | val_1_rmse: 0.09342 |  0:01:41s
epoch 106| loss: 0.00891 | val_0_rmse: 0.08819 | val_1_rmse: 0.09157 |  0:01:42s
epoch 107| loss: 0.00883 | val_0_rmse: 0.093   | val_1_rmse: 0.09532 |  0:01:43s
epoch 108| loss: 0.00896 | val_0_rmse: 0.09118 | val_1_rmse: 0.09468 |  0:01:44s
epoch 109| loss: 0.0091  | val_0_rmse: 0.08896 | val_1_rmse: 0.09242 |  0:01:45s
epoch 110| loss: 0.00883 | val_0_rmse: 0.08902 | val_1_rmse: 0.09192 |  0:01:46s
epoch 111| loss: 0.00877 | val_0_rmse: 0.089   | val_1_rmse: 0.09246 |  0:01:47s
epoch 112| loss: 0.00859 | val_0_rmse: 0.08669 | val_1_rmse: 0.08987 |  0:01:48s
epoch 113| loss: 0.00853 | val_0_rmse: 0.08762 | val_1_rmse: 0.0915  |  0:01:49s
epoch 114| loss: 0.00866 | val_0_rmse: 0.08799 | val_1_rmse: 0.0922  |  0:01:50s
epoch 115| loss: 0.00892 | val_0_rmse: 0.08999 | val_1_rmse: 0.09316 |  0:01:51s
epoch 116| loss: 0.00893 | val_0_rmse: 0.11043 | val_1_rmse: 0.11248 |  0:01:52s
epoch 117| loss: 0.00886 | val_0_rmse: 0.09342 | val_1_rmse: 0.09624 |  0:01:53s
epoch 118| loss: 0.00848 | val_0_rmse: 0.08738 | val_1_rmse: 0.09109 |  0:01:54s
epoch 119| loss: 0.00852 | val_0_rmse: 0.08652 | val_1_rmse: 0.08976 |  0:01:55s
epoch 120| loss: 0.00884 | val_0_rmse: 0.097   | val_1_rmse: 0.10047 |  0:01:56s
epoch 121| loss: 0.00959 | val_0_rmse: 0.09544 | val_1_rmse: 0.0978  |  0:01:57s
epoch 122| loss: 0.00891 | val_0_rmse: 0.09245 | val_1_rmse: 0.09476 |  0:01:57s
epoch 123| loss: 0.00892 | val_0_rmse: 0.09631 | val_1_rmse: 0.09969 |  0:01:58s
epoch 124| loss: 0.00916 | val_0_rmse: 0.09519 | val_1_rmse: 0.09776 |  0:01:59s
epoch 125| loss: 0.00913 | val_0_rmse: 0.08789 | val_1_rmse: 0.09159 |  0:02:00s
epoch 126| loss: 0.00874 | val_0_rmse: 0.09047 | val_1_rmse: 0.09329 |  0:02:01s
epoch 127| loss: 0.00867 | val_0_rmse: 0.08776 | val_1_rmse: 0.09091 |  0:02:02s
epoch 128| loss: 0.00905 | val_0_rmse: 0.08907 | val_1_rmse: 0.0928  |  0:02:03s
epoch 129| loss: 0.00865 | val_0_rmse: 0.09299 | val_1_rmse: 0.09667 |  0:02:04s
epoch 130| loss: 0.00873 | val_0_rmse: 0.10079 | val_1_rmse: 0.10344 |  0:02:05s
epoch 131| loss: 0.00873 | val_0_rmse: 0.09121 | val_1_rmse: 0.09411 |  0:02:06s
epoch 132| loss: 0.00883 | val_0_rmse: 0.08818 | val_1_rmse: 0.09156 |  0:02:07s
epoch 133| loss: 0.00873 | val_0_rmse: 0.08947 | val_1_rmse: 0.09252 |  0:02:08s
epoch 134| loss: 0.00894 | val_0_rmse: 0.08902 | val_1_rmse: 0.09253 |  0:02:09s
epoch 135| loss: 0.00895 | val_0_rmse: 0.09837 | val_1_rmse: 0.10183 |  0:02:10s
epoch 136| loss: 0.00884 | val_0_rmse: 0.09518 | val_1_rmse: 0.09823 |  0:02:11s
epoch 137| loss: 0.00904 | val_0_rmse: 0.08799 | val_1_rmse: 0.09181 |  0:02:12s
epoch 138| loss: 0.00861 | val_0_rmse: 0.08911 | val_1_rmse: 0.09236 |  0:02:13s
epoch 139| loss: 0.00877 | val_0_rmse: 0.09689 | val_1_rmse: 0.10057 |  0:02:14s
epoch 140| loss: 0.00931 | val_0_rmse: 0.08909 | val_1_rmse: 0.09207 |  0:02:15s
epoch 141| loss: 0.00902 | val_0_rmse: 0.08873 | val_1_rmse: 0.09228 |  0:02:16s
epoch 142| loss: 0.0092  | val_0_rmse: 0.09164 | val_1_rmse: 0.09375 |  0:02:17s
epoch 143| loss: 0.00916 | val_0_rmse: 0.09297 | val_1_rmse: 0.09539 |  0:02:18s
epoch 144| loss: 0.00924 | val_0_rmse: 0.08953 | val_1_rmse: 0.09196 |  0:02:19s
epoch 145| loss: 0.00899 | val_0_rmse: 0.08997 | val_1_rmse: 0.09359 |  0:02:19s
epoch 146| loss: 0.00875 | val_0_rmse: 0.08907 | val_1_rmse: 0.09275 |  0:02:20s
epoch 147| loss: 0.009   | val_0_rmse: 0.09379 | val_1_rmse: 0.09622 |  0:02:21s
epoch 148| loss: 0.00874 | val_0_rmse: 0.09665 | val_1_rmse: 0.09984 |  0:02:22s
epoch 149| loss: 0.00897 | val_0_rmse: 0.09325 | val_1_rmse: 0.09555 |  0:02:23s

Early stopping occured at epoch 149 with best_epoch = 119 and best_val_1_rmse = 0.08976
Best weights from best epoch are automatically used!
ended training at: 06:20:50
Feature importance:
[('Area', 0.24177633330813628), ('Baths', 0.09420167490313355), ('Beds', 0.03849745774429997), ('Latitude', 0.23907426075315025), ('Longitude', 0.31727197438979476), ('Month', 0.052997327412949796), ('Year', 0.016180971488535405)]
Mean squared error is of 6141962876.020234
Mean absolute error:56178.381852006794
MAPE:0.15215696807229628
R2 score:0.8019054377376258
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:20:50
epoch 0  | loss: 0.36119 | val_0_rmse: 0.20663 | val_1_rmse: 0.20892 |  0:00:00s
epoch 1  | loss: 0.03826 | val_0_rmse: 0.20816 | val_1_rmse: 0.20777 |  0:00:00s
epoch 2  | loss: 0.02843 | val_0_rmse: 0.21902 | val_1_rmse: 0.21762 |  0:00:01s
epoch 3  | loss: 0.02484 | val_0_rmse: 0.21473 | val_1_rmse: 0.21314 |  0:00:01s
epoch 4  | loss: 0.02237 | val_0_rmse: 0.21703 | val_1_rmse: 0.21518 |  0:00:02s
epoch 5  | loss: 0.0207  | val_0_rmse: 0.2303  | val_1_rmse: 0.22821 |  0:00:02s
epoch 6  | loss: 0.01997 | val_0_rmse: 0.22149 | val_1_rmse: 0.2195  |  0:00:03s
epoch 7  | loss: 0.01901 | val_0_rmse: 0.22649 | val_1_rmse: 0.22413 |  0:00:03s
epoch 8  | loss: 0.01833 | val_0_rmse: 0.24598 | val_1_rmse: 0.24346 |  0:00:04s
epoch 9  | loss: 0.01799 | val_0_rmse: 0.24015 | val_1_rmse: 0.23766 |  0:00:04s
epoch 10 | loss: 0.01758 | val_0_rmse: 0.23065 | val_1_rmse: 0.22886 |  0:00:05s
epoch 11 | loss: 0.01667 | val_0_rmse: 0.23907 | val_1_rmse: 0.23725 |  0:00:05s
epoch 12 | loss: 0.01622 | val_0_rmse: 0.25071 | val_1_rmse: 0.24863 |  0:00:05s
epoch 13 | loss: 0.01518 | val_0_rmse: 0.24931 | val_1_rmse: 0.24729 |  0:00:06s
epoch 14 | loss: 0.01497 | val_0_rmse: 0.24547 | val_1_rmse: 0.24327 |  0:00:06s
epoch 15 | loss: 0.01492 | val_0_rmse: 0.23753 | val_1_rmse: 0.23516 |  0:00:07s
epoch 16 | loss: 0.01472 | val_0_rmse: 0.24585 | val_1_rmse: 0.24311 |  0:00:07s
epoch 17 | loss: 0.01395 | val_0_rmse: 0.25675 | val_1_rmse: 0.254   |  0:00:08s
epoch 18 | loss: 0.01382 | val_0_rmse: 0.26001 | val_1_rmse: 0.25701 |  0:00:08s
epoch 19 | loss: 0.01385 | val_0_rmse: 0.26435 | val_1_rmse: 0.26132 |  0:00:09s
epoch 20 | loss: 0.01376 | val_0_rmse: 0.24633 | val_1_rmse: 0.24361 |  0:00:09s
epoch 21 | loss: 0.01335 | val_0_rmse: 0.25535 | val_1_rmse: 0.25266 |  0:00:10s
epoch 22 | loss: 0.01289 | val_0_rmse: 0.26585 | val_1_rmse: 0.26318 |  0:00:10s
epoch 23 | loss: 0.01268 | val_0_rmse: 0.26956 | val_1_rmse: 0.26712 |  0:00:10s
epoch 24 | loss: 0.01287 | val_0_rmse: 0.24834 | val_1_rmse: 0.24591 |  0:00:11s
epoch 25 | loss: 0.01312 | val_0_rmse: 0.25915 | val_1_rmse: 0.25662 |  0:00:11s
epoch 26 | loss: 0.01299 | val_0_rmse: 0.24549 | val_1_rmse: 0.2428  |  0:00:12s
epoch 27 | loss: 0.01336 | val_0_rmse: 0.26111 | val_1_rmse: 0.25853 |  0:00:12s
epoch 28 | loss: 0.01325 | val_0_rmse: 0.26884 | val_1_rmse: 0.26585 |  0:00:13s
epoch 29 | loss: 0.01316 | val_0_rmse: 0.25    | val_1_rmse: 0.24725 |  0:00:13s
epoch 30 | loss: 0.01289 | val_0_rmse: 0.25792 | val_1_rmse: 0.25546 |  0:00:14s
epoch 31 | loss: 0.01255 | val_0_rmse: 0.24185 | val_1_rmse: 0.23967 |  0:00:14s

Early stopping occured at epoch 31 with best_epoch = 1 and best_val_1_rmse = 0.20777
Best weights from best epoch are automatically used!
ended training at: 06:21:05
Feature importance:
[('Area', 0.09581473643344189), ('Baths', 0.18297819305968852), ('Beds', 0.01804547727940534), ('Latitude', 0.11713469848498813), ('Longitude', 0.2931231372944899), ('Month', 0.0), ('Year', 0.29290375744798625)]
Mean squared error is of 87688585806.76889
Mean absolute error:246478.412798633
MAPE:0.5419131533406817
R2 score:-0.0642099673067773
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:21:06
epoch 0  | loss: 0.37627 | val_0_rmse: 0.20662 | val_1_rmse: 0.20894 |  0:00:00s
epoch 1  | loss: 0.03922 | val_0_rmse: 0.18706 | val_1_rmse: 0.18611 |  0:00:00s
epoch 2  | loss: 0.02773 | val_0_rmse: 0.19989 | val_1_rmse: 0.19726 |  0:00:01s
epoch 3  | loss: 0.02432 | val_0_rmse: 0.22406 | val_1_rmse: 0.22291 |  0:00:01s
epoch 4  | loss: 0.02185 | val_0_rmse: 0.21727 | val_1_rmse: 0.21527 |  0:00:02s
epoch 5  | loss: 0.02068 | val_0_rmse: 0.22208 | val_1_rmse: 0.22009 |  0:00:02s
epoch 6  | loss: 0.01939 | val_0_rmse: 0.23269 | val_1_rmse: 0.23137 |  0:00:03s
epoch 7  | loss: 0.01862 | val_0_rmse: 0.23115 | val_1_rmse: 0.23005 |  0:00:03s
epoch 8  | loss: 0.0179  | val_0_rmse: 0.24511 | val_1_rmse: 0.24457 |  0:00:04s
epoch 9  | loss: 0.0179  | val_0_rmse: 0.24879 | val_1_rmse: 0.24856 |  0:00:04s
epoch 10 | loss: 0.01691 | val_0_rmse: 0.24653 | val_1_rmse: 0.24651 |  0:00:05s
epoch 11 | loss: 0.0168  | val_0_rmse: 0.2401  | val_1_rmse: 0.24004 |  0:00:05s
epoch 12 | loss: 0.01714 | val_0_rmse: 0.25051 | val_1_rmse: 0.25065 |  0:00:05s
epoch 13 | loss: 0.01611 | val_0_rmse: 0.24333 | val_1_rmse: 0.24337 |  0:00:06s
epoch 14 | loss: 0.01641 | val_0_rmse: 0.25431 | val_1_rmse: 0.25475 |  0:00:06s
epoch 15 | loss: 0.01574 | val_0_rmse: 0.24569 | val_1_rmse: 0.24627 |  0:00:07s
epoch 16 | loss: 0.01568 | val_0_rmse: 0.24854 | val_1_rmse: 0.24925 |  0:00:07s
epoch 17 | loss: 0.01557 | val_0_rmse: 0.25415 | val_1_rmse: 0.25518 |  0:00:08s
epoch 18 | loss: 0.01564 | val_0_rmse: 0.26588 | val_1_rmse: 0.26722 |  0:00:08s
epoch 19 | loss: 0.01539 | val_0_rmse: 0.26063 | val_1_rmse: 0.26176 |  0:00:09s
epoch 20 | loss: 0.01517 | val_0_rmse: 0.25123 | val_1_rmse: 0.25224 |  0:00:09s
epoch 21 | loss: 0.01495 | val_0_rmse: 0.24933 | val_1_rmse: 0.2501  |  0:00:10s
epoch 22 | loss: 0.0148  | val_0_rmse: 0.24834 | val_1_rmse: 0.24925 |  0:00:10s
epoch 23 | loss: 0.01445 | val_0_rmse: 0.24818 | val_1_rmse: 0.24922 |  0:00:10s
epoch 24 | loss: 0.01451 | val_0_rmse: 0.26324 | val_1_rmse: 0.26468 |  0:00:11s
epoch 25 | loss: 0.01418 | val_0_rmse: 0.2511  | val_1_rmse: 0.25257 |  0:00:11s
epoch 26 | loss: 0.01432 | val_0_rmse: 0.24906 | val_1_rmse: 0.25059 |  0:00:12s
epoch 27 | loss: 0.01395 | val_0_rmse: 0.24233 | val_1_rmse: 0.24394 |  0:00:12s
epoch 28 | loss: 0.01405 | val_0_rmse: 0.26343 | val_1_rmse: 0.26562 |  0:00:13s
epoch 29 | loss: 0.01357 | val_0_rmse: 0.24644 | val_1_rmse: 0.24842 |  0:00:13s
epoch 30 | loss: 0.01373 | val_0_rmse: 0.24927 | val_1_rmse: 0.2516  |  0:00:14s
epoch 31 | loss: 0.01352 | val_0_rmse: 0.23271 | val_1_rmse: 0.23526 |  0:00:14s

Early stopping occured at epoch 31 with best_epoch = 1 and best_val_1_rmse = 0.18611
Best weights from best epoch are automatically used!
ended training at: 06:21:21
Feature importance:
[('Area', 0.11400557710521346), ('Baths', 0.1333084048066002), ('Beds', 0.006617234465537994), ('Latitude', 0.2884172055197893), ('Longitude', 0.2953848088735036), ('Month', 0.01083587880808433), ('Year', 0.15143089042127117)]
Mean squared error is of 69247462629.47244
Mean absolute error:215311.83218546267
MAPE:0.44228860662999125
R2 score:0.15499384049324139
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:21:21
epoch 0  | loss: 0.38869 | val_0_rmse: 0.21551 | val_1_rmse: 0.21546 |  0:00:00s
epoch 1  | loss: 0.04304 | val_0_rmse: 0.19802 | val_1_rmse: 0.196   |  0:00:00s
epoch 2  | loss: 0.03193 | val_0_rmse: 0.19949 | val_1_rmse: 0.19607 |  0:00:01s
epoch 3  | loss: 0.02485 | val_0_rmse: 0.20848 | val_1_rmse: 0.207   |  0:00:01s
epoch 4  | loss: 0.0223  | val_0_rmse: 0.20923 | val_1_rmse: 0.20806 |  0:00:02s
epoch 5  | loss: 0.0205  | val_0_rmse: 0.21167 | val_1_rmse: 0.2112  |  0:00:02s
epoch 6  | loss: 0.01978 | val_0_rmse: 0.20228 | val_1_rmse: 0.20119 |  0:00:03s
epoch 7  | loss: 0.01916 | val_0_rmse: 0.21014 | val_1_rmse: 0.20955 |  0:00:03s
epoch 8  | loss: 0.01833 | val_0_rmse: 0.21794 | val_1_rmse: 0.21737 |  0:00:04s
epoch 9  | loss: 0.01758 | val_0_rmse: 0.2071  | val_1_rmse: 0.20672 |  0:00:04s
epoch 10 | loss: 0.0175  | val_0_rmse: 0.22244 | val_1_rmse: 0.22247 |  0:00:05s
epoch 11 | loss: 0.01715 | val_0_rmse: 0.22269 | val_1_rmse: 0.22286 |  0:00:05s
epoch 12 | loss: 0.01657 | val_0_rmse: 0.22347 | val_1_rmse: 0.22382 |  0:00:05s
epoch 13 | loss: 0.01662 | val_0_rmse: 0.22725 | val_1_rmse: 0.22742 |  0:00:06s
epoch 14 | loss: 0.01617 | val_0_rmse: 0.22614 | val_1_rmse: 0.22665 |  0:00:06s
epoch 15 | loss: 0.01596 | val_0_rmse: 0.23297 | val_1_rmse: 0.23354 |  0:00:07s
epoch 16 | loss: 0.01645 | val_0_rmse: 0.23923 | val_1_rmse: 0.23973 |  0:00:07s
epoch 17 | loss: 0.01596 | val_0_rmse: 0.23502 | val_1_rmse: 0.23567 |  0:00:08s
epoch 18 | loss: 0.01537 | val_0_rmse: 0.26134 | val_1_rmse: 0.26243 |  0:00:08s
epoch 19 | loss: 0.01488 | val_0_rmse: 0.26593 | val_1_rmse: 0.26658 |  0:00:09s
epoch 20 | loss: 0.0153  | val_0_rmse: 0.27056 | val_1_rmse: 0.27134 |  0:00:09s
epoch 21 | loss: 0.01457 | val_0_rmse: 0.27017 | val_1_rmse: 0.27095 |  0:00:10s
epoch 22 | loss: 0.01439 | val_0_rmse: 0.26879 | val_1_rmse: 0.26911 |  0:00:10s
epoch 23 | loss: 0.01429 | val_0_rmse: 0.26613 | val_1_rmse: 0.26685 |  0:00:11s
epoch 24 | loss: 0.01394 | val_0_rmse: 0.25951 | val_1_rmse: 0.25997 |  0:00:11s
epoch 25 | loss: 0.01416 | val_0_rmse: 0.26443 | val_1_rmse: 0.26456 |  0:00:11s
epoch 26 | loss: 0.01376 | val_0_rmse: 0.27305 | val_1_rmse: 0.27367 |  0:00:12s
epoch 27 | loss: 0.01358 | val_0_rmse: 0.25985 | val_1_rmse: 0.26087 |  0:00:12s
epoch 28 | loss: 0.01349 | val_0_rmse: 0.2489  | val_1_rmse: 0.24977 |  0:00:13s
epoch 29 | loss: 0.01348 | val_0_rmse: 0.24945 | val_1_rmse: 0.24958 |  0:00:13s
epoch 30 | loss: 0.01387 | val_0_rmse: 0.24298 | val_1_rmse: 0.24412 |  0:00:14s
epoch 31 | loss: 0.01373 | val_0_rmse: 0.22845 | val_1_rmse: 0.22908 |  0:00:14s

Early stopping occured at epoch 31 with best_epoch = 1 and best_val_1_rmse = 0.196
Best weights from best epoch are automatically used!
ended training at: 06:21:36
Feature importance:
[('Area', 0.1089420691525929), ('Baths', 0.18602735709376989), ('Beds', 0.00014969894393507933), ('Latitude', 0.25855970797876116), ('Longitude', 0.318832247220316), ('Month', 0.03179613039729965), ('Year', 0.09569278921332533)]
Mean squared error is of 81639307807.79906
Mean absolute error:225010.88152799685
MAPE:0.4062356334108925
R2 score:0.024289748547328682
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:21:36
epoch 0  | loss: 0.37471 | val_0_rmse: 0.2331  | val_1_rmse: 0.23472 |  0:00:00s
epoch 1  | loss: 0.04147 | val_0_rmse: 0.19199 | val_1_rmse: 0.19286 |  0:00:00s
epoch 2  | loss: 0.02966 | val_0_rmse: 0.19353 | val_1_rmse: 0.1942  |  0:00:01s
epoch 3  | loss: 0.02615 | val_0_rmse: 0.20183 | val_1_rmse: 0.2028  |  0:00:01s
epoch 4  | loss: 0.02308 | val_0_rmse: 0.20192 | val_1_rmse: 0.20271 |  0:00:02s
epoch 5  | loss: 0.02121 | val_0_rmse: 0.19936 | val_1_rmse: 0.19976 |  0:00:02s
epoch 6  | loss: 0.01952 | val_0_rmse: 0.19925 | val_1_rmse: 0.20065 |  0:00:03s
epoch 7  | loss: 0.0185  | val_0_rmse: 0.19616 | val_1_rmse: 0.19766 |  0:00:03s
epoch 8  | loss: 0.01803 | val_0_rmse: 0.20945 | val_1_rmse: 0.21208 |  0:00:04s
epoch 9  | loss: 0.0175  | val_0_rmse: 0.21356 | val_1_rmse: 0.21629 |  0:00:04s
epoch 10 | loss: 0.01687 | val_0_rmse: 0.21732 | val_1_rmse: 0.22005 |  0:00:05s
epoch 11 | loss: 0.01671 | val_0_rmse: 0.20857 | val_1_rmse: 0.21063 |  0:00:05s
epoch 12 | loss: 0.01618 | val_0_rmse: 0.22082 | val_1_rmse: 0.22352 |  0:00:05s
epoch 13 | loss: 0.01598 | val_0_rmse: 0.22536 | val_1_rmse: 0.22826 |  0:00:06s
epoch 14 | loss: 0.01553 | val_0_rmse: 0.22867 | val_1_rmse: 0.23174 |  0:00:06s
epoch 15 | loss: 0.01549 | val_0_rmse: 0.2268  | val_1_rmse: 0.22971 |  0:00:07s
epoch 16 | loss: 0.01519 | val_0_rmse: 0.23427 | val_1_rmse: 0.23739 |  0:00:07s
epoch 17 | loss: 0.01484 | val_0_rmse: 0.23535 | val_1_rmse: 0.2385  |  0:00:08s
epoch 18 | loss: 0.01464 | val_0_rmse: 0.23246 | val_1_rmse: 0.23539 |  0:00:08s
epoch 19 | loss: 0.01461 | val_0_rmse: 0.25684 | val_1_rmse: 0.25991 |  0:00:09s
epoch 20 | loss: 0.01421 | val_0_rmse: 0.24113 | val_1_rmse: 0.24387 |  0:00:09s
epoch 21 | loss: 0.01394 | val_0_rmse: 0.24001 | val_1_rmse: 0.24285 |  0:00:09s
epoch 22 | loss: 0.01394 | val_0_rmse: 0.23376 | val_1_rmse: 0.23631 |  0:00:10s
epoch 23 | loss: 0.01369 | val_0_rmse: 0.24219 | val_1_rmse: 0.24529 |  0:00:10s
epoch 24 | loss: 0.01349 | val_0_rmse: 0.23382 | val_1_rmse: 0.23673 |  0:00:11s
epoch 25 | loss: 0.01409 | val_0_rmse: 0.23549 | val_1_rmse: 0.23852 |  0:00:11s
epoch 26 | loss: 0.01353 | val_0_rmse: 0.22991 | val_1_rmse: 0.23293 |  0:00:12s
epoch 27 | loss: 0.0135  | val_0_rmse: 0.22476 | val_1_rmse: 0.22766 |  0:00:12s
epoch 28 | loss: 0.01348 | val_0_rmse: 0.22949 | val_1_rmse: 0.23255 |  0:00:13s
epoch 29 | loss: 0.01298 | val_0_rmse: 0.24038 | val_1_rmse: 0.24362 |  0:00:13s
epoch 30 | loss: 0.01311 | val_0_rmse: 0.23232 | val_1_rmse: 0.23537 |  0:00:14s
epoch 31 | loss: 0.01274 | val_0_rmse: 0.23571 | val_1_rmse: 0.23865 |  0:00:14s

Early stopping occured at epoch 31 with best_epoch = 1 and best_val_1_rmse = 0.19286
Best weights from best epoch are automatically used!
ended training at: 06:21:50
Feature importance:
[('Area', 0.14318934650237541), ('Baths', 0.17362703187082248), ('Beds', 0.26909116828864366), ('Latitude', 0.0004187179874673603), ('Longitude', 0.004195600695562867), ('Month', 0.3677388688570119), ('Year', 0.04173926579811628)]
Mean squared error is of 79319080816.22629
Mean absolute error:233984.22177418505
MAPE:0.48745247686123283
R2 score:0.08223671921750042
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:21:50
epoch 0  | loss: 0.38208 | val_0_rmse: 0.21149 | val_1_rmse: 0.20934 |  0:00:00s
epoch 1  | loss: 0.042   | val_0_rmse: 0.21326 | val_1_rmse: 0.21331 |  0:00:00s
epoch 2  | loss: 0.03141 | val_0_rmse: 0.20424 | val_1_rmse: 0.20379 |  0:00:01s
epoch 3  | loss: 0.02707 | val_0_rmse: 0.20204 | val_1_rmse: 0.19971 |  0:00:01s
epoch 4  | loss: 0.02453 | val_0_rmse: 0.20231 | val_1_rmse: 0.19925 |  0:00:02s
epoch 5  | loss: 0.02248 | val_0_rmse: 0.21006 | val_1_rmse: 0.20605 |  0:00:02s
epoch 6  | loss: 0.02127 | val_0_rmse: 0.20873 | val_1_rmse: 0.20481 |  0:00:03s
epoch 7  | loss: 0.02071 | val_0_rmse: 0.21658 | val_1_rmse: 0.21254 |  0:00:03s
epoch 8  | loss: 0.01973 | val_0_rmse: 0.22747 | val_1_rmse: 0.22358 |  0:00:04s
epoch 9  | loss: 0.01916 | val_0_rmse: 0.23982 | val_1_rmse: 0.23566 |  0:00:04s
epoch 10 | loss: 0.01811 | val_0_rmse: 0.23374 | val_1_rmse: 0.2304  |  0:00:05s
epoch 11 | loss: 0.01791 | val_0_rmse: 0.23925 | val_1_rmse: 0.23749 |  0:00:05s
epoch 12 | loss: 0.01698 | val_0_rmse: 0.24952 | val_1_rmse: 0.24835 |  0:00:05s
epoch 13 | loss: 0.01607 | val_0_rmse: 0.25081 | val_1_rmse: 0.24954 |  0:00:06s
epoch 14 | loss: 0.01567 | val_0_rmse: 0.23357 | val_1_rmse: 0.23239 |  0:00:06s
epoch 15 | loss: 0.01576 | val_0_rmse: 0.24333 | val_1_rmse: 0.24209 |  0:00:07s
epoch 16 | loss: 0.01619 | val_0_rmse: 0.24301 | val_1_rmse: 0.24179 |  0:00:07s
epoch 17 | loss: 0.01552 | val_0_rmse: 0.2417  | val_1_rmse: 0.24122 |  0:00:08s
epoch 18 | loss: 0.01487 | val_0_rmse: 0.25542 | val_1_rmse: 0.25367 |  0:00:08s
epoch 19 | loss: 0.01437 | val_0_rmse: 0.25952 | val_1_rmse: 0.2591  |  0:00:09s
epoch 20 | loss: 0.01399 | val_0_rmse: 0.26119 | val_1_rmse: 0.26119 |  0:00:09s
epoch 21 | loss: 0.01418 | val_0_rmse: 0.2621  | val_1_rmse: 0.26166 |  0:00:10s
epoch 22 | loss: 0.01368 | val_0_rmse: 0.24852 | val_1_rmse: 0.24847 |  0:00:10s
epoch 23 | loss: 0.01379 | val_0_rmse: 0.24613 | val_1_rmse: 0.24576 |  0:00:10s
epoch 24 | loss: 0.01403 | val_0_rmse: 0.24484 | val_1_rmse: 0.24378 |  0:00:11s
epoch 25 | loss: 0.01385 | val_0_rmse: 0.23807 | val_1_rmse: 0.23746 |  0:00:11s
epoch 26 | loss: 0.01404 | val_0_rmse: 0.23472 | val_1_rmse: 0.2336  |  0:00:12s
epoch 27 | loss: 0.0136  | val_0_rmse: 0.23103 | val_1_rmse: 0.23007 |  0:00:12s
epoch 28 | loss: 0.01333 | val_0_rmse: 0.24492 | val_1_rmse: 0.24384 |  0:00:13s
epoch 29 | loss: 0.01319 | val_0_rmse: 0.24037 | val_1_rmse: 0.24032 |  0:00:13s
epoch 30 | loss: 0.01306 | val_0_rmse: 0.25208 | val_1_rmse: 0.2524  |  0:00:14s
epoch 31 | loss: 0.01318 | val_0_rmse: 0.25512 | val_1_rmse: 0.2566  |  0:00:14s
epoch 32 | loss: 0.01306 | val_0_rmse: 0.24333 | val_1_rmse: 0.24437 |  0:00:15s
epoch 33 | loss: 0.01286 | val_0_rmse: 0.25814 | val_1_rmse: 0.25952 |  0:00:15s
epoch 34 | loss: 0.01277 | val_0_rmse: 0.25666 | val_1_rmse: 0.25819 |  0:00:15s

Early stopping occured at epoch 34 with best_epoch = 4 and best_val_1_rmse = 0.19925
Best weights from best epoch are automatically used!
ended training at: 06:22:07
Feature importance:
[('Area', 0.010051421755673128), ('Baths', 0.01151474751091165), ('Beds', 0.2264701569602564), ('Latitude', 0.2996036958022325), ('Longitude', 0.05761345163928705), ('Month', 0.0), ('Year', 0.3947465263316392)]
Mean squared error is of 78379007740.69185
Mean absolute error:233710.3731926919
MAPE:0.5142814453617884
R2 score:0.004319547984020788
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:22:07
epoch 0  | loss: 0.97832 | val_0_rmse: 0.38275 | val_1_rmse: 0.37792 |  0:00:00s
epoch 1  | loss: 0.16216 | val_0_rmse: 0.3391  | val_1_rmse: 0.3381  |  0:00:00s
epoch 2  | loss: 0.06842 | val_0_rmse: 0.35042 | val_1_rmse: 0.35469 |  0:00:00s
epoch 3  | loss: 0.04314 | val_0_rmse: 0.28733 | val_1_rmse: 0.29513 |  0:00:00s
epoch 4  | loss: 0.03957 | val_0_rmse: 0.27227 | val_1_rmse: 0.28641 |  0:00:00s
epoch 5  | loss: 0.03386 | val_0_rmse: 0.25622 | val_1_rmse: 0.26792 |  0:00:00s
epoch 6  | loss: 0.03092 | val_0_rmse: 0.23    | val_1_rmse: 0.23514 |  0:00:00s
epoch 7  | loss: 0.02959 | val_0_rmse: 0.21977 | val_1_rmse: 0.21888 |  0:00:01s
epoch 8  | loss: 0.02723 | val_0_rmse: 0.2126  | val_1_rmse: 0.21089 |  0:00:01s
epoch 9  | loss: 0.02553 | val_0_rmse: 0.21013 | val_1_rmse: 0.20937 |  0:00:01s
epoch 10 | loss: 0.02466 | val_0_rmse: 0.20994 | val_1_rmse: 0.20995 |  0:00:01s
epoch 11 | loss: 0.0253  | val_0_rmse: 0.21165 | val_1_rmse: 0.2133  |  0:00:01s
epoch 12 | loss: 0.02252 | val_0_rmse: 0.20998 | val_1_rmse: 0.21294 |  0:00:01s
epoch 13 | loss: 0.02312 | val_0_rmse: 0.21717 | val_1_rmse: 0.22175 |  0:00:01s
epoch 14 | loss: 0.022   | val_0_rmse: 0.21897 | val_1_rmse: 0.22416 |  0:00:02s
epoch 15 | loss: 0.02173 | val_0_rmse: 0.21586 | val_1_rmse: 0.22181 |  0:00:02s
epoch 16 | loss: 0.02199 | val_0_rmse: 0.22181 | val_1_rmse: 0.229   |  0:00:02s
epoch 17 | loss: 0.02181 | val_0_rmse: 0.21379 | val_1_rmse: 0.2204  |  0:00:02s
epoch 18 | loss: 0.02175 | val_0_rmse: 0.20458 | val_1_rmse: 0.21021 |  0:00:02s
epoch 19 | loss: 0.02116 | val_0_rmse: 0.20566 | val_1_rmse: 0.21181 |  0:00:02s
epoch 20 | loss: 0.02115 | val_0_rmse: 0.20325 | val_1_rmse: 0.20942 |  0:00:02s
epoch 21 | loss: 0.02146 | val_0_rmse: 0.192   | val_1_rmse: 0.19631 |  0:00:02s
epoch 22 | loss: 0.02094 | val_0_rmse: 0.19058 | val_1_rmse: 0.19487 |  0:00:03s
epoch 23 | loss: 0.02059 | val_0_rmse: 0.19059 | val_1_rmse: 0.19501 |  0:00:03s
epoch 24 | loss: 0.02044 | val_0_rmse: 0.18972 | val_1_rmse: 0.19372 |  0:00:03s
epoch 25 | loss: 0.02051 | val_0_rmse: 0.1905  | val_1_rmse: 0.19452 |  0:00:03s
epoch 26 | loss: 0.02037 | val_0_rmse: 0.19207 | val_1_rmse: 0.19643 |  0:00:03s
epoch 27 | loss: 0.02065 | val_0_rmse: 0.19182 | val_1_rmse: 0.19596 |  0:00:03s
epoch 28 | loss: 0.02054 | val_0_rmse: 0.1912  | val_1_rmse: 0.19497 |  0:00:03s
epoch 29 | loss: 0.02021 | val_0_rmse: 0.19196 | val_1_rmse: 0.19567 |  0:00:04s
epoch 30 | loss: 0.02035 | val_0_rmse: 0.19736 | val_1_rmse: 0.20244 |  0:00:04s
epoch 31 | loss: 0.02108 | val_0_rmse: 0.19093 | val_1_rmse: 0.19303 |  0:00:04s
epoch 32 | loss: 0.02045 | val_0_rmse: 0.19155 | val_1_rmse: 0.1946  |  0:00:04s
epoch 33 | loss: 0.02082 | val_0_rmse: 0.19137 | val_1_rmse: 0.1946  |  0:00:04s
epoch 34 | loss: 0.02084 | val_0_rmse: 0.18997 | val_1_rmse: 0.19062 |  0:00:04s
epoch 35 | loss: 0.02103 | val_0_rmse: 0.19341 | val_1_rmse: 0.19768 |  0:00:04s
epoch 36 | loss: 0.02037 | val_0_rmse: 0.18971 | val_1_rmse: 0.19105 |  0:00:04s
epoch 37 | loss: 0.02051 | val_0_rmse: 0.19358 | val_1_rmse: 0.19755 |  0:00:05s
epoch 38 | loss: 0.02035 | val_0_rmse: 0.1923  | val_1_rmse: 0.19526 |  0:00:05s
epoch 39 | loss: 0.01978 | val_0_rmse: 0.19174 | val_1_rmse: 0.1935  |  0:00:05s
epoch 40 | loss: 0.02025 | val_0_rmse: 0.19125 | val_1_rmse: 0.19421 |  0:00:05s
epoch 41 | loss: 0.01937 | val_0_rmse: 0.18726 | val_1_rmse: 0.18855 |  0:00:05s
epoch 42 | loss: 0.01975 | val_0_rmse: 0.18502 | val_1_rmse: 0.18697 |  0:00:05s
epoch 43 | loss: 0.02022 | val_0_rmse: 0.18602 | val_1_rmse: 0.18649 |  0:00:05s
epoch 44 | loss: 0.02017 | val_0_rmse: 0.18938 | val_1_rmse: 0.19196 |  0:00:06s
epoch 45 | loss: 0.01948 | val_0_rmse: 0.19198 | val_1_rmse: 0.19491 |  0:00:06s
epoch 46 | loss: 0.02009 | val_0_rmse: 0.19087 | val_1_rmse: 0.19296 |  0:00:06s
epoch 47 | loss: 0.01958 | val_0_rmse: 0.18978 | val_1_rmse: 0.19231 |  0:00:06s
epoch 48 | loss: 0.0202  | val_0_rmse: 0.18744 | val_1_rmse: 0.18863 |  0:00:06s
epoch 49 | loss: 0.02035 | val_0_rmse: 0.18547 | val_1_rmse: 0.18764 |  0:00:06s
epoch 50 | loss: 0.01981 | val_0_rmse: 0.19065 | val_1_rmse: 0.18853 |  0:00:06s
epoch 51 | loss: 0.01991 | val_0_rmse: 0.18623 | val_1_rmse: 0.18763 |  0:00:06s
epoch 52 | loss: 0.02038 | val_0_rmse: 0.18947 | val_1_rmse: 0.18905 |  0:00:07s
epoch 53 | loss: 0.02018 | val_0_rmse: 0.19038 | val_1_rmse: 0.18887 |  0:00:07s
epoch 54 | loss: 0.01959 | val_0_rmse: 0.18662 | val_1_rmse: 0.18593 |  0:00:07s
epoch 55 | loss: 0.01951 | val_0_rmse: 0.19183 | val_1_rmse: 0.18846 |  0:00:07s
epoch 56 | loss: 0.01978 | val_0_rmse: 0.18516 | val_1_rmse: 0.18661 |  0:00:07s
epoch 57 | loss: 0.02046 | val_0_rmse: 0.18882 | val_1_rmse: 0.18678 |  0:00:07s
epoch 58 | loss: 0.01982 | val_0_rmse: 0.189   | val_1_rmse: 0.18589 |  0:00:07s
epoch 59 | loss: 0.01976 | val_0_rmse: 0.17938 | val_1_rmse: 0.17992 |  0:00:08s
epoch 60 | loss: 0.0198  | val_0_rmse: 0.19143 | val_1_rmse: 0.1856  |  0:00:08s
epoch 61 | loss: 0.0201  | val_0_rmse: 0.17843 | val_1_rmse: 0.1768  |  0:00:08s
epoch 62 | loss: 0.01965 | val_0_rmse: 0.17949 | val_1_rmse: 0.17819 |  0:00:08s
epoch 63 | loss: 0.01935 | val_0_rmse: 0.18921 | val_1_rmse: 0.18395 |  0:00:08s
epoch 64 | loss: 0.01965 | val_0_rmse: 0.17589 | val_1_rmse: 0.1752  |  0:00:08s
epoch 65 | loss: 0.01958 | val_0_rmse: 0.17754 | val_1_rmse: 0.17313 |  0:00:08s
epoch 66 | loss: 0.0191  | val_0_rmse: 0.17676 | val_1_rmse: 0.1723  |  0:00:08s
epoch 67 | loss: 0.01924 | val_0_rmse: 0.17318 | val_1_rmse: 0.17204 |  0:00:09s
epoch 68 | loss: 0.01888 | val_0_rmse: 0.17746 | val_1_rmse: 0.17305 |  0:00:09s
epoch 69 | loss: 0.01948 | val_0_rmse: 0.17348 | val_1_rmse: 0.16964 |  0:00:09s
epoch 70 | loss: 0.01934 | val_0_rmse: 0.17149 | val_1_rmse: 0.16752 |  0:00:09s
epoch 71 | loss: 0.01967 | val_0_rmse: 0.17311 | val_1_rmse: 0.16776 |  0:00:09s
epoch 72 | loss: 0.01885 | val_0_rmse: 0.16889 | val_1_rmse: 0.16581 |  0:00:09s
epoch 73 | loss: 0.01891 | val_0_rmse: 0.17333 | val_1_rmse: 0.16943 |  0:00:09s
epoch 74 | loss: 0.01922 | val_0_rmse: 0.16879 | val_1_rmse: 0.16658 |  0:00:10s
epoch 75 | loss: 0.01973 | val_0_rmse: 0.1652  | val_1_rmse: 0.1639  |  0:00:10s
epoch 76 | loss: 0.019   | val_0_rmse: 0.17096 | val_1_rmse: 0.1648  |  0:00:10s
epoch 77 | loss: 0.02012 | val_0_rmse: 0.15979 | val_1_rmse: 0.15972 |  0:00:10s
epoch 78 | loss: 0.01917 | val_0_rmse: 0.15935 | val_1_rmse: 0.15934 |  0:00:10s
epoch 79 | loss: 0.01868 | val_0_rmse: 0.16221 | val_1_rmse: 0.15836 |  0:00:10s
epoch 80 | loss: 0.01932 | val_0_rmse: 0.16197 | val_1_rmse: 0.16345 |  0:00:10s
epoch 81 | loss: 0.02043 | val_0_rmse: 0.1594  | val_1_rmse: 0.15838 |  0:00:11s
epoch 82 | loss: 0.01906 | val_0_rmse: 0.16415 | val_1_rmse: 0.15682 |  0:00:11s
epoch 83 | loss: 0.01997 | val_0_rmse: 0.16044 | val_1_rmse: 0.16137 |  0:00:11s
epoch 84 | loss: 0.01946 | val_0_rmse: 0.15634 | val_1_rmse: 0.15435 |  0:00:11s
epoch 85 | loss: 0.01846 | val_0_rmse: 0.15905 | val_1_rmse: 0.15117 |  0:00:11s
epoch 86 | loss: 0.01952 | val_0_rmse: 0.15443 | val_1_rmse: 0.15157 |  0:00:11s
epoch 87 | loss: 0.01878 | val_0_rmse: 0.15333 | val_1_rmse: 0.14953 |  0:00:11s
epoch 88 | loss: 0.01865 | val_0_rmse: 0.15697 | val_1_rmse: 0.1486  |  0:00:11s
epoch 89 | loss: 0.01882 | val_0_rmse: 0.15266 | val_1_rmse: 0.14909 |  0:00:12s
epoch 90 | loss: 0.01855 | val_0_rmse: 0.15323 | val_1_rmse: 0.15052 |  0:00:12s
epoch 91 | loss: 0.01878 | val_0_rmse: 0.15412 | val_1_rmse: 0.14787 |  0:00:12s
epoch 92 | loss: 0.01809 | val_0_rmse: 0.15422 | val_1_rmse: 0.14689 |  0:00:12s
epoch 93 | loss: 0.0189  | val_0_rmse: 0.15209 | val_1_rmse: 0.14745 |  0:00:12s
epoch 94 | loss: 0.01804 | val_0_rmse: 0.15395 | val_1_rmse: 0.14629 |  0:00:12s
epoch 95 | loss: 0.01821 | val_0_rmse: 0.15026 | val_1_rmse: 0.14625 |  0:00:12s
epoch 96 | loss: 0.01787 | val_0_rmse: 0.14953 | val_1_rmse: 0.14592 |  0:00:13s
epoch 97 | loss: 0.01766 | val_0_rmse: 0.14872 | val_1_rmse: 0.14387 |  0:00:13s
epoch 98 | loss: 0.0179  | val_0_rmse: 0.14924 | val_1_rmse: 0.14408 |  0:00:13s
epoch 99 | loss: 0.01765 | val_0_rmse: 0.14984 | val_1_rmse: 0.14607 |  0:00:13s
epoch 100| loss: 0.01773 | val_0_rmse: 0.14926 | val_1_rmse: 0.14436 |  0:00:13s
epoch 101| loss: 0.01769 | val_0_rmse: 0.14755 | val_1_rmse: 0.14175 |  0:00:13s
epoch 102| loss: 0.0178  | val_0_rmse: 0.14657 | val_1_rmse: 0.14304 |  0:00:13s
epoch 103| loss: 0.01827 | val_0_rmse: 0.14716 | val_1_rmse: 0.14168 |  0:00:13s
epoch 104| loss: 0.01828 | val_0_rmse: 0.14842 | val_1_rmse: 0.14252 |  0:00:14s
epoch 105| loss: 0.01789 | val_0_rmse: 0.15043 | val_1_rmse: 0.14894 |  0:00:14s
epoch 106| loss: 0.0183  | val_0_rmse: 0.14815 | val_1_rmse: 0.14242 |  0:00:14s
epoch 107| loss: 0.01835 | val_0_rmse: 0.14637 | val_1_rmse: 0.14301 |  0:00:14s
epoch 108| loss: 0.01774 | val_0_rmse: 0.15135 | val_1_rmse: 0.15287 |  0:00:14s
epoch 109| loss: 0.01844 | val_0_rmse: 0.1485  | val_1_rmse: 0.14413 |  0:00:14s
epoch 110| loss: 0.01795 | val_0_rmse: 0.1473  | val_1_rmse: 0.14377 |  0:00:14s
epoch 111| loss: 0.01834 | val_0_rmse: 0.14805 | val_1_rmse: 0.1474  |  0:00:15s
epoch 112| loss: 0.01797 | val_0_rmse: 0.14591 | val_1_rmse: 0.13907 |  0:00:15s
epoch 113| loss: 0.01775 | val_0_rmse: 0.14532 | val_1_rmse: 0.14256 |  0:00:15s
epoch 114| loss: 0.01776 | val_0_rmse: 0.14602 | val_1_rmse: 0.14412 |  0:00:15s
epoch 115| loss: 0.01792 | val_0_rmse: 0.14462 | val_1_rmse: 0.13835 |  0:00:15s
epoch 116| loss: 0.01781 | val_0_rmse: 0.14472 | val_1_rmse: 0.14137 |  0:00:15s
epoch 117| loss: 0.01761 | val_0_rmse: 0.14453 | val_1_rmse: 0.14061 |  0:00:15s
epoch 118| loss: 0.01762 | val_0_rmse: 0.14401 | val_1_rmse: 0.13858 |  0:00:15s
epoch 119| loss: 0.01809 | val_0_rmse: 0.14387 | val_1_rmse: 0.13922 |  0:00:16s
epoch 120| loss: 0.01816 | val_0_rmse: 0.14396 | val_1_rmse: 0.1414  |  0:00:16s
epoch 121| loss: 0.01814 | val_0_rmse: 0.14386 | val_1_rmse: 0.14181 |  0:00:16s
epoch 122| loss: 0.01785 | val_0_rmse: 0.14327 | val_1_rmse: 0.13531 |  0:00:16s
epoch 123| loss: 0.01779 | val_0_rmse: 0.14255 | val_1_rmse: 0.13657 |  0:00:16s
epoch 124| loss: 0.01762 | val_0_rmse: 0.14344 | val_1_rmse: 0.1393  |  0:00:16s
epoch 125| loss: 0.01777 | val_0_rmse: 0.1424  | val_1_rmse: 0.13464 |  0:00:16s
epoch 126| loss: 0.01838 | val_0_rmse: 0.14193 | val_1_rmse: 0.13507 |  0:00:17s
epoch 127| loss: 0.01707 | val_0_rmse: 0.14174 | val_1_rmse: 0.13601 |  0:00:17s
epoch 128| loss: 0.01805 | val_0_rmse: 0.14146 | val_1_rmse: 0.13563 |  0:00:17s
epoch 129| loss: 0.01772 | val_0_rmse: 0.14299 | val_1_rmse: 0.13977 |  0:00:17s
epoch 130| loss: 0.01822 | val_0_rmse: 0.1448  | val_1_rmse: 0.14306 |  0:00:17s
epoch 131| loss: 0.01825 | val_0_rmse: 0.14374 | val_1_rmse: 0.14042 |  0:00:17s
epoch 132| loss: 0.01793 | val_0_rmse: 0.14257 | val_1_rmse: 0.13803 |  0:00:17s
epoch 133| loss: 0.01794 | val_0_rmse: 0.14694 | val_1_rmse: 0.14567 |  0:00:17s
epoch 134| loss: 0.01836 | val_0_rmse: 0.14423 | val_1_rmse: 0.14212 |  0:00:18s
epoch 135| loss: 0.01722 | val_0_rmse: 0.14072 | val_1_rmse: 0.13472 |  0:00:18s
epoch 136| loss: 0.01813 | val_0_rmse: 0.14653 | val_1_rmse: 0.14661 |  0:00:18s
epoch 137| loss: 0.01783 | val_0_rmse: 0.14789 | val_1_rmse: 0.14969 |  0:00:18s
epoch 138| loss: 0.01816 | val_0_rmse: 0.14025 | val_1_rmse: 0.1366  |  0:00:18s
epoch 139| loss: 0.018   | val_0_rmse: 0.14296 | val_1_rmse: 0.14244 |  0:00:18s
epoch 140| loss: 0.01814 | val_0_rmse: 0.14114 | val_1_rmse: 0.13893 |  0:00:18s
epoch 141| loss: 0.01729 | val_0_rmse: 0.1386  | val_1_rmse: 0.13285 |  0:00:19s
epoch 142| loss: 0.01755 | val_0_rmse: 0.14627 | val_1_rmse: 0.14595 |  0:00:19s
epoch 143| loss: 0.0183  | val_0_rmse: 0.14393 | val_1_rmse: 0.14255 |  0:00:19s
epoch 144| loss: 0.0177  | val_0_rmse: 0.13806 | val_1_rmse: 0.13351 |  0:00:19s
epoch 145| loss: 0.01768 | val_0_rmse: 0.14155 | val_1_rmse: 0.14132 |  0:00:19s
epoch 146| loss: 0.01741 | val_0_rmse: 0.13988 | val_1_rmse: 0.13958 |  0:00:19s
epoch 147| loss: 0.0173  | val_0_rmse: 0.13746 | val_1_rmse: 0.13393 |  0:00:19s
epoch 148| loss: 0.01721 | val_0_rmse: 0.139   | val_1_rmse: 0.13891 |  0:00:19s
epoch 149| loss: 0.0176  | val_0_rmse: 0.13731 | val_1_rmse: 0.1351  |  0:00:20s
Stop training because you reached max_epochs = 150 with best_epoch = 141 and best_val_1_rmse = 0.13285
Best weights from best epoch are automatically used!
ended training at: 06:22:27
Feature importance:
[('Area', 0.24394891752865175), ('Baths', 0.18146744686409075), ('Beds', 0.09483633355156176), ('Latitude', 0.06822010452271512), ('Longitude', 0.06545768987871575), ('Month', 0.02483268513311423), ('Year', 0.3212368225211506)]
Mean squared error is of 3506289588.956177
Mean absolute error:44601.986626167585
MAPE:0.39913221969495694
R2 score:0.5545258905661626
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:22:27
epoch 0  | loss: 0.97931 | val_0_rmse: 0.79029 | val_1_rmse: 0.81431 |  0:00:00s
epoch 1  | loss: 0.18978 | val_0_rmse: 0.28416 | val_1_rmse: 0.30414 |  0:00:00s
epoch 2  | loss: 0.06647 | val_0_rmse: 0.22483 | val_1_rmse: 0.23755 |  0:00:00s
epoch 3  | loss: 0.05882 | val_0_rmse: 0.16763 | val_1_rmse: 0.16854 |  0:00:00s
epoch 4  | loss: 0.04216 | val_0_rmse: 0.17106 | val_1_rmse: 0.16972 |  0:00:00s
epoch 5  | loss: 0.03181 | val_0_rmse: 0.18012 | val_1_rmse: 0.1786  |  0:00:00s
epoch 6  | loss: 0.03183 | val_0_rmse: 0.17995 | val_1_rmse: 0.17362 |  0:00:00s
epoch 7  | loss: 0.02985 | val_0_rmse: 0.18323 | val_1_rmse: 0.17308 |  0:00:01s
epoch 8  | loss: 0.02842 | val_0_rmse: 0.18215 | val_1_rmse: 0.17235 |  0:00:01s
epoch 9  | loss: 0.02758 | val_0_rmse: 0.18181 | val_1_rmse: 0.17302 |  0:00:01s
epoch 10 | loss: 0.02463 | val_0_rmse: 0.18301 | val_1_rmse: 0.17578 |  0:00:01s
epoch 11 | loss: 0.02495 | val_0_rmse: 0.18726 | val_1_rmse: 0.18206 |  0:00:01s
epoch 12 | loss: 0.02492 | val_0_rmse: 0.19088 | val_1_rmse: 0.18699 |  0:00:01s
epoch 13 | loss: 0.02314 | val_0_rmse: 0.19058 | val_1_rmse: 0.18728 |  0:00:01s
epoch 14 | loss: 0.02264 | val_0_rmse: 0.19097 | val_1_rmse: 0.18824 |  0:00:01s
epoch 15 | loss: 0.02253 | val_0_rmse: 0.1913  | val_1_rmse: 0.18854 |  0:00:02s
epoch 16 | loss: 0.02295 | val_0_rmse: 0.18992 | val_1_rmse: 0.18668 |  0:00:02s
epoch 17 | loss: 0.02244 | val_0_rmse: 0.19706 | val_1_rmse: 0.19527 |  0:00:02s
epoch 18 | loss: 0.02183 | val_0_rmse: 0.20628 | val_1_rmse: 0.20534 |  0:00:02s
epoch 19 | loss: 0.02234 | val_0_rmse: 0.1966  | val_1_rmse: 0.19313 |  0:00:02s
epoch 20 | loss: 0.02113 | val_0_rmse: 0.18842 | val_1_rmse: 0.18245 |  0:00:02s
epoch 21 | loss: 0.02216 | val_0_rmse: 0.18946 | val_1_rmse: 0.18413 |  0:00:02s
epoch 22 | loss: 0.02047 | val_0_rmse: 0.19829 | val_1_rmse: 0.1957  |  0:00:03s
epoch 23 | loss: 0.02186 | val_0_rmse: 0.193   | val_1_rmse: 0.18959 |  0:00:03s
epoch 24 | loss: 0.02083 | val_0_rmse: 0.1839  | val_1_rmse: 0.17798 |  0:00:03s
epoch 25 | loss: 0.02127 | val_0_rmse: 0.18734 | val_1_rmse: 0.18245 |  0:00:03s
epoch 26 | loss: 0.02047 | val_0_rmse: 0.19723 | val_1_rmse: 0.19415 |  0:00:03s
epoch 27 | loss: 0.02109 | val_0_rmse: 0.18867 | val_1_rmse: 0.18324 |  0:00:03s
epoch 28 | loss: 0.02086 | val_0_rmse: 0.18438 | val_1_rmse: 0.17777 |  0:00:03s
epoch 29 | loss: 0.02086 | val_0_rmse: 0.18773 | val_1_rmse: 0.18342 |  0:00:03s
epoch 30 | loss: 0.0203  | val_0_rmse: 0.19292 | val_1_rmse: 0.19036 |  0:00:04s
epoch 31 | loss: 0.02126 | val_0_rmse: 0.18427 | val_1_rmse: 0.17932 |  0:00:04s
epoch 32 | loss: 0.02037 | val_0_rmse: 0.18174 | val_1_rmse: 0.17465 |  0:00:04s
epoch 33 | loss: 0.02059 | val_0_rmse: 0.18511 | val_1_rmse: 0.17903 |  0:00:04s

Early stopping occured at epoch 33 with best_epoch = 3 and best_val_1_rmse = 0.16854
Best weights from best epoch are automatically used!
ended training at: 06:22:32
Feature importance:
[('Area', 0.1367971687765314), ('Baths', 0.11152799844210194), ('Beds', 0.25513747311813373), ('Latitude', 0.1990340611868969), ('Longitude', 0.011487474946997278), ('Month', 0.21116274433273272), ('Year', 0.07485307919660601)]
Mean squared error is of 4200952264.734756
Mean absolute error:50486.40961662088
MAPE:0.48452565378807794
R2 score:0.36035195528264385
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:22:32
epoch 0  | loss: 1.01714 | val_0_rmse: 0.3336  | val_1_rmse: 0.34581 |  0:00:00s
epoch 1  | loss: 0.21432 | val_0_rmse: 0.22097 | val_1_rmse: 0.2131  |  0:00:00s
epoch 2  | loss: 0.06481 | val_0_rmse: 0.22525 | val_1_rmse: 0.22852 |  0:00:00s
epoch 3  | loss: 0.04375 | val_0_rmse: 0.21558 | val_1_rmse: 0.21836 |  0:00:00s
epoch 4  | loss: 0.03727 | val_0_rmse: 0.21227 | val_1_rmse: 0.21339 |  0:00:00s
epoch 5  | loss: 0.03356 | val_0_rmse: 0.21942 | val_1_rmse: 0.22051 |  0:00:00s
epoch 6  | loss: 0.02942 | val_0_rmse: 0.22103 | val_1_rmse: 0.22227 |  0:00:00s
epoch 7  | loss: 0.02695 | val_0_rmse: 0.21671 | val_1_rmse: 0.21706 |  0:00:01s
epoch 8  | loss: 0.02573 | val_0_rmse: 0.21693 | val_1_rmse: 0.21729 |  0:00:01s
epoch 9  | loss: 0.02563 | val_0_rmse: 0.22208 | val_1_rmse: 0.22301 |  0:00:01s
epoch 10 | loss: 0.02478 | val_0_rmse: 0.22395 | val_1_rmse: 0.2253  |  0:00:01s
epoch 11 | loss: 0.02403 | val_0_rmse: 0.2256  | val_1_rmse: 0.22756 |  0:00:01s
epoch 12 | loss: 0.02385 | val_0_rmse: 0.22543 | val_1_rmse: 0.22797 |  0:00:01s
epoch 13 | loss: 0.02408 | val_0_rmse: 0.22561 | val_1_rmse: 0.22894 |  0:00:01s
epoch 14 | loss: 0.02352 | val_0_rmse: 0.22201 | val_1_rmse: 0.22543 |  0:00:01s
epoch 15 | loss: 0.02373 | val_0_rmse: 0.21418 | val_1_rmse: 0.21669 |  0:00:02s
epoch 16 | loss: 0.02293 | val_0_rmse: 0.21357 | val_1_rmse: 0.21611 |  0:00:02s
epoch 17 | loss: 0.02332 | val_0_rmse: 0.21658 | val_1_rmse: 0.22013 |  0:00:02s
epoch 18 | loss: 0.02282 | val_0_rmse: 0.21288 | val_1_rmse: 0.21612 |  0:00:02s
epoch 19 | loss: 0.02276 | val_0_rmse: 0.21361 | val_1_rmse: 0.21713 |  0:00:02s
epoch 20 | loss: 0.02286 | val_0_rmse: 0.21043 | val_1_rmse: 0.21379 |  0:00:02s
epoch 21 | loss: 0.02306 | val_0_rmse: 0.2033  | val_1_rmse: 0.20511 |  0:00:02s
epoch 22 | loss: 0.0231  | val_0_rmse: 0.20333 | val_1_rmse: 0.20541 |  0:00:03s
epoch 23 | loss: 0.02234 | val_0_rmse: 0.20764 | val_1_rmse: 0.21064 |  0:00:03s
epoch 24 | loss: 0.02292 | val_0_rmse: 0.20173 | val_1_rmse: 0.20361 |  0:00:03s
epoch 25 | loss: 0.0221  | val_0_rmse: 0.19732 | val_1_rmse: 0.198   |  0:00:03s
epoch 26 | loss: 0.02277 | val_0_rmse: 0.2069  | val_1_rmse: 0.2098  |  0:00:03s
epoch 27 | loss: 0.02227 | val_0_rmse: 0.20205 | val_1_rmse: 0.20431 |  0:00:03s
epoch 28 | loss: 0.02198 | val_0_rmse: 0.1981  | val_1_rmse: 0.19993 |  0:00:03s
epoch 29 | loss: 0.02321 | val_0_rmse: 0.20214 | val_1_rmse: 0.2046  |  0:00:03s
epoch 30 | loss: 0.02252 | val_0_rmse: 0.19571 | val_1_rmse: 0.19706 |  0:00:04s
epoch 31 | loss: 0.02189 | val_0_rmse: 0.19824 | val_1_rmse: 0.20018 |  0:00:04s
epoch 32 | loss: 0.02147 | val_0_rmse: 0.19939 | val_1_rmse: 0.20169 |  0:00:04s
epoch 33 | loss: 0.02107 | val_0_rmse: 0.19507 | val_1_rmse: 0.19642 |  0:00:04s
epoch 34 | loss: 0.02092 | val_0_rmse: 0.19903 | val_1_rmse: 0.20167 |  0:00:04s
epoch 35 | loss: 0.02156 | val_0_rmse: 0.1937  | val_1_rmse: 0.19518 |  0:00:04s
epoch 36 | loss: 0.0214  | val_0_rmse: 0.19196 | val_1_rmse: 0.19286 |  0:00:04s
epoch 37 | loss: 0.02142 | val_0_rmse: 0.19747 | val_1_rmse: 0.20039 |  0:00:05s
epoch 38 | loss: 0.02109 | val_0_rmse: 0.19259 | val_1_rmse: 0.19468 |  0:00:05s
epoch 39 | loss: 0.02083 | val_0_rmse: 0.18952 | val_1_rmse: 0.19105 |  0:00:05s
epoch 40 | loss: 0.02115 | val_0_rmse: 0.19042 | val_1_rmse: 0.19296 |  0:00:05s
epoch 41 | loss: 0.02062 | val_0_rmse: 0.18618 | val_1_rmse: 0.18791 |  0:00:05s
epoch 42 | loss: 0.02154 | val_0_rmse: 0.18546 | val_1_rmse: 0.18697 |  0:00:05s
epoch 43 | loss: 0.02057 | val_0_rmse: 0.18949 | val_1_rmse: 0.19207 |  0:00:05s
epoch 44 | loss: 0.0206  | val_0_rmse: 0.193   | val_1_rmse: 0.19629 |  0:00:05s
epoch 45 | loss: 0.02093 | val_0_rmse: 0.18532 | val_1_rmse: 0.18745 |  0:00:06s
epoch 46 | loss: 0.02056 | val_0_rmse: 0.1836  | val_1_rmse: 0.18543 |  0:00:06s
epoch 47 | loss: 0.02115 | val_0_rmse: 0.18516 | val_1_rmse: 0.18718 |  0:00:06s
epoch 48 | loss: 0.02006 | val_0_rmse: 0.18578 | val_1_rmse: 0.18778 |  0:00:06s
epoch 49 | loss: 0.02067 | val_0_rmse: 0.19118 | val_1_rmse: 0.19431 |  0:00:06s
epoch 50 | loss: 0.02113 | val_0_rmse: 0.18349 | val_1_rmse: 0.18503 |  0:00:06s
epoch 51 | loss: 0.02077 | val_0_rmse: 0.18203 | val_1_rmse: 0.18352 |  0:00:06s
epoch 52 | loss: 0.02049 | val_0_rmse: 0.18841 | val_1_rmse: 0.19203 |  0:00:07s
epoch 53 | loss: 0.02122 | val_0_rmse: 0.17703 | val_1_rmse: 0.17746 |  0:00:07s
epoch 54 | loss: 0.0211  | val_0_rmse: 0.1773  | val_1_rmse: 0.17836 |  0:00:07s
epoch 55 | loss: 0.02082 | val_0_rmse: 0.18204 | val_1_rmse: 0.18471 |  0:00:07s
epoch 56 | loss: 0.0208  | val_0_rmse: 0.17736 | val_1_rmse: 0.17715 |  0:00:07s
epoch 57 | loss: 0.02071 | val_0_rmse: 0.18447 | val_1_rmse: 0.18683 |  0:00:07s
epoch 58 | loss: 0.02093 | val_0_rmse: 0.18278 | val_1_rmse: 0.18506 |  0:00:07s
epoch 59 | loss: 0.02021 | val_0_rmse: 0.17106 | val_1_rmse: 0.16924 |  0:00:08s
epoch 60 | loss: 0.02191 | val_0_rmse: 0.17186 | val_1_rmse: 0.17248 |  0:00:08s
epoch 61 | loss: 0.02069 | val_0_rmse: 0.17516 | val_1_rmse: 0.17706 |  0:00:08s
epoch 62 | loss: 0.02056 | val_0_rmse: 0.17136 | val_1_rmse: 0.17224 |  0:00:08s
epoch 63 | loss: 0.02087 | val_0_rmse: 0.17368 | val_1_rmse: 0.176   |  0:00:08s
epoch 64 | loss: 0.02025 | val_0_rmse: 0.17297 | val_1_rmse: 0.17562 |  0:00:08s
epoch 65 | loss: 0.0203  | val_0_rmse: 0.16692 | val_1_rmse: 0.16791 |  0:00:08s
epoch 66 | loss: 0.02065 | val_0_rmse: 0.16899 | val_1_rmse: 0.17091 |  0:00:08s
epoch 67 | loss: 0.01941 | val_0_rmse: 0.17441 | val_1_rmse: 0.17783 |  0:00:09s
epoch 68 | loss: 0.01976 | val_0_rmse: 0.1743  | val_1_rmse: 0.17805 |  0:00:09s
epoch 69 | loss: 0.01976 | val_0_rmse: 0.16755 | val_1_rmse: 0.17007 |  0:00:09s
epoch 70 | loss: 0.01978 | val_0_rmse: 0.16557 | val_1_rmse: 0.16827 |  0:00:09s
epoch 71 | loss: 0.02032 | val_0_rmse: 0.16455 | val_1_rmse: 0.16684 |  0:00:09s
epoch 72 | loss: 0.01974 | val_0_rmse: 0.16523 | val_1_rmse: 0.16696 |  0:00:09s
epoch 73 | loss: 0.01912 | val_0_rmse: 0.16866 | val_1_rmse: 0.17153 |  0:00:09s
epoch 74 | loss: 0.01923 | val_0_rmse: 0.16715 | val_1_rmse: 0.16883 |  0:00:10s
epoch 75 | loss: 0.02016 | val_0_rmse: 0.16504 | val_1_rmse: 0.16617 |  0:00:10s
epoch 76 | loss: 0.01977 | val_0_rmse: 0.16379 | val_1_rmse: 0.16578 |  0:00:10s
epoch 77 | loss: 0.01898 | val_0_rmse: 0.16184 | val_1_rmse: 0.16141 |  0:00:10s
epoch 78 | loss: 0.01993 | val_0_rmse: 0.16294 | val_1_rmse: 0.16345 |  0:00:10s
epoch 79 | loss: 0.01886 | val_0_rmse: 0.16452 | val_1_rmse: 0.16631 |  0:00:10s
epoch 80 | loss: 0.01959 | val_0_rmse: 0.16437 | val_1_rmse: 0.1653  |  0:00:10s
epoch 81 | loss: 0.01985 | val_0_rmse: 0.16621 | val_1_rmse: 0.16817 |  0:00:10s
epoch 82 | loss: 0.01964 | val_0_rmse: 0.16531 | val_1_rmse: 0.16691 |  0:00:11s
epoch 83 | loss: 0.01894 | val_0_rmse: 0.16361 | val_1_rmse: 0.16435 |  0:00:11s
epoch 84 | loss: 0.01897 | val_0_rmse: 0.16625 | val_1_rmse: 0.16913 |  0:00:11s
epoch 85 | loss: 0.01936 | val_0_rmse: 0.16082 | val_1_rmse: 0.16148 |  0:00:11s
epoch 86 | loss: 0.01882 | val_0_rmse: 0.15908 | val_1_rmse: 0.15816 |  0:00:11s
epoch 87 | loss: 0.01978 | val_0_rmse: 0.16136 | val_1_rmse: 0.16263 |  0:00:11s
epoch 88 | loss: 0.0196  | val_0_rmse: 0.15818 | val_1_rmse: 0.15689 |  0:00:11s
epoch 89 | loss: 0.0196  | val_0_rmse: 0.1577  | val_1_rmse: 0.1578  |  0:00:12s
epoch 90 | loss: 0.01925 | val_0_rmse: 0.15805 | val_1_rmse: 0.15904 |  0:00:12s
epoch 91 | loss: 0.01896 | val_0_rmse: 0.15865 | val_1_rmse: 0.15698 |  0:00:12s
epoch 92 | loss: 0.0197  | val_0_rmse: 0.15845 | val_1_rmse: 0.1593  |  0:00:12s
epoch 93 | loss: 0.01928 | val_0_rmse: 0.15693 | val_1_rmse: 0.15643 |  0:00:12s
epoch 94 | loss: 0.0194  | val_0_rmse: 0.15863 | val_1_rmse: 0.16081 |  0:00:12s
epoch 95 | loss: 0.01961 | val_0_rmse: 0.15568 | val_1_rmse: 0.15645 |  0:00:12s
epoch 96 | loss: 0.01863 | val_0_rmse: 0.15445 | val_1_rmse: 0.15231 |  0:00:12s
epoch 97 | loss: 0.01975 | val_0_rmse: 0.15567 | val_1_rmse: 0.15824 |  0:00:13s
epoch 98 | loss: 0.01858 | val_0_rmse: 0.15295 | val_1_rmse: 0.15194 |  0:00:13s
epoch 99 | loss: 0.01964 | val_0_rmse: 0.15349 | val_1_rmse: 0.15447 |  0:00:13s
epoch 100| loss: 0.01902 | val_0_rmse: 0.15562 | val_1_rmse: 0.15779 |  0:00:13s
epoch 101| loss: 0.01871 | val_0_rmse: 0.15384 | val_1_rmse: 0.15108 |  0:00:13s
epoch 102| loss: 0.0192  | val_0_rmse: 0.15283 | val_1_rmse: 0.15313 |  0:00:13s
epoch 103| loss: 0.01932 | val_0_rmse: 0.15269 | val_1_rmse: 0.14977 |  0:00:13s
epoch 104| loss: 0.019   | val_0_rmse: 0.15605 | val_1_rmse: 0.15052 |  0:00:14s
epoch 105| loss: 0.01914 | val_0_rmse: 0.15238 | val_1_rmse: 0.14965 |  0:00:14s
epoch 106| loss: 0.01832 | val_0_rmse: 0.15737 | val_1_rmse: 0.15192 |  0:00:14s
epoch 107| loss: 0.01815 | val_0_rmse: 0.15854 | val_1_rmse: 0.15287 |  0:00:14s
epoch 108| loss: 0.01851 | val_0_rmse: 0.15448 | val_1_rmse: 0.15078 |  0:00:14s
epoch 109| loss: 0.01855 | val_0_rmse: 0.15645 | val_1_rmse: 0.15135 |  0:00:14s
epoch 110| loss: 0.0185  | val_0_rmse: 0.15419 | val_1_rmse: 0.14965 |  0:00:14s
epoch 111| loss: 0.01827 | val_0_rmse: 0.14981 | val_1_rmse: 0.14715 |  0:00:14s
epoch 112| loss: 0.01778 | val_0_rmse: 0.15091 | val_1_rmse: 0.14641 |  0:00:15s
epoch 113| loss: 0.01825 | val_0_rmse: 0.14976 | val_1_rmse: 0.14845 |  0:00:15s
epoch 114| loss: 0.01805 | val_0_rmse: 0.14956 | val_1_rmse: 0.14803 |  0:00:15s
epoch 115| loss: 0.01897 | val_0_rmse: 0.15012 | val_1_rmse: 0.14618 |  0:00:15s
epoch 116| loss: 0.01869 | val_0_rmse: 0.14903 | val_1_rmse: 0.14615 |  0:00:15s
epoch 117| loss: 0.01821 | val_0_rmse: 0.14745 | val_1_rmse: 0.14692 |  0:00:15s
epoch 118| loss: 0.01841 | val_0_rmse: 0.16561 | val_1_rmse: 0.15691 |  0:00:15s
epoch 119| loss: 0.01931 | val_0_rmse: 0.16357 | val_1_rmse: 0.15586 |  0:00:16s
epoch 120| loss: 0.01893 | val_0_rmse: 0.15753 | val_1_rmse: 0.1514  |  0:00:16s
epoch 121| loss: 0.01808 | val_0_rmse: 0.1681  | val_1_rmse: 0.15709 |  0:00:16s
epoch 122| loss: 0.01913 | val_0_rmse: 0.14939 | val_1_rmse: 0.14599 |  0:00:16s
epoch 123| loss: 0.01959 | val_0_rmse: 0.15294 | val_1_rmse: 0.14624 |  0:00:16s
epoch 124| loss: 0.01942 | val_0_rmse: 0.15358 | val_1_rmse: 0.14621 |  0:00:16s
epoch 125| loss: 0.01788 | val_0_rmse: 0.14565 | val_1_rmse: 0.14309 |  0:00:16s
epoch 126| loss: 0.01808 | val_0_rmse: 0.1609  | val_1_rmse: 0.15118 |  0:00:16s
epoch 127| loss: 0.01853 | val_0_rmse: 0.15177 | val_1_rmse: 0.14558 |  0:00:17s
epoch 128| loss: 0.0181  | val_0_rmse: 0.15114 | val_1_rmse: 0.14535 |  0:00:17s
epoch 129| loss: 0.01827 | val_0_rmse: 0.14965 | val_1_rmse: 0.14341 |  0:00:17s
epoch 130| loss: 0.01775 | val_0_rmse: 0.14497 | val_1_rmse: 0.14201 |  0:00:17s
epoch 131| loss: 0.01732 | val_0_rmse: 0.14757 | val_1_rmse: 0.14096 |  0:00:17s
epoch 132| loss: 0.0177  | val_0_rmse: 0.14826 | val_1_rmse: 0.14319 |  0:00:17s
epoch 133| loss: 0.01789 | val_0_rmse: 0.15191 | val_1_rmse: 0.14544 |  0:00:17s
epoch 134| loss: 0.0176  | val_0_rmse: 0.16702 | val_1_rmse: 0.15622 |  0:00:18s
epoch 135| loss: 0.01718 | val_0_rmse: 0.16142 | val_1_rmse: 0.15188 |  0:00:18s
epoch 136| loss: 0.01743 | val_0_rmse: 0.16888 | val_1_rmse: 0.15813 |  0:00:18s
epoch 137| loss: 0.01714 | val_0_rmse: 0.16867 | val_1_rmse: 0.15733 |  0:00:18s
epoch 138| loss: 0.01657 | val_0_rmse: 0.14711 | val_1_rmse: 0.14108 |  0:00:18s
epoch 139| loss: 0.01743 | val_0_rmse: 0.14952 | val_1_rmse: 0.1419  |  0:00:18s
epoch 140| loss: 0.01701 | val_0_rmse: 0.15869 | val_1_rmse: 0.14895 |  0:00:18s
epoch 141| loss: 0.01702 | val_0_rmse: 0.16104 | val_1_rmse: 0.15163 |  0:00:18s
epoch 142| loss: 0.01604 | val_0_rmse: 0.16951 | val_1_rmse: 0.1589  |  0:00:19s
epoch 143| loss: 0.01615 | val_0_rmse: 0.16818 | val_1_rmse: 0.15777 |  0:00:19s
epoch 144| loss: 0.01592 | val_0_rmse: 0.14511 | val_1_rmse: 0.14039 |  0:00:19s
epoch 145| loss: 0.01634 | val_0_rmse: 0.15234 | val_1_rmse: 0.1441  |  0:00:19s
epoch 146| loss: 0.01603 | val_0_rmse: 0.15219 | val_1_rmse: 0.14433 |  0:00:19s
epoch 147| loss: 0.01625 | val_0_rmse: 0.14404 | val_1_rmse: 0.13961 |  0:00:19s
epoch 148| loss: 0.01608 | val_0_rmse: 0.15212 | val_1_rmse: 0.1445  |  0:00:19s
epoch 149| loss: 0.01595 | val_0_rmse: 0.14995 | val_1_rmse: 0.14368 |  0:00:20s
Stop training because you reached max_epochs = 150 with best_epoch = 147 and best_val_1_rmse = 0.13961
Best weights from best epoch are automatically used!
ended training at: 06:22:52
Feature importance:
[('Area', 0.29795965995320955), ('Baths', 0.10925139695059903), ('Beds', 0.040357196345045906), ('Latitude', 0.42954916346714656), ('Longitude', 0.014253648185300328), ('Month', 0.09926197402276157), ('Year', 0.009366961075937)]
Mean squared error is of 2860318390.7563453
Mean absolute error:40787.28689210165
MAPE:0.3661673171321929
R2 score:0.6078788574012896
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:22:52
epoch 0  | loss: 0.97918 | val_0_rmse: 0.28771 | val_1_rmse: 0.27726 |  0:00:00s
epoch 1  | loss: 0.17585 | val_0_rmse: 0.22245 | val_1_rmse: 0.21528 |  0:00:00s
epoch 2  | loss: 0.07423 | val_0_rmse: 0.21856 | val_1_rmse: 0.2167  |  0:00:00s
epoch 3  | loss: 0.0515  | val_0_rmse: 0.23204 | val_1_rmse: 0.22823 |  0:00:00s
epoch 4  | loss: 0.03812 | val_0_rmse: 0.2404  | val_1_rmse: 0.23499 |  0:00:00s
epoch 5  | loss: 0.03325 | val_0_rmse: 0.25354 | val_1_rmse: 0.24718 |  0:00:00s
epoch 6  | loss: 0.03282 | val_0_rmse: 0.26122 | val_1_rmse: 0.253   |  0:00:00s
epoch 7  | loss: 0.02962 | val_0_rmse: 0.2581  | val_1_rmse: 0.25025 |  0:00:01s
epoch 8  | loss: 0.02773 | val_0_rmse: 0.2614  | val_1_rmse: 0.25485 |  0:00:01s
epoch 9  | loss: 0.02711 | val_0_rmse: 0.26595 | val_1_rmse: 0.25872 |  0:00:01s
epoch 10 | loss: 0.02608 | val_0_rmse: 0.27305 | val_1_rmse: 0.26569 |  0:00:01s
epoch 11 | loss: 0.02406 | val_0_rmse: 0.2899  | val_1_rmse: 0.28165 |  0:00:01s
epoch 12 | loss: 0.02291 | val_0_rmse: 0.28914 | val_1_rmse: 0.28029 |  0:00:01s
epoch 13 | loss: 0.02263 | val_0_rmse: 0.27697 | val_1_rmse: 0.26806 |  0:00:01s
epoch 14 | loss: 0.02391 | val_0_rmse: 0.28998 | val_1_rmse: 0.28181 |  0:00:02s
epoch 15 | loss: 0.02321 | val_0_rmse: 0.32215 | val_1_rmse: 0.31434 |  0:00:02s
epoch 16 | loss: 0.02262 | val_0_rmse: 0.32703 | val_1_rmse: 0.3193  |  0:00:02s
epoch 17 | loss: 0.02294 | val_0_rmse: 0.29994 | val_1_rmse: 0.29355 |  0:00:02s
epoch 18 | loss: 0.02168 | val_0_rmse: 0.29359 | val_1_rmse: 0.2878  |  0:00:02s
epoch 19 | loss: 0.02124 | val_0_rmse: 0.30769 | val_1_rmse: 0.3018  |  0:00:02s
epoch 20 | loss: 0.02086 | val_0_rmse: 0.30226 | val_1_rmse: 0.2963  |  0:00:02s
epoch 21 | loss: 0.02127 | val_0_rmse: 0.27746 | val_1_rmse: 0.27173 |  0:00:02s
epoch 22 | loss: 0.02072 | val_0_rmse: 0.27084 | val_1_rmse: 0.26491 |  0:00:03s
epoch 23 | loss: 0.0199  | val_0_rmse: 0.27157 | val_1_rmse: 0.26528 |  0:00:03s
epoch 24 | loss: 0.0204  | val_0_rmse: 0.27199 | val_1_rmse: 0.26564 |  0:00:03s
epoch 25 | loss: 0.02066 | val_0_rmse: 0.25955 | val_1_rmse: 0.25378 |  0:00:03s
epoch 26 | loss: 0.02103 | val_0_rmse: 0.24649 | val_1_rmse: 0.24113 |  0:00:03s
epoch 27 | loss: 0.02063 | val_0_rmse: 0.2496  | val_1_rmse: 0.24408 |  0:00:03s
epoch 28 | loss: 0.0196  | val_0_rmse: 0.26009 | val_1_rmse: 0.25406 |  0:00:03s
epoch 29 | loss: 0.02036 | val_0_rmse: 0.23684 | val_1_rmse: 0.23141 |  0:00:03s
epoch 30 | loss: 0.02017 | val_0_rmse: 0.23021 | val_1_rmse: 0.22499 |  0:00:04s
epoch 31 | loss: 0.02024 | val_0_rmse: 0.24351 | val_1_rmse: 0.23806 |  0:00:04s

Early stopping occured at epoch 31 with best_epoch = 1 and best_val_1_rmse = 0.21528
Best weights from best epoch are automatically used!
ended training at: 06:22:56
Feature importance:
[('Area', 0.07938031476043618), ('Baths', 0.0008013723536135058), ('Beds', 0.3869060015112657), ('Latitude', 0.06574206598260975), ('Longitude', 0.18358854597187163), ('Month', 0.23152394521366798), ('Year', 0.052057754206535245)]
Mean squared error is of 7293106505.645879
Mean absolute error:64664.67240493003
MAPE:0.5855151612410906
R2 score:-0.07632577089776649
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:22:56
epoch 0  | loss: 0.93994 | val_0_rmse: 0.6316  | val_1_rmse: 0.64674 |  0:00:00s
epoch 1  | loss: 0.15526 | val_0_rmse: 0.58969 | val_1_rmse: 0.60842 |  0:00:00s
epoch 2  | loss: 0.09015 | val_0_rmse: 0.37488 | val_1_rmse: 0.38206 |  0:00:00s
epoch 3  | loss: 0.05041 | val_0_rmse: 0.30164 | val_1_rmse: 0.30445 |  0:00:00s
epoch 4  | loss: 0.04884 | val_0_rmse: 0.21221 | val_1_rmse: 0.20342 |  0:00:00s
epoch 5  | loss: 0.0351  | val_0_rmse: 0.20237 | val_1_rmse: 0.19288 |  0:00:00s
epoch 6  | loss: 0.03587 | val_0_rmse: 0.19692 | val_1_rmse: 0.18856 |  0:00:00s
epoch 7  | loss: 0.0336  | val_0_rmse: 0.19703 | val_1_rmse: 0.18718 |  0:00:01s
epoch 8  | loss: 0.02766 | val_0_rmse: 0.19838 | val_1_rmse: 0.18909 |  0:00:01s
epoch 9  | loss: 0.02678 | val_0_rmse: 0.2034  | val_1_rmse: 0.19438 |  0:00:01s
epoch 10 | loss: 0.02662 | val_0_rmse: 0.20065 | val_1_rmse: 0.18865 |  0:00:01s
epoch 11 | loss: 0.02444 | val_0_rmse: 0.20271 | val_1_rmse: 0.18818 |  0:00:01s
epoch 12 | loss: 0.02488 | val_0_rmse: 0.20385 | val_1_rmse: 0.18934 |  0:00:01s
epoch 13 | loss: 0.02412 | val_0_rmse: 0.20289 | val_1_rmse: 0.1908  |  0:00:01s
epoch 14 | loss: 0.02336 | val_0_rmse: 0.20718 | val_1_rmse: 0.19566 |  0:00:02s
epoch 15 | loss: 0.02228 | val_0_rmse: 0.20611 | val_1_rmse: 0.19348 |  0:00:02s
epoch 16 | loss: 0.02326 | val_0_rmse: 0.20672 | val_1_rmse: 0.19425 |  0:00:02s
epoch 17 | loss: 0.0225  | val_0_rmse: 0.20982 | val_1_rmse: 0.19869 |  0:00:02s
epoch 18 | loss: 0.02233 | val_0_rmse: 0.20612 | val_1_rmse: 0.19435 |  0:00:02s
epoch 19 | loss: 0.02108 | val_0_rmse: 0.20299 | val_1_rmse: 0.18961 |  0:00:02s
epoch 20 | loss: 0.02156 | val_0_rmse: 0.20342 | val_1_rmse: 0.19046 |  0:00:02s
epoch 21 | loss: 0.02176 | val_0_rmse: 0.20611 | val_1_rmse: 0.194   |  0:00:02s
epoch 22 | loss: 0.02072 | val_0_rmse: 0.20455 | val_1_rmse: 0.19161 |  0:00:03s
epoch 23 | loss: 0.02062 | val_0_rmse: 0.20649 | val_1_rmse: 0.19366 |  0:00:03s
epoch 24 | loss: 0.02089 | val_0_rmse: 0.20692 | val_1_rmse: 0.19419 |  0:00:03s
epoch 25 | loss: 0.02062 | val_0_rmse: 0.20829 | val_1_rmse: 0.19585 |  0:00:03s
epoch 26 | loss: 0.0207  | val_0_rmse: 0.20807 | val_1_rmse: 0.19553 |  0:00:03s
epoch 27 | loss: 0.02082 | val_0_rmse: 0.20753 | val_1_rmse: 0.19401 |  0:00:03s
epoch 28 | loss: 0.02134 | val_0_rmse: 0.20711 | val_1_rmse: 0.19375 |  0:00:03s
epoch 29 | loss: 0.01984 | val_0_rmse: 0.20635 | val_1_rmse: 0.19463 |  0:00:03s
epoch 30 | loss: 0.02114 | val_0_rmse: 0.20539 | val_1_rmse: 0.1926  |  0:00:04s
epoch 31 | loss: 0.02057 | val_0_rmse: 0.20516 | val_1_rmse: 0.19177 |  0:00:04s
epoch 32 | loss: 0.02077 | val_0_rmse: 0.20383 | val_1_rmse: 0.19112 |  0:00:04s
epoch 33 | loss: 0.01999 | val_0_rmse: 0.20361 | val_1_rmse: 0.18987 |  0:00:04s
epoch 34 | loss: 0.02004 | val_0_rmse: 0.20486 | val_1_rmse: 0.18987 |  0:00:04s
epoch 35 | loss: 0.0203  | val_0_rmse: 0.20353 | val_1_rmse: 0.18848 |  0:00:04s
epoch 36 | loss: 0.02018 | val_0_rmse: 0.20305 | val_1_rmse: 0.18698 |  0:00:04s
epoch 37 | loss: 0.02005 | val_0_rmse: 0.20335 | val_1_rmse: 0.18657 |  0:00:05s
epoch 38 | loss: 0.02008 | val_0_rmse: 0.20148 | val_1_rmse: 0.18511 |  0:00:05s
epoch 39 | loss: 0.01971 | val_0_rmse: 0.20086 | val_1_rmse: 0.18447 |  0:00:05s
epoch 40 | loss: 0.02007 | val_0_rmse: 0.20104 | val_1_rmse: 0.18427 |  0:00:05s
epoch 41 | loss: 0.01968 | val_0_rmse: 0.20089 | val_1_rmse: 0.18407 |  0:00:05s
epoch 42 | loss: 0.01927 | val_0_rmse: 0.20091 | val_1_rmse: 0.18328 |  0:00:05s
epoch 43 | loss: 0.0196  | val_0_rmse: 0.20005 | val_1_rmse: 0.18214 |  0:00:05s
epoch 44 | loss: 0.01981 | val_0_rmse: 0.19961 | val_1_rmse: 0.18136 |  0:00:06s
epoch 45 | loss: 0.01942 | val_0_rmse: 0.20161 | val_1_rmse: 0.18211 |  0:00:06s
epoch 46 | loss: 0.01927 | val_0_rmse: 0.19786 | val_1_rmse: 0.18058 |  0:00:06s
epoch 47 | loss: 0.01997 | val_0_rmse: 0.19859 | val_1_rmse: 0.18063 |  0:00:06s
epoch 48 | loss: 0.0193  | val_0_rmse: 0.20424 | val_1_rmse: 0.18546 |  0:00:06s
epoch 49 | loss: 0.02025 | val_0_rmse: 0.19752 | val_1_rmse: 0.18103 |  0:00:06s
epoch 50 | loss: 0.01975 | val_0_rmse: 0.19888 | val_1_rmse: 0.18022 |  0:00:06s
epoch 51 | loss: 0.01911 | val_0_rmse: 0.20065 | val_1_rmse: 0.18046 |  0:00:06s
epoch 52 | loss: 0.02053 | val_0_rmse: 0.19389 | val_1_rmse: 0.17644 |  0:00:07s
epoch 53 | loss: 0.01922 | val_0_rmse: 0.19609 | val_1_rmse: 0.17637 |  0:00:07s
epoch 54 | loss: 0.01929 | val_0_rmse: 0.19686 | val_1_rmse: 0.17647 |  0:00:07s
epoch 55 | loss: 0.01891 | val_0_rmse: 0.19273 | val_1_rmse: 0.17678 |  0:00:07s
epoch 56 | loss: 0.01939 | val_0_rmse: 0.1941  | val_1_rmse: 0.17518 |  0:00:07s
epoch 57 | loss: 0.01917 | val_0_rmse: 0.19772 | val_1_rmse: 0.17718 |  0:00:07s
epoch 58 | loss: 0.01988 | val_0_rmse: 0.1932  | val_1_rmse: 0.17753 |  0:00:07s
epoch 59 | loss: 0.02019 | val_0_rmse: 0.19175 | val_1_rmse: 0.17426 |  0:00:08s
epoch 60 | loss: 0.01935 | val_0_rmse: 0.19408 | val_1_rmse: 0.17427 |  0:00:08s
epoch 61 | loss: 0.01928 | val_0_rmse: 0.18764 | val_1_rmse: 0.17267 |  0:00:08s
epoch 62 | loss: 0.01974 | val_0_rmse: 0.18716 | val_1_rmse: 0.17058 |  0:00:08s
epoch 63 | loss: 0.01901 | val_0_rmse: 0.18761 | val_1_rmse: 0.17095 |  0:00:08s
epoch 64 | loss: 0.01922 | val_0_rmse: 0.18239 | val_1_rmse: 0.17178 |  0:00:08s
epoch 65 | loss: 0.0202  | val_0_rmse: 0.18091 | val_1_rmse: 0.16821 |  0:00:08s
epoch 66 | loss: 0.01943 | val_0_rmse: 0.18262 | val_1_rmse: 0.1681  |  0:00:08s
epoch 67 | loss: 0.01872 | val_0_rmse: 0.18077 | val_1_rmse: 0.17087 |  0:00:09s
epoch 68 | loss: 0.02001 | val_0_rmse: 0.18047 | val_1_rmse: 0.16776 |  0:00:09s
epoch 69 | loss: 0.01907 | val_0_rmse: 0.18628 | val_1_rmse: 0.17011 |  0:00:09s
epoch 70 | loss: 0.02052 | val_0_rmse: 0.1817  | val_1_rmse: 0.16906 |  0:00:09s
epoch 71 | loss: 0.01924 | val_0_rmse: 0.18092 | val_1_rmse: 0.17004 |  0:00:09s
epoch 72 | loss: 0.01921 | val_0_rmse: 0.18383 | val_1_rmse: 0.16843 |  0:00:09s
epoch 73 | loss: 0.01942 | val_0_rmse: 0.17859 | val_1_rmse: 0.16478 |  0:00:09s
epoch 74 | loss: 0.01897 | val_0_rmse: 0.17526 | val_1_rmse: 0.1645  |  0:00:10s
epoch 75 | loss: 0.01923 | val_0_rmse: 0.17465 | val_1_rmse: 0.16211 |  0:00:10s
epoch 76 | loss: 0.01868 | val_0_rmse: 0.17372 | val_1_rmse: 0.16167 |  0:00:10s
epoch 77 | loss: 0.01863 | val_0_rmse: 0.17083 | val_1_rmse: 0.1627  |  0:00:10s
epoch 78 | loss: 0.01858 | val_0_rmse: 0.17072 | val_1_rmse: 0.16103 |  0:00:10s
epoch 79 | loss: 0.01867 | val_0_rmse: 0.1698  | val_1_rmse: 0.15997 |  0:00:10s
epoch 80 | loss: 0.01856 | val_0_rmse: 0.1676  | val_1_rmse: 0.16036 |  0:00:10s
epoch 81 | loss: 0.01889 | val_0_rmse: 0.16734 | val_1_rmse: 0.1598  |  0:00:11s
epoch 82 | loss: 0.01892 | val_0_rmse: 0.16874 | val_1_rmse: 0.16038 |  0:00:11s
epoch 83 | loss: 0.01839 | val_0_rmse: 0.16839 | val_1_rmse: 0.16127 |  0:00:11s
epoch 84 | loss: 0.01864 | val_0_rmse: 0.16907 | val_1_rmse: 0.16253 |  0:00:11s
epoch 85 | loss: 0.01846 | val_0_rmse: 0.17278 | val_1_rmse: 0.16328 |  0:00:11s
epoch 86 | loss: 0.01918 | val_0_rmse: 0.16885 | val_1_rmse: 0.16176 |  0:00:11s
epoch 87 | loss: 0.01857 | val_0_rmse: 0.1672  | val_1_rmse: 0.16041 |  0:00:11s
epoch 88 | loss: 0.01903 | val_0_rmse: 0.16947 | val_1_rmse: 0.15954 |  0:00:11s
epoch 89 | loss: 0.01847 | val_0_rmse: 0.16662 | val_1_rmse: 0.15815 |  0:00:12s
epoch 90 | loss: 0.0182  | val_0_rmse: 0.16484 | val_1_rmse: 0.15757 |  0:00:12s
epoch 91 | loss: 0.01843 | val_0_rmse: 0.16462 | val_1_rmse: 0.15621 |  0:00:12s
epoch 92 | loss: 0.0185  | val_0_rmse: 0.16453 | val_1_rmse: 0.15607 |  0:00:12s
epoch 93 | loss: 0.01854 | val_0_rmse: 0.1636  | val_1_rmse: 0.15609 |  0:00:12s
epoch 94 | loss: 0.01817 | val_0_rmse: 0.16461 | val_1_rmse: 0.15673 |  0:00:12s
epoch 95 | loss: 0.01839 | val_0_rmse: 0.16353 | val_1_rmse: 0.15675 |  0:00:12s
epoch 96 | loss: 0.01836 | val_0_rmse: 0.16228 | val_1_rmse: 0.15687 |  0:00:13s
epoch 97 | loss: 0.01841 | val_0_rmse: 0.16235 | val_1_rmse: 0.15666 |  0:00:13s
epoch 98 | loss: 0.01829 | val_0_rmse: 0.16296 | val_1_rmse: 0.15713 |  0:00:13s
epoch 99 | loss: 0.01864 | val_0_rmse: 0.16434 | val_1_rmse: 0.15774 |  0:00:13s
epoch 100| loss: 0.01862 | val_0_rmse: 0.16302 | val_1_rmse: 0.1574  |  0:00:13s
epoch 101| loss: 0.01859 | val_0_rmse: 0.16485 | val_1_rmse: 0.15694 |  0:00:13s
epoch 102| loss: 0.01858 | val_0_rmse: 0.1625  | val_1_rmse: 0.15608 |  0:00:13s
epoch 103| loss: 0.01851 | val_0_rmse: 0.16357 | val_1_rmse: 0.15668 |  0:00:14s
epoch 104| loss: 0.01819 | val_0_rmse: 0.16871 | val_1_rmse: 0.15971 |  0:00:14s
epoch 105| loss: 0.0188  | val_0_rmse: 0.1623  | val_1_rmse: 0.15726 |  0:00:14s
epoch 106| loss: 0.01868 | val_0_rmse: 0.16218 | val_1_rmse: 0.15758 |  0:00:14s
epoch 107| loss: 0.01807 | val_0_rmse: 0.16732 | val_1_rmse: 0.15835 |  0:00:14s
epoch 108| loss: 0.01834 | val_0_rmse: 0.1669  | val_1_rmse: 0.15754 |  0:00:14s
epoch 109| loss: 0.01803 | val_0_rmse: 0.16356 | val_1_rmse: 0.15659 |  0:00:14s
epoch 110| loss: 0.01835 | val_0_rmse: 0.17044 | val_1_rmse: 0.15942 |  0:00:14s
epoch 111| loss: 0.0186  | val_0_rmse: 0.16684 | val_1_rmse: 0.15766 |  0:00:15s
epoch 112| loss: 0.01789 | val_0_rmse: 0.16184 | val_1_rmse: 0.15724 |  0:00:15s
epoch 113| loss: 0.01864 | val_0_rmse: 0.16429 | val_1_rmse: 0.15669 |  0:00:15s
epoch 114| loss: 0.01815 | val_0_rmse: 0.16388 | val_1_rmse: 0.15659 |  0:00:15s
epoch 115| loss: 0.01843 | val_0_rmse: 0.15996 | val_1_rmse: 0.15664 |  0:00:15s
epoch 116| loss: 0.0177  | val_0_rmse: 0.16177 | val_1_rmse: 0.15665 |  0:00:15s
epoch 117| loss: 0.01836 | val_0_rmse: 0.16218 | val_1_rmse: 0.15716 |  0:00:15s
epoch 118| loss: 0.01863 | val_0_rmse: 0.15926 | val_1_rmse: 0.15685 |  0:00:15s
epoch 119| loss: 0.01824 | val_0_rmse: 0.15901 | val_1_rmse: 0.15526 |  0:00:16s
epoch 120| loss: 0.01775 | val_0_rmse: 0.15865 | val_1_rmse: 0.15477 |  0:00:16s
epoch 121| loss: 0.01738 | val_0_rmse: 0.15738 | val_1_rmse: 0.15489 |  0:00:16s
epoch 122| loss: 0.01815 | val_0_rmse: 0.15796 | val_1_rmse: 0.15319 |  0:00:16s
epoch 123| loss: 0.01886 | val_0_rmse: 0.15517 | val_1_rmse: 0.15168 |  0:00:16s
epoch 124| loss: 0.0184  | val_0_rmse: 0.15375 | val_1_rmse: 0.15491 |  0:00:16s
epoch 125| loss: 0.01879 | val_0_rmse: 0.15425 | val_1_rmse: 0.15147 |  0:00:16s
epoch 126| loss: 0.01837 | val_0_rmse: 0.15496 | val_1_rmse: 0.15196 |  0:00:17s
epoch 127| loss: 0.01768 | val_0_rmse: 0.15217 | val_1_rmse: 0.15182 |  0:00:17s
epoch 128| loss: 0.01838 | val_0_rmse: 0.1519  | val_1_rmse: 0.15002 |  0:00:17s
epoch 129| loss: 0.01769 | val_0_rmse: 0.1525  | val_1_rmse: 0.15015 |  0:00:17s
epoch 130| loss: 0.01798 | val_0_rmse: 0.14841 | val_1_rmse: 0.15057 |  0:00:17s
epoch 131| loss: 0.01787 | val_0_rmse: 0.1473  | val_1_rmse: 0.14952 |  0:00:17s
epoch 132| loss: 0.01751 | val_0_rmse: 0.14723 | val_1_rmse: 0.14987 |  0:00:17s
epoch 133| loss: 0.01782 | val_0_rmse: 0.14785 | val_1_rmse: 0.15346 |  0:00:17s
epoch 134| loss: 0.01787 | val_0_rmse: 0.14828 | val_1_rmse: 0.15152 |  0:00:18s
epoch 135| loss: 0.01732 | val_0_rmse: 0.15161 | val_1_rmse: 0.15042 |  0:00:18s
epoch 136| loss: 0.01791 | val_0_rmse: 0.14914 | val_1_rmse: 0.14911 |  0:00:18s
epoch 137| loss: 0.01838 | val_0_rmse: 0.14968 | val_1_rmse: 0.14781 |  0:00:18s
epoch 138| loss: 0.01777 | val_0_rmse: 0.15279 | val_1_rmse: 0.14844 |  0:00:18s
epoch 139| loss: 0.01802 | val_0_rmse: 0.14924 | val_1_rmse: 0.1471  |  0:00:18s
epoch 140| loss: 0.01749 | val_0_rmse: 0.14792 | val_1_rmse: 0.1466  |  0:00:18s
epoch 141| loss: 0.01775 | val_0_rmse: 0.14781 | val_1_rmse: 0.14505 |  0:00:19s
epoch 142| loss: 0.01778 | val_0_rmse: 0.14746 | val_1_rmse: 0.14493 |  0:00:19s
epoch 143| loss: 0.01784 | val_0_rmse: 0.14785 | val_1_rmse: 0.14514 |  0:00:19s
epoch 144| loss: 0.01745 | val_0_rmse: 0.14863 | val_1_rmse: 0.14578 |  0:00:19s
epoch 145| loss: 0.01748 | val_0_rmse: 0.14734 | val_1_rmse: 0.14645 |  0:00:19s
epoch 146| loss: 0.01766 | val_0_rmse: 0.14725 | val_1_rmse: 0.14719 |  0:00:19s
epoch 147| loss: 0.0174  | val_0_rmse: 0.14694 | val_1_rmse: 0.14745 |  0:00:19s
epoch 148| loss: 0.01764 | val_0_rmse: 0.1471  | val_1_rmse: 0.1466  |  0:00:20s
epoch 149| loss: 0.01749 | val_0_rmse: 0.14598 | val_1_rmse: 0.14582 |  0:00:20s
Stop training because you reached max_epochs = 150 with best_epoch = 142 and best_val_1_rmse = 0.14493
Best weights from best epoch are automatically used!
ended training at: 06:23:16
Feature importance:
[('Area', 0.23960251782894434), ('Baths', 0.055601075977294266), ('Beds', 0.22265340398272424), ('Latitude', 0.05248337520157749), ('Longitude', 0.1687081739798561), ('Month', 0.1774072109054246), ('Year', 0.08354424212417898)]
Mean squared error is of 3582074149.1210656
Mean absolute error:47456.918928640116
MAPE:0.4872004946887192
R2 score:0.5238948246055022
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:23:17
epoch 0  | loss: 0.60684 | val_0_rmse: 0.27787 | val_1_rmse: 0.27566 |  0:00:00s
epoch 1  | loss: 0.06435 | val_0_rmse: 0.15636 | val_1_rmse: 0.14835 |  0:00:00s
epoch 2  | loss: 0.03671 | val_0_rmse: 0.14859 | val_1_rmse: 0.14024 |  0:00:00s
epoch 3  | loss: 0.02896 | val_0_rmse: 0.16248 | val_1_rmse: 0.15211 |  0:00:01s
epoch 4  | loss: 0.02341 | val_0_rmse: 0.13777 | val_1_rmse: 0.13197 |  0:00:01s
epoch 5  | loss: 0.02035 | val_0_rmse: 0.14569 | val_1_rmse: 0.13972 |  0:00:01s
epoch 6  | loss: 0.0202  | val_0_rmse: 0.14394 | val_1_rmse: 0.13987 |  0:00:02s
epoch 7  | loss: 0.01909 | val_0_rmse: 0.14261 | val_1_rmse: 0.13934 |  0:00:02s
epoch 8  | loss: 0.01939 | val_0_rmse: 0.14508 | val_1_rmse: 0.14111 |  0:00:02s
epoch 9  | loss: 0.01841 | val_0_rmse: 0.14644 | val_1_rmse: 0.14246 |  0:00:02s
epoch 10 | loss: 0.01809 | val_0_rmse: 0.14867 | val_1_rmse: 0.14559 |  0:00:03s
epoch 11 | loss: 0.01756 | val_0_rmse: 0.14712 | val_1_rmse: 0.14349 |  0:00:03s
epoch 12 | loss: 0.01715 | val_0_rmse: 0.14675 | val_1_rmse: 0.14365 |  0:00:03s
epoch 13 | loss: 0.01777 | val_0_rmse: 0.1448  | val_1_rmse: 0.14066 |  0:00:04s
epoch 14 | loss: 0.01704 | val_0_rmse: 0.14745 | val_1_rmse: 0.14455 |  0:00:04s
epoch 15 | loss: 0.01687 | val_0_rmse: 0.14521 | val_1_rmse: 0.14096 |  0:00:04s
epoch 16 | loss: 0.01654 | val_0_rmse: 0.14675 | val_1_rmse: 0.14354 |  0:00:04s
epoch 17 | loss: 0.01687 | val_0_rmse: 0.14634 | val_1_rmse: 0.14294 |  0:00:05s
epoch 18 | loss: 0.01672 | val_0_rmse: 0.14405 | val_1_rmse: 0.13972 |  0:00:05s
epoch 19 | loss: 0.01667 | val_0_rmse: 0.14352 | val_1_rmse: 0.13972 |  0:00:05s
epoch 20 | loss: 0.01644 | val_0_rmse: 0.14623 | val_1_rmse: 0.14328 |  0:00:06s
epoch 21 | loss: 0.01663 | val_0_rmse: 0.14343 | val_1_rmse: 0.1399  |  0:00:06s
epoch 22 | loss: 0.01651 | val_0_rmse: 0.1442  | val_1_rmse: 0.14116 |  0:00:06s
epoch 23 | loss: 0.01585 | val_0_rmse: 0.14273 | val_1_rmse: 0.13948 |  0:00:07s
epoch 24 | loss: 0.01623 | val_0_rmse: 0.14205 | val_1_rmse: 0.13856 |  0:00:07s
epoch 25 | loss: 0.01619 | val_0_rmse: 0.14324 | val_1_rmse: 0.14041 |  0:00:07s
epoch 26 | loss: 0.01579 | val_0_rmse: 0.14263 | val_1_rmse: 0.13994 |  0:00:07s
epoch 27 | loss: 0.01569 | val_0_rmse: 0.14414 | val_1_rmse: 0.1418  |  0:00:08s
epoch 28 | loss: 0.01579 | val_0_rmse: 0.14566 | val_1_rmse: 0.14383 |  0:00:08s
epoch 29 | loss: 0.01598 | val_0_rmse: 0.14134 | val_1_rmse: 0.13859 |  0:00:08s
epoch 30 | loss: 0.01529 | val_0_rmse: 0.14445 | val_1_rmse: 0.14232 |  0:00:09s
epoch 31 | loss: 0.01552 | val_0_rmse: 0.14141 | val_1_rmse: 0.13901 |  0:00:09s
epoch 32 | loss: 0.01551 | val_0_rmse: 0.14165 | val_1_rmse: 0.13857 |  0:00:09s
epoch 33 | loss: 0.01572 | val_0_rmse: 0.14626 | val_1_rmse: 0.14519 |  0:00:09s
epoch 34 | loss: 0.01508 | val_0_rmse: 0.13946 | val_1_rmse: 0.13747 |  0:00:10s

Early stopping occured at epoch 34 with best_epoch = 4 and best_val_1_rmse = 0.13197
Best weights from best epoch are automatically used!
ended training at: 06:23:27
Feature importance:
[('Area', 0.24767907219422802), ('Baths', 0.35437883855484875), ('Beds', 0.08648862383862138), ('Latitude', 0.024905746595681472), ('Longitude', 0.18099536169639416), ('Month', 0.09592504957130986), ('Year', 0.009627307548916345)]
Mean squared error is of 4323554503.524336
Mean absolute error:47698.94972644313
MAPE:0.43941572353773645
R2 score:0.448106544720685
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:23:27
epoch 0  | loss: 0.56372 | val_0_rmse: 0.18746 | val_1_rmse: 0.1815  |  0:00:00s
epoch 1  | loss: 0.07324 | val_0_rmse: 0.16812 | val_1_rmse: 0.16008 |  0:00:00s
epoch 2  | loss: 0.03776 | val_0_rmse: 0.16905 | val_1_rmse: 0.15985 |  0:00:00s
epoch 3  | loss: 0.02937 | val_0_rmse: 0.15534 | val_1_rmse: 0.14822 |  0:00:01s
epoch 4  | loss: 0.02474 | val_0_rmse: 0.15229 | val_1_rmse: 0.14537 |  0:00:01s
epoch 5  | loss: 0.022   | val_0_rmse: 0.15645 | val_1_rmse: 0.15023 |  0:00:01s
epoch 6  | loss: 0.02153 | val_0_rmse: 0.15204 | val_1_rmse: 0.14626 |  0:00:02s
epoch 7  | loss: 0.02016 | val_0_rmse: 0.15969 | val_1_rmse: 0.15434 |  0:00:02s
epoch 8  | loss: 0.0197  | val_0_rmse: 0.15395 | val_1_rmse: 0.14836 |  0:00:02s
epoch 9  | loss: 0.01922 | val_0_rmse: 0.16306 | val_1_rmse: 0.15747 |  0:00:02s
epoch 10 | loss: 0.01914 | val_0_rmse: 0.15559 | val_1_rmse: 0.1496  |  0:00:03s
epoch 11 | loss: 0.01864 | val_0_rmse: 0.15562 | val_1_rmse: 0.14976 |  0:00:03s
epoch 12 | loss: 0.01847 | val_0_rmse: 0.15684 | val_1_rmse: 0.15107 |  0:00:03s
epoch 13 | loss: 0.01846 | val_0_rmse: 0.1537  | val_1_rmse: 0.1479  |  0:00:04s
epoch 14 | loss: 0.01911 | val_0_rmse: 0.15814 | val_1_rmse: 0.15262 |  0:00:04s
epoch 15 | loss: 0.01861 | val_0_rmse: 0.15444 | val_1_rmse: 0.14894 |  0:00:04s
epoch 16 | loss: 0.01803 | val_0_rmse: 0.15463 | val_1_rmse: 0.14927 |  0:00:04s
epoch 17 | loss: 0.01792 | val_0_rmse: 0.15364 | val_1_rmse: 0.14816 |  0:00:05s
epoch 18 | loss: 0.01779 | val_0_rmse: 0.1522  | val_1_rmse: 0.14661 |  0:00:05s
epoch 19 | loss: 0.01786 | val_0_rmse: 0.15198 | val_1_rmse: 0.14648 |  0:00:05s
epoch 20 | loss: 0.01814 | val_0_rmse: 0.15012 | val_1_rmse: 0.1446  |  0:00:06s
epoch 21 | loss: 0.01781 | val_0_rmse: 0.15218 | val_1_rmse: 0.14661 |  0:00:06s
epoch 22 | loss: 0.01796 | val_0_rmse: 0.15228 | val_1_rmse: 0.14641 |  0:00:06s
epoch 23 | loss: 0.01803 | val_0_rmse: 0.15274 | val_1_rmse: 0.14679 |  0:00:07s
epoch 24 | loss: 0.01781 | val_0_rmse: 0.15007 | val_1_rmse: 0.14413 |  0:00:07s
epoch 25 | loss: 0.01785 | val_0_rmse: 0.15151 | val_1_rmse: 0.14519 |  0:00:07s
epoch 26 | loss: 0.01777 | val_0_rmse: 0.14765 | val_1_rmse: 0.14174 |  0:00:07s
epoch 27 | loss: 0.01786 | val_0_rmse: 0.14673 | val_1_rmse: 0.14079 |  0:00:08s
epoch 28 | loss: 0.01767 | val_0_rmse: 0.14637 | val_1_rmse: 0.14034 |  0:00:08s
epoch 29 | loss: 0.01765 | val_0_rmse: 0.14563 | val_1_rmse: 0.13911 |  0:00:08s
epoch 30 | loss: 0.01756 | val_0_rmse: 0.14677 | val_1_rmse: 0.13984 |  0:00:09s
epoch 31 | loss: 0.01801 | val_0_rmse: 0.14719 | val_1_rmse: 0.13998 |  0:00:09s
epoch 32 | loss: 0.01738 | val_0_rmse: 0.14483 | val_1_rmse: 0.13756 |  0:00:09s
epoch 33 | loss: 0.01701 | val_0_rmse: 0.14346 | val_1_rmse: 0.13597 |  0:00:09s
epoch 34 | loss: 0.01668 | val_0_rmse: 0.14283 | val_1_rmse: 0.13493 |  0:00:10s
epoch 35 | loss: 0.01688 | val_0_rmse: 0.14191 | val_1_rmse: 0.13415 |  0:00:10s
epoch 36 | loss: 0.01635 | val_0_rmse: 0.13981 | val_1_rmse: 0.13173 |  0:00:10s
epoch 37 | loss: 0.01646 | val_0_rmse: 0.14156 | val_1_rmse: 0.1334  |  0:00:11s
epoch 38 | loss: 0.0164  | val_0_rmse: 0.13983 | val_1_rmse: 0.1314  |  0:00:11s
epoch 39 | loss: 0.01624 | val_0_rmse: 0.13913 | val_1_rmse: 0.13002 |  0:00:11s
epoch 40 | loss: 0.01643 | val_0_rmse: 0.14233 | val_1_rmse: 0.13342 |  0:00:12s
epoch 41 | loss: 0.01608 | val_0_rmse: 0.13992 | val_1_rmse: 0.13097 |  0:00:12s
epoch 42 | loss: 0.01637 | val_0_rmse: 0.14167 | val_1_rmse: 0.13342 |  0:00:12s
epoch 43 | loss: 0.01616 | val_0_rmse: 0.13999 | val_1_rmse: 0.13184 |  0:00:12s
epoch 44 | loss: 0.01633 | val_0_rmse: 0.13836 | val_1_rmse: 0.13027 |  0:00:13s
epoch 45 | loss: 0.01624 | val_0_rmse: 0.14135 | val_1_rmse: 0.13325 |  0:00:13s
epoch 46 | loss: 0.01669 | val_0_rmse: 0.13774 | val_1_rmse: 0.12931 |  0:00:13s
epoch 47 | loss: 0.01605 | val_0_rmse: 0.13944 | val_1_rmse: 0.13081 |  0:00:14s
epoch 48 | loss: 0.01591 | val_0_rmse: 0.13772 | val_1_rmse: 0.12855 |  0:00:14s
epoch 49 | loss: 0.01597 | val_0_rmse: 0.13779 | val_1_rmse: 0.1284  |  0:00:14s
epoch 50 | loss: 0.01588 | val_0_rmse: 0.13869 | val_1_rmse: 0.12948 |  0:00:15s
epoch 51 | loss: 0.01577 | val_0_rmse: 0.13563 | val_1_rmse: 0.12651 |  0:00:15s
epoch 52 | loss: 0.01617 | val_0_rmse: 0.13786 | val_1_rmse: 0.12881 |  0:00:15s
epoch 53 | loss: 0.01653 | val_0_rmse: 0.1389  | val_1_rmse: 0.13004 |  0:00:15s
epoch 54 | loss: 0.01591 | val_0_rmse: 0.1387  | val_1_rmse: 0.12969 |  0:00:16s
epoch 55 | loss: 0.01587 | val_0_rmse: 0.13851 | val_1_rmse: 0.1295  |  0:00:16s
epoch 56 | loss: 0.0158  | val_0_rmse: 0.13781 | val_1_rmse: 0.12902 |  0:00:16s
epoch 57 | loss: 0.01564 | val_0_rmse: 0.13812 | val_1_rmse: 0.12957 |  0:00:17s
epoch 58 | loss: 0.01576 | val_0_rmse: 0.13911 | val_1_rmse: 0.13049 |  0:00:17s
epoch 59 | loss: 0.01565 | val_0_rmse: 0.13759 | val_1_rmse: 0.12845 |  0:00:17s
epoch 60 | loss: 0.01577 | val_0_rmse: 0.13691 | val_1_rmse: 0.12833 |  0:00:17s
epoch 61 | loss: 0.0157  | val_0_rmse: 0.13549 | val_1_rmse: 0.12675 |  0:00:18s
epoch 62 | loss: 0.01578 | val_0_rmse: 0.14421 | val_1_rmse: 0.13606 |  0:00:18s
epoch 63 | loss: 0.01581 | val_0_rmse: 0.13769 | val_1_rmse: 0.12956 |  0:00:18s
epoch 64 | loss: 0.016   | val_0_rmse: 0.13561 | val_1_rmse: 0.12663 |  0:00:19s
epoch 65 | loss: 0.01534 | val_0_rmse: 0.13715 | val_1_rmse: 0.12876 |  0:00:19s
epoch 66 | loss: 0.0153  | val_0_rmse: 0.13914 | val_1_rmse: 0.13109 |  0:00:19s
epoch 67 | loss: 0.01525 | val_0_rmse: 0.13598 | val_1_rmse: 0.12747 |  0:00:19s
epoch 68 | loss: 0.01512 | val_0_rmse: 0.13761 | val_1_rmse: 0.12908 |  0:00:20s
epoch 69 | loss: 0.01542 | val_0_rmse: 0.14015 | val_1_rmse: 0.13198 |  0:00:20s
epoch 70 | loss: 0.0155  | val_0_rmse: 0.13538 | val_1_rmse: 0.12613 |  0:00:20s
epoch 71 | loss: 0.01575 | val_0_rmse: 0.13786 | val_1_rmse: 0.12886 |  0:00:21s
epoch 72 | loss: 0.01562 | val_0_rmse: 0.14237 | val_1_rmse: 0.13465 |  0:00:21s
epoch 73 | loss: 0.01556 | val_0_rmse: 0.13911 | val_1_rmse: 0.13087 |  0:00:21s
epoch 74 | loss: 0.01515 | val_0_rmse: 0.13463 | val_1_rmse: 0.12579 |  0:00:22s
epoch 75 | loss: 0.01507 | val_0_rmse: 0.13677 | val_1_rmse: 0.1286  |  0:00:22s
epoch 76 | loss: 0.01502 | val_0_rmse: 0.13859 | val_1_rmse: 0.13076 |  0:00:22s
epoch 77 | loss: 0.01497 | val_0_rmse: 0.1387  | val_1_rmse: 0.13072 |  0:00:22s
epoch 78 | loss: 0.01503 | val_0_rmse: 0.13478 | val_1_rmse: 0.12613 |  0:00:23s
epoch 79 | loss: 0.01499 | val_0_rmse: 0.13487 | val_1_rmse: 0.12677 |  0:00:23s
epoch 80 | loss: 0.01499 | val_0_rmse: 0.14195 | val_1_rmse: 0.13398 |  0:00:23s
epoch 81 | loss: 0.01495 | val_0_rmse: 0.13609 | val_1_rmse: 0.12781 |  0:00:24s
epoch 82 | loss: 0.01495 | val_0_rmse: 0.13707 | val_1_rmse: 0.12862 |  0:00:24s
epoch 83 | loss: 0.01458 | val_0_rmse: 0.13638 | val_1_rmse: 0.12795 |  0:00:24s
epoch 84 | loss: 0.01487 | val_0_rmse: 0.13553 | val_1_rmse: 0.12727 |  0:00:24s
epoch 85 | loss: 0.01529 | val_0_rmse: 0.13313 | val_1_rmse: 0.12514 |  0:00:25s
epoch 86 | loss: 0.01475 | val_0_rmse: 0.1366  | val_1_rmse: 0.12939 |  0:00:25s
epoch 87 | loss: 0.01443 | val_0_rmse: 0.13733 | val_1_rmse: 0.13019 |  0:00:25s
epoch 88 | loss: 0.01482 | val_0_rmse: 0.13477 | val_1_rmse: 0.12735 |  0:00:26s
epoch 89 | loss: 0.01478 | val_0_rmse: 0.13725 | val_1_rmse: 0.12945 |  0:00:26s
epoch 90 | loss: 0.01469 | val_0_rmse: 0.13355 | val_1_rmse: 0.12511 |  0:00:26s
epoch 91 | loss: 0.01482 | val_0_rmse: 0.13079 | val_1_rmse: 0.1227  |  0:00:26s
epoch 92 | loss: 0.01509 | val_0_rmse: 0.1422  | val_1_rmse: 0.13591 |  0:00:27s
epoch 93 | loss: 0.01441 | val_0_rmse: 0.13324 | val_1_rmse: 0.1256  |  0:00:27s
epoch 94 | loss: 0.01444 | val_0_rmse: 0.1313  | val_1_rmse: 0.12306 |  0:00:27s
epoch 95 | loss: 0.01459 | val_0_rmse: 0.13259 | val_1_rmse: 0.12523 |  0:00:28s
epoch 96 | loss: 0.01438 | val_0_rmse: 0.1355  | val_1_rmse: 0.12875 |  0:00:28s
epoch 97 | loss: 0.01457 | val_0_rmse: 0.13016 | val_1_rmse: 0.12291 |  0:00:28s
epoch 98 | loss: 0.01517 | val_0_rmse: 0.1288  | val_1_rmse: 0.12025 |  0:00:28s
epoch 99 | loss: 0.01521 | val_0_rmse: 0.13259 | val_1_rmse: 0.12536 |  0:00:29s
epoch 100| loss: 0.01414 | val_0_rmse: 0.13699 | val_1_rmse: 0.13113 |  0:00:29s
epoch 101| loss: 0.01418 | val_0_rmse: 0.12953 | val_1_rmse: 0.12187 |  0:00:29s
epoch 102| loss: 0.01382 | val_0_rmse: 0.13227 | val_1_rmse: 0.12474 |  0:00:30s
epoch 103| loss: 0.01384 | val_0_rmse: 0.13143 | val_1_rmse: 0.12472 |  0:00:30s
epoch 104| loss: 0.01438 | val_0_rmse: 0.12881 | val_1_rmse: 0.12131 |  0:00:30s
epoch 105| loss: 0.0136  | val_0_rmse: 0.12622 | val_1_rmse: 0.11816 |  0:00:31s
epoch 106| loss: 0.01327 | val_0_rmse: 0.13837 | val_1_rmse: 0.13227 |  0:00:31s
epoch 107| loss: 0.01363 | val_0_rmse: 0.13652 | val_1_rmse: 0.13199 |  0:00:31s
epoch 108| loss: 0.01357 | val_0_rmse: 0.12905 | val_1_rmse: 0.12212 |  0:00:31s
epoch 109| loss: 0.01414 | val_0_rmse: 0.13941 | val_1_rmse: 0.13472 |  0:00:32s
epoch 110| loss: 0.01535 | val_0_rmse: 0.13516 | val_1_rmse: 0.12906 |  0:00:32s
epoch 111| loss: 0.01559 | val_0_rmse: 0.13264 | val_1_rmse: 0.12585 |  0:00:32s
epoch 112| loss: 0.01518 | val_0_rmse: 0.13797 | val_1_rmse: 0.1309  |  0:00:33s
epoch 113| loss: 0.01457 | val_0_rmse: 0.12752 | val_1_rmse: 0.12135 |  0:00:33s
epoch 114| loss: 0.01442 | val_0_rmse: 0.13135 | val_1_rmse: 0.12411 |  0:00:33s
epoch 115| loss: 0.01408 | val_0_rmse: 0.13666 | val_1_rmse: 0.13106 |  0:00:33s
epoch 116| loss: 0.01418 | val_0_rmse: 0.12541 | val_1_rmse: 0.11664 |  0:00:34s
epoch 117| loss: 0.01412 | val_0_rmse: 0.12379 | val_1_rmse: 0.11663 |  0:00:34s
epoch 118| loss: 0.01332 | val_0_rmse: 0.12275 | val_1_rmse: 0.11719 |  0:00:34s
epoch 119| loss: 0.0133  | val_0_rmse: 0.12399 | val_1_rmse: 0.11886 |  0:00:35s
epoch 120| loss: 0.01351 | val_0_rmse: 0.12304 | val_1_rmse: 0.11654 |  0:00:35s
epoch 121| loss: 0.0137  | val_0_rmse: 0.12334 | val_1_rmse: 0.11579 |  0:00:35s
epoch 122| loss: 0.01365 | val_0_rmse: 0.12881 | val_1_rmse: 0.12041 |  0:00:36s
epoch 123| loss: 0.01374 | val_0_rmse: 0.12255 | val_1_rmse: 0.11526 |  0:00:36s
epoch 124| loss: 0.01317 | val_0_rmse: 0.12209 | val_1_rmse: 0.11453 |  0:00:36s
epoch 125| loss: 0.01286 | val_0_rmse: 0.12502 | val_1_rmse: 0.11751 |  0:00:36s
epoch 126| loss: 0.01321 | val_0_rmse: 0.12295 | val_1_rmse: 0.11549 |  0:00:37s
epoch 127| loss: 0.01302 | val_0_rmse: 0.12881 | val_1_rmse: 0.12273 |  0:00:37s
epoch 128| loss: 0.01303 | val_0_rmse: 0.12585 | val_1_rmse: 0.1187  |  0:00:37s
epoch 129| loss: 0.01317 | val_0_rmse: 0.11996 | val_1_rmse: 0.11264 |  0:00:38s
epoch 130| loss: 0.01278 | val_0_rmse: 0.12841 | val_1_rmse: 0.1213  |  0:00:38s
epoch 131| loss: 0.01294 | val_0_rmse: 0.12472 | val_1_rmse: 0.11873 |  0:00:38s
epoch 132| loss: 0.01293 | val_0_rmse: 0.12219 | val_1_rmse: 0.11683 |  0:00:38s
epoch 133| loss: 0.01292 | val_0_rmse: 0.12721 | val_1_rmse: 0.12203 |  0:00:39s
epoch 134| loss: 0.01264 | val_0_rmse: 0.11826 | val_1_rmse: 0.11266 |  0:00:39s
epoch 135| loss: 0.01263 | val_0_rmse: 0.11976 | val_1_rmse: 0.11384 |  0:00:39s
epoch 136| loss: 0.01241 | val_0_rmse: 0.11856 | val_1_rmse: 0.11315 |  0:00:40s
epoch 137| loss: 0.01245 | val_0_rmse: 0.12088 | val_1_rmse: 0.11349 |  0:00:40s
epoch 138| loss: 0.01372 | val_0_rmse: 0.11849 | val_1_rmse: 0.11417 |  0:00:40s
epoch 139| loss: 0.01381 | val_0_rmse: 0.12541 | val_1_rmse: 0.12105 |  0:00:41s
epoch 140| loss: 0.01322 | val_0_rmse: 0.11772 | val_1_rmse: 0.113   |  0:00:41s
epoch 141| loss: 0.01265 | val_0_rmse: 0.12183 | val_1_rmse: 0.11658 |  0:00:41s
epoch 142| loss: 0.01301 | val_0_rmse: 0.11938 | val_1_rmse: 0.11515 |  0:00:41s
epoch 143| loss: 0.01249 | val_0_rmse: 0.11809 | val_1_rmse: 0.11371 |  0:00:42s
epoch 144| loss: 0.01258 | val_0_rmse: 0.11759 | val_1_rmse: 0.11313 |  0:00:42s
epoch 145| loss: 0.01306 | val_0_rmse: 0.11751 | val_1_rmse: 0.11235 |  0:00:42s
epoch 146| loss: 0.0136  | val_0_rmse: 0.11832 | val_1_rmse: 0.11288 |  0:00:43s
epoch 147| loss: 0.01302 | val_0_rmse: 0.11509 | val_1_rmse: 0.10937 |  0:00:43s
epoch 148| loss: 0.01304 | val_0_rmse: 0.11586 | val_1_rmse: 0.11048 |  0:00:43s
epoch 149| loss: 0.01284 | val_0_rmse: 0.12559 | val_1_rmse: 0.12167 |  0:00:43s
Stop training because you reached max_epochs = 150 with best_epoch = 147 and best_val_1_rmse = 0.10937
Best weights from best epoch are automatically used!
ended training at: 06:24:11
Feature importance:
[('Area', 0.2469197209262284), ('Baths', 0.20179125403461576), ('Beds', 0.22927553020720776), ('Latitude', 0.0025401734885246246), ('Longitude', 0.07606891949329267), ('Month', 0.0), ('Year', 0.24340440185013082)]
Mean squared error is of 4016166345.6487184
Mean absolute error:43420.783576506794
MAPE:0.429056496779745
R2 score:0.5413067180346698
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:24:12
epoch 0  | loss: 0.62325 | val_0_rmse: 0.36868 | val_1_rmse: 0.3824  |  0:00:00s
epoch 1  | loss: 0.05376 | val_0_rmse: 0.22381 | val_1_rmse: 0.23222 |  0:00:00s
epoch 2  | loss: 0.03263 | val_0_rmse: 0.18962 | val_1_rmse: 0.19437 |  0:00:00s
epoch 3  | loss: 0.02496 | val_0_rmse: 0.17627 | val_1_rmse: 0.17977 |  0:00:01s
epoch 4  | loss: 0.02178 | val_0_rmse: 0.18027 | val_1_rmse: 0.18283 |  0:00:01s
epoch 5  | loss: 0.02207 | val_0_rmse: 0.17779 | val_1_rmse: 0.17919 |  0:00:01s
epoch 6  | loss: 0.02055 | val_0_rmse: 0.18084 | val_1_rmse: 0.18175 |  0:00:02s
epoch 7  | loss: 0.01961 | val_0_rmse: 0.18094 | val_1_rmse: 0.18135 |  0:00:02s
epoch 8  | loss: 0.0191  | val_0_rmse: 0.18226 | val_1_rmse: 0.1824  |  0:00:02s
epoch 9  | loss: 0.01903 | val_0_rmse: 0.18527 | val_1_rmse: 0.18538 |  0:00:02s
epoch 10 | loss: 0.01889 | val_0_rmse: 0.18834 | val_1_rmse: 0.18825 |  0:00:03s
epoch 11 | loss: 0.01847 | val_0_rmse: 0.18542 | val_1_rmse: 0.1857  |  0:00:03s
epoch 12 | loss: 0.01811 | val_0_rmse: 0.1774  | val_1_rmse: 0.17805 |  0:00:03s
epoch 13 | loss: 0.01728 | val_0_rmse: 0.18221 | val_1_rmse: 0.18267 |  0:00:04s
epoch 14 | loss: 0.01704 | val_0_rmse: 0.18475 | val_1_rmse: 0.18568 |  0:00:04s
epoch 15 | loss: 0.0172  | val_0_rmse: 0.17628 | val_1_rmse: 0.17662 |  0:00:04s
epoch 16 | loss: 0.01725 | val_0_rmse: 0.16722 | val_1_rmse: 0.16867 |  0:00:04s
epoch 17 | loss: 0.01809 | val_0_rmse: 0.17143 | val_1_rmse: 0.17384 |  0:00:05s
epoch 18 | loss: 0.01745 | val_0_rmse: 0.16269 | val_1_rmse: 0.16561 |  0:00:05s
epoch 19 | loss: 0.01731 | val_0_rmse: 0.165   | val_1_rmse: 0.1671  |  0:00:05s
epoch 20 | loss: 0.01714 | val_0_rmse: 0.1608  | val_1_rmse: 0.16372 |  0:00:06s
epoch 21 | loss: 0.01616 | val_0_rmse: 0.16446 | val_1_rmse: 0.16659 |  0:00:06s
epoch 22 | loss: 0.01618 | val_0_rmse: 0.15633 | val_1_rmse: 0.15906 |  0:00:06s
epoch 23 | loss: 0.01527 | val_0_rmse: 0.15888 | val_1_rmse: 0.16091 |  0:00:07s
epoch 24 | loss: 0.01518 | val_0_rmse: 0.15669 | val_1_rmse: 0.15925 |  0:00:07s
epoch 25 | loss: 0.01488 | val_0_rmse: 0.16034 | val_1_rmse: 0.16189 |  0:00:07s
epoch 26 | loss: 0.01489 | val_0_rmse: 0.15864 | val_1_rmse: 0.1605  |  0:00:07s
epoch 27 | loss: 0.0148  | val_0_rmse: 0.16233 | val_1_rmse: 0.16361 |  0:00:08s
epoch 28 | loss: 0.01533 | val_0_rmse: 0.15483 | val_1_rmse: 0.15676 |  0:00:08s
epoch 29 | loss: 0.01456 | val_0_rmse: 0.15341 | val_1_rmse: 0.15604 |  0:00:08s
epoch 30 | loss: 0.01441 | val_0_rmse: 0.15511 | val_1_rmse: 0.15763 |  0:00:09s
epoch 31 | loss: 0.01439 | val_0_rmse: 0.15116 | val_1_rmse: 0.15422 |  0:00:09s
epoch 32 | loss: 0.01417 | val_0_rmse: 0.15394 | val_1_rmse: 0.15656 |  0:00:09s
epoch 33 | loss: 0.01395 | val_0_rmse: 0.16307 | val_1_rmse: 0.16559 |  0:00:10s
epoch 34 | loss: 0.01491 | val_0_rmse: 0.15274 | val_1_rmse: 0.15598 |  0:00:10s
epoch 35 | loss: 0.01434 | val_0_rmse: 0.15156 | val_1_rmse: 0.15416 |  0:00:10s
epoch 36 | loss: 0.01422 | val_0_rmse: 0.16522 | val_1_rmse: 0.16883 |  0:00:10s
epoch 37 | loss: 0.01401 | val_0_rmse: 0.16138 | val_1_rmse: 0.16484 |  0:00:11s
epoch 38 | loss: 0.01378 | val_0_rmse: 0.14815 | val_1_rmse: 0.15121 |  0:00:11s
epoch 39 | loss: 0.01448 | val_0_rmse: 0.15777 | val_1_rmse: 0.16214 |  0:00:11s
epoch 40 | loss: 0.015   | val_0_rmse: 0.16882 | val_1_rmse: 0.17268 |  0:00:12s
epoch 41 | loss: 0.01462 | val_0_rmse: 0.14674 | val_1_rmse: 0.15087 |  0:00:12s
epoch 42 | loss: 0.01437 | val_0_rmse: 0.14608 | val_1_rmse: 0.1491  |  0:00:12s
epoch 43 | loss: 0.0147  | val_0_rmse: 0.13761 | val_1_rmse: 0.14124 |  0:00:12s
epoch 44 | loss: 0.01482 | val_0_rmse: 0.1434  | val_1_rmse: 0.14668 |  0:00:13s
epoch 45 | loss: 0.0145  | val_0_rmse: 0.149   | val_1_rmse: 0.15205 |  0:00:13s
epoch 46 | loss: 0.01494 | val_0_rmse: 0.14169 | val_1_rmse: 0.14585 |  0:00:13s
epoch 47 | loss: 0.01497 | val_0_rmse: 0.13613 | val_1_rmse: 0.14086 |  0:00:14s
epoch 48 | loss: 0.01526 | val_0_rmse: 0.14633 | val_1_rmse: 0.14967 |  0:00:14s
epoch 49 | loss: 0.01446 | val_0_rmse: 0.14143 | val_1_rmse: 0.14525 |  0:00:14s
epoch 50 | loss: 0.01428 | val_0_rmse: 0.13452 | val_1_rmse: 0.13926 |  0:00:14s
epoch 51 | loss: 0.01446 | val_0_rmse: 0.14459 | val_1_rmse: 0.14927 |  0:00:15s
epoch 52 | loss: 0.01412 | val_0_rmse: 0.14992 | val_1_rmse: 0.15471 |  0:00:15s
epoch 53 | loss: 0.01415 | val_0_rmse: 0.13889 | val_1_rmse: 0.14316 |  0:00:15s
epoch 54 | loss: 0.01392 | val_0_rmse: 0.13503 | val_1_rmse: 0.14    |  0:00:16s
epoch 55 | loss: 0.01439 | val_0_rmse: 0.15855 | val_1_rmse: 0.16272 |  0:00:16s
epoch 56 | loss: 0.01491 | val_0_rmse: 0.14814 | val_1_rmse: 0.15276 |  0:00:16s
epoch 57 | loss: 0.0136  | val_0_rmse: 0.13662 | val_1_rmse: 0.14215 |  0:00:17s
epoch 58 | loss: 0.01394 | val_0_rmse: 0.14852 | val_1_rmse: 0.1543  |  0:00:17s
epoch 59 | loss: 0.01373 | val_0_rmse: 0.14626 | val_1_rmse: 0.15266 |  0:00:17s
epoch 60 | loss: 0.01387 | val_0_rmse: 0.13726 | val_1_rmse: 0.14337 |  0:00:17s
epoch 61 | loss: 0.01308 | val_0_rmse: 0.14242 | val_1_rmse: 0.14868 |  0:00:18s
epoch 62 | loss: 0.01281 | val_0_rmse: 0.15278 | val_1_rmse: 0.15868 |  0:00:18s
epoch 63 | loss: 0.0131  | val_0_rmse: 0.14091 | val_1_rmse: 0.1476  |  0:00:18s
epoch 64 | loss: 0.01302 | val_0_rmse: 0.1336  | val_1_rmse: 0.14089 |  0:00:19s
epoch 65 | loss: 0.01324 | val_0_rmse: 0.15237 | val_1_rmse: 0.15804 |  0:00:19s
epoch 66 | loss: 0.01355 | val_0_rmse: 0.13557 | val_1_rmse: 0.14206 |  0:00:19s
epoch 67 | loss: 0.01296 | val_0_rmse: 0.13054 | val_1_rmse: 0.13775 |  0:00:19s
epoch 68 | loss: 0.01323 | val_0_rmse: 0.1344  | val_1_rmse: 0.14099 |  0:00:20s
epoch 69 | loss: 0.0125  | val_0_rmse: 0.14119 | val_1_rmse: 0.1477  |  0:00:20s
epoch 70 | loss: 0.013   | val_0_rmse: 0.13764 | val_1_rmse: 0.1448  |  0:00:20s
epoch 71 | loss: 0.01246 | val_0_rmse: 0.13302 | val_1_rmse: 0.14077 |  0:00:21s
epoch 72 | loss: 0.01246 | val_0_rmse: 0.13154 | val_1_rmse: 0.13851 |  0:00:21s
epoch 73 | loss: 0.01241 | val_0_rmse: 0.13323 | val_1_rmse: 0.14024 |  0:00:21s
epoch 74 | loss: 0.01247 | val_0_rmse: 0.12827 | val_1_rmse: 0.13508 |  0:00:21s
epoch 75 | loss: 0.01283 | val_0_rmse: 0.13182 | val_1_rmse: 0.13864 |  0:00:22s
epoch 76 | loss: 0.01302 | val_0_rmse: 0.13613 | val_1_rmse: 0.14363 |  0:00:22s
epoch 77 | loss: 0.01306 | val_0_rmse: 0.12873 | val_1_rmse: 0.13663 |  0:00:22s
epoch 78 | loss: 0.013   | val_0_rmse: 0.12909 | val_1_rmse: 0.13619 |  0:00:23s
epoch 79 | loss: 0.01266 | val_0_rmse: 0.12817 | val_1_rmse: 0.13464 |  0:00:23s
epoch 80 | loss: 0.01333 | val_0_rmse: 0.12577 | val_1_rmse: 0.13204 |  0:00:23s
epoch 81 | loss: 0.01295 | val_0_rmse: 0.13702 | val_1_rmse: 0.14181 |  0:00:24s
epoch 82 | loss: 0.01413 | val_0_rmse: 0.14128 | val_1_rmse: 0.14644 |  0:00:24s
epoch 83 | loss: 0.01421 | val_0_rmse: 0.14144 | val_1_rmse: 0.14623 |  0:00:24s
epoch 84 | loss: 0.01493 | val_0_rmse: 0.12517 | val_1_rmse: 0.12983 |  0:00:24s
epoch 85 | loss: 0.01569 | val_0_rmse: 0.12521 | val_1_rmse: 0.13073 |  0:00:25s
epoch 86 | loss: 0.01486 | val_0_rmse: 0.13423 | val_1_rmse: 0.13878 |  0:00:25s
epoch 87 | loss: 0.01388 | val_0_rmse: 0.13129 | val_1_rmse: 0.13685 |  0:00:25s
epoch 88 | loss: 0.01357 | val_0_rmse: 0.12634 | val_1_rmse: 0.13283 |  0:00:26s
epoch 89 | loss: 0.01338 | val_0_rmse: 0.13855 | val_1_rmse: 0.14424 |  0:00:26s
epoch 90 | loss: 0.01325 | val_0_rmse: 0.13175 | val_1_rmse: 0.1384  |  0:00:26s
epoch 91 | loss: 0.01325 | val_0_rmse: 0.12533 | val_1_rmse: 0.13243 |  0:00:26s
epoch 92 | loss: 0.01315 | val_0_rmse: 0.13381 | val_1_rmse: 0.14016 |  0:00:27s
epoch 93 | loss: 0.0127  | val_0_rmse: 0.12341 | val_1_rmse: 0.13126 |  0:00:27s
epoch 94 | loss: 0.01257 | val_0_rmse: 0.12521 | val_1_rmse: 0.13296 |  0:00:27s
epoch 95 | loss: 0.0125  | val_0_rmse: 0.12715 | val_1_rmse: 0.1346  |  0:00:28s
epoch 96 | loss: 0.01252 | val_0_rmse: 0.13105 | val_1_rmse: 0.13724 |  0:00:28s
epoch 97 | loss: 0.0123  | val_0_rmse: 0.12323 | val_1_rmse: 0.13006 |  0:00:28s
epoch 98 | loss: 0.01229 | val_0_rmse: 0.1253  | val_1_rmse: 0.13206 |  0:00:29s
epoch 99 | loss: 0.01198 | val_0_rmse: 0.12323 | val_1_rmse: 0.13074 |  0:00:29s
epoch 100| loss: 0.01214 | val_0_rmse: 0.12443 | val_1_rmse: 0.13146 |  0:00:29s
epoch 101| loss: 0.0133  | val_0_rmse: 0.12749 | val_1_rmse: 0.13488 |  0:00:29s
epoch 102| loss: 0.0132  | val_0_rmse: 0.13191 | val_1_rmse: 0.13945 |  0:00:30s
epoch 103| loss: 0.01319 | val_0_rmse: 0.12224 | val_1_rmse: 0.12979 |  0:00:30s
epoch 104| loss: 0.01297 | val_0_rmse: 0.12087 | val_1_rmse: 0.12904 |  0:00:30s
epoch 105| loss: 0.01271 | val_0_rmse: 0.14023 | val_1_rmse: 0.14721 |  0:00:31s
epoch 106| loss: 0.01234 | val_0_rmse: 0.12028 | val_1_rmse: 0.12853 |  0:00:31s
epoch 107| loss: 0.01219 | val_0_rmse: 0.11856 | val_1_rmse: 0.1274  |  0:00:31s
epoch 108| loss: 0.01173 | val_0_rmse: 0.12107 | val_1_rmse: 0.13068 |  0:00:31s
epoch 109| loss: 0.01184 | val_0_rmse: 0.12379 | val_1_rmse: 0.13307 |  0:00:32s
epoch 110| loss: 0.01206 | val_0_rmse: 0.12089 | val_1_rmse: 0.13002 |  0:00:32s
epoch 111| loss: 0.01186 | val_0_rmse: 0.12219 | val_1_rmse: 0.13152 |  0:00:32s
epoch 112| loss: 0.01212 | val_0_rmse: 0.11804 | val_1_rmse: 0.12823 |  0:00:33s
epoch 113| loss: 0.01221 | val_0_rmse: 0.11595 | val_1_rmse: 0.1253  |  0:00:33s
epoch 114| loss: 0.01172 | val_0_rmse: 0.12198 | val_1_rmse: 0.13116 |  0:00:33s
epoch 115| loss: 0.0122  | val_0_rmse: 0.11772 | val_1_rmse: 0.1267  |  0:00:34s
epoch 116| loss: 0.01175 | val_0_rmse: 0.11696 | val_1_rmse: 0.12604 |  0:00:34s
epoch 117| loss: 0.01165 | val_0_rmse: 0.12722 | val_1_rmse: 0.13554 |  0:00:34s
epoch 118| loss: 0.0117  | val_0_rmse: 0.11629 | val_1_rmse: 0.12493 |  0:00:34s
epoch 119| loss: 0.01151 | val_0_rmse: 0.11833 | val_1_rmse: 0.12778 |  0:00:35s
epoch 120| loss: 0.01147 | val_0_rmse: 0.12915 | val_1_rmse: 0.13796 |  0:00:35s
epoch 121| loss: 0.01196 | val_0_rmse: 0.11628 | val_1_rmse: 0.12679 |  0:00:35s
epoch 122| loss: 0.01188 | val_0_rmse: 0.11491 | val_1_rmse: 0.12634 |  0:00:36s
epoch 123| loss: 0.01142 | val_0_rmse: 0.11631 | val_1_rmse: 0.12655 |  0:00:36s
epoch 124| loss: 0.01174 | val_0_rmse: 0.12204 | val_1_rmse: 0.13117 |  0:00:36s
epoch 125| loss: 0.01257 | val_0_rmse: 0.11411 | val_1_rmse: 0.12431 |  0:00:36s
epoch 126| loss: 0.01188 | val_0_rmse: 0.11544 | val_1_rmse: 0.12453 |  0:00:37s
epoch 127| loss: 0.0116  | val_0_rmse: 0.11302 | val_1_rmse: 0.12205 |  0:00:37s
epoch 128| loss: 0.01128 | val_0_rmse: 0.11592 | val_1_rmse: 0.12496 |  0:00:37s
epoch 129| loss: 0.01165 | val_0_rmse: 0.11506 | val_1_rmse: 0.12387 |  0:00:38s
epoch 130| loss: 0.01126 | val_0_rmse: 0.11324 | val_1_rmse: 0.12335 |  0:00:38s
epoch 131| loss: 0.0116  | val_0_rmse: 0.11079 | val_1_rmse: 0.12075 |  0:00:38s
epoch 132| loss: 0.01148 | val_0_rmse: 0.11613 | val_1_rmse: 0.12474 |  0:00:38s
epoch 133| loss: 0.01132 | val_0_rmse: 0.11357 | val_1_rmse: 0.1224  |  0:00:39s
epoch 134| loss: 0.01139 | val_0_rmse: 0.1118  | val_1_rmse: 0.12008 |  0:00:39s
epoch 135| loss: 0.01155 | val_0_rmse: 0.11053 | val_1_rmse: 0.11908 |  0:00:39s
epoch 136| loss: 0.0114  | val_0_rmse: 0.11065 | val_1_rmse: 0.1199  |  0:00:40s
epoch 137| loss: 0.0114  | val_0_rmse: 0.11123 | val_1_rmse: 0.11969 |  0:00:40s
epoch 138| loss: 0.01156 | val_0_rmse: 0.11015 | val_1_rmse: 0.11824 |  0:00:40s
epoch 139| loss: 0.01141 | val_0_rmse: 0.10994 | val_1_rmse: 0.11834 |  0:00:41s
epoch 140| loss: 0.01145 | val_0_rmse: 0.11084 | val_1_rmse: 0.11996 |  0:00:41s
epoch 141| loss: 0.01167 | val_0_rmse: 0.11141 | val_1_rmse: 0.12097 |  0:00:41s
epoch 142| loss: 0.01156 | val_0_rmse: 0.11334 | val_1_rmse: 0.12184 |  0:00:41s
epoch 143| loss: 0.01166 | val_0_rmse: 0.10989 | val_1_rmse: 0.11929 |  0:00:42s
epoch 144| loss: 0.01101 | val_0_rmse: 0.10853 | val_1_rmse: 0.11786 |  0:00:42s
epoch 145| loss: 0.011   | val_0_rmse: 0.10832 | val_1_rmse: 0.11735 |  0:00:42s
epoch 146| loss: 0.01107 | val_0_rmse: 0.10825 | val_1_rmse: 0.11775 |  0:00:43s
epoch 147| loss: 0.01153 | val_0_rmse: 0.11941 | val_1_rmse: 0.12793 |  0:00:43s
epoch 148| loss: 0.01132 | val_0_rmse: 0.10883 | val_1_rmse: 0.11812 |  0:00:43s
epoch 149| loss: 0.01116 | val_0_rmse: 0.1102  | val_1_rmse: 0.11875 |  0:00:43s
Stop training because you reached max_epochs = 150 with best_epoch = 145 and best_val_1_rmse = 0.11735
Best weights from best epoch are automatically used!
ended training at: 06:24:56
Feature importance:
[('Area', 0.09237773411751336), ('Baths', 0.06782387962810069), ('Beds', 0.22569138485587995), ('Latitude', 0.10423965652198304), ('Longitude', 0.24193940541933404), ('Month', 0.14588329142656656), ('Year', 0.1220446480306224)]
Mean squared error is of 3851383224.0558114
Mean absolute error:41746.60474235993
MAPE:0.3895264014592714
R2 score:0.5627739798162007
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:24:56
epoch 0  | loss: 0.60481 | val_0_rmse: 0.24174 | val_1_rmse: 0.24351 |  0:00:00s
epoch 1  | loss: 0.05517 | val_0_rmse: 0.21113 | val_1_rmse: 0.20899 |  0:00:00s
epoch 2  | loss: 0.02979 | val_0_rmse: 0.16638 | val_1_rmse: 0.17053 |  0:00:00s
epoch 3  | loss: 0.02593 | val_0_rmse: 0.16826 | val_1_rmse: 0.17233 |  0:00:01s
epoch 4  | loss: 0.02391 | val_0_rmse: 0.16251 | val_1_rmse: 0.16688 |  0:00:01s
epoch 5  | loss: 0.02191 | val_0_rmse: 0.16679 | val_1_rmse: 0.17135 |  0:00:01s
epoch 6  | loss: 0.02156 | val_0_rmse: 0.15725 | val_1_rmse: 0.16179 |  0:00:02s
epoch 7  | loss: 0.02039 | val_0_rmse: 0.15804 | val_1_rmse: 0.16134 |  0:00:02s
epoch 8  | loss: 0.02023 | val_0_rmse: 0.15735 | val_1_rmse: 0.15975 |  0:00:02s
epoch 9  | loss: 0.01958 | val_0_rmse: 0.1527  | val_1_rmse: 0.15509 |  0:00:03s
epoch 10 | loss: 0.01929 | val_0_rmse: 0.15212 | val_1_rmse: 0.15462 |  0:00:03s
epoch 11 | loss: 0.0189  | val_0_rmse: 0.14803 | val_1_rmse: 0.15136 |  0:00:03s
epoch 12 | loss: 0.01847 | val_0_rmse: 0.14657 | val_1_rmse: 0.15074 |  0:00:03s
epoch 13 | loss: 0.01793 | val_0_rmse: 0.14733 | val_1_rmse: 0.15266 |  0:00:04s
epoch 14 | loss: 0.01861 | val_0_rmse: 0.14655 | val_1_rmse: 0.15222 |  0:00:04s
epoch 15 | loss: 0.01846 | val_0_rmse: 0.15026 | val_1_rmse: 0.15573 |  0:00:04s
epoch 16 | loss: 0.01862 | val_0_rmse: 0.14789 | val_1_rmse: 0.15372 |  0:00:05s
epoch 17 | loss: 0.01786 | val_0_rmse: 0.14707 | val_1_rmse: 0.15289 |  0:00:05s
epoch 18 | loss: 0.01766 | val_0_rmse: 0.14684 | val_1_rmse: 0.15248 |  0:00:05s
epoch 19 | loss: 0.01744 | val_0_rmse: 0.14435 | val_1_rmse: 0.14962 |  0:00:05s
epoch 20 | loss: 0.01758 | val_0_rmse: 0.14445 | val_1_rmse: 0.14932 |  0:00:06s
epoch 21 | loss: 0.0174  | val_0_rmse: 0.14556 | val_1_rmse: 0.15123 |  0:00:06s
epoch 22 | loss: 0.0171  | val_0_rmse: 0.14312 | val_1_rmse: 0.14908 |  0:00:06s
epoch 23 | loss: 0.01688 | val_0_rmse: 0.14203 | val_1_rmse: 0.14776 |  0:00:07s
epoch 24 | loss: 0.01718 | val_0_rmse: 0.1472  | val_1_rmse: 0.15311 |  0:00:07s
epoch 25 | loss: 0.01721 | val_0_rmse: 0.14415 | val_1_rmse: 0.15022 |  0:00:07s
epoch 26 | loss: 0.01686 | val_0_rmse: 0.14276 | val_1_rmse: 0.14924 |  0:00:08s
epoch 27 | loss: 0.01726 | val_0_rmse: 0.14325 | val_1_rmse: 0.14994 |  0:00:08s
epoch 28 | loss: 0.01704 | val_0_rmse: 0.14327 | val_1_rmse: 0.14949 |  0:00:08s
epoch 29 | loss: 0.01681 | val_0_rmse: 0.14268 | val_1_rmse: 0.14852 |  0:00:08s
epoch 30 | loss: 0.01675 | val_0_rmse: 0.14135 | val_1_rmse: 0.14726 |  0:00:09s
epoch 31 | loss: 0.01679 | val_0_rmse: 0.13957 | val_1_rmse: 0.14525 |  0:00:09s
epoch 32 | loss: 0.01669 | val_0_rmse: 0.1365  | val_1_rmse: 0.14097 |  0:00:09s
epoch 33 | loss: 0.01634 | val_0_rmse: 0.13739 | val_1_rmse: 0.14089 |  0:00:10s
epoch 34 | loss: 0.01663 | val_0_rmse: 0.13388 | val_1_rmse: 0.13705 |  0:00:10s
epoch 35 | loss: 0.0166  | val_0_rmse: 0.13324 | val_1_rmse: 0.13682 |  0:00:10s
epoch 36 | loss: 0.01654 | val_0_rmse: 0.13384 | val_1_rmse: 0.13812 |  0:00:10s
epoch 37 | loss: 0.01655 | val_0_rmse: 0.13343 | val_1_rmse: 0.13819 |  0:00:11s
epoch 38 | loss: 0.01651 | val_0_rmse: 0.13245 | val_1_rmse: 0.13698 |  0:00:11s
epoch 39 | loss: 0.01635 | val_0_rmse: 0.13167 | val_1_rmse: 0.13647 |  0:00:11s
epoch 40 | loss: 0.01633 | val_0_rmse: 0.13279 | val_1_rmse: 0.13831 |  0:00:12s
epoch 41 | loss: 0.01588 | val_0_rmse: 0.13272 | val_1_rmse: 0.13777 |  0:00:12s
epoch 42 | loss: 0.01637 | val_0_rmse: 0.13219 | val_1_rmse: 0.13665 |  0:00:12s
epoch 43 | loss: 0.01601 | val_0_rmse: 0.13296 | val_1_rmse: 0.13747 |  0:00:13s
epoch 44 | loss: 0.01658 | val_0_rmse: 0.13234 | val_1_rmse: 0.13714 |  0:00:13s
epoch 45 | loss: 0.0166  | val_0_rmse: 0.13423 | val_1_rmse: 0.13897 |  0:00:13s
epoch 46 | loss: 0.01716 | val_0_rmse: 0.12999 | val_1_rmse: 0.13451 |  0:00:13s
epoch 47 | loss: 0.0163  | val_0_rmse: 0.12955 | val_1_rmse: 0.13369 |  0:00:14s
epoch 48 | loss: 0.01597 | val_0_rmse: 0.12837 | val_1_rmse: 0.13289 |  0:00:14s
epoch 49 | loss: 0.01573 | val_0_rmse: 0.12919 | val_1_rmse: 0.13331 |  0:00:14s
epoch 50 | loss: 0.0154  | val_0_rmse: 0.12933 | val_1_rmse: 0.13285 |  0:00:15s
epoch 51 | loss: 0.01541 | val_0_rmse: 0.12884 | val_1_rmse: 0.1321  |  0:00:15s
epoch 52 | loss: 0.01548 | val_0_rmse: 0.13019 | val_1_rmse: 0.13381 |  0:00:15s
epoch 53 | loss: 0.01567 | val_0_rmse: 0.12978 | val_1_rmse: 0.13322 |  0:00:15s
epoch 54 | loss: 0.01561 | val_0_rmse: 0.12857 | val_1_rmse: 0.13183 |  0:00:16s
epoch 55 | loss: 0.01517 | val_0_rmse: 0.12863 | val_1_rmse: 0.1321  |  0:00:16s
epoch 56 | loss: 0.01544 | val_0_rmse: 0.12912 | val_1_rmse: 0.13442 |  0:00:16s
epoch 57 | loss: 0.01585 | val_0_rmse: 0.12813 | val_1_rmse: 0.13289 |  0:00:17s
epoch 58 | loss: 0.0157  | val_0_rmse: 0.12862 | val_1_rmse: 0.13285 |  0:00:17s
epoch 59 | loss: 0.01622 | val_0_rmse: 0.13093 | val_1_rmse: 0.13531 |  0:00:17s
epoch 60 | loss: 0.01625 | val_0_rmse: 0.1286  | val_1_rmse: 0.13261 |  0:00:18s
epoch 61 | loss: 0.01595 | val_0_rmse: 0.12812 | val_1_rmse: 0.13203 |  0:00:18s
epoch 62 | loss: 0.01568 | val_0_rmse: 0.12604 | val_1_rmse: 0.12812 |  0:00:18s
epoch 63 | loss: 0.01574 | val_0_rmse: 0.12593 | val_1_rmse: 0.12726 |  0:00:18s
epoch 64 | loss: 0.01574 | val_0_rmse: 0.12639 | val_1_rmse: 0.12939 |  0:00:19s
epoch 65 | loss: 0.01554 | val_0_rmse: 0.12492 | val_1_rmse: 0.12832 |  0:00:19s
epoch 66 | loss: 0.01526 | val_0_rmse: 0.12625 | val_1_rmse: 0.12896 |  0:00:19s
epoch 67 | loss: 0.01552 | val_0_rmse: 0.12749 | val_1_rmse: 0.13096 |  0:00:20s
epoch 68 | loss: 0.01548 | val_0_rmse: 0.12608 | val_1_rmse: 0.12962 |  0:00:20s
epoch 69 | loss: 0.01555 | val_0_rmse: 0.12657 | val_1_rmse: 0.12927 |  0:00:20s
epoch 70 | loss: 0.01499 | val_0_rmse: 0.12693 | val_1_rmse: 0.12966 |  0:00:20s
epoch 71 | loss: 0.01532 | val_0_rmse: 0.12595 | val_1_rmse: 0.12894 |  0:00:21s
epoch 72 | loss: 0.01549 | val_0_rmse: 0.12579 | val_1_rmse: 0.12832 |  0:00:21s
epoch 73 | loss: 0.01521 | val_0_rmse: 0.12648 | val_1_rmse: 0.12871 |  0:00:21s
epoch 74 | loss: 0.01505 | val_0_rmse: 0.12482 | val_1_rmse: 0.12837 |  0:00:22s
epoch 75 | loss: 0.01477 | val_0_rmse: 0.1242  | val_1_rmse: 0.12723 |  0:00:22s
epoch 76 | loss: 0.01487 | val_0_rmse: 0.12506 | val_1_rmse: 0.12765 |  0:00:22s
epoch 77 | loss: 0.01484 | val_0_rmse: 0.12624 | val_1_rmse: 0.12846 |  0:00:22s
epoch 78 | loss: 0.01506 | val_0_rmse: 0.12768 | val_1_rmse: 0.12922 |  0:00:23s
epoch 79 | loss: 0.01502 | val_0_rmse: 0.12867 | val_1_rmse: 0.13004 |  0:00:23s
epoch 80 | loss: 0.01462 | val_0_rmse: 0.12593 | val_1_rmse: 0.12698 |  0:00:23s
epoch 81 | loss: 0.01483 | val_0_rmse: 0.12555 | val_1_rmse: 0.12612 |  0:00:24s
epoch 82 | loss: 0.01488 | val_0_rmse: 0.1288  | val_1_rmse: 0.12916 |  0:00:24s
epoch 83 | loss: 0.01483 | val_0_rmse: 0.12472 | val_1_rmse: 0.12574 |  0:00:24s
epoch 84 | loss: 0.01473 | val_0_rmse: 0.12498 | val_1_rmse: 0.12609 |  0:00:25s
epoch 85 | loss: 0.0146  | val_0_rmse: 0.12459 | val_1_rmse: 0.12567 |  0:00:25s
epoch 86 | loss: 0.0144  | val_0_rmse: 0.12458 | val_1_rmse: 0.12594 |  0:00:25s
epoch 87 | loss: 0.01427 | val_0_rmse: 0.12687 | val_1_rmse: 0.12786 |  0:00:25s
epoch 88 | loss: 0.01433 | val_0_rmse: 0.12418 | val_1_rmse: 0.12598 |  0:00:26s
epoch 89 | loss: 0.01453 | val_0_rmse: 0.12546 | val_1_rmse: 0.12673 |  0:00:26s
epoch 90 | loss: 0.01428 | val_0_rmse: 0.12542 | val_1_rmse: 0.12705 |  0:00:26s
epoch 91 | loss: 0.01422 | val_0_rmse: 0.12261 | val_1_rmse: 0.12433 |  0:00:27s
epoch 92 | loss: 0.01398 | val_0_rmse: 0.12341 | val_1_rmse: 0.12482 |  0:00:27s
epoch 93 | loss: 0.01414 | val_0_rmse: 0.12772 | val_1_rmse: 0.12938 |  0:00:27s
epoch 94 | loss: 0.01403 | val_0_rmse: 0.1223  | val_1_rmse: 0.1243  |  0:00:28s
epoch 95 | loss: 0.014   | val_0_rmse: 0.12227 | val_1_rmse: 0.12333 |  0:00:28s
epoch 96 | loss: 0.01401 | val_0_rmse: 0.13294 | val_1_rmse: 0.13323 |  0:00:28s
epoch 97 | loss: 0.01416 | val_0_rmse: 0.12624 | val_1_rmse: 0.1286  |  0:00:28s
epoch 98 | loss: 0.01381 | val_0_rmse: 0.1229  | val_1_rmse: 0.12466 |  0:00:29s
epoch 99 | loss: 0.01338 | val_0_rmse: 0.12303 | val_1_rmse: 0.12435 |  0:00:29s
epoch 100| loss: 0.01364 | val_0_rmse: 0.12282 | val_1_rmse: 0.12288 |  0:00:29s
epoch 101| loss: 0.01361 | val_0_rmse: 0.13258 | val_1_rmse: 0.13398 |  0:00:30s
epoch 102| loss: 0.0138  | val_0_rmse: 0.1297  | val_1_rmse: 0.13028 |  0:00:30s
epoch 103| loss: 0.01328 | val_0_rmse: 0.12406 | val_1_rmse: 0.12594 |  0:00:30s
epoch 104| loss: 0.01348 | val_0_rmse: 0.12173 | val_1_rmse: 0.12425 |  0:00:30s
epoch 105| loss: 0.01331 | val_0_rmse: 0.12719 | val_1_rmse: 0.12803 |  0:00:31s
epoch 106| loss: 0.0133  | val_0_rmse: 0.12549 | val_1_rmse: 0.12614 |  0:00:31s
epoch 107| loss: 0.01324 | val_0_rmse: 0.12127 | val_1_rmse: 0.12112 |  0:00:31s
epoch 108| loss: 0.01345 | val_0_rmse: 0.12392 | val_1_rmse: 0.12498 |  0:00:32s
epoch 109| loss: 0.01292 | val_0_rmse: 0.12235 | val_1_rmse: 0.12451 |  0:00:32s
epoch 110| loss: 0.01351 | val_0_rmse: 0.12409 | val_1_rmse: 0.12669 |  0:00:32s
epoch 111| loss: 0.01311 | val_0_rmse: 0.12835 | val_1_rmse: 0.13021 |  0:00:32s
epoch 112| loss: 0.01304 | val_0_rmse: 0.12205 | val_1_rmse: 0.12219 |  0:00:33s
epoch 113| loss: 0.01328 | val_0_rmse: 0.12548 | val_1_rmse: 0.12628 |  0:00:33s
epoch 114| loss: 0.01313 | val_0_rmse: 0.13175 | val_1_rmse: 0.13351 |  0:00:33s
epoch 115| loss: 0.01283 | val_0_rmse: 0.12073 | val_1_rmse: 0.12298 |  0:00:34s
epoch 116| loss: 0.01328 | val_0_rmse: 0.12123 | val_1_rmse: 0.12286 |  0:00:34s
epoch 117| loss: 0.01339 | val_0_rmse: 0.13464 | val_1_rmse: 0.13616 |  0:00:34s
epoch 118| loss: 0.01376 | val_0_rmse: 0.12315 | val_1_rmse: 0.12465 |  0:00:35s
epoch 119| loss: 0.01362 | val_0_rmse: 0.12131 | val_1_rmse: 0.12159 |  0:00:35s
epoch 120| loss: 0.01364 | val_0_rmse: 0.12791 | val_1_rmse: 0.12788 |  0:00:35s
epoch 121| loss: 0.01299 | val_0_rmse: 0.12177 | val_1_rmse: 0.12194 |  0:00:35s
epoch 122| loss: 0.01353 | val_0_rmse: 0.12116 | val_1_rmse: 0.12348 |  0:00:36s
epoch 123| loss: 0.01322 | val_0_rmse: 0.12305 | val_1_rmse: 0.12334 |  0:00:36s
epoch 124| loss: 0.01312 | val_0_rmse: 0.12665 | val_1_rmse: 0.12873 |  0:00:36s
epoch 125| loss: 0.0132  | val_0_rmse: 0.12047 | val_1_rmse: 0.1216  |  0:00:37s
epoch 126| loss: 0.01318 | val_0_rmse: 0.11962 | val_1_rmse: 0.11941 |  0:00:37s
epoch 127| loss: 0.0132  | val_0_rmse: 0.11888 | val_1_rmse: 0.12131 |  0:00:37s
epoch 128| loss: 0.01323 | val_0_rmse: 0.11853 | val_1_rmse: 0.11971 |  0:00:37s
epoch 129| loss: 0.01283 | val_0_rmse: 0.12115 | val_1_rmse: 0.12181 |  0:00:38s
epoch 130| loss: 0.01287 | val_0_rmse: 0.11802 | val_1_rmse: 0.11872 |  0:00:38s
epoch 131| loss: 0.0126  | val_0_rmse: 0.11724 | val_1_rmse: 0.12031 |  0:00:38s
epoch 132| loss: 0.0126  | val_0_rmse: 0.12018 | val_1_rmse: 0.11989 |  0:00:39s
epoch 133| loss: 0.01276 | val_0_rmse: 0.11688 | val_1_rmse: 0.11701 |  0:00:39s
epoch 134| loss: 0.01244 | val_0_rmse: 0.1198  | val_1_rmse: 0.12014 |  0:00:39s
epoch 135| loss: 0.01249 | val_0_rmse: 0.11514 | val_1_rmse: 0.1161  |  0:00:40s
epoch 136| loss: 0.01251 | val_0_rmse: 0.11569 | val_1_rmse: 0.11619 |  0:00:40s
epoch 137| loss: 0.01216 | val_0_rmse: 0.11686 | val_1_rmse: 0.11615 |  0:00:40s
epoch 138| loss: 0.01252 | val_0_rmse: 0.11835 | val_1_rmse: 0.11883 |  0:00:40s
epoch 139| loss: 0.0127  | val_0_rmse: 0.11312 | val_1_rmse: 0.11475 |  0:00:41s
epoch 140| loss: 0.01194 | val_0_rmse: 0.11806 | val_1_rmse: 0.11665 |  0:00:41s
epoch 141| loss: 0.0125  | val_0_rmse: 0.12026 | val_1_rmse: 0.11998 |  0:00:41s
epoch 142| loss: 0.01271 | val_0_rmse: 0.11537 | val_1_rmse: 0.11708 |  0:00:42s
epoch 143| loss: 0.01221 | val_0_rmse: 0.11444 | val_1_rmse: 0.11617 |  0:00:42s
epoch 144| loss: 0.01192 | val_0_rmse: 0.1138  | val_1_rmse: 0.11413 |  0:00:42s
epoch 145| loss: 0.01241 | val_0_rmse: 0.11324 | val_1_rmse: 0.11418 |  0:00:42s
epoch 146| loss: 0.01205 | val_0_rmse: 0.11188 | val_1_rmse: 0.11347 |  0:00:43s
epoch 147| loss: 0.01198 | val_0_rmse: 0.113   | val_1_rmse: 0.11504 |  0:00:43s
epoch 148| loss: 0.01186 | val_0_rmse: 0.11519 | val_1_rmse: 0.11798 |  0:00:43s
epoch 149| loss: 0.01183 | val_0_rmse: 0.1143  | val_1_rmse: 0.11691 |  0:00:44s
Stop training because you reached max_epochs = 150 with best_epoch = 146 and best_val_1_rmse = 0.11347
Best weights from best epoch are automatically used!
ended training at: 06:25:40
Feature importance:
[('Area', 0.25451357262384394), ('Baths', 0.2547608027273694), ('Beds', 0.23391100299811293), ('Latitude', 0.0), ('Longitude', 0.06316823647401261), ('Month', 0.16273239700232583), ('Year', 0.030913988174335306)]
Mean squared error is of 3577898551.5884213
Mean absolute error:41051.58317863964
MAPE:0.34418137512199654
R2 score:0.5472338436490258
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:25:40
epoch 0  | loss: 0.66365 | val_0_rmse: 0.23348 | val_1_rmse: 0.23514 |  0:00:00s
epoch 1  | loss: 0.07595 | val_0_rmse: 0.17675 | val_1_rmse: 0.1814  |  0:00:00s
epoch 2  | loss: 0.0374  | val_0_rmse: 0.14357 | val_1_rmse: 0.15327 |  0:00:00s
epoch 3  | loss: 0.02336 | val_0_rmse: 0.15979 | val_1_rmse: 0.1657  |  0:00:01s
epoch 4  | loss: 0.02032 | val_0_rmse: 0.16038 | val_1_rmse: 0.16787 |  0:00:01s
epoch 5  | loss: 0.01881 | val_0_rmse: 0.17349 | val_1_rmse: 0.17844 |  0:00:01s
epoch 6  | loss: 0.01911 | val_0_rmse: 0.16777 | val_1_rmse: 0.17464 |  0:00:02s
epoch 7  | loss: 0.01805 | val_0_rmse: 0.1723  | val_1_rmse: 0.17849 |  0:00:02s
epoch 8  | loss: 0.01733 | val_0_rmse: 0.17283 | val_1_rmse: 0.17904 |  0:00:02s
epoch 9  | loss: 0.01702 | val_0_rmse: 0.16566 | val_1_rmse: 0.17318 |  0:00:02s
epoch 10 | loss: 0.01661 | val_0_rmse: 0.16672 | val_1_rmse: 0.17352 |  0:00:03s
epoch 11 | loss: 0.01639 | val_0_rmse: 0.16081 | val_1_rmse: 0.16892 |  0:00:03s
epoch 12 | loss: 0.01628 | val_0_rmse: 0.16555 | val_1_rmse: 0.17257 |  0:00:03s
epoch 13 | loss: 0.01555 | val_0_rmse: 0.16173 | val_1_rmse: 0.17001 |  0:00:04s
epoch 14 | loss: 0.01564 | val_0_rmse: 0.16386 | val_1_rmse: 0.17145 |  0:00:04s
epoch 15 | loss: 0.01512 | val_0_rmse: 0.16105 | val_1_rmse: 0.16913 |  0:00:04s
epoch 16 | loss: 0.01526 | val_0_rmse: 0.16202 | val_1_rmse: 0.16928 |  0:00:05s
epoch 17 | loss: 0.01529 | val_0_rmse: 0.15964 | val_1_rmse: 0.16759 |  0:00:05s
epoch 18 | loss: 0.01527 | val_0_rmse: 0.16208 | val_1_rmse: 0.16949 |  0:00:05s
epoch 19 | loss: 0.01511 | val_0_rmse: 0.15914 | val_1_rmse: 0.167   |  0:00:05s
epoch 20 | loss: 0.01541 | val_0_rmse: 0.15831 | val_1_rmse: 0.16629 |  0:00:06s
epoch 21 | loss: 0.01499 | val_0_rmse: 0.15795 | val_1_rmse: 0.16582 |  0:00:06s
epoch 22 | loss: 0.01475 | val_0_rmse: 0.15562 | val_1_rmse: 0.16392 |  0:00:06s
epoch 23 | loss: 0.01479 | val_0_rmse: 0.15654 | val_1_rmse: 0.16455 |  0:00:07s
epoch 24 | loss: 0.01521 | val_0_rmse: 0.15702 | val_1_rmse: 0.16495 |  0:00:07s
epoch 25 | loss: 0.01499 | val_0_rmse: 0.1587  | val_1_rmse: 0.16607 |  0:00:07s
epoch 26 | loss: 0.0148  | val_0_rmse: 0.15394 | val_1_rmse: 0.16271 |  0:00:07s
epoch 27 | loss: 0.01476 | val_0_rmse: 0.16011 | val_1_rmse: 0.16745 |  0:00:08s
epoch 28 | loss: 0.0148  | val_0_rmse: 0.15359 | val_1_rmse: 0.16234 |  0:00:08s
epoch 29 | loss: 0.01446 | val_0_rmse: 0.1514  | val_1_rmse: 0.16073 |  0:00:08s
epoch 30 | loss: 0.01437 | val_0_rmse: 0.1542  | val_1_rmse: 0.16275 |  0:00:09s
epoch 31 | loss: 0.01442 | val_0_rmse: 0.15263 | val_1_rmse: 0.16138 |  0:00:09s
epoch 32 | loss: 0.01459 | val_0_rmse: 0.15159 | val_1_rmse: 0.16063 |  0:00:09s

Early stopping occured at epoch 32 with best_epoch = 2 and best_val_1_rmse = 0.15327
Best weights from best epoch are automatically used!
ended training at: 06:25:50
Feature importance:
[('Area', 0.031982928511758324), ('Baths', 0.16857505879524132), ('Beds', 0.22659301268139537), ('Latitude', 0.18341004520532636), ('Longitude', 0.1622819300259611), ('Month', 0.002830934784620247), ('Year', 0.22432608999569725)]
Mean squared error is of 5348496324.237502
Mean absolute error:52031.74001506791
MAPE:0.45806558503415357
R2 score:0.361519490801395
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:25:50
epoch 0  | loss: 0.06072 | val_0_rmse: 0.17807 | val_1_rmse: 0.17906 |  0:00:03s
epoch 1  | loss: 0.01973 | val_0_rmse: 0.17397 | val_1_rmse: 0.17422 |  0:00:07s
epoch 2  | loss: 0.01909 | val_0_rmse: 0.16666 | val_1_rmse: 0.16729 |  0:00:11s
epoch 3  | loss: 0.01876 | val_0_rmse: 0.16106 | val_1_rmse: 0.16156 |  0:00:15s
epoch 4  | loss: 0.01849 | val_0_rmse: 0.14358 | val_1_rmse: 0.14407 |  0:00:19s
epoch 5  | loss: 0.01851 | val_0_rmse: 0.14525 | val_1_rmse: 0.14544 |  0:00:23s
epoch 6  | loss: 0.01845 | val_0_rmse: 0.13725 | val_1_rmse: 0.13833 |  0:00:27s
epoch 7  | loss: 0.01848 | val_0_rmse: 0.13524 | val_1_rmse: 0.13606 |  0:00:30s
epoch 8  | loss: 0.01816 | val_0_rmse: 0.133   | val_1_rmse: 0.13416 |  0:00:34s
epoch 9  | loss: 0.01812 | val_0_rmse: 0.13598 | val_1_rmse: 0.13671 |  0:00:38s
epoch 10 | loss: 0.01815 | val_0_rmse: 0.13326 | val_1_rmse: 0.13462 |  0:00:42s
epoch 11 | loss: 0.01803 | val_0_rmse: 0.13706 | val_1_rmse: 0.1386  |  0:00:46s
epoch 12 | loss: 0.018   | val_0_rmse: 0.13164 | val_1_rmse: 0.1329  |  0:00:50s
epoch 13 | loss: 0.01821 | val_0_rmse: 0.13145 | val_1_rmse: 0.13296 |  0:00:53s
epoch 14 | loss: 0.0179  | val_0_rmse: 0.13097 | val_1_rmse: 0.13258 |  0:00:57s
epoch 15 | loss: 0.01772 | val_0_rmse: 0.13032 | val_1_rmse: 0.13206 |  0:01:01s
epoch 16 | loss: 0.01774 | val_0_rmse: 0.13506 | val_1_rmse: 0.13663 |  0:01:05s
epoch 17 | loss: 0.018   | val_0_rmse: 0.13163 | val_1_rmse: 0.13318 |  0:01:09s
epoch 18 | loss: 0.01807 | val_0_rmse: 0.13289 | val_1_rmse: 0.13428 |  0:01:13s
epoch 19 | loss: 0.01776 | val_0_rmse: 0.1328  | val_1_rmse: 0.13406 |  0:01:16s
epoch 20 | loss: 0.01818 | val_0_rmse: 0.13603 | val_1_rmse: 0.13718 |  0:01:20s
epoch 21 | loss: 0.019   | val_0_rmse: 0.14599 | val_1_rmse: 0.16896 |  0:01:24s
epoch 22 | loss: 0.01886 | val_0_rmse: 0.13499 | val_1_rmse: 0.14708 |  0:01:28s
epoch 23 | loss: 0.01838 | val_0_rmse: 0.13479 | val_1_rmse: 0.14782 |  0:01:32s
epoch 24 | loss: 0.01843 | val_0_rmse: 0.13595 | val_1_rmse: 0.15028 |  0:01:36s
epoch 25 | loss: 0.01892 | val_0_rmse: 0.13767 | val_1_rmse: 0.14283 |  0:01:40s
epoch 26 | loss: 0.01828 | val_0_rmse: 0.13622 | val_1_rmse: 0.14528 |  0:01:44s
epoch 27 | loss: 0.01822 | val_0_rmse: 0.13198 | val_1_rmse: 0.1492  |  0:01:47s
epoch 28 | loss: 0.01798 | val_0_rmse: 0.1328  | val_1_rmse: 0.14854 |  0:01:51s
epoch 29 | loss: 0.01788 | val_0_rmse: 0.1347  | val_1_rmse: 0.14262 |  0:01:55s
epoch 30 | loss: 0.01826 | val_0_rmse: 0.13544 | val_1_rmse: 0.14367 |  0:01:59s
epoch 31 | loss: 0.01803 | val_0_rmse: 0.13586 | val_1_rmse: 0.15764 |  0:02:03s
epoch 32 | loss: 0.01793 | val_0_rmse: 0.13561 | val_1_rmse: 0.15193 |  0:02:07s
epoch 33 | loss: 0.01784 | val_0_rmse: 0.13385 | val_1_rmse: 0.14359 |  0:02:10s
epoch 34 | loss: 0.0181  | val_0_rmse: 0.13377 | val_1_rmse: 0.15271 |  0:02:14s
epoch 35 | loss: 0.01768 | val_0_rmse: 0.13359 | val_1_rmse: 0.16182 |  0:02:18s
epoch 36 | loss: 0.01779 | val_0_rmse: 0.13177 | val_1_rmse: 0.1657  |  0:02:22s
epoch 37 | loss: 0.01782 | val_0_rmse: 0.14883 | val_1_rmse: 0.19677 |  0:02:26s
epoch 38 | loss: 0.01783 | val_0_rmse: 0.13384 | val_1_rmse: 0.13902 |  0:02:30s
epoch 39 | loss: 0.01808 | val_0_rmse: 0.13671 | val_1_rmse: 0.17515 |  0:02:33s
epoch 40 | loss: 0.01789 | val_0_rmse: 0.13396 | val_1_rmse: 0.14538 |  0:02:37s
epoch 41 | loss: 0.01764 | val_0_rmse: 0.13357 | val_1_rmse: 0.15016 |  0:02:41s
epoch 42 | loss: 0.01755 | val_0_rmse: 0.13076 | val_1_rmse: 0.13491 |  0:02:45s
epoch 43 | loss: 0.01754 | val_0_rmse: 0.13252 | val_1_rmse: 0.14927 |  0:02:49s
epoch 44 | loss: 0.01759 | val_0_rmse: 0.1315  | val_1_rmse: 0.15179 |  0:02:53s
epoch 45 | loss: 0.01787 | val_0_rmse: 0.138   | val_1_rmse: 0.24442 |  0:02:57s

Early stopping occured at epoch 45 with best_epoch = 15 and best_val_1_rmse = 0.13206
Best weights from best epoch are automatically used!
ended training at: 06:28:48
Feature importance:
[('Area', 0.360930756039306), ('Baths', 0.22212501471703705), ('Beds', 0.13692810336921427), ('Latitude', 0.03505317744039901), ('Longitude', 0.16719640817587775), ('Month', 0.016344516415745886), ('Year', 0.06142202384242001)]
Mean squared error is of 1081723087.6263192
Mean absolute error:22576.84890887875
MAPE:0.3653054740440472
R2 score:0.6852985885348906
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:28:49
epoch 0  | loss: 0.05778 | val_0_rmse: 0.20084 | val_1_rmse: 0.20058 |  0:00:03s
epoch 1  | loss: 0.02042 | val_0_rmse: 0.18901 | val_1_rmse: 0.18848 |  0:00:07s
epoch 2  | loss: 0.01984 | val_0_rmse: 0.1814  | val_1_rmse: 0.18034 |  0:00:11s
epoch 3  | loss: 0.01914 | val_0_rmse: 0.16795 | val_1_rmse: 0.16704 |  0:00:15s
epoch 4  | loss: 0.01975 | val_0_rmse: 0.14887 | val_1_rmse: 0.14731 |  0:00:19s
epoch 5  | loss: 0.01877 | val_0_rmse: 0.14921 | val_1_rmse: 0.14806 |  0:00:23s
epoch 6  | loss: 0.01867 | val_0_rmse: 0.14359 | val_1_rmse: 0.14232 |  0:00:27s
epoch 7  | loss: 0.01842 | val_0_rmse: 0.14426 | val_1_rmse: 0.14242 |  0:00:30s
epoch 8  | loss: 0.01867 | val_0_rmse: 0.13545 | val_1_rmse: 0.13382 |  0:00:34s
epoch 9  | loss: 0.01852 | val_0_rmse: 0.13722 | val_1_rmse: 0.136   |  0:00:38s
epoch 10 | loss: 0.01827 | val_0_rmse: 0.13362 | val_1_rmse: 0.13219 |  0:00:42s
epoch 11 | loss: 0.01818 | val_0_rmse: 0.13284 | val_1_rmse: 0.13133 |  0:00:46s
epoch 12 | loss: 0.01811 | val_0_rmse: 0.13281 | val_1_rmse: 0.1316  |  0:00:50s
epoch 13 | loss: 0.01792 | val_0_rmse: 0.13086 | val_1_rmse: 0.12976 |  0:00:54s
epoch 14 | loss: 0.01778 | val_0_rmse: 0.13065 | val_1_rmse: 0.12926 |  0:00:58s
epoch 15 | loss: 0.01796 | val_0_rmse: 0.13112 | val_1_rmse: 0.13008 |  0:01:01s
epoch 16 | loss: 0.01793 | val_0_rmse: 0.13142 | val_1_rmse: 0.13019 |  0:01:05s
epoch 17 | loss: 0.01785 | val_0_rmse: 0.13089 | val_1_rmse: 0.12943 |  0:01:09s
epoch 18 | loss: 0.01798 | val_0_rmse: 0.13473 | val_1_rmse: 0.13344 |  0:01:13s
epoch 19 | loss: 0.01778 | val_0_rmse: 0.13621 | val_1_rmse: 0.13505 |  0:01:17s
epoch 20 | loss: 0.01787 | val_0_rmse: 0.12999 | val_1_rmse: 0.1287  |  0:01:21s
epoch 21 | loss: 0.01754 | val_0_rmse: 0.13041 | val_1_rmse: 0.12934 |  0:01:25s
epoch 22 | loss: 0.01752 | val_0_rmse: 0.13052 | val_1_rmse: 0.12878 |  0:01:28s
epoch 23 | loss: 0.01753 | val_0_rmse: 0.13094 | val_1_rmse: 0.12953 |  0:01:32s
epoch 24 | loss: 0.01768 | val_0_rmse: 0.12947 | val_1_rmse: 0.12824 |  0:01:36s
epoch 25 | loss: 0.01716 | val_0_rmse: 0.13087 | val_1_rmse: 0.12964 |  0:01:40s
epoch 26 | loss: 0.01688 | val_0_rmse: 0.21967 | val_1_rmse: 0.21607 |  0:01:44s
epoch 27 | loss: 0.01722 | val_0_rmse: 0.1348  | val_1_rmse: 0.13337 |  0:01:48s
epoch 28 | loss: 0.01843 | val_0_rmse: 0.13103 | val_1_rmse: 0.13011 |  0:01:52s
epoch 29 | loss: 0.01772 | val_0_rmse: 0.13318 | val_1_rmse: 0.13272 |  0:01:55s
epoch 30 | loss: 0.01703 | val_0_rmse: 0.22721 | val_1_rmse: 0.22414 |  0:01:59s
epoch 31 | loss: 0.01691 | val_0_rmse: 0.1299  | val_1_rmse: 0.129   |  0:02:03s
epoch 32 | loss: 0.01718 | val_0_rmse: 0.13264 | val_1_rmse: 0.13176 |  0:02:07s
epoch 33 | loss: 0.01698 | val_0_rmse: 0.13141 | val_1_rmse: 0.13024 |  0:02:11s
epoch 34 | loss: 0.01686 | val_0_rmse: 0.14952 | val_1_rmse: 0.1472  |  0:02:15s
epoch 35 | loss: 0.01698 | val_0_rmse: 0.13705 | val_1_rmse: 0.13629 |  0:02:19s
epoch 36 | loss: 0.01664 | val_0_rmse: 0.13207 | val_1_rmse: 0.13099 |  0:02:22s
epoch 37 | loss: 0.01663 | val_0_rmse: 0.12866 | val_1_rmse: 0.12722 |  0:02:26s
epoch 38 | loss: 0.01657 | val_0_rmse: 0.13375 | val_1_rmse: 0.13175 |  0:02:30s
epoch 39 | loss: 0.01667 | val_0_rmse: 0.12825 | val_1_rmse: 0.12661 |  0:02:34s
epoch 40 | loss: 0.01646 | val_0_rmse: 0.13319 | val_1_rmse: 0.1323  |  0:02:38s
epoch 41 | loss: 0.01661 | val_0_rmse: 0.14445 | val_1_rmse: 0.14353 |  0:02:42s
epoch 42 | loss: 0.01654 | val_0_rmse: 0.14313 | val_1_rmse: 0.14238 |  0:02:45s
epoch 43 | loss: 0.01628 | val_0_rmse: 0.13214 | val_1_rmse: 0.13017 |  0:02:49s
epoch 44 | loss: 0.01666 | val_0_rmse: 0.13606 | val_1_rmse: 0.1349  |  0:02:53s
epoch 45 | loss: 0.01686 | val_0_rmse: 0.14215 | val_1_rmse: 0.1416  |  0:02:57s
epoch 46 | loss: 0.01671 | val_0_rmse: 0.12587 | val_1_rmse: 0.12427 |  0:03:01s
epoch 47 | loss: 0.01628 | val_0_rmse: 0.12489 | val_1_rmse: 0.12363 |  0:03:05s
epoch 48 | loss: 0.01639 | val_0_rmse: 0.12924 | val_1_rmse: 0.12748 |  0:03:09s
epoch 49 | loss: 0.01754 | val_0_rmse: 0.14135 | val_1_rmse: 0.13878 |  0:03:12s
epoch 50 | loss: 0.01827 | val_0_rmse: 0.13238 | val_1_rmse: 0.12975 |  0:03:16s
epoch 51 | loss: 0.01775 | val_0_rmse: 0.13491 | val_1_rmse: 0.13214 |  0:03:20s
epoch 52 | loss: 0.01778 | val_0_rmse: 0.13242 | val_1_rmse: 0.13079 |  0:03:24s
epoch 53 | loss: 0.01756 | val_0_rmse: 0.13234 | val_1_rmse: 0.13018 |  0:03:28s
epoch 54 | loss: 0.0175  | val_0_rmse: 0.13083 | val_1_rmse: 0.12849 |  0:03:32s
epoch 55 | loss: 0.01755 | val_0_rmse: 0.13096 | val_1_rmse: 0.12908 |  0:03:35s
epoch 56 | loss: 0.01761 | val_0_rmse: 0.13266 | val_1_rmse: 0.13104 |  0:03:39s
epoch 57 | loss: 0.01746 | val_0_rmse: 0.13146 | val_1_rmse: 0.12916 |  0:03:43s
epoch 58 | loss: 0.01877 | val_0_rmse: 0.13629 | val_1_rmse: 0.13327 |  0:03:47s
epoch 59 | loss: 0.01808 | val_0_rmse: 0.14824 | val_1_rmse: 0.14679 |  0:03:51s
epoch 60 | loss: 0.01783 | val_0_rmse: 0.13324 | val_1_rmse: 0.131   |  0:03:55s
epoch 61 | loss: 0.01784 | val_0_rmse: 0.13688 | val_1_rmse: 0.13483 |  0:03:59s
epoch 62 | loss: 0.01799 | val_0_rmse: 0.1339  | val_1_rmse: 0.13148 |  0:04:02s
epoch 63 | loss: 0.01815 | val_0_rmse: 0.14184 | val_1_rmse: 0.13877 |  0:04:06s
epoch 64 | loss: 0.01826 | val_0_rmse: 0.13307 | val_1_rmse: 0.13076 |  0:04:10s
epoch 65 | loss: 0.01792 | val_0_rmse: 0.13381 | val_1_rmse: 0.13212 |  0:04:14s
epoch 66 | loss: 0.01786 | val_0_rmse: 0.13619 | val_1_rmse: 0.13415 |  0:04:18s
epoch 67 | loss: 0.01782 | val_0_rmse: 0.1365  | val_1_rmse: 0.1346  |  0:04:22s
epoch 68 | loss: 0.01785 | val_0_rmse: 0.13416 | val_1_rmse: 0.13146 |  0:04:26s
epoch 69 | loss: 0.01793 | val_0_rmse: 0.13279 | val_1_rmse: 0.13086 |  0:04:29s
epoch 70 | loss: 0.01804 | val_0_rmse: 0.13518 | val_1_rmse: 0.13316 |  0:04:33s
epoch 71 | loss: 0.01782 | val_0_rmse: 0.13197 | val_1_rmse: 0.12978 |  0:04:37s
epoch 72 | loss: 0.01787 | val_0_rmse: 0.13299 | val_1_rmse: 0.13095 |  0:04:41s
epoch 73 | loss: 0.01799 | val_0_rmse: 0.14049 | val_1_rmse: 0.13907 |  0:04:45s
epoch 74 | loss: 0.01785 | val_0_rmse: 0.13185 | val_1_rmse: 0.12987 |  0:04:49s
epoch 75 | loss: 0.01768 | val_0_rmse: 0.13438 | val_1_rmse: 0.13225 |  0:04:53s
epoch 76 | loss: 0.01754 | val_0_rmse: 0.14343 | val_1_rmse: 0.14256 |  0:04:56s
epoch 77 | loss: 0.01776 | val_0_rmse: 0.14629 | val_1_rmse: 0.14679 |  0:05:00s

Early stopping occured at epoch 77 with best_epoch = 47 and best_val_1_rmse = 0.12363
Best weights from best epoch are automatically used!
ended training at: 06:33:51
Feature importance:
[('Area', 0.4467233333275699), ('Baths', 0.0), ('Beds', 0.229315312728212), ('Latitude', 0.1819430352133952), ('Longitude', 0.021687757925432805), ('Month', 0.008143040835328479), ('Year', 0.1121875199700616)]
Mean squared error is of 961915477.8831916
Mean absolute error:21390.926995852984
MAPE:0.34600538257875046
R2 score:0.7135859762637684
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:33:52
epoch 0  | loss: 0.0633  | val_0_rmse: 0.19458 | val_1_rmse: 0.193   |  0:00:03s
epoch 1  | loss: 0.01906 | val_0_rmse: 0.18792 | val_1_rmse: 0.1862  |  0:00:07s
epoch 2  | loss: 0.0188  | val_0_rmse: 0.18578 | val_1_rmse: 0.18405 |  0:00:11s
epoch 3  | loss: 0.01841 | val_0_rmse: 0.16772 | val_1_rmse: 0.16621 |  0:00:15s
epoch 4  | loss: 0.01856 | val_0_rmse: 0.17459 | val_1_rmse: 0.17313 |  0:00:19s
epoch 5  | loss: 0.01811 | val_0_rmse: 0.15976 | val_1_rmse: 0.15857 |  0:00:23s
epoch 6  | loss: 0.01794 | val_0_rmse: 0.16256 | val_1_rmse: 0.16143 |  0:00:27s
epoch 7  | loss: 0.01828 | val_0_rmse: 0.14778 | val_1_rmse: 0.1467  |  0:00:30s
epoch 8  | loss: 0.0181  | val_0_rmse: 0.14309 | val_1_rmse: 0.14241 |  0:00:34s
epoch 9  | loss: 0.01819 | val_0_rmse: 0.13811 | val_1_rmse: 0.1377  |  0:00:38s
epoch 10 | loss: 0.01814 | val_0_rmse: 0.13468 | val_1_rmse: 0.1346  |  0:00:42s
epoch 11 | loss: 0.01804 | val_0_rmse: 0.13316 | val_1_rmse: 0.1325  |  0:00:46s
epoch 12 | loss: 0.01803 | val_0_rmse: 0.13497 | val_1_rmse: 0.13491 |  0:00:50s
epoch 13 | loss: 0.01808 | val_0_rmse: 0.13615 | val_1_rmse: 0.13554 |  0:00:54s
epoch 14 | loss: 0.01782 | val_0_rmse: 0.13233 | val_1_rmse: 0.13284 |  0:00:57s
epoch 15 | loss: 0.01793 | val_0_rmse: 0.13602 | val_1_rmse: 0.13528 |  0:01:01s
epoch 16 | loss: 0.01789 | val_0_rmse: 0.13513 | val_1_rmse: 0.13445 |  0:01:05s
epoch 17 | loss: 0.01981 | val_0_rmse: 0.14017 | val_1_rmse: 0.13988 |  0:01:09s
epoch 18 | loss: 0.01972 | val_0_rmse: 0.14238 | val_1_rmse: 0.16067 |  0:01:13s
epoch 19 | loss: 0.01986 | val_0_rmse: 0.14552 | val_1_rmse: 0.18436 |  0:01:17s
epoch 20 | loss: 0.0193  | val_0_rmse: 0.1453  | val_1_rmse: 0.1693  |  0:01:21s
epoch 21 | loss: 0.0194  | val_0_rmse: 0.14    | val_1_rmse: 0.15177 |  0:01:24s
epoch 22 | loss: 0.01883 | val_0_rmse: 0.14163 | val_1_rmse: 0.15049 |  0:01:28s
epoch 23 | loss: 0.01874 | val_0_rmse: 0.14187 | val_1_rmse: 0.14161 |  0:01:32s
epoch 24 | loss: 0.01861 | val_0_rmse: 0.14075 | val_1_rmse: 0.13989 |  0:01:36s
epoch 25 | loss: 0.0183  | val_0_rmse: 0.13423 | val_1_rmse: 0.13387 |  0:01:40s
epoch 26 | loss: 0.01838 | val_0_rmse: 0.13591 | val_1_rmse: 0.13494 |  0:01:44s
epoch 27 | loss: 0.01805 | val_0_rmse: 0.13661 | val_1_rmse: 0.13582 |  0:01:47s
epoch 28 | loss: 0.01819 | val_0_rmse: 0.13346 | val_1_rmse: 0.13426 |  0:01:51s
epoch 29 | loss: 0.01812 | val_0_rmse: 0.13347 | val_1_rmse: 0.13362 |  0:01:55s
epoch 30 | loss: 0.01814 | val_0_rmse: 0.13514 | val_1_rmse: 0.13465 |  0:01:59s
epoch 31 | loss: 0.0187  | val_0_rmse: 0.13684 | val_1_rmse: 0.13642 |  0:02:03s
epoch 32 | loss: 0.01894 | val_0_rmse: 0.13432 | val_1_rmse: 0.13394 |  0:02:07s
epoch 33 | loss: 0.01884 | val_0_rmse: 0.13553 | val_1_rmse: 0.13546 |  0:02:11s
epoch 34 | loss: 0.01846 | val_0_rmse: 0.13498 | val_1_rmse: 0.1346  |  0:02:14s
epoch 35 | loss: 0.0191  | val_0_rmse: 0.13463 | val_1_rmse: 0.13415 |  0:02:18s
epoch 36 | loss: 0.01862 | val_0_rmse: 0.13429 | val_1_rmse: 0.13382 |  0:02:22s
epoch 37 | loss: 0.01871 | val_0_rmse: 0.13833 | val_1_rmse: 0.13801 |  0:02:26s
epoch 38 | loss: 0.01859 | val_0_rmse: 0.13454 | val_1_rmse: 0.13352 |  0:02:30s
epoch 39 | loss: 0.01856 | val_0_rmse: 0.134   | val_1_rmse: 0.1333  |  0:02:34s
epoch 40 | loss: 0.01839 | val_0_rmse: 0.13371 | val_1_rmse: 0.13314 |  0:02:37s
epoch 41 | loss: 0.0185  | val_0_rmse: 0.13593 | val_1_rmse: 0.13548 |  0:02:41s

Early stopping occured at epoch 41 with best_epoch = 11 and best_val_1_rmse = 0.1325
Best weights from best epoch are automatically used!
ended training at: 06:36:35
Feature importance:
[('Area', 0.36249215083937036), ('Baths', 0.18874943168536548), ('Beds', 0.1606246502753001), ('Latitude', 0.04142040442342688), ('Longitude', 0.1703265785791927), ('Month', 0.039436557039216204), ('Year', 0.036950227158128256)]
Mean squared error is of 1071481317.0632616
Mean absolute error:23217.72036389992
MAPE:0.386591811208811
R2 score:0.6791859482621634
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:36:35
epoch 0  | loss: 0.0617  | val_0_rmse: 0.18254 | val_1_rmse: 0.18207 |  0:00:03s
epoch 1  | loss: 0.01996 | val_0_rmse: 0.18935 | val_1_rmse: 0.18914 |  0:00:07s
epoch 2  | loss: 0.01963 | val_0_rmse: 0.17935 | val_1_rmse: 0.17908 |  0:00:11s
epoch 3  | loss: 0.0192  | val_0_rmse: 0.16283 | val_1_rmse: 0.16267 |  0:00:15s
epoch 4  | loss: 0.01932 | val_0_rmse: 0.15903 | val_1_rmse: 0.15894 |  0:00:19s
epoch 5  | loss: 0.01893 | val_0_rmse: 0.15742 | val_1_rmse: 0.15733 |  0:00:23s
epoch 6  | loss: 0.01872 | val_0_rmse: 0.15324 | val_1_rmse: 0.15339 |  0:00:27s
epoch 7  | loss: 0.01924 | val_0_rmse: 0.14638 | val_1_rmse: 0.14617 |  0:00:30s
epoch 8  | loss: 0.01908 | val_0_rmse: 0.14211 | val_1_rmse: 0.14176 |  0:00:34s
epoch 9  | loss: 0.0185  | val_0_rmse: 0.14453 | val_1_rmse: 0.14463 |  0:00:38s
epoch 10 | loss: 0.01907 | val_0_rmse: 0.13659 | val_1_rmse: 0.13656 |  0:00:42s
epoch 11 | loss: 0.01861 | val_0_rmse: 0.13562 | val_1_rmse: 0.13531 |  0:00:46s
epoch 12 | loss: 0.01846 | val_0_rmse: 0.13199 | val_1_rmse: 0.1315  |  0:00:50s
epoch 13 | loss: 0.01867 | val_0_rmse: 0.14109 | val_1_rmse: 0.14108 |  0:00:54s
epoch 14 | loss: 0.01847 | val_0_rmse: 0.13236 | val_1_rmse: 0.13191 |  0:00:57s
epoch 15 | loss: 0.01823 | val_0_rmse: 0.14133 | val_1_rmse: 0.14214 |  0:01:01s
epoch 16 | loss: 0.01808 | val_0_rmse: 0.13189 | val_1_rmse: 0.13145 |  0:01:05s
epoch 17 | loss: 0.01863 | val_0_rmse: 0.13269 | val_1_rmse: 0.13219 |  0:01:09s
epoch 18 | loss: 0.01859 | val_0_rmse: 0.1344  | val_1_rmse: 0.13412 |  0:01:13s
epoch 19 | loss: 0.01857 | val_0_rmse: 0.13249 | val_1_rmse: 0.13193 |  0:01:17s
epoch 20 | loss: 0.01818 | val_0_rmse: 0.1328  | val_1_rmse: 0.13204 |  0:01:20s
epoch 21 | loss: 0.0183  | val_0_rmse: 0.13765 | val_1_rmse: 0.1379  |  0:01:24s
epoch 22 | loss: 0.01841 | val_0_rmse: 0.14576 | val_1_rmse: 0.14493 |  0:01:28s
epoch 23 | loss: 0.01749 | val_0_rmse: 0.14081 | val_1_rmse: 0.14174 |  0:01:32s
epoch 24 | loss: 0.01735 | val_0_rmse: 0.12816 | val_1_rmse: 0.12794 |  0:01:36s
epoch 25 | loss: 0.01705 | val_0_rmse: 0.13576 | val_1_rmse: 0.13664 |  0:01:40s
epoch 26 | loss: 0.01698 | val_0_rmse: 0.12769 | val_1_rmse: 0.12796 |  0:01:44s
epoch 27 | loss: 0.01713 | val_0_rmse: 0.13957 | val_1_rmse: 0.14022 |  0:01:47s
epoch 28 | loss: 0.01723 | val_0_rmse: 0.14064 | val_1_rmse: 0.14127 |  0:01:51s
epoch 29 | loss: 0.01703 | val_0_rmse: 0.14076 | val_1_rmse: 0.14129 |  0:01:55s
epoch 30 | loss: 0.01706 | val_0_rmse: 0.13077 | val_1_rmse: 0.13078 |  0:01:59s
epoch 31 | loss: 0.01719 | val_0_rmse: 0.12987 | val_1_rmse: 0.1298  |  0:02:03s
epoch 32 | loss: 0.01687 | val_0_rmse: 0.13154 | val_1_rmse: 0.13209 |  0:02:07s
epoch 33 | loss: 0.01712 | val_0_rmse: 0.13875 | val_1_rmse: 0.13871 |  0:02:11s
epoch 34 | loss: 0.01753 | val_0_rmse: 0.13773 | val_1_rmse: 0.1378  |  0:02:14s
epoch 35 | loss: 0.01738 | val_0_rmse: 0.13946 | val_1_rmse: 0.13978 |  0:02:18s
epoch 36 | loss: 0.01733 | val_0_rmse: 0.1298  | val_1_rmse: 0.12984 |  0:02:22s
epoch 37 | loss: 0.01713 | val_0_rmse: 0.12904 | val_1_rmse: 0.129   |  0:02:26s
epoch 38 | loss: 0.01695 | val_0_rmse: 0.13247 | val_1_rmse: 0.13261 |  0:02:30s
epoch 39 | loss: 0.01714 | val_0_rmse: 0.1294  | val_1_rmse: 0.1297  |  0:02:34s
epoch 40 | loss: 0.0171  | val_0_rmse: 0.13499 | val_1_rmse: 0.13511 |  0:02:38s
epoch 41 | loss: 0.01696 | val_0_rmse: 0.12987 | val_1_rmse: 0.12993 |  0:02:41s
epoch 42 | loss: 0.01693 | val_0_rmse: 0.15003 | val_1_rmse: 0.15007 |  0:02:45s
epoch 43 | loss: 0.01701 | val_0_rmse: 0.1283  | val_1_rmse: 0.12844 |  0:02:49s
epoch 44 | loss: 0.01687 | val_0_rmse: 0.13529 | val_1_rmse: 0.13569 |  0:02:53s
epoch 45 | loss: 0.01674 | val_0_rmse: 0.13558 | val_1_rmse: 0.13583 |  0:02:57s
epoch 46 | loss: 0.01669 | val_0_rmse: 0.12838 | val_1_rmse: 0.12838 |  0:03:00s
epoch 47 | loss: 0.01675 | val_0_rmse: 0.12842 | val_1_rmse: 0.12862 |  0:03:04s
epoch 48 | loss: 0.01677 | val_0_rmse: 0.14519 | val_1_rmse: 0.14571 |  0:03:08s
epoch 49 | loss: 0.01674 | val_0_rmse: 0.12804 | val_1_rmse: 0.12805 |  0:03:12s
epoch 50 | loss: 0.01671 | val_0_rmse: 0.13179 | val_1_rmse: 0.13226 |  0:03:16s
epoch 51 | loss: 0.01683 | val_0_rmse: 0.12951 | val_1_rmse: 0.1297  |  0:03:20s
epoch 52 | loss: 0.01685 | val_0_rmse: 0.14008 | val_1_rmse: 0.14083 |  0:03:24s
epoch 53 | loss: 0.01695 | val_0_rmse: 0.14274 | val_1_rmse: 0.14257 |  0:03:27s
epoch 54 | loss: 0.01699 | val_0_rmse: 0.14838 | val_1_rmse: 0.15034 |  0:03:31s

Early stopping occured at epoch 54 with best_epoch = 24 and best_val_1_rmse = 0.12794
Best weights from best epoch are automatically used!
ended training at: 06:40:08
Feature importance:
[('Area', 0.4858386437092794), ('Baths', 0.08053701319030715), ('Beds', 0.21510990742398114), ('Latitude', 0.19857947027823025), ('Longitude', 0.010110829359291962), ('Month', 0.0), ('Year', 0.009824136038910058)]
Mean squared error is of 980436988.3187562
Mean absolute error:22158.0277750381
MAPE:0.3761330077764744
R2 score:0.7094699030792657
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:40:09
epoch 0  | loss: 0.0622  | val_0_rmse: 0.2268  | val_1_rmse: 0.22175 |  0:00:03s
epoch 1  | loss: 0.02035 | val_0_rmse: 0.18959 | val_1_rmse: 0.18553 |  0:00:07s
epoch 2  | loss: 0.01954 | val_0_rmse: 0.18469 | val_1_rmse: 0.18038 |  0:00:11s
epoch 3  | loss: 0.01927 | val_0_rmse: 0.18231 | val_1_rmse: 0.17887 |  0:00:15s
epoch 4  | loss: 0.01893 | val_0_rmse: 0.18252 | val_1_rmse: 0.17906 |  0:00:19s
epoch 5  | loss: 0.01918 | val_0_rmse: 0.18892 | val_1_rmse: 0.18657 |  0:00:23s
epoch 6  | loss: 0.01858 | val_0_rmse: 0.19046 | val_1_rmse: 0.18813 |  0:00:26s
epoch 7  | loss: 0.01842 | val_0_rmse: 0.18114 | val_1_rmse: 0.17888 |  0:00:30s
epoch 8  | loss: 0.01866 | val_0_rmse: 0.1749  | val_1_rmse: 0.17272 |  0:00:34s
epoch 9  | loss: 0.01816 | val_0_rmse: 0.14791 | val_1_rmse: 0.14444 |  0:00:38s
epoch 10 | loss: 0.01831 | val_0_rmse: 0.15267 | val_1_rmse: 0.14832 |  0:00:42s
epoch 11 | loss: 0.01787 | val_0_rmse: 0.1508  | val_1_rmse: 0.14776 |  0:00:46s
epoch 12 | loss: 0.01769 | val_0_rmse: 0.15729 | val_1_rmse: 0.15541 |  0:00:50s
epoch 13 | loss: 0.01793 | val_0_rmse: 0.15759 | val_1_rmse: 0.15379 |  0:00:53s
epoch 14 | loss: 0.01758 | val_0_rmse: 0.1465  | val_1_rmse: 0.14565 |  0:00:57s
epoch 15 | loss: 0.01734 | val_0_rmse: 0.1402  | val_1_rmse: 0.13805 |  0:01:01s
epoch 16 | loss: 0.01701 | val_0_rmse: 0.13816 | val_1_rmse: 0.13534 |  0:01:05s
epoch 17 | loss: 0.01719 | val_0_rmse: 0.13489 | val_1_rmse: 0.13207 |  0:01:09s
epoch 18 | loss: 0.01689 | val_0_rmse: 0.12803 | val_1_rmse: 0.12594 |  0:01:13s
epoch 19 | loss: 0.01673 | val_0_rmse: 0.13294 | val_1_rmse: 0.13004 |  0:01:17s
epoch 20 | loss: 0.01685 | val_0_rmse: 0.12732 | val_1_rmse: 0.12449 |  0:01:20s
epoch 21 | loss: 0.01653 | val_0_rmse: 0.13515 | val_1_rmse: 0.13349 |  0:01:24s
epoch 22 | loss: 0.01672 | val_0_rmse: 0.13381 | val_1_rmse: 0.13237 |  0:01:28s
epoch 23 | loss: 0.01647 | val_0_rmse: 0.12605 | val_1_rmse: 0.12352 |  0:01:32s
epoch 24 | loss: 0.01657 | val_0_rmse: 0.13993 | val_1_rmse: 0.13755 |  0:01:36s
epoch 25 | loss: 0.01665 | val_0_rmse: 0.1329  | val_1_rmse: 0.12985 |  0:01:40s
epoch 26 | loss: 0.01642 | val_0_rmse: 0.12551 | val_1_rmse: 0.12338 |  0:01:44s
epoch 27 | loss: 0.01629 | val_0_rmse: 0.12616 | val_1_rmse: 0.12434 |  0:01:47s
epoch 28 | loss: 0.01629 | val_0_rmse: 0.1256  | val_1_rmse: 0.12333 |  0:01:51s
epoch 29 | loss: 0.01623 | val_0_rmse: 0.12729 | val_1_rmse: 0.12533 |  0:01:55s
epoch 30 | loss: 0.01655 | val_0_rmse: 0.17991 | val_1_rmse: 0.17588 |  0:01:59s
epoch 31 | loss: 0.01619 | val_0_rmse: 0.13666 | val_1_rmse: 0.13469 |  0:02:03s
epoch 32 | loss: 0.01649 | val_0_rmse: 0.14139 | val_1_rmse: 0.13963 |  0:02:07s
epoch 33 | loss: 0.01641 | val_0_rmse: 0.12613 | val_1_rmse: 0.12341 |  0:02:11s
epoch 34 | loss: 0.01637 | val_0_rmse: 0.1255  | val_1_rmse: 0.12408 |  0:02:14s
epoch 35 | loss: 0.01613 | val_0_rmse: 0.12874 | val_1_rmse: 0.12596 |  0:02:18s
epoch 36 | loss: 0.0162  | val_0_rmse: 0.13024 | val_1_rmse: 0.12749 |  0:02:22s
epoch 37 | loss: 0.016   | val_0_rmse: 0.12473 | val_1_rmse: 0.12248 |  0:02:26s
epoch 38 | loss: 0.01645 | val_0_rmse: 0.12764 | val_1_rmse: 0.1253  |  0:02:30s
epoch 39 | loss: 0.01652 | val_0_rmse: 0.12996 | val_1_rmse: 0.1271  |  0:02:34s
epoch 40 | loss: 0.01638 | val_0_rmse: 0.127   | val_1_rmse: 0.12407 |  0:02:37s
epoch 41 | loss: 0.01635 | val_0_rmse: 0.12344 | val_1_rmse: 0.12106 |  0:02:41s
epoch 42 | loss: 0.0163  | val_0_rmse: 0.12599 | val_1_rmse: 0.12387 |  0:02:45s
epoch 43 | loss: 0.01602 | val_0_rmse: 0.13679 | val_1_rmse: 0.13393 |  0:02:49s
epoch 44 | loss: 0.01602 | val_0_rmse: 0.12729 | val_1_rmse: 0.12534 |  0:02:53s
epoch 45 | loss: 0.0161  | val_0_rmse: 0.13133 | val_1_rmse: 0.12883 |  0:02:57s
epoch 46 | loss: 0.01584 | val_0_rmse: 0.16361 | val_1_rmse: 0.15951 |  0:03:00s
epoch 47 | loss: 0.01604 | val_0_rmse: 0.13998 | val_1_rmse: 0.13769 |  0:03:04s
epoch 48 | loss: 0.01596 | val_0_rmse: 0.12492 | val_1_rmse: 0.12253 |  0:03:08s
epoch 49 | loss: 0.01599 | val_0_rmse: 0.21023 | val_1_rmse: 0.20547 |  0:03:12s
epoch 50 | loss: 0.01582 | val_0_rmse: 0.12368 | val_1_rmse: 0.12124 |  0:03:16s
epoch 51 | loss: 0.01597 | val_0_rmse: 0.12695 | val_1_rmse: 0.12385 |  0:03:20s
epoch 52 | loss: 0.01561 | val_0_rmse: 0.20848 | val_1_rmse: 0.20369 |  0:03:24s
epoch 53 | loss: 0.0159  | val_0_rmse: 0.13403 | val_1_rmse: 0.13202 |  0:03:27s
epoch 54 | loss: 0.01595 | val_0_rmse: 0.12863 | val_1_rmse: 0.12622 |  0:03:31s
epoch 55 | loss: 0.01599 | val_0_rmse: 0.12869 | val_1_rmse: 0.12642 |  0:03:35s
epoch 56 | loss: 0.01586 | val_0_rmse: 0.12479 | val_1_rmse: 0.1226  |  0:03:39s
epoch 57 | loss: 0.01567 | val_0_rmse: 0.12274 | val_1_rmse: 0.12062 |  0:03:43s
epoch 58 | loss: 0.01572 | val_0_rmse: 0.28004 | val_1_rmse: 0.27369 |  0:03:47s
epoch 59 | loss: 0.01584 | val_0_rmse: 0.13608 | val_1_rmse: 0.13395 |  0:03:51s
epoch 60 | loss: 0.0158  | val_0_rmse: 0.12613 | val_1_rmse: 0.12354 |  0:03:54s
epoch 61 | loss: 0.01574 | val_0_rmse: 0.13118 | val_1_rmse: 0.12902 |  0:03:58s
epoch 62 | loss: 0.01577 | val_0_rmse: 0.13965 | val_1_rmse: 0.13771 |  0:04:02s
epoch 63 | loss: 0.0157  | val_0_rmse: 0.14511 | val_1_rmse: 0.14214 |  0:04:06s
epoch 64 | loss: 0.01576 | val_0_rmse: 0.13821 | val_1_rmse: 0.13596 |  0:04:10s
epoch 65 | loss: 0.01576 | val_0_rmse: 0.13346 | val_1_rmse: 0.1308  |  0:04:14s
epoch 66 | loss: 0.01582 | val_0_rmse: 0.12514 | val_1_rmse: 0.12238 |  0:04:17s
epoch 67 | loss: 0.01588 | val_0_rmse: 0.12206 | val_1_rmse: 0.12002 |  0:04:21s
epoch 68 | loss: 0.01576 | val_0_rmse: 0.12668 | val_1_rmse: 0.12395 |  0:04:25s
epoch 69 | loss: 0.01567 | val_0_rmse: 0.12862 | val_1_rmse: 0.12611 |  0:04:29s
epoch 70 | loss: 0.01571 | val_0_rmse: 0.13073 | val_1_rmse: 0.1283  |  0:04:33s
epoch 71 | loss: 0.01581 | val_0_rmse: 0.13634 | val_1_rmse: 0.1339  |  0:04:37s
epoch 72 | loss: 0.0158  | val_0_rmse: 0.12651 | val_1_rmse: 0.12393 |  0:04:41s
epoch 73 | loss: 0.01681 | val_0_rmse: 0.12797 | val_1_rmse: 0.12559 |  0:04:44s
epoch 74 | loss: 0.0168  | val_0_rmse: 0.13035 | val_1_rmse: 0.1277  |  0:04:48s
epoch 75 | loss: 0.0161  | val_0_rmse: 0.14768 | val_1_rmse: 0.14439 |  0:04:52s
epoch 76 | loss: 0.01603 | val_0_rmse: 0.12754 | val_1_rmse: 0.12454 |  0:04:56s
epoch 77 | loss: 0.01676 | val_0_rmse: 0.13683 | val_1_rmse: 0.13493 |  0:05:00s
epoch 78 | loss: 0.01621 | val_0_rmse: 0.13236 | val_1_rmse: 0.12985 |  0:05:04s
epoch 79 | loss: 0.01615 | val_0_rmse: 0.12615 | val_1_rmse: 0.1244  |  0:05:08s
epoch 80 | loss: 0.0165  | val_0_rmse: 0.13466 | val_1_rmse: 0.13271 |  0:05:11s
epoch 81 | loss: 0.0161  | val_0_rmse: 0.12909 | val_1_rmse: 0.12682 |  0:05:15s
epoch 82 | loss: 0.0161  | val_0_rmse: 0.1324  | val_1_rmse: 0.12946 |  0:05:19s
epoch 83 | loss: 0.0162  | val_0_rmse: 0.15508 | val_1_rmse: 0.15239 |  0:05:23s
epoch 84 | loss: 0.01596 | val_0_rmse: 0.1252  | val_1_rmse: 0.12252 |  0:05:27s
epoch 85 | loss: 0.01608 | val_0_rmse: 0.13885 | val_1_rmse: 0.13607 |  0:05:31s
epoch 86 | loss: 0.01597 | val_0_rmse: 0.40263 | val_1_rmse: 0.39195 |  0:05:34s
epoch 87 | loss: 0.01598 | val_0_rmse: 0.13468 | val_1_rmse: 0.13363 |  0:05:38s
epoch 88 | loss: 0.0161  | val_0_rmse: 0.12425 | val_1_rmse: 0.13823 |  0:05:42s
epoch 89 | loss: 0.01597 | val_0_rmse: 0.12436 | val_1_rmse: 0.12743 |  0:05:46s
epoch 90 | loss: 0.0158  | val_0_rmse: 0.15205 | val_1_rmse: 0.16251 |  0:05:50s
epoch 91 | loss: 0.01594 | val_0_rmse: 0.12505 | val_1_rmse: 0.1311  |  0:05:54s
epoch 92 | loss: 0.01586 | val_0_rmse: 0.1378  | val_1_rmse: 0.16244 |  0:05:58s
epoch 93 | loss: 0.01594 | val_0_rmse: 0.12933 | val_1_rmse: 0.14294 |  0:06:01s
epoch 94 | loss: 0.01599 | val_0_rmse: 0.12557 | val_1_rmse: 0.14194 |  0:06:05s
epoch 95 | loss: 0.01581 | val_0_rmse: 0.12962 | val_1_rmse: 0.13825 |  0:06:09s
epoch 96 | loss: 0.01578 | val_0_rmse: 0.12365 | val_1_rmse: 0.13715 |  0:06:13s
epoch 97 | loss: 0.01588 | val_0_rmse: 0.12345 | val_1_rmse: 0.14429 |  0:06:17s

Early stopping occured at epoch 97 with best_epoch = 67 and best_val_1_rmse = 0.12002
Best weights from best epoch are automatically used!
ended training at: 06:46:27
Feature importance:
[('Area', 0.3767444573167081), ('Baths', 0.004977298857050446), ('Beds', 0.0817551974908469), ('Latitude', 0.27448770732555344), ('Longitude', 0.136257421076609), ('Month', 0.012634649057800533), ('Year', 0.1131432688754316)]
Mean squared error is of 927841124.5625902
Mean absolute error:21309.215999338463
MAPE:0.3607461438751624
R2 score:0.7219430366126189
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:46:28
epoch 0  | loss: 0.01678 | val_0_rmse: 0.09843 | val_1_rmse: 0.09833 |  0:00:17s
epoch 1  | loss: 0.00675 | val_0_rmse: 0.08156 | val_1_rmse: 0.08153 |  0:00:35s
epoch 2  | loss: 0.00673 | val_0_rmse: 0.07779 | val_1_rmse: 0.07798 |  0:00:53s
epoch 3  | loss: 0.0063  | val_0_rmse: 0.07882 | val_1_rmse: 0.07877 |  0:01:11s
epoch 4  | loss: 0.00624 | val_0_rmse: 0.07748 | val_1_rmse: 0.07745 |  0:01:29s
epoch 5  | loss: 0.00611 | val_0_rmse: 0.07816 | val_1_rmse: 0.07831 |  0:01:47s
epoch 6  | loss: 0.00608 | val_0_rmse: 0.07784 | val_1_rmse: 0.07793 |  0:02:04s
epoch 7  | loss: 0.00605 | val_0_rmse: 0.07682 | val_1_rmse: 0.07671 |  0:02:22s
epoch 8  | loss: 0.00602 | val_0_rmse: 0.0788  | val_1_rmse: 0.07889 |  0:02:40s
epoch 9  | loss: 0.00601 | val_0_rmse: 0.07576 | val_1_rmse: 0.07581 |  0:02:58s
epoch 10 | loss: 0.00596 | val_0_rmse: 0.07762 | val_1_rmse: 0.07749 |  0:03:16s
epoch 11 | loss: 0.00591 | val_0_rmse: 0.07548 | val_1_rmse: 0.07575 |  0:03:34s
epoch 12 | loss: 0.00589 | val_0_rmse: 0.0794  | val_1_rmse: 0.07965 |  0:03:52s
epoch 13 | loss: 0.00586 | val_0_rmse: 0.08042 | val_1_rmse: 0.08017 |  0:04:09s
epoch 14 | loss: 0.00588 | val_0_rmse: 0.07429 | val_1_rmse: 0.0745  |  0:04:27s
epoch 15 | loss: 0.00584 | val_0_rmse: 0.07839 | val_1_rmse: 0.07857 |  0:04:45s
epoch 16 | loss: 0.00583 | val_0_rmse: 0.07897 | val_1_rmse: 0.07913 |  0:05:03s
epoch 17 | loss: 0.0058  | val_0_rmse: 0.07873 | val_1_rmse: 0.07896 |  0:05:21s
epoch 18 | loss: 0.00578 | val_0_rmse: 0.07572 | val_1_rmse: 0.07586 |  0:05:39s
epoch 19 | loss: 0.00581 | val_0_rmse: 0.07493 | val_1_rmse: 0.07518 |  0:05:57s
epoch 20 | loss: 0.00576 | val_0_rmse: 0.07449 | val_1_rmse: 0.07471 |  0:06:14s
epoch 21 | loss: 0.00572 | val_0_rmse: 0.07976 | val_1_rmse: 0.07977 |  0:06:32s
epoch 22 | loss: 0.00574 | val_0_rmse: 0.07904 | val_1_rmse: 0.07903 |  0:06:50s
epoch 23 | loss: 0.00578 | val_0_rmse: 0.0824  | val_1_rmse: 0.08266 |  0:07:08s
epoch 24 | loss: 0.00581 | val_0_rmse: 0.07664 | val_1_rmse: 0.07683 |  0:07:26s
epoch 25 | loss: 0.00587 | val_0_rmse: 0.07643 | val_1_rmse: 0.07649 |  0:07:44s
epoch 26 | loss: 0.00575 | val_0_rmse: 0.07427 | val_1_rmse: 0.07447 |  0:08:01s
epoch 27 | loss: 0.00578 | val_0_rmse: 0.07808 | val_1_rmse: 0.07848 |  0:08:19s
epoch 28 | loss: 0.00571 | val_0_rmse: 0.07347 | val_1_rmse: 0.07382 |  0:08:37s
epoch 29 | loss: 0.0058  | val_0_rmse: 0.07416 | val_1_rmse: 0.07445 |  0:08:55s
epoch 30 | loss: 0.00575 | val_0_rmse: 0.07377 | val_1_rmse: 0.07411 |  0:09:13s
epoch 31 | loss: 0.00579 | val_0_rmse: 0.07525 | val_1_rmse: 0.0755  |  0:09:30s
epoch 32 | loss: 0.00569 | val_0_rmse: 0.07702 | val_1_rmse: 0.07713 |  0:09:48s
epoch 33 | loss: 0.00568 | val_0_rmse: 0.07871 | val_1_rmse: 0.07885 |  0:10:06s
epoch 34 | loss: 0.00593 | val_0_rmse: 0.07556 | val_1_rmse: 0.07585 |  0:10:24s
epoch 35 | loss: 0.00592 | val_0_rmse: 0.07664 | val_1_rmse: 0.07674 |  0:10:42s
epoch 36 | loss: 0.00652 | val_0_rmse: 0.07973 | val_1_rmse: 0.0795  |  0:11:00s
epoch 37 | loss: 0.00701 | val_0_rmse: 0.08316 | val_1_rmse: 0.08327 |  0:11:17s
epoch 38 | loss: 0.00658 | val_0_rmse: 0.08059 | val_1_rmse: 0.08042 |  0:11:35s
epoch 39 | loss: 0.00639 | val_0_rmse: 0.07982 | val_1_rmse: 0.07969 |  0:11:53s
epoch 40 | loss: 0.00632 | val_0_rmse: 0.07802 | val_1_rmse: 0.07799 |  0:12:11s
epoch 41 | loss: 0.0062  | val_0_rmse: 0.07709 | val_1_rmse: 0.07725 |  0:12:29s
epoch 42 | loss: 0.0063  | val_0_rmse: 0.07918 | val_1_rmse: 0.07927 |  0:12:46s
epoch 43 | loss: 0.00617 | val_0_rmse: 0.0795  | val_1_rmse: 0.07972 |  0:13:04s
epoch 44 | loss: 0.00617 | val_0_rmse: 0.07725 | val_1_rmse: 0.07727 |  0:13:22s
epoch 45 | loss: 0.00642 | val_0_rmse: 0.07834 | val_1_rmse: 0.0784  |  0:13:40s
epoch 46 | loss: 0.00617 | val_0_rmse: 0.07663 | val_1_rmse: 0.07664 |  0:13:58s
epoch 47 | loss: 0.00599 | val_0_rmse: 0.07568 | val_1_rmse: 0.07595 |  0:14:16s
epoch 48 | loss: 0.00594 | val_0_rmse: 0.07687 | val_1_rmse: 0.07695 |  0:14:34s
epoch 49 | loss: 0.00594 | val_0_rmse: 0.0762  | val_1_rmse: 0.07637 |  0:14:51s
epoch 50 | loss: 0.00591 | val_0_rmse: 0.0758  | val_1_rmse: 0.07595 |  0:15:09s
epoch 51 | loss: 0.00588 | val_0_rmse: 0.07639 | val_1_rmse: 0.07635 |  0:15:27s
epoch 52 | loss: 0.0059  | val_0_rmse: 0.07801 | val_1_rmse: 0.07814 |  0:15:45s
epoch 53 | loss: 0.0059  | val_0_rmse: 0.07545 | val_1_rmse: 0.07565 |  0:16:03s
epoch 54 | loss: 0.00671 | val_0_rmse: 0.08255 | val_1_rmse: 0.08233 |  0:16:20s
epoch 55 | loss: 0.00676 | val_0_rmse: 0.08265 | val_1_rmse: 0.08239 |  0:16:38s
epoch 56 | loss: 0.00665 | val_0_rmse: 0.08057 | val_1_rmse: 0.08033 |  0:16:56s
epoch 57 | loss: 0.00653 | val_0_rmse: 0.07988 | val_1_rmse: 0.07966 |  0:17:14s
epoch 58 | loss: 0.00648 | val_0_rmse: 0.08146 | val_1_rmse: 0.08138 |  0:17:32s

Early stopping occured at epoch 58 with best_epoch = 28 and best_val_1_rmse = 0.07382
Best weights from best epoch are automatically used!
ended training at: 07:04:05
Feature importance:
[('Area', 0.007542631960064026), ('Baths', 0.03166066067766178), ('Beds', 0.1765234294414303), ('Latitude', 0.4053455610679169), ('Longitude', 0.1722612958262795), ('Month', 0.037556406903387816), ('Year', 0.1691100141232597)]
Mean squared error is of 11967542250.592505
Mean absolute error:68845.15421769643
MAPE:0.4717228193820307
R2 score:0.6948392613261847
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_0.zip
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:04:08
epoch 0  | loss: 0.01471 | val_0_rmse: 0.10372 | val_1_rmse: 0.10351 |  0:00:17s
epoch 1  | loss: 0.00688 | val_0_rmse: 0.08377 | val_1_rmse: 0.08373 |  0:00:35s
epoch 2  | loss: 0.00642 | val_0_rmse: 0.07706 | val_1_rmse: 0.07675 |  0:00:53s
epoch 3  | loss: 0.00627 | val_0_rmse: 0.07939 | val_1_rmse: 0.07905 |  0:01:11s
epoch 4  | loss: 0.00626 | val_0_rmse: 0.07929 | val_1_rmse: 0.07871 |  0:01:29s
epoch 5  | loss: 0.00612 | val_0_rmse: 0.07514 | val_1_rmse: 0.07469 |  0:01:47s
epoch 6  | loss: 0.00632 | val_0_rmse: 0.07695 | val_1_rmse: 0.07638 |  0:02:04s
epoch 7  | loss: 0.00619 | val_0_rmse: 0.07598 | val_1_rmse: 0.07557 |  0:02:22s
epoch 8  | loss: 0.00598 | val_0_rmse: 0.07719 | val_1_rmse: 0.07706 |  0:02:40s
epoch 9  | loss: 0.00609 | val_0_rmse: 0.07699 | val_1_rmse: 0.07674 |  0:02:58s
epoch 10 | loss: 0.00609 | val_0_rmse: 0.07598 | val_1_rmse: 0.07584 |  0:03:16s
epoch 11 | loss: 0.0061  | val_0_rmse: 0.0744  | val_1_rmse: 0.07407 |  0:03:34s
epoch 12 | loss: 0.00593 | val_0_rmse: 0.07466 | val_1_rmse: 0.07439 |  0:03:52s
epoch 13 | loss: 0.00598 | val_0_rmse: 0.07612 | val_1_rmse: 0.07584 |  0:04:09s
epoch 14 | loss: 0.0061  | val_0_rmse: 0.07625 | val_1_rmse: 0.07575 |  0:04:27s
epoch 15 | loss: 0.00604 | val_0_rmse: 0.07774 | val_1_rmse: 0.07774 |  0:04:45s
epoch 16 | loss: 0.0062  | val_0_rmse: 0.07697 | val_1_rmse: 0.07665 |  0:05:03s
epoch 17 | loss: 0.0059  | val_0_rmse: 0.07706 | val_1_rmse: 0.07724 |  0:05:21s
epoch 18 | loss: 0.00581 | val_0_rmse: 0.07458 | val_1_rmse: 0.0742  |  0:05:39s
epoch 19 | loss: 0.00577 | val_0_rmse: 0.07497 | val_1_rmse: 0.07446 |  0:05:56s
epoch 20 | loss: 0.00593 | val_0_rmse: 0.07517 | val_1_rmse: 0.07483 |  0:06:14s
epoch 21 | loss: 0.00575 | val_0_rmse: 0.0736  | val_1_rmse: 0.07348 |  0:06:32s
epoch 22 | loss: 0.00597 | val_0_rmse: 0.07644 | val_1_rmse: 0.07596 |  0:06:50s
epoch 23 | loss: 0.00596 | val_0_rmse: 0.08056 | val_1_rmse: 0.08045 |  0:07:08s
epoch 24 | loss: 0.00661 | val_0_rmse: 0.08514 | val_1_rmse: 0.0842  |  0:07:26s
epoch 25 | loss: 0.00653 | val_0_rmse: 0.07843 | val_1_rmse: 0.07812 |  0:07:43s
epoch 26 | loss: 0.00621 | val_0_rmse: 0.07802 | val_1_rmse: 0.07793 |  0:08:01s
epoch 27 | loss: 0.00611 | val_0_rmse: 0.07668 | val_1_rmse: 0.07641 |  0:08:19s
epoch 28 | loss: 0.00605 | val_0_rmse: 0.07625 | val_1_rmse: 0.07579 |  0:08:37s
epoch 29 | loss: 0.00601 | val_0_rmse: 0.07836 | val_1_rmse: 0.07779 |  0:08:55s
epoch 30 | loss: 0.00597 | val_0_rmse: 0.0802  | val_1_rmse: 0.07987 |  0:09:12s
epoch 31 | loss: 0.0066  | val_0_rmse: 0.0795  | val_1_rmse: 0.07932 |  0:09:30s
epoch 32 | loss: 0.00642 | val_0_rmse: 0.07841 | val_1_rmse: 0.0782  |  0:09:48s
epoch 33 | loss: 0.00642 | val_0_rmse: 0.07915 | val_1_rmse: 0.07871 |  0:10:06s
epoch 34 | loss: 0.00636 | val_0_rmse: 0.07912 | val_1_rmse: 0.07904 |  0:10:24s
epoch 35 | loss: 0.00638 | val_0_rmse: 0.07879 | val_1_rmse: 0.07851 |  0:10:42s
epoch 36 | loss: 0.00631 | val_0_rmse: 0.07887 | val_1_rmse: 0.07847 |  0:11:00s
epoch 37 | loss: 0.00651 | val_0_rmse: 0.08138 | val_1_rmse: 0.08096 |  0:11:18s
epoch 38 | loss: 0.00661 | val_0_rmse: 0.07872 | val_1_rmse: 0.07811 |  0:11:35s
epoch 39 | loss: 0.00637 | val_0_rmse: 0.07782 | val_1_rmse: 0.07754 |  0:11:53s
epoch 40 | loss: 0.00641 | val_0_rmse: 0.07707 | val_1_rmse: 0.07673 |  0:12:11s
epoch 41 | loss: 0.00616 | val_0_rmse: 0.07575 | val_1_rmse: 0.07538 |  0:12:29s
epoch 42 | loss: 0.00637 | val_0_rmse: 0.09357 | val_1_rmse: 0.09348 |  0:12:47s
epoch 43 | loss: 0.00784 | val_0_rmse: 0.08415 | val_1_rmse: 0.08369 |  0:13:05s
epoch 44 | loss: 0.00709 | val_0_rmse: 0.08089 | val_1_rmse: 0.08036 |  0:13:22s
epoch 45 | loss: 0.00672 | val_0_rmse: 0.0805  | val_1_rmse: 0.0801  |  0:13:40s
epoch 46 | loss: 0.00667 | val_0_rmse: 0.08015 | val_1_rmse: 0.07968 |  0:13:58s
epoch 47 | loss: 0.0066  | val_0_rmse: 0.08061 | val_1_rmse: 0.08007 |  0:14:16s
epoch 48 | loss: 0.00658 | val_0_rmse: 0.08011 | val_1_rmse: 0.07968 |  0:14:34s
epoch 49 | loss: 0.00656 | val_0_rmse: 0.07994 | val_1_rmse: 0.07956 |  0:14:51s
epoch 50 | loss: 0.00652 | val_0_rmse: 0.07958 | val_1_rmse: 0.07921 |  0:15:09s
epoch 51 | loss: 0.00651 | val_0_rmse: 0.07964 | val_1_rmse: 0.07919 |  0:15:27s

Early stopping occured at epoch 51 with best_epoch = 21 and best_val_1_rmse = 0.07348
Best weights from best epoch are automatically used!
ended training at: 07:19:41
Feature importance:
[('Area', 0.06897473208877555), ('Baths', 0.31107757607007197), ('Beds', 0.08976749453253073), ('Latitude', 0.2186218119357042), ('Longitude', 5.278121822456208e-06), ('Month', 0.0), ('Year', 0.311553107251095)]
Mean squared error is of 12009829772.83717
Mean absolute error:71860.9249995212
MAPE:0.5428442950991799
R2 score:0.6946413051151469
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_1.zip
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:19:43
epoch 0  | loss: 0.01657 | val_0_rmse: 0.10559 | val_1_rmse: 0.10526 |  0:00:18s
epoch 1  | loss: 0.0067  | val_0_rmse: 0.07878 | val_1_rmse: 0.07864 |  0:00:35s
epoch 2  | loss: 0.00618 | val_0_rmse: 0.07642 | val_1_rmse: 0.0766  |  0:00:53s
epoch 3  | loss: 0.00602 | val_0_rmse: 0.07501 | val_1_rmse: 0.07519 |  0:01:11s
epoch 4  | loss: 0.00594 | val_0_rmse: 0.07639 | val_1_rmse: 0.07626 |  0:01:29s
epoch 5  | loss: 0.00589 | val_0_rmse: 0.07426 | val_1_rmse: 0.07444 |  0:01:47s
epoch 6  | loss: 0.00582 | val_0_rmse: 0.07457 | val_1_rmse: 0.07485 |  0:02:05s
epoch 7  | loss: 0.00576 | val_0_rmse: 0.07362 | val_1_rmse: 0.07368 |  0:02:22s
epoch 8  | loss: 0.00564 | val_0_rmse: 0.07355 | val_1_rmse: 0.07358 |  0:02:40s
epoch 9  | loss: 0.00565 | val_0_rmse: 0.07359 | val_1_rmse: 0.07366 |  0:02:58s
epoch 10 | loss: 0.00556 | val_0_rmse: 0.07328 | val_1_rmse: 0.07341 |  0:03:16s
epoch 11 | loss: 0.00558 | val_0_rmse: 0.07347 | val_1_rmse: 0.07344 |  0:03:33s
epoch 12 | loss: 0.00554 | val_0_rmse: 0.08068 | val_1_rmse: 0.08063 |  0:03:51s
epoch 13 | loss: 0.00555 | val_0_rmse: 0.07309 | val_1_rmse: 0.07306 |  0:04:09s
epoch 14 | loss: 0.0055  | val_0_rmse: 0.084   | val_1_rmse: 0.08396 |  0:04:27s
epoch 15 | loss: 0.00553 | val_0_rmse: 0.0872  | val_1_rmse: 0.08723 |  0:04:44s
epoch 16 | loss: 0.00553 | val_0_rmse: 0.07553 | val_1_rmse: 0.07569 |  0:05:02s
epoch 17 | loss: 0.00552 | val_0_rmse: 0.07259 | val_1_rmse: 0.07256 |  0:05:20s
epoch 18 | loss: 0.0055  | val_0_rmse: 0.08117 | val_1_rmse: 0.08098 |  0:05:38s
epoch 19 | loss: 0.00551 | val_0_rmse: 0.07648 | val_1_rmse: 0.07655 |  0:05:56s
epoch 20 | loss: 0.0055  | val_0_rmse: 0.07293 | val_1_rmse: 0.07291 |  0:06:14s
epoch 21 | loss: 0.00549 | val_0_rmse: 0.07523 | val_1_rmse: 0.07515 |  0:06:31s
epoch 22 | loss: 0.0055  | val_0_rmse: 0.07711 | val_1_rmse: 0.07693 |  0:06:49s
epoch 23 | loss: 0.00561 | val_0_rmse: 0.08247 | val_1_rmse: 0.08231 |  0:07:07s
epoch 24 | loss: 0.00549 | val_0_rmse: 0.07479 | val_1_rmse: 0.07473 |  0:07:25s
epoch 25 | loss: 0.00542 | val_0_rmse: 0.07913 | val_1_rmse: 0.07915 |  0:07:42s
epoch 26 | loss: 0.00591 | val_0_rmse: 0.07487 | val_1_rmse: 0.07481 |  0:08:00s
epoch 27 | loss: 0.00562 | val_0_rmse: 0.07388 | val_1_rmse: 0.07374 |  0:08:18s
epoch 28 | loss: 0.00551 | val_0_rmse: 0.07292 | val_1_rmse: 0.07288 |  0:08:36s
epoch 29 | loss: 0.0055  | val_0_rmse: 0.0749  | val_1_rmse: 0.07473 |  0:08:54s
epoch 30 | loss: 0.00771 | val_0_rmse: 0.0892  | val_1_rmse: 0.0891  |  0:09:12s
epoch 31 | loss: 0.00787 | val_0_rmse: 0.08743 | val_1_rmse: 0.08738 |  0:09:29s
epoch 32 | loss: 0.0077  | val_0_rmse: 0.0874  | val_1_rmse: 0.08726 |  0:09:47s
epoch 33 | loss: 0.00771 | val_0_rmse: 0.08727 | val_1_rmse: 0.08717 |  0:10:05s
epoch 34 | loss: 0.00798 | val_0_rmse: 0.08847 | val_1_rmse: 0.08828 |  0:10:23s
epoch 35 | loss: 0.00788 | val_0_rmse: 0.08822 | val_1_rmse: 0.08812 |  0:10:41s
epoch 36 | loss: 0.00778 | val_0_rmse: 0.08748 | val_1_rmse: 0.08739 |  0:10:58s
epoch 37 | loss: 0.00769 | val_0_rmse: 0.08722 | val_1_rmse: 0.08709 |  0:11:16s
epoch 38 | loss: 0.00767 | val_0_rmse: 0.08661 | val_1_rmse: 0.08653 |  0:11:34s
epoch 39 | loss: 0.00762 | val_0_rmse: 0.08637 | val_1_rmse: 0.08626 |  0:11:52s
epoch 40 | loss: 0.00763 | val_0_rmse: 0.08847 | val_1_rmse: 0.0884  |  0:12:10s
epoch 41 | loss: 0.00765 | val_0_rmse: 0.08786 | val_1_rmse: 0.08771 |  0:12:28s
epoch 42 | loss: 0.00755 | val_0_rmse: 0.08387 | val_1_rmse: 0.08357 |  0:12:45s
epoch 43 | loss: 0.0071  | val_0_rmse: 0.08227 | val_1_rmse: 0.08222 |  0:13:03s
epoch 44 | loss: 0.00697 | val_0_rmse: 0.08198 | val_1_rmse: 0.08192 |  0:13:21s
epoch 45 | loss: 0.00696 | val_0_rmse: 0.08266 | val_1_rmse: 0.08262 |  0:13:39s
epoch 46 | loss: 0.00694 | val_0_rmse: 0.08403 | val_1_rmse: 0.08398 |  0:13:57s
epoch 47 | loss: 0.00899 | val_0_rmse: 0.08695 | val_1_rmse: 0.08698 |  0:14:14s

Early stopping occured at epoch 47 with best_epoch = 17 and best_val_1_rmse = 0.07256
Best weights from best epoch are automatically used!
ended training at: 07:34:03
Feature importance:
[('Area', 0.15596691720526373), ('Baths', 0.12484036764148007), ('Beds', 0.0), ('Latitude', 0.23841011523937022), ('Longitude', 0.27981684837867105), ('Month', 0.1117971383916007), ('Year', 0.08916861314361418)]
Mean squared error is of 11937513933.404758
Mean absolute error:69625.11148913956
MAPE:0.47570474777932015
R2 score:0.6999446905657551
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_2.zip
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:34:06
epoch 0  | loss: 0.01558 | val_0_rmse: 0.10626 | val_1_rmse: 0.1056  |  0:00:18s
epoch 1  | loss: 0.00647 | val_0_rmse: 0.07777 | val_1_rmse: 0.07772 |  0:00:35s
epoch 2  | loss: 0.00627 | val_0_rmse: 0.07796 | val_1_rmse: 0.07756 |  0:00:53s
epoch 3  | loss: 0.00617 | val_0_rmse: 0.07597 | val_1_rmse: 0.07563 |  0:01:11s
epoch 4  | loss: 0.00616 | val_0_rmse: 0.07529 | val_1_rmse: 0.07521 |  0:01:29s
epoch 5  | loss: 0.00595 | val_0_rmse: 0.07409 | val_1_rmse: 0.07382 |  0:01:46s
epoch 6  | loss: 0.00596 | val_0_rmse: 0.07532 | val_1_rmse: 0.0752  |  0:02:04s
epoch 7  | loss: 0.00586 | val_0_rmse: 0.07704 | val_1_rmse: 0.07687 |  0:02:22s
epoch 8  | loss: 0.00587 | val_0_rmse: 0.07489 | val_1_rmse: 0.07481 |  0:02:40s
epoch 9  | loss: 0.00583 | val_0_rmse: 0.0732  | val_1_rmse: 0.07324 |  0:02:57s
epoch 10 | loss: 0.00588 | val_0_rmse: 0.07554 | val_1_rmse: 0.07541 |  0:03:15s
epoch 11 | loss: 0.00579 | val_0_rmse: 0.07355 | val_1_rmse: 0.07362 |  0:03:33s
epoch 12 | loss: 0.00601 | val_0_rmse: 0.07469 | val_1_rmse: 0.07454 |  0:03:51s
epoch 13 | loss: 0.00601 | val_0_rmse: 0.07462 | val_1_rmse: 0.07471 |  0:04:08s
epoch 14 | loss: 0.00585 | val_0_rmse: 0.07413 | val_1_rmse: 0.0741  |  0:04:26s
epoch 15 | loss: 0.00592 | val_0_rmse: 0.07592 | val_1_rmse: 0.07597 |  0:04:44s
epoch 16 | loss: 0.00594 | val_0_rmse: 0.08005 | val_1_rmse: 0.07967 |  0:05:02s
epoch 17 | loss: 0.00596 | val_0_rmse: 0.08613 | val_1_rmse: 0.08637 |  0:05:19s
epoch 18 | loss: 0.0062  | val_0_rmse: 0.07538 | val_1_rmse: 0.07513 |  0:05:37s
epoch 19 | loss: 0.00601 | val_0_rmse: 0.07477 | val_1_rmse: 0.07466 |  0:05:55s
epoch 20 | loss: 0.00591 | val_0_rmse: 0.07634 | val_1_rmse: 0.07626 |  0:06:13s
epoch 21 | loss: 0.00594 | val_0_rmse: 0.07662 | val_1_rmse: 0.07642 |  0:06:30s
epoch 22 | loss: 0.00587 | val_0_rmse: 0.07807 | val_1_rmse: 0.07774 |  0:06:48s
epoch 23 | loss: 0.00585 | val_0_rmse: 0.07559 | val_1_rmse: 0.0755  |  0:07:06s
epoch 24 | loss: 0.00581 | val_0_rmse: 0.07666 | val_1_rmse: 0.0766  |  0:07:24s
epoch 25 | loss: 0.0058  | val_0_rmse: 0.0754  | val_1_rmse: 0.07518 |  0:07:41s
epoch 26 | loss: 0.00578 | val_0_rmse: 0.07427 | val_1_rmse: 0.07431 |  0:07:59s
epoch 27 | loss: 0.00585 | val_0_rmse: 0.0837  | val_1_rmse: 0.08334 |  0:08:17s
epoch 28 | loss: 0.00579 | val_0_rmse: 0.07659 | val_1_rmse: 0.07653 |  0:08:35s
epoch 29 | loss: 0.00582 | val_0_rmse: 0.07518 | val_1_rmse: 0.07505 |  0:08:52s
epoch 30 | loss: 0.00577 | val_0_rmse: 0.07429 | val_1_rmse: 0.07423 |  0:09:10s
epoch 31 | loss: 0.00578 | val_0_rmse: 0.07626 | val_1_rmse: 0.07612 |  0:09:28s
epoch 32 | loss: 0.00575 | val_0_rmse: 0.07743 | val_1_rmse: 0.07737 |  0:09:45s
epoch 33 | loss: 0.00575 | val_0_rmse: 0.07756 | val_1_rmse: 0.07735 |  0:10:03s
epoch 34 | loss: 0.00573 | val_0_rmse: 0.07427 | val_1_rmse: 0.07431 |  0:10:21s
epoch 35 | loss: 0.00588 | val_0_rmse: 0.0917  | val_1_rmse: 0.09153 |  0:10:39s
epoch 36 | loss: 0.00633 | val_0_rmse: 0.07538 | val_1_rmse: 0.07522 |  0:10:57s
epoch 37 | loss: 0.00581 | val_0_rmse: 0.07416 | val_1_rmse: 0.07413 |  0:11:14s
epoch 38 | loss: 0.00571 | val_0_rmse: 0.07743 | val_1_rmse: 0.07743 |  0:11:32s
epoch 39 | loss: 0.00568 | val_0_rmse: 0.07441 | val_1_rmse: 0.07433 |  0:11:50s

Early stopping occured at epoch 39 with best_epoch = 9 and best_val_1_rmse = 0.07324
Best weights from best epoch are automatically used!
ended training at: 07:46:01
Feature importance:
[('Area', 0.07797562158240634), ('Baths', 0.06620654419527798), ('Beds', 0.029402453593545184), ('Latitude', 0.4129710179355385), ('Longitude', 0.20174873061045764), ('Month', 0.14464846704509815), ('Year', 0.06704716503767622)]
Mean squared error is of 12234862871.043236
Mean absolute error:71566.55266628238
MAPE:0.5271220531133316
R2 score:0.6933641543318377
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_3.zip
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:46:04
epoch 0  | loss: 0.01658 | val_0_rmse: 0.10513 | val_1_rmse: 0.10492 |  0:00:17s
epoch 1  | loss: 0.00722 | val_0_rmse: 0.07916 | val_1_rmse: 0.07896 |  0:00:35s
epoch 2  | loss: 0.00678 | val_0_rmse: 0.0797  | val_1_rmse: 0.07936 |  0:00:53s
epoch 3  | loss: 0.00669 | val_0_rmse: 0.077   | val_1_rmse: 0.07669 |  0:01:11s
epoch 4  | loss: 0.00649 | val_0_rmse: 0.07567 | val_1_rmse: 0.07552 |  0:01:29s
epoch 5  | loss: 0.00623 | val_0_rmse: 0.07401 | val_1_rmse: 0.07371 |  0:01:46s
epoch 6  | loss: 0.00603 | val_0_rmse: 0.09194 | val_1_rmse: 0.09185 |  0:02:04s
epoch 7  | loss: 0.00603 | val_0_rmse: 0.07536 | val_1_rmse: 0.07498 |  0:02:22s
epoch 8  | loss: 0.00602 | val_0_rmse: 0.07498 | val_1_rmse: 0.07454 |  0:02:40s
epoch 9  | loss: 0.00631 | val_0_rmse: 0.09309 | val_1_rmse: 0.09234 |  0:02:58s
epoch 10 | loss: 0.0064  | val_0_rmse: 0.07573 | val_1_rmse: 0.07538 |  0:03:15s
epoch 11 | loss: 0.00598 | val_0_rmse: 0.0743  | val_1_rmse: 0.07389 |  0:03:33s
epoch 12 | loss: 0.00589 | val_0_rmse: 0.074   | val_1_rmse: 0.07352 |  0:03:51s
epoch 13 | loss: 0.00607 | val_0_rmse: 0.0746  | val_1_rmse: 0.07421 |  0:04:09s
epoch 14 | loss: 0.0059  | val_0_rmse: 0.07374 | val_1_rmse: 0.07342 |  0:04:27s
epoch 15 | loss: 0.00584 | val_0_rmse: 0.07378 | val_1_rmse: 0.0735  |  0:04:44s
epoch 16 | loss: 0.00583 | val_0_rmse: 0.0747  | val_1_rmse: 0.07429 |  0:05:02s
epoch 17 | loss: 0.00596 | val_0_rmse: 0.0738  | val_1_rmse: 0.07352 |  0:05:20s
epoch 18 | loss: 0.00575 | val_0_rmse: 0.07627 | val_1_rmse: 0.076   |  0:05:38s
epoch 19 | loss: 0.00572 | val_0_rmse: 0.07645 | val_1_rmse: 0.07595 |  0:05:56s
epoch 20 | loss: 0.00575 | val_0_rmse: 0.07989 | val_1_rmse: 0.07961 |  0:06:14s
epoch 21 | loss: 0.00568 | val_0_rmse: 0.0752  | val_1_rmse: 0.07471 |  0:06:31s
epoch 22 | loss: 0.00558 | val_0_rmse: 0.07384 | val_1_rmse: 0.0735  |  0:06:49s
epoch 23 | loss: 0.00554 | val_0_rmse: 0.07345 | val_1_rmse: 0.07336 |  0:07:07s
epoch 24 | loss: 0.00556 | val_0_rmse: 0.07366 | val_1_rmse: 0.0735  |  0:07:25s
epoch 25 | loss: 0.00551 | val_0_rmse: 0.07187 | val_1_rmse: 0.07168 |  0:07:43s
epoch 26 | loss: 0.00549 | val_0_rmse: 0.07236 | val_1_rmse: 0.07208 |  0:08:01s
epoch 27 | loss: 0.0055  | val_0_rmse: 0.07482 | val_1_rmse: 0.07432 |  0:08:18s
epoch 28 | loss: 0.00551 | val_0_rmse: 0.07337 | val_1_rmse: 0.07288 |  0:08:36s
epoch 29 | loss: 0.00547 | val_0_rmse: 0.0724  | val_1_rmse: 0.07205 |  0:08:54s
epoch 30 | loss: 0.00551 | val_0_rmse: 0.07175 | val_1_rmse: 0.07139 |  0:09:12s
epoch 31 | loss: 0.00544 | val_0_rmse: 0.07189 | val_1_rmse: 0.07166 |  0:09:30s
epoch 32 | loss: 0.0055  | val_0_rmse: 0.07387 | val_1_rmse: 0.07373 |  0:09:47s
epoch 33 | loss: 0.00551 | val_0_rmse: 0.07579 | val_1_rmse: 0.07511 |  0:10:05s
epoch 34 | loss: 0.00557 | val_0_rmse: 0.07358 | val_1_rmse: 0.07348 |  0:10:23s
epoch 35 | loss: 0.00552 | val_0_rmse: 0.07216 | val_1_rmse: 0.07205 |  0:10:41s
epoch 36 | loss: 0.00563 | val_0_rmse: 0.0727  | val_1_rmse: 0.07238 |  0:10:59s
epoch 37 | loss: 0.00547 | val_0_rmse: 0.07236 | val_1_rmse: 0.07227 |  0:11:17s
epoch 38 | loss: 0.00597 | val_0_rmse: 0.07519 | val_1_rmse: 0.07487 |  0:11:34s
epoch 39 | loss: 0.006   | val_0_rmse: 0.0801  | val_1_rmse: 0.0799  |  0:11:52s
epoch 40 | loss: 0.00591 | val_0_rmse: 0.07424 | val_1_rmse: 0.07389 |  0:12:10s
epoch 41 | loss: 0.00563 | val_0_rmse: 0.07265 | val_1_rmse: 0.07232 |  0:12:28s
epoch 42 | loss: 0.00566 | val_0_rmse: 0.07327 | val_1_rmse: 0.07297 |  0:12:45s
epoch 43 | loss: 0.00792 | val_0_rmse: 0.0869  | val_1_rmse: 0.0863  |  0:13:03s
epoch 44 | loss: 0.00762 | val_0_rmse: 0.08874 | val_1_rmse: 0.08865 |  0:13:21s
epoch 45 | loss: 0.00757 | val_0_rmse: 0.08636 | val_1_rmse: 0.08604 |  0:13:39s
epoch 46 | loss: 0.00754 | val_0_rmse: 0.08634 | val_1_rmse: 0.08599 |  0:13:57s
epoch 47 | loss: 0.0075  | val_0_rmse: 0.08585 | val_1_rmse: 0.08549 |  0:14:15s
epoch 48 | loss: 0.00797 | val_0_rmse: 0.08824 | val_1_rmse: 0.08775 |  0:14:33s
epoch 49 | loss: 0.00756 | val_0_rmse: 0.0865  | val_1_rmse: 0.08619 |  0:14:50s
epoch 50 | loss: 0.00778 | val_0_rmse: 0.08642 | val_1_rmse: 0.08602 |  0:15:08s
epoch 51 | loss: 0.00754 | val_0_rmse: 0.08612 | val_1_rmse: 0.08568 |  0:15:26s
epoch 52 | loss: 0.00752 | val_0_rmse: 0.08618 | val_1_rmse: 0.08571 |  0:15:44s
epoch 53 | loss: 0.00747 | val_0_rmse: 0.0847  | val_1_rmse: 0.08429 |  0:16:02s
epoch 54 | loss: 0.00722 | val_0_rmse: 0.08418 | val_1_rmse: 0.08368 |  0:16:19s
epoch 55 | loss: 0.0072  | val_0_rmse: 0.08437 | val_1_rmse: 0.08399 |  0:16:37s
epoch 56 | loss: 0.00718 | val_0_rmse: 0.084   | val_1_rmse: 0.0835  |  0:16:55s
epoch 57 | loss: 0.00718 | val_0_rmse: 0.0842  | val_1_rmse: 0.08375 |  0:17:13s
epoch 58 | loss: 0.00714 | val_0_rmse: 0.08412 | val_1_rmse: 0.08374 |  0:17:30s
epoch 59 | loss: 0.00717 | val_0_rmse: 0.0844  | val_1_rmse: 0.08406 |  0:17:48s
epoch 60 | loss: 0.00717 | val_0_rmse: 0.08415 | val_1_rmse: 0.08366 |  0:18:06s

Early stopping occured at epoch 60 with best_epoch = 30 and best_val_1_rmse = 0.07139
Best weights from best epoch are automatically used!
ended training at: 08:04:15
Feature importance:
[('Area', 0.028273914842875482), ('Baths', 0.27814546206518237), ('Beds', 0.1084938144476641), ('Latitude', 0.3644915607693068), ('Longitude', 0.10960358838197938), ('Month', 0.11094913729256878), ('Year', 4.252220042314694e-05)]
Mean squared error is of 11095445103.05444
Mean absolute error:67563.19884016458
MAPE:0.4829786467187457
R2 score:0.7126426943821949
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\MinMaxNormalization\Logs\\\Model_saves\tabnet_model_test_4.zip
