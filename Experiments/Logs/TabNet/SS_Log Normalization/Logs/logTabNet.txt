TabNet Logs:

Saving copy of script...
In this script all the datasets are used and the price is normalized using the logarithmic function
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:31:26
epoch 0  | loss: 48.22174| val_0_rmse: 2.3099  | val_1_rmse: 2.29079 |  0:00:04s
epoch 1  | loss: 0.68569 | val_0_rmse: 1.01308 | val_1_rmse: 1.00953 |  0:00:06s
epoch 2  | loss: 0.27897 | val_0_rmse: 0.66738 | val_1_rmse: 0.66388 |  0:00:08s
epoch 3  | loss: 0.18608 | val_0_rmse: 0.47674 | val_1_rmse: 0.481   |  0:00:09s
epoch 4  | loss: 0.15332 | val_0_rmse: 0.44106 | val_1_rmse: 0.44522 |  0:00:11s
epoch 5  | loss: 0.19671 | val_0_rmse: 0.34802 | val_1_rmse: 0.35507 |  0:00:13s
epoch 6  | loss: 0.13454 | val_0_rmse: 0.3562  | val_1_rmse: 0.35996 |  0:00:15s
epoch 7  | loss: 0.16458 | val_0_rmse: 0.3466  | val_1_rmse: 0.3548  |  0:00:16s
epoch 8  | loss: 0.15445 | val_0_rmse: 0.34717 | val_1_rmse: 0.35821 |  0:00:19s
epoch 9  | loss: 0.1487  | val_0_rmse: 0.35216 | val_1_rmse: 0.35797 |  0:00:20s
epoch 10 | loss: 0.15024 | val_0_rmse: 0.35746 | val_1_rmse: 0.36532 |  0:00:22s
epoch 11 | loss: 0.14515 | val_0_rmse: 0.32399 | val_1_rmse: 0.32807 |  0:00:24s
epoch 12 | loss: 0.1394  | val_0_rmse: 0.33478 | val_1_rmse: 0.33911 |  0:00:26s
epoch 13 | loss: 0.13931 | val_0_rmse: 0.36721 | val_1_rmse: 0.37099 |  0:00:27s
epoch 14 | loss: 0.14267 | val_0_rmse: 0.33626 | val_1_rmse: 0.34283 |  0:00:29s
epoch 15 | loss: 0.13593 | val_0_rmse: 0.33567 | val_1_rmse: 0.34114 |  0:00:31s
epoch 16 | loss: 0.13397 | val_0_rmse: 0.3424  | val_1_rmse: 0.34985 |  0:00:33s
epoch 17 | loss: 0.13559 | val_0_rmse: 0.32775 | val_1_rmse: 0.33275 |  0:00:34s
epoch 18 | loss: 0.12849 | val_0_rmse: 0.32267 | val_1_rmse: 0.32759 |  0:00:36s
epoch 19 | loss: 0.13166 | val_0_rmse: 0.33794 | val_1_rmse: 0.34466 |  0:00:38s
epoch 20 | loss: 0.12688 | val_0_rmse: 0.32255 | val_1_rmse: 0.32756 |  0:00:40s
epoch 21 | loss: 0.12636 | val_0_rmse: 0.34441 | val_1_rmse: 0.34898 |  0:00:42s
epoch 22 | loss: 0.12293 | val_0_rmse: 0.31759 | val_1_rmse: 0.32492 |  0:00:44s
epoch 23 | loss: 0.12302 | val_0_rmse: 0.3158  | val_1_rmse: 0.32309 |  0:00:46s
epoch 24 | loss: 0.11768 | val_0_rmse: 0.30626 | val_1_rmse: 0.31401 |  0:00:47s
epoch 25 | loss: 0.11824 | val_0_rmse: 0.34801 | val_1_rmse: 0.3563  |  0:00:49s
epoch 26 | loss: 0.11492 | val_0_rmse: 0.28616 | val_1_rmse: 0.29282 |  0:00:51s
epoch 27 | loss: 0.11204 | val_0_rmse: 0.29735 | val_1_rmse: 0.30398 |  0:00:53s
epoch 28 | loss: 0.11296 | val_0_rmse: 0.30603 | val_1_rmse: 0.31133 |  0:00:55s
epoch 29 | loss: 0.11624 | val_0_rmse: 0.29392 | val_1_rmse: 0.30132 |  0:00:57s
epoch 30 | loss: 0.10959 | val_0_rmse: 0.30425 | val_1_rmse: 0.31252 |  0:00:59s
epoch 31 | loss: 0.12082 | val_0_rmse: 0.32125 | val_1_rmse: 0.32817 |  0:01:00s
epoch 32 | loss: 0.11791 | val_0_rmse: 0.30158 | val_1_rmse: 0.3109  |  0:01:02s
epoch 33 | loss: 0.11266 | val_0_rmse: 0.31408 | val_1_rmse: 0.32536 |  0:01:04s
epoch 34 | loss: 0.1143  | val_0_rmse: 0.30789 | val_1_rmse: 0.31526 |  0:01:05s
epoch 35 | loss: 0.1081  | val_0_rmse: 0.28275 | val_1_rmse: 0.29481 |  0:01:07s
epoch 36 | loss: 0.11158 | val_0_rmse: 0.32502 | val_1_rmse: 0.3349  |  0:01:09s
epoch 37 | loss: 0.10908 | val_0_rmse: 0.30794 | val_1_rmse: 0.31668 |  0:01:11s
epoch 38 | loss: 0.11334 | val_0_rmse: 0.29021 | val_1_rmse: 0.29997 |  0:01:12s
epoch 39 | loss: 0.10415 | val_0_rmse: 0.33307 | val_1_rmse: 0.34186 |  0:01:14s
epoch 40 | loss: 0.10927 | val_0_rmse: 0.2793  | val_1_rmse: 0.28898 |  0:01:16s
epoch 41 | loss: 0.10357 | val_0_rmse: 0.27771 | val_1_rmse: 0.28374 |  0:01:18s
epoch 42 | loss: 0.10201 | val_0_rmse: 0.30117 | val_1_rmse: 0.30833 |  0:01:19s
epoch 43 | loss: 0.1073  | val_0_rmse: 0.3157  | val_1_rmse: 0.325   |  0:01:21s
epoch 44 | loss: 0.10512 | val_0_rmse: 0.26187 | val_1_rmse: 0.26983 |  0:01:23s
epoch 45 | loss: 0.09983 | val_0_rmse: 0.31144 | val_1_rmse: 0.31965 |  0:01:25s
epoch 46 | loss: 0.10483 | val_0_rmse: 0.30187 | val_1_rmse: 0.30838 |  0:01:26s
epoch 47 | loss: 0.10341 | val_0_rmse: 0.28007 | val_1_rmse: 0.28823 |  0:01:28s
epoch 48 | loss: 0.10184 | val_0_rmse: 0.3074  | val_1_rmse: 0.31577 |  0:01:30s
epoch 49 | loss: 0.10432 | val_0_rmse: 0.28832 | val_1_rmse: 0.29645 |  0:01:31s
epoch 50 | loss: 0.10919 | val_0_rmse: 0.30509 | val_1_rmse: 0.31475 |  0:01:33s
epoch 51 | loss: 0.10151 | val_0_rmse: 0.2624  | val_1_rmse: 0.27092 |  0:01:35s
epoch 52 | loss: 0.10603 | val_0_rmse: 0.29831 | val_1_rmse: 0.30551 |  0:01:36s
epoch 53 | loss: 0.10527 | val_0_rmse: 0.28275 | val_1_rmse: 0.29081 |  0:01:38s
epoch 54 | loss: 0.10689 | val_0_rmse: 0.28171 | val_1_rmse: 0.28914 |  0:01:40s
epoch 55 | loss: 0.10442 | val_0_rmse: 0.29057 | val_1_rmse: 0.29849 |  0:01:41s
epoch 56 | loss: 0.10563 | val_0_rmse: 0.28377 | val_1_rmse: 0.2908  |  0:01:43s
epoch 57 | loss: 0.1072  | val_0_rmse: 0.29401 | val_1_rmse: 0.30216 |  0:01:45s
epoch 58 | loss: 0.10892 | val_0_rmse: 0.31805 | val_1_rmse: 0.3273  |  0:01:46s
epoch 59 | loss: 0.10524 | val_0_rmse: 0.26647 | val_1_rmse: 0.27725 |  0:01:48s
epoch 60 | loss: 0.10281 | val_0_rmse: 0.2818  | val_1_rmse: 0.29109 |  0:01:50s
epoch 61 | loss: 0.10175 | val_0_rmse: 0.31712 | val_1_rmse: 0.32417 |  0:01:51s
epoch 62 | loss: 0.10702 | val_0_rmse: 0.28091 | val_1_rmse: 0.28995 |  0:01:53s
epoch 63 | loss: 0.10003 | val_0_rmse: 0.29918 | val_1_rmse: 0.30542 |  0:01:55s
epoch 64 | loss: 0.1011  | val_0_rmse: 0.32377 | val_1_rmse: 0.33118 |  0:01:56s
epoch 65 | loss: 0.10374 | val_0_rmse: 0.27849 | val_1_rmse: 0.28758 |  0:01:58s
epoch 66 | loss: 0.10078 | val_0_rmse: 0.28985 | val_1_rmse: 0.29892 |  0:02:00s
epoch 67 | loss: 0.10016 | val_0_rmse: 0.28061 | val_1_rmse: 0.28988 |  0:02:01s
epoch 68 | loss: 0.09946 | val_0_rmse: 0.27429 | val_1_rmse: 0.2822  |  0:02:03s
epoch 69 | loss: 0.09766 | val_0_rmse: 0.28284 | val_1_rmse: 0.29228 |  0:02:05s
epoch 70 | loss: 0.10039 | val_0_rmse: 0.31366 | val_1_rmse: 0.32064 |  0:02:06s
epoch 71 | loss: 0.09773 | val_0_rmse: 0.28539 | val_1_rmse: 0.29473 |  0:02:08s
epoch 72 | loss: 0.09727 | val_0_rmse: 0.29159 | val_1_rmse: 0.30035 |  0:02:10s
epoch 73 | loss: 0.09677 | val_0_rmse: 0.28589 | val_1_rmse: 0.29517 |  0:02:11s
epoch 74 | loss: 0.09608 | val_0_rmse: 0.27319 | val_1_rmse: 0.28226 |  0:02:13s

Early stopping occured at epoch 74 with best_epoch = 44 and best_val_1_rmse = 0.26983
Best weights from best epoch are automatically used!
ended training at: 03:33:40
Feature importance:
[('Area', 0.1023850376835895), ('Baths', 0.04498098532476864), ('Beds', 0.0), ('Latitude', 0.3532778690603262), ('Longitude', 0.4010097888549212), ('Month', 0.09275078106061098), ('Year', 0.005595538015783425)]
Mean squared error is of 9146398618.631592
Mean absolute error:64622.469020146884
MAPE:0.18744481110448483
R2 score:0.5987394206124558
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:33:41
epoch 0  | loss: 47.9356 | val_0_rmse: 1.86963 | val_1_rmse: 1.87294 |  0:00:01s
epoch 1  | loss: 0.65904 | val_0_rmse: 1.07489 | val_1_rmse: 1.06877 |  0:00:03s
epoch 2  | loss: 0.39472 | val_0_rmse: 0.51926 | val_1_rmse: 0.53356 |  0:00:05s
epoch 3  | loss: 0.25904 | val_0_rmse: 0.58902 | val_1_rmse: 0.58404 |  0:00:06s
epoch 4  | loss: 0.2448  | val_0_rmse: 0.42719 | val_1_rmse: 0.43458 |  0:00:08s
epoch 5  | loss: 0.22568 | val_0_rmse: 0.49306 | val_1_rmse: 0.49025 |  0:00:10s
epoch 6  | loss: 0.21771 | val_0_rmse: 0.39414 | val_1_rmse: 0.40176 |  0:00:11s
epoch 7  | loss: 0.21352 | val_0_rmse: 0.44945 | val_1_rmse: 0.45037 |  0:00:13s
epoch 8  | loss: 0.20687 | val_0_rmse: 0.43216 | val_1_rmse: 0.43405 |  0:00:15s
epoch 9  | loss: 0.2027  | val_0_rmse: 0.4609  | val_1_rmse: 0.45613 |  0:00:17s
epoch 10 | loss: 0.21099 | val_0_rmse: 0.40068 | val_1_rmse: 0.40674 |  0:00:19s
epoch 11 | loss: 0.20262 | val_0_rmse: 0.4402  | val_1_rmse: 0.44441 |  0:00:21s
epoch 12 | loss: 0.21071 | val_0_rmse: 0.39657 | val_1_rmse: 0.39972 |  0:00:22s
epoch 13 | loss: 0.20394 | val_0_rmse: 0.4575  | val_1_rmse: 0.4609  |  0:00:24s
epoch 14 | loss: 0.18412 | val_0_rmse: 0.39002 | val_1_rmse: 0.39379 |  0:00:26s
epoch 15 | loss: 0.18746 | val_0_rmse: 0.47607 | val_1_rmse: 0.48414 |  0:00:28s
epoch 16 | loss: 0.20216 | val_0_rmse: 0.3629  | val_1_rmse: 0.36796 |  0:00:29s
epoch 17 | loss: 0.18812 | val_0_rmse: 0.50186 | val_1_rmse: 0.50309 |  0:00:31s
epoch 18 | loss: 0.18093 | val_0_rmse: 0.39651 | val_1_rmse: 0.40117 |  0:00:33s
epoch 19 | loss: 0.17975 | val_0_rmse: 0.42923 | val_1_rmse: 0.42864 |  0:00:34s
epoch 20 | loss: 0.17731 | val_0_rmse: 0.38077 | val_1_rmse: 0.38641 |  0:00:36s
epoch 21 | loss: 0.17542 | val_0_rmse: 0.43452 | val_1_rmse: 0.43472 |  0:00:38s
epoch 22 | loss: 0.17427 | val_0_rmse: 0.39841 | val_1_rmse: 0.40394 |  0:00:39s
epoch 23 | loss: 0.17419 | val_0_rmse: 0.45042 | val_1_rmse: 0.45026 |  0:00:41s
epoch 24 | loss: 0.17227 | val_0_rmse: 0.37506 | val_1_rmse: 0.37824 |  0:00:43s
epoch 25 | loss: 0.1759  | val_0_rmse: 0.47683 | val_1_rmse: 0.47753 |  0:00:45s
epoch 26 | loss: 0.17174 | val_0_rmse: 0.37709 | val_1_rmse: 0.3802  |  0:00:46s
epoch 27 | loss: 0.17383 | val_0_rmse: 0.41408 | val_1_rmse: 0.41546 |  0:00:48s
epoch 28 | loss: 0.17323 | val_0_rmse: 0.4254  | val_1_rmse: 0.4286  |  0:00:50s
epoch 29 | loss: 0.1724  | val_0_rmse: 0.39145 | val_1_rmse: 0.39219 |  0:00:51s
epoch 30 | loss: 0.17302 | val_0_rmse: 0.44314 | val_1_rmse: 0.44614 |  0:00:53s
epoch 31 | loss: 0.18014 | val_0_rmse: 0.45784 | val_1_rmse: 0.45197 |  0:00:55s
epoch 32 | loss: 0.16915 | val_0_rmse: 0.30229 | val_1_rmse: 0.30235 |  0:00:56s
epoch 33 | loss: 0.09907 | val_0_rmse: 0.28709 | val_1_rmse: 0.29106 |  0:00:58s
epoch 34 | loss: 0.09823 | val_0_rmse: 0.30235 | val_1_rmse: 0.3079  |  0:01:00s
epoch 35 | loss: 0.10477 | val_0_rmse: 0.30668 | val_1_rmse: 0.31404 |  0:01:01s
epoch 36 | loss: 0.09736 | val_0_rmse: 0.29995 | val_1_rmse: 0.30946 |  0:01:03s
epoch 37 | loss: 0.11112 | val_0_rmse: 0.28532 | val_1_rmse: 0.28976 |  0:01:05s
epoch 38 | loss: 0.09402 | val_0_rmse: 0.29568 | val_1_rmse: 0.29687 |  0:01:07s
epoch 39 | loss: 0.08782 | val_0_rmse: 0.27969 | val_1_rmse: 0.28309 |  0:01:08s
epoch 40 | loss: 0.10314 | val_0_rmse: 0.30624 | val_1_rmse: 0.307   |  0:01:10s
epoch 41 | loss: 0.10489 | val_0_rmse: 0.27076 | val_1_rmse: 0.27664 |  0:01:12s
epoch 42 | loss: 0.1055  | val_0_rmse: 0.27597 | val_1_rmse: 0.27821 |  0:01:13s
epoch 43 | loss: 0.10611 | val_0_rmse: 0.30078 | val_1_rmse: 0.30745 |  0:01:15s
epoch 44 | loss: 0.10678 | val_0_rmse: 0.30407 | val_1_rmse: 0.30806 |  0:01:17s
epoch 45 | loss: 0.10545 | val_0_rmse: 0.29444 | val_1_rmse: 0.29604 |  0:01:18s
epoch 46 | loss: 0.10525 | val_0_rmse: 0.30396 | val_1_rmse: 0.31124 |  0:01:20s
epoch 47 | loss: 0.10582 | val_0_rmse: 0.31571 | val_1_rmse: 0.3157  |  0:01:22s
epoch 48 | loss: 0.10681 | val_0_rmse: 0.29932 | val_1_rmse: 0.30092 |  0:01:24s
epoch 49 | loss: 0.10433 | val_0_rmse: 0.30257 | val_1_rmse: 0.30883 |  0:01:25s
epoch 50 | loss: 0.10434 | val_0_rmse: 0.31257 | val_1_rmse: 0.31534 |  0:01:27s
epoch 51 | loss: 0.10313 | val_0_rmse: 0.28948 | val_1_rmse: 0.29181 |  0:01:29s
epoch 52 | loss: 0.10283 | val_0_rmse: 0.28999 | val_1_rmse: 0.29672 |  0:01:30s
epoch 53 | loss: 0.10777 | val_0_rmse: 0.29851 | val_1_rmse: 0.30101 |  0:01:32s
epoch 54 | loss: 0.10441 | val_0_rmse: 0.28372 | val_1_rmse: 0.28691 |  0:01:34s
epoch 55 | loss: 0.10202 | val_0_rmse: 0.34632 | val_1_rmse: 0.35179 |  0:01:35s
epoch 56 | loss: 0.11025 | val_0_rmse: 0.28987 | val_1_rmse: 0.29144 |  0:01:37s
epoch 57 | loss: 0.10043 | val_0_rmse: 0.30212 | val_1_rmse: 0.30859 |  0:01:39s
epoch 58 | loss: 0.12359 | val_0_rmse: 0.3548  | val_1_rmse: 0.35896 |  0:01:41s
epoch 59 | loss: 0.10954 | val_0_rmse: 0.29148 | val_1_rmse: 0.29421 |  0:01:42s
epoch 60 | loss: 0.10336 | val_0_rmse: 0.30155 | val_1_rmse: 0.30441 |  0:01:44s
epoch 61 | loss: 0.10252 | val_0_rmse: 0.33224 | val_1_rmse: 0.33974 |  0:01:46s
epoch 62 | loss: 0.08735 | val_0_rmse: 0.2986  | val_1_rmse: 0.30156 |  0:01:47s
epoch 63 | loss: 0.08304 | val_0_rmse: 0.27647 | val_1_rmse: 0.2816  |  0:01:49s
epoch 64 | loss: 0.07889 | val_0_rmse: 0.26563 | val_1_rmse: 0.26899 |  0:01:51s
epoch 65 | loss: 0.07889 | val_0_rmse: 0.26962 | val_1_rmse: 0.27278 |  0:01:53s
epoch 66 | loss: 0.07848 | val_0_rmse: 0.26147 | val_1_rmse: 0.26624 |  0:01:54s
epoch 67 | loss: 0.11215 | val_0_rmse: 0.26822 | val_1_rmse: 0.27364 |  0:01:56s
epoch 68 | loss: 0.09859 | val_0_rmse: 0.27801 | val_1_rmse: 0.28188 |  0:01:58s
epoch 69 | loss: 0.08766 | val_0_rmse: 0.26827 | val_1_rmse: 0.2752  |  0:01:59s
epoch 70 | loss: 0.08424 | val_0_rmse: 0.26846 | val_1_rmse: 0.2743  |  0:02:01s
epoch 71 | loss: 0.08424 | val_0_rmse: 0.28722 | val_1_rmse: 0.29243 |  0:02:03s
epoch 72 | loss: 0.09168 | val_0_rmse: 0.33565 | val_1_rmse: 0.34329 |  0:02:04s
epoch 73 | loss: 0.11573 | val_0_rmse: 0.26358 | val_1_rmse: 0.26911 |  0:02:06s
epoch 74 | loss: 0.07734 | val_0_rmse: 0.27548 | val_1_rmse: 0.28186 |  0:02:08s
epoch 75 | loss: 0.08248 | val_0_rmse: 0.26499 | val_1_rmse: 0.27142 |  0:02:09s
epoch 76 | loss: 0.07446 | val_0_rmse: 0.25924 | val_1_rmse: 0.26409 |  0:02:11s
epoch 77 | loss: 0.07532 | val_0_rmse: 0.27089 | val_1_rmse: 0.27584 |  0:02:13s
epoch 78 | loss: 0.08175 | val_0_rmse: 0.3087  | val_1_rmse: 0.31117 |  0:02:15s
epoch 79 | loss: 0.08983 | val_0_rmse: 0.26887 | val_1_rmse: 0.27487 |  0:02:16s
epoch 80 | loss: 0.08245 | val_0_rmse: 0.26494 | val_1_rmse: 0.27162 |  0:02:18s
epoch 81 | loss: 0.0807  | val_0_rmse: 0.28983 | val_1_rmse: 0.29325 |  0:02:20s
epoch 82 | loss: 0.08511 | val_0_rmse: 0.25957 | val_1_rmse: 0.26414 |  0:02:21s
epoch 83 | loss: 0.08654 | val_0_rmse: 0.27731 | val_1_rmse: 0.27825 |  0:02:23s
epoch 84 | loss: 0.07808 | val_0_rmse: 0.27264 | val_1_rmse: 0.27687 |  0:02:25s
epoch 85 | loss: 0.08421 | val_0_rmse: 0.28615 | val_1_rmse: 0.28646 |  0:02:26s
epoch 86 | loss: 0.08139 | val_0_rmse: 0.27545 | val_1_rmse: 0.27726 |  0:02:28s
epoch 87 | loss: 0.08395 | val_0_rmse: 0.27017 | val_1_rmse: 0.27776 |  0:02:30s
epoch 88 | loss: 0.08966 | val_0_rmse: 0.41333 | val_1_rmse: 0.41732 |  0:02:31s
epoch 89 | loss: 0.12645 | val_0_rmse: 0.28886 | val_1_rmse: 0.29397 |  0:02:33s
epoch 90 | loss: 0.10849 | val_0_rmse: 0.3201  | val_1_rmse: 0.32632 |  0:02:35s
epoch 91 | loss: 0.10723 | val_0_rmse: 0.3008  | val_1_rmse: 0.30262 |  0:02:36s
epoch 92 | loss: 0.10863 | val_0_rmse: 0.30263 | val_1_rmse: 0.30288 |  0:02:38s
epoch 93 | loss: 0.10602 | val_0_rmse: 0.31426 | val_1_rmse: 0.32037 |  0:02:40s
epoch 94 | loss: 0.10292 | val_0_rmse: 0.30343 | val_1_rmse: 0.30735 |  0:02:41s
epoch 95 | loss: 0.10504 | val_0_rmse: 0.29182 | val_1_rmse: 0.29433 |  0:02:43s
epoch 96 | loss: 0.10309 | val_0_rmse: 0.29803 | val_1_rmse: 0.30521 |  0:02:45s
epoch 97 | loss: 0.10515 | val_0_rmse: 0.30012 | val_1_rmse: 0.30217 |  0:02:47s
epoch 98 | loss: 0.10497 | val_0_rmse: 0.30249 | val_1_rmse: 0.30434 |  0:02:48s
epoch 99 | loss: 0.1056  | val_0_rmse: 0.30304 | val_1_rmse: 0.30925 |  0:02:50s
epoch 100| loss: 0.10668 | val_0_rmse: 0.29488 | val_1_rmse: 0.29753 |  0:02:52s
epoch 101| loss: 0.10254 | val_0_rmse: 0.28228 | val_1_rmse: 0.28355 |  0:02:53s
epoch 102| loss: 0.1025  | val_0_rmse: 0.30632 | val_1_rmse: 0.31381 |  0:02:55s
epoch 103| loss: 0.10687 | val_0_rmse: 0.31686 | val_1_rmse: 0.31835 |  0:02:57s
epoch 104| loss: 0.10371 | val_0_rmse: 0.28601 | val_1_rmse: 0.28879 |  0:02:58s
epoch 105| loss: 0.10404 | val_0_rmse: 0.30782 | val_1_rmse: 0.315   |  0:03:00s
epoch 106| loss: 0.10334 | val_0_rmse: 0.29536 | val_1_rmse: 0.29763 |  0:03:02s

Early stopping occured at epoch 106 with best_epoch = 76 and best_val_1_rmse = 0.26409
Best weights from best epoch are automatically used!
ended training at: 03:36:44
Feature importance:
[('Area', 0.3059052593934517), ('Baths', 0.008667472191827199), ('Beds', 0.08382685531654585), ('Latitude', 0.1994004020813037), ('Longitude', 0.3931885453592495), ('Month', 0.0), ('Year', 0.009011465657622019)]
Mean squared error is of 7728473682.939001
Mean absolute error:60555.45963637193
MAPE:0.19615925244862709
R2 score:0.6523612718543883
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:36:44
epoch 0  | loss: 47.48808| val_0_rmse: 2.06074 | val_1_rmse: 2.01614 |  0:00:01s
epoch 1  | loss: 0.96927 | val_0_rmse: 0.65015 | val_1_rmse: 0.64804 |  0:00:03s
epoch 2  | loss: 0.36608 | val_0_rmse: 0.60226 | val_1_rmse: 0.59741 |  0:00:05s
epoch 3  | loss: 0.24628 | val_0_rmse: 0.41824 | val_1_rmse: 0.41927 |  0:00:06s
epoch 4  | loss: 0.19345 | val_0_rmse: 0.39231 | val_1_rmse: 0.39339 |  0:00:08s
epoch 5  | loss: 0.19713 | val_0_rmse: 0.45244 | val_1_rmse: 0.45406 |  0:00:10s
epoch 6  | loss: 0.25479 | val_0_rmse: 0.43172 | val_1_rmse: 0.43175 |  0:00:12s
epoch 7  | loss: 0.33062 | val_0_rmse: 0.73938 | val_1_rmse: 0.72567 |  0:00:13s
epoch 8  | loss: 0.28336 | val_0_rmse: 0.38915 | val_1_rmse: 0.39311 |  0:00:15s
epoch 9  | loss: 0.16823 | val_0_rmse: 0.41266 | val_1_rmse: 0.41865 |  0:00:17s
epoch 10 | loss: 0.1756  | val_0_rmse: 0.38872 | val_1_rmse: 0.38444 |  0:00:18s
epoch 11 | loss: 0.18645 | val_0_rmse: 0.38281 | val_1_rmse: 0.38029 |  0:00:20s
epoch 12 | loss: 0.16998 | val_0_rmse: 0.39695 | val_1_rmse: 0.39821 |  0:00:22s
epoch 13 | loss: 0.14642 | val_0_rmse: 0.33922 | val_1_rmse: 0.34116 |  0:00:23s
epoch 14 | loss: 0.13043 | val_0_rmse: 0.34344 | val_1_rmse: 0.34807 |  0:00:25s
epoch 15 | loss: 0.1253  | val_0_rmse: 0.3506  | val_1_rmse: 0.35125 |  0:00:27s
epoch 16 | loss: 0.12542 | val_0_rmse: 0.44129 | val_1_rmse: 0.43758 |  0:00:29s
epoch 17 | loss: 0.155   | val_0_rmse: 0.34827 | val_1_rmse: 0.34998 |  0:00:30s
epoch 18 | loss: 0.16607 | val_0_rmse: 0.34575 | val_1_rmse: 0.34541 |  0:00:32s
epoch 19 | loss: 0.1296  | val_0_rmse: 0.35511 | val_1_rmse: 0.35644 |  0:00:34s
epoch 20 | loss: 0.12873 | val_0_rmse: 0.33672 | val_1_rmse: 0.34001 |  0:00:35s
epoch 21 | loss: 0.1263  | val_0_rmse: 0.34299 | val_1_rmse: 0.33927 |  0:00:37s
epoch 22 | loss: 0.15028 | val_0_rmse: 0.36659 | val_1_rmse: 0.36251 |  0:00:39s
epoch 23 | loss: 0.12432 | val_0_rmse: 0.37241 | val_1_rmse: 0.36911 |  0:00:40s
epoch 24 | loss: 0.11972 | val_0_rmse: 0.31408 | val_1_rmse: 0.31417 |  0:00:42s
epoch 25 | loss: 0.111   | val_0_rmse: 0.31449 | val_1_rmse: 0.3121  |  0:00:44s
epoch 26 | loss: 0.10667 | val_0_rmse: 0.30962 | val_1_rmse: 0.30772 |  0:00:46s
epoch 27 | loss: 0.10841 | val_0_rmse: 0.36813 | val_1_rmse: 0.36261 |  0:00:47s
epoch 28 | loss: 0.1038  | val_0_rmse: 0.35252 | val_1_rmse: 0.3531  |  0:00:49s
epoch 29 | loss: 0.10195 | val_0_rmse: 0.28912 | val_1_rmse: 0.2892  |  0:00:51s
epoch 30 | loss: 0.10575 | val_0_rmse: 0.41419 | val_1_rmse: 0.41379 |  0:00:52s
epoch 31 | loss: 0.12231 | val_0_rmse: 0.29866 | val_1_rmse: 0.29992 |  0:00:54s
epoch 32 | loss: 0.10348 | val_0_rmse: 0.29797 | val_1_rmse: 0.3     |  0:00:56s
epoch 33 | loss: 0.10428 | val_0_rmse: 0.33805 | val_1_rmse: 0.3393  |  0:00:57s
epoch 34 | loss: 0.10736 | val_0_rmse: 0.31249 | val_1_rmse: 0.31104 |  0:00:59s
epoch 35 | loss: 0.10334 | val_0_rmse: 0.3361  | val_1_rmse: 0.33656 |  0:01:01s
epoch 36 | loss: 0.09855 | val_0_rmse: 0.31231 | val_1_rmse: 0.31516 |  0:01:03s
epoch 37 | loss: 0.096   | val_0_rmse: 0.28385 | val_1_rmse: 0.28437 |  0:01:04s
epoch 38 | loss: 0.09008 | val_0_rmse: 0.31523 | val_1_rmse: 0.31903 |  0:01:06s
epoch 39 | loss: 0.16818 | val_0_rmse: 0.33747 | val_1_rmse: 0.33501 |  0:01:08s
epoch 40 | loss: 0.13962 | val_0_rmse: 0.30376 | val_1_rmse: 0.30381 |  0:01:09s
epoch 41 | loss: 0.08842 | val_0_rmse: 0.27499 | val_1_rmse: 0.27421 |  0:01:11s
epoch 42 | loss: 0.10419 | val_0_rmse: 0.30222 | val_1_rmse: 0.30586 |  0:01:13s
epoch 43 | loss: 0.10444 | val_0_rmse: 0.29205 | val_1_rmse: 0.29312 |  0:01:15s
epoch 44 | loss: 0.0939  | val_0_rmse: 0.27194 | val_1_rmse: 0.27265 |  0:01:16s
epoch 45 | loss: 0.08838 | val_0_rmse: 0.30072 | val_1_rmse: 0.30001 |  0:01:18s
epoch 46 | loss: 0.1042  | val_0_rmse: 0.28125 | val_1_rmse: 0.28169 |  0:01:20s
epoch 47 | loss: 0.11858 | val_0_rmse: 0.32356 | val_1_rmse: 0.32455 |  0:01:21s
epoch 48 | loss: 0.11719 | val_0_rmse: 0.29289 | val_1_rmse: 0.29781 |  0:01:23s
epoch 49 | loss: 0.11041 | val_0_rmse: 0.30193 | val_1_rmse: 0.30554 |  0:01:25s
epoch 50 | loss: 0.10768 | val_0_rmse: 0.35114 | val_1_rmse: 0.35395 |  0:01:27s
epoch 51 | loss: 0.10879 | val_0_rmse: 0.27444 | val_1_rmse: 0.278   |  0:01:29s
epoch 52 | loss: 0.10669 | val_0_rmse: 0.26637 | val_1_rmse: 0.26782 |  0:01:30s
epoch 53 | loss: 0.16475 | val_0_rmse: 0.54261 | val_1_rmse: 0.54383 |  0:01:32s
epoch 54 | loss: 0.17906 | val_0_rmse: 0.27132 | val_1_rmse: 0.27179 |  0:01:34s
epoch 55 | loss: 0.10952 | val_0_rmse: 0.37572 | val_1_rmse: 0.37589 |  0:01:36s
epoch 56 | loss: 0.10988 | val_0_rmse: 0.28002 | val_1_rmse: 0.2829  |  0:01:37s
epoch 57 | loss: 0.09041 | val_0_rmse: 0.27847 | val_1_rmse: 0.28147 |  0:01:39s
epoch 58 | loss: 0.12005 | val_0_rmse: 0.30215 | val_1_rmse: 0.30116 |  0:01:41s
epoch 59 | loss: 0.09847 | val_0_rmse: 0.28127 | val_1_rmse: 0.28868 |  0:01:42s
epoch 60 | loss: 0.08224 | val_0_rmse: 0.26514 | val_1_rmse: 0.27086 |  0:01:44s
epoch 61 | loss: 0.08587 | val_0_rmse: 0.26388 | val_1_rmse: 0.2654  |  0:01:45s
epoch 62 | loss: 0.10875 | val_0_rmse: 0.29897 | val_1_rmse: 0.3026  |  0:01:47s
epoch 63 | loss: 0.1053  | val_0_rmse: 0.28628 | val_1_rmse: 0.2906  |  0:01:48s
epoch 64 | loss: 0.08067 | val_0_rmse: 0.26699 | val_1_rmse: 0.26865 |  0:01:50s
epoch 65 | loss: 0.08084 | val_0_rmse: 0.27089 | val_1_rmse: 0.27382 |  0:01:51s
epoch 66 | loss: 0.11868 | val_0_rmse: 0.40392 | val_1_rmse: 0.4063  |  0:01:52s
epoch 67 | loss: 0.1369  | val_0_rmse: 0.29253 | val_1_rmse: 0.29724 |  0:01:54s
epoch 68 | loss: 0.08431 | val_0_rmse: 0.27952 | val_1_rmse: 0.28118 |  0:01:55s
epoch 69 | loss: 0.08258 | val_0_rmse: 0.27675 | val_1_rmse: 0.27913 |  0:01:57s
epoch 70 | loss: 0.09368 | val_0_rmse: 0.31922 | val_1_rmse: 0.32472 |  0:01:58s
epoch 71 | loss: 0.11354 | val_0_rmse: 0.30605 | val_1_rmse: 0.30834 |  0:02:00s
epoch 72 | loss: 0.11055 | val_0_rmse: 0.26297 | val_1_rmse: 0.26706 |  0:02:01s
epoch 73 | loss: 0.10241 | val_0_rmse: 0.26217 | val_1_rmse: 0.26666 |  0:02:02s
epoch 74 | loss: 0.10372 | val_0_rmse: 0.3712  | val_1_rmse: 0.37466 |  0:02:04s
epoch 75 | loss: 0.10231 | val_0_rmse: 0.26292 | val_1_rmse: 0.26942 |  0:02:05s
epoch 76 | loss: 0.1042  | val_0_rmse: 0.27841 | val_1_rmse: 0.283   |  0:02:07s
epoch 77 | loss: 0.1044  | val_0_rmse: 0.31846 | val_1_rmse: 0.32221 |  0:02:08s
epoch 78 | loss: 0.10192 | val_0_rmse: 0.2978  | val_1_rmse: 0.30257 |  0:02:10s
epoch 79 | loss: 0.10321 | val_0_rmse: 0.28874 | val_1_rmse: 0.29421 |  0:02:11s
epoch 80 | loss: 0.09841 | val_0_rmse: 0.33236 | val_1_rmse: 0.33763 |  0:02:12s
epoch 81 | loss: 0.10083 | val_0_rmse: 0.27086 | val_1_rmse: 0.27626 |  0:02:14s
epoch 82 | loss: 0.09936 | val_0_rmse: 0.27113 | val_1_rmse: 0.27517 |  0:02:15s
epoch 83 | loss: 0.10324 | val_0_rmse: 0.3448  | val_1_rmse: 0.34654 |  0:02:17s
epoch 84 | loss: 0.10296 | val_0_rmse: 0.27903 | val_1_rmse: 0.28238 |  0:02:18s
epoch 85 | loss: 0.07852 | val_0_rmse: 0.27168 | val_1_rmse: 0.27772 |  0:02:20s
epoch 86 | loss: 0.07924 | val_0_rmse: 0.27496 | val_1_rmse: 0.27833 |  0:02:21s
epoch 87 | loss: 0.09236 | val_0_rmse: 0.28484 | val_1_rmse: 0.28648 |  0:02:23s
epoch 88 | loss: 0.08559 | val_0_rmse: 0.28185 | val_1_rmse: 0.28528 |  0:02:24s
epoch 89 | loss: 0.08253 | val_0_rmse: 0.27232 | val_1_rmse: 0.27803 |  0:02:25s
epoch 90 | loss: 0.08312 | val_0_rmse: 0.26202 | val_1_rmse: 0.26806 |  0:02:27s
epoch 91 | loss: 0.07995 | val_0_rmse: 0.28627 | val_1_rmse: 0.29121 |  0:02:28s

Early stopping occured at epoch 91 with best_epoch = 61 and best_val_1_rmse = 0.2654
Best weights from best epoch are automatically used!
ended training at: 03:39:13
Feature importance:
[('Area', 0.3030993430741105), ('Baths', 0.030802927029806013), ('Beds', 0.22835747720147573), ('Latitude', 0.09842971429724894), ('Longitude', 0.29244925450001213), ('Month', 0.02707918468742202), ('Year', 0.019782099209924613)]
Mean squared error is of 9324934698.443476
Mean absolute error:64579.05454486894
MAPE:0.1946674680824027
R2 score:0.5974143605658006
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:39:13
epoch 0  | loss: 46.55288| val_0_rmse: 2.98073 | val_1_rmse: 2.97302 |  0:00:01s
epoch 1  | loss: 0.61267 | val_0_rmse: 0.89369 | val_1_rmse: 0.88906 |  0:00:02s
epoch 2  | loss: 0.21243 | val_0_rmse: 0.44894 | val_1_rmse: 0.45666 |  0:00:04s
epoch 3  | loss: 0.17088 | val_0_rmse: 0.37545 | val_1_rmse: 0.38207 |  0:00:05s
epoch 4  | loss: 0.15551 | val_0_rmse: 0.34869 | val_1_rmse: 0.35184 |  0:00:07s
epoch 5  | loss: 0.13938 | val_0_rmse: 0.39289 | val_1_rmse: 0.39929 |  0:00:08s
epoch 6  | loss: 0.14554 | val_0_rmse: 0.39676 | val_1_rmse: 0.40053 |  0:00:10s
epoch 7  | loss: 0.17271 | val_0_rmse: 0.35485 | val_1_rmse: 0.36122 |  0:00:11s
epoch 8  | loss: 0.13929 | val_0_rmse: 0.33456 | val_1_rmse: 0.347   |  0:00:13s
epoch 9  | loss: 0.12428 | val_0_rmse: 0.34057 | val_1_rmse: 0.35197 |  0:00:14s
epoch 10 | loss: 0.12436 | val_0_rmse: 0.32615 | val_1_rmse: 0.33599 |  0:00:15s
epoch 11 | loss: 0.11931 | val_0_rmse: 0.33676 | val_1_rmse: 0.34078 |  0:00:17s
epoch 12 | loss: 0.12043 | val_0_rmse: 0.31434 | val_1_rmse: 0.32223 |  0:00:18s
epoch 13 | loss: 0.11316 | val_0_rmse: 0.32057 | val_1_rmse: 0.33206 |  0:00:20s
epoch 14 | loss: 0.12423 | val_0_rmse: 0.35292 | val_1_rmse: 0.36049 |  0:00:21s
epoch 15 | loss: 0.14338 | val_0_rmse: 0.33527 | val_1_rmse: 0.34188 |  0:00:23s
epoch 16 | loss: 0.18649 | val_0_rmse: 0.31367 | val_1_rmse: 0.31961 |  0:00:24s
epoch 17 | loss: 0.16036 | val_0_rmse: 0.33113 | val_1_rmse: 0.34126 |  0:00:26s
epoch 18 | loss: 0.13037 | val_0_rmse: 0.29927 | val_1_rmse: 0.3073  |  0:00:27s
epoch 19 | loss: 0.16604 | val_0_rmse: 0.30597 | val_1_rmse: 0.3156  |  0:00:28s
epoch 20 | loss: 0.1101  | val_0_rmse: 0.34977 | val_1_rmse: 0.35829 |  0:00:30s
epoch 21 | loss: 0.1035  | val_0_rmse: 0.29775 | val_1_rmse: 0.30805 |  0:00:31s
epoch 22 | loss: 0.10905 | val_0_rmse: 0.32078 | val_1_rmse: 0.32883 |  0:00:33s
epoch 23 | loss: 0.10469 | val_0_rmse: 0.33441 | val_1_rmse: 0.34041 |  0:00:34s
epoch 24 | loss: 0.12699 | val_0_rmse: 0.29111 | val_1_rmse: 0.30224 |  0:00:36s
epoch 25 | loss: 0.09426 | val_0_rmse: 0.27941 | val_1_rmse: 0.29126 |  0:00:37s
epoch 26 | loss: 0.09372 | val_0_rmse: 0.27668 | val_1_rmse: 0.28917 |  0:00:38s
epoch 27 | loss: 0.09788 | val_0_rmse: 0.2813  | val_1_rmse: 0.29176 |  0:00:40s
epoch 28 | loss: 0.10047 | val_0_rmse: 0.28072 | val_1_rmse: 0.29398 |  0:00:41s
epoch 29 | loss: 0.11414 | val_0_rmse: 0.36063 | val_1_rmse: 0.36799 |  0:00:43s
epoch 30 | loss: 0.11614 | val_0_rmse: 0.29028 | val_1_rmse: 0.30312 |  0:00:45s
epoch 31 | loss: 0.11578 | val_0_rmse: 0.28066 | val_1_rmse: 0.29297 |  0:00:46s
epoch 32 | loss: 0.11429 | val_0_rmse: 0.34168 | val_1_rmse: 0.34777 |  0:00:48s
epoch 33 | loss: 0.11313 | val_0_rmse: 0.2966  | val_1_rmse: 0.30618 |  0:00:49s
epoch 34 | loss: 0.11336 | val_0_rmse: 0.27617 | val_1_rmse: 0.28831 |  0:00:51s
epoch 35 | loss: 0.11158 | val_0_rmse: 0.37148 | val_1_rmse: 0.3764  |  0:00:52s
epoch 36 | loss: 0.11462 | val_0_rmse: 0.3057  | val_1_rmse: 0.31667 |  0:00:53s
epoch 37 | loss: 0.11412 | val_0_rmse: 0.31383 | val_1_rmse: 0.32423 |  0:00:55s
epoch 38 | loss: 0.11029 | val_0_rmse: 0.32873 | val_1_rmse: 0.33662 |  0:00:57s
epoch 39 | loss: 0.11241 | val_0_rmse: 0.28624 | val_1_rmse: 0.29597 |  0:00:58s
epoch 40 | loss: 0.10918 | val_0_rmse: 0.27248 | val_1_rmse: 0.28265 |  0:01:00s
epoch 41 | loss: 0.10911 | val_0_rmse: 0.35834 | val_1_rmse: 0.36363 |  0:01:01s
epoch 42 | loss: 0.11021 | val_0_rmse: 0.29048 | val_1_rmse: 0.30357 |  0:01:03s
epoch 43 | loss: 0.10687 | val_0_rmse: 0.27429 | val_1_rmse: 0.28527 |  0:01:04s
epoch 44 | loss: 0.10508 | val_0_rmse: 0.33351 | val_1_rmse: 0.34111 |  0:01:05s
epoch 45 | loss: 0.10684 | val_0_rmse: 0.2771  | val_1_rmse: 0.29039 |  0:01:07s
epoch 46 | loss: 0.10825 | val_0_rmse: 0.28297 | val_1_rmse: 0.29612 |  0:01:08s
epoch 47 | loss: 0.1062  | val_0_rmse: 0.34031 | val_1_rmse: 0.34772 |  0:01:11s
epoch 48 | loss: 0.11231 | val_0_rmse: 0.28786 | val_1_rmse: 0.29992 |  0:01:12s
epoch 49 | loss: 0.10637 | val_0_rmse: 0.29373 | val_1_rmse: 0.30619 |  0:01:14s
epoch 50 | loss: 0.10624 | val_0_rmse: 0.31354 | val_1_rmse: 0.32034 |  0:01:15s
epoch 51 | loss: 0.10823 | val_0_rmse: 0.28408 | val_1_rmse: 0.29551 |  0:01:17s
epoch 52 | loss: 0.10348 | val_0_rmse: 0.26326 | val_1_rmse: 0.27476 |  0:01:18s
epoch 53 | loss: 0.10442 | val_0_rmse: 0.32539 | val_1_rmse: 0.33386 |  0:01:19s
epoch 54 | loss: 0.1043  | val_0_rmse: 0.27037 | val_1_rmse: 0.2836  |  0:01:21s
epoch 55 | loss: 0.10434 | val_0_rmse: 0.27545 | val_1_rmse: 0.28717 |  0:01:23s
epoch 56 | loss: 0.09874 | val_0_rmse: 0.3194  | val_1_rmse: 0.32488 |  0:01:25s
epoch 57 | loss: 0.10114 | val_0_rmse: 0.27591 | val_1_rmse: 0.28585 |  0:01:27s
epoch 58 | loss: 0.10339 | val_0_rmse: 0.26965 | val_1_rmse: 0.2797  |  0:01:28s
epoch 59 | loss: 0.10254 | val_0_rmse: 0.31762 | val_1_rmse: 0.32354 |  0:01:30s
epoch 60 | loss: 0.10529 | val_0_rmse: 0.2667  | val_1_rmse: 0.27968 |  0:01:32s
epoch 61 | loss: 0.10165 | val_0_rmse: 0.26615 | val_1_rmse: 0.27743 |  0:01:33s
epoch 62 | loss: 0.09935 | val_0_rmse: 0.32015 | val_1_rmse: 0.3269  |  0:01:35s
epoch 63 | loss: 0.10226 | val_0_rmse: 0.2869  | val_1_rmse: 0.29733 |  0:01:36s
epoch 64 | loss: 0.09972 | val_0_rmse: 0.2731  | val_1_rmse: 0.28403 |  0:01:37s
epoch 65 | loss: 0.10116 | val_0_rmse: 0.33422 | val_1_rmse: 0.33984 |  0:01:39s
epoch 66 | loss: 0.10025 | val_0_rmse: 0.27474 | val_1_rmse: 0.28518 |  0:01:41s
epoch 67 | loss: 0.1004  | val_0_rmse: 0.27389 | val_1_rmse: 0.28378 |  0:01:42s
epoch 68 | loss: 0.1002  | val_0_rmse: 0.30603 | val_1_rmse: 0.3133  |  0:01:44s
epoch 69 | loss: 0.09987 | val_0_rmse: 0.27847 | val_1_rmse: 0.28992 |  0:01:45s
epoch 70 | loss: 0.10029 | val_0_rmse: 0.27208 | val_1_rmse: 0.28207 |  0:01:47s
epoch 71 | loss: 0.09887 | val_0_rmse: 0.32198 | val_1_rmse: 0.32686 |  0:01:49s
epoch 72 | loss: 0.10167 | val_0_rmse: 0.26375 | val_1_rmse: 0.27581 |  0:01:50s
epoch 73 | loss: 0.10109 | val_0_rmse: 0.25907 | val_1_rmse: 0.26958 |  0:01:51s
epoch 74 | loss: 0.10113 | val_0_rmse: 0.30444 | val_1_rmse: 0.30962 |  0:01:53s
epoch 75 | loss: 0.10069 | val_0_rmse: 0.28448 | val_1_rmse: 0.29502 |  0:01:54s
epoch 76 | loss: 0.09895 | val_0_rmse: 0.2614  | val_1_rmse: 0.27159 |  0:01:56s
epoch 77 | loss: 0.09378 | val_0_rmse: 0.32935 | val_1_rmse: 0.33397 |  0:01:57s
epoch 78 | loss: 0.09951 | val_0_rmse: 0.2559  | val_1_rmse: 0.26637 |  0:01:59s
epoch 79 | loss: 0.09604 | val_0_rmse: 0.25345 | val_1_rmse: 0.26045 |  0:02:00s
epoch 80 | loss: 0.09545 | val_0_rmse: 0.32545 | val_1_rmse: 0.3304  |  0:02:02s
epoch 81 | loss: 0.09803 | val_0_rmse: 0.27795 | val_1_rmse: 0.28768 |  0:02:03s
epoch 82 | loss: 0.09812 | val_0_rmse: 0.2651  | val_1_rmse: 0.27535 |  0:02:04s
epoch 83 | loss: 0.09638 | val_0_rmse: 0.32295 | val_1_rmse: 0.3269  |  0:02:06s
epoch 84 | loss: 0.09894 | val_0_rmse: 0.26565 | val_1_rmse: 0.27569 |  0:02:07s
epoch 85 | loss: 0.09436 | val_0_rmse: 0.26623 | val_1_rmse: 0.27571 |  0:02:09s
epoch 86 | loss: 0.09518 | val_0_rmse: 0.31882 | val_1_rmse: 0.32269 |  0:02:10s
epoch 87 | loss: 0.0955  | val_0_rmse: 0.27059 | val_1_rmse: 0.28088 |  0:02:12s
epoch 88 | loss: 0.09526 | val_0_rmse: 0.26087 | val_1_rmse: 0.27151 |  0:02:13s
epoch 89 | loss: 0.09212 | val_0_rmse: 0.30053 | val_1_rmse: 0.30558 |  0:02:15s
epoch 90 | loss: 0.09419 | val_0_rmse: 0.2642  | val_1_rmse: 0.27608 |  0:02:16s
epoch 91 | loss: 0.09518 | val_0_rmse: 0.25965 | val_1_rmse: 0.27042 |  0:02:17s
epoch 92 | loss: 0.09582 | val_0_rmse: 0.2973  | val_1_rmse: 0.30486 |  0:02:19s
epoch 93 | loss: 0.09798 | val_0_rmse: 0.30839 | val_1_rmse: 0.31462 |  0:02:20s
epoch 94 | loss: 0.09543 | val_0_rmse: 0.25558 | val_1_rmse: 0.26633 |  0:02:22s
epoch 95 | loss: 0.09344 | val_0_rmse: 0.3146  | val_1_rmse: 0.32161 |  0:02:23s
epoch 96 | loss: 0.09591 | val_0_rmse: 0.2747  | val_1_rmse: 0.28197 |  0:02:25s
epoch 97 | loss: 0.09341 | val_0_rmse: 0.24297 | val_1_rmse: 0.25367 |  0:02:26s
epoch 98 | loss: 0.09251 | val_0_rmse: 0.37507 | val_1_rmse: 0.37884 |  0:02:27s
epoch 99 | loss: 0.09819 | val_0_rmse: 0.25987 | val_1_rmse: 0.26754 |  0:02:29s
epoch 100| loss: 0.0938  | val_0_rmse: 0.25447 | val_1_rmse: 0.26486 |  0:02:30s
epoch 101| loss: 0.09105 | val_0_rmse: 0.32563 | val_1_rmse: 0.33001 |  0:02:32s
epoch 102| loss: 0.09309 | val_0_rmse: 0.27588 | val_1_rmse: 0.28303 |  0:02:33s
epoch 103| loss: 0.09444 | val_0_rmse: 0.25371 | val_1_rmse: 0.2635  |  0:02:35s
epoch 104| loss: 0.08983 | val_0_rmse: 0.31    | val_1_rmse: 0.31743 |  0:02:36s
epoch 105| loss: 0.09184 | val_0_rmse: 0.2764  | val_1_rmse: 0.28888 |  0:02:37s
epoch 106| loss: 0.09225 | val_0_rmse: 0.25784 | val_1_rmse: 0.26871 |  0:02:39s
epoch 107| loss: 0.09305 | val_0_rmse: 0.33256 | val_1_rmse: 0.34055 |  0:02:40s
epoch 108| loss: 0.09444 | val_0_rmse: 0.26017 | val_1_rmse: 0.27107 |  0:02:42s
epoch 109| loss: 0.09289 | val_0_rmse: 0.27275 | val_1_rmse: 0.28338 |  0:02:44s
epoch 110| loss: 0.08868 | val_0_rmse: 0.34075 | val_1_rmse: 0.34872 |  0:02:45s
epoch 111| loss: 0.09661 | val_0_rmse: 0.26636 | val_1_rmse: 0.27669 |  0:02:47s
epoch 112| loss: 0.09322 | val_0_rmse: 0.25207 | val_1_rmse: 0.26316 |  0:02:48s
epoch 113| loss: 0.09089 | val_0_rmse: 0.31908 | val_1_rmse: 0.32774 |  0:02:50s
epoch 114| loss: 0.0906  | val_0_rmse: 0.27762 | val_1_rmse: 0.28819 |  0:02:51s
epoch 115| loss: 0.08991 | val_0_rmse: 0.24554 | val_1_rmse: 0.25889 |  0:02:53s
epoch 116| loss: 0.09105 | val_0_rmse: 0.34496 | val_1_rmse: 0.35171 |  0:02:54s
epoch 117| loss: 0.09246 | val_0_rmse: 0.27205 | val_1_rmse: 0.2828  |  0:02:56s
epoch 118| loss: 0.09003 | val_0_rmse: 0.24994 | val_1_rmse: 0.26174 |  0:02:58s
epoch 119| loss: 0.08783 | val_0_rmse: 0.30793 | val_1_rmse: 0.31588 |  0:02:59s
epoch 120| loss: 0.09038 | val_0_rmse: 0.25088 | val_1_rmse: 0.26172 |  0:03:01s
epoch 121| loss: 0.09005 | val_0_rmse: 0.26264 | val_1_rmse: 0.2746  |  0:03:03s
epoch 122| loss: 0.08794 | val_0_rmse: 0.30797 | val_1_rmse: 0.31506 |  0:03:05s
epoch 123| loss: 0.09238 | val_0_rmse: 0.26839 | val_1_rmse: 0.27928 |  0:03:06s
epoch 124| loss: 0.0919  | val_0_rmse: 0.24913 | val_1_rmse: 0.25961 |  0:03:08s
epoch 125| loss: 0.08795 | val_0_rmse: 0.33533 | val_1_rmse: 0.34247 |  0:03:10s
epoch 126| loss: 0.09104 | val_0_rmse: 0.26417 | val_1_rmse: 0.27626 |  0:03:11s
epoch 127| loss: 0.08906 | val_0_rmse: 0.24177 | val_1_rmse: 0.25554 |  0:03:13s

Early stopping occured at epoch 127 with best_epoch = 97 and best_val_1_rmse = 0.25367
Best weights from best epoch are automatically used!
ended training at: 03:42:27
Feature importance:
[('Area', 0.10048954317902019), ('Baths', 0.23621623091076413), ('Beds', 0.042806505554241474), ('Latitude', 0.09335977057875686), ('Longitude', 0.3010481559744532), ('Month', 0.15778086993700513), ('Year', 0.06829892386575902)]
Mean squared error is of 6789513396.252318
Mean absolute error:58019.82274826022
MAPE:0.18894037054840307
R2 score:0.6927261021793985
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:42:27
epoch 0  | loss: 47.72906| val_0_rmse: 2.15239 | val_1_rmse: 2.10862 |  0:00:01s
epoch 1  | loss: 0.87542 | val_0_rmse: 0.82469 | val_1_rmse: 0.8293  |  0:00:03s
epoch 2  | loss: 0.37049 | val_0_rmse: 0.65011 | val_1_rmse: 0.65361 |  0:00:05s
epoch 3  | loss: 0.20564 | val_0_rmse: 0.40706 | val_1_rmse: 0.41896 |  0:00:07s
epoch 4  | loss: 0.16293 | val_0_rmse: 0.35585 | val_1_rmse: 0.36336 |  0:00:09s
epoch 5  | loss: 0.14268 | val_0_rmse: 0.33844 | val_1_rmse: 0.34946 |  0:00:10s
epoch 6  | loss: 0.17278 | val_0_rmse: 0.38509 | val_1_rmse: 0.39092 |  0:00:12s
epoch 7  | loss: 0.16429 | val_0_rmse: 0.35073 | val_1_rmse: 0.35901 |  0:00:13s
epoch 8  | loss: 0.15119 | val_0_rmse: 0.38188 | val_1_rmse: 0.38975 |  0:00:15s
epoch 9  | loss: 0.15097 | val_0_rmse: 0.33391 | val_1_rmse: 0.34074 |  0:00:17s
epoch 10 | loss: 0.1468  | val_0_rmse: 0.34439 | val_1_rmse: 0.34842 |  0:00:18s
epoch 11 | loss: 0.14443 | val_0_rmse: 0.36667 | val_1_rmse: 0.37037 |  0:00:20s
epoch 12 | loss: 0.14619 | val_0_rmse: 0.32135 | val_1_rmse: 0.32427 |  0:00:22s
epoch 13 | loss: 0.13809 | val_0_rmse: 0.31619 | val_1_rmse: 0.31654 |  0:00:23s
epoch 14 | loss: 0.13725 | val_0_rmse: 0.39452 | val_1_rmse: 0.40218 |  0:00:24s
epoch 15 | loss: 0.13582 | val_0_rmse: 0.32957 | val_1_rmse: 0.33067 |  0:00:26s
epoch 16 | loss: 0.13109 | val_0_rmse: 0.32883 | val_1_rmse: 0.33595 |  0:00:28s
epoch 17 | loss: 0.13164 | val_0_rmse: 0.32994 | val_1_rmse: 0.33987 |  0:00:30s
epoch 18 | loss: 0.13244 | val_0_rmse: 0.34014 | val_1_rmse: 0.3415  |  0:00:31s
epoch 19 | loss: 0.13593 | val_0_rmse: 0.33727 | val_1_rmse: 0.34027 |  0:00:33s
epoch 20 | loss: 0.13047 | val_0_rmse: 0.35164 | val_1_rmse: 0.36002 |  0:00:34s
epoch 21 | loss: 0.13295 | val_0_rmse: 0.33665 | val_1_rmse: 0.33974 |  0:00:36s
epoch 22 | loss: 0.12972 | val_0_rmse: 0.32376 | val_1_rmse: 0.32866 |  0:00:37s
epoch 23 | loss: 0.12669 | val_0_rmse: 0.34214 | val_1_rmse: 0.35084 |  0:00:39s
epoch 24 | loss: 0.14538 | val_0_rmse: 0.42095 | val_1_rmse: 0.42349 |  0:00:41s
epoch 25 | loss: 0.13879 | val_0_rmse: 0.35239 | val_1_rmse: 0.35438 |  0:00:42s
epoch 26 | loss: 0.10876 | val_0_rmse: 0.30445 | val_1_rmse: 0.31066 |  0:00:44s
epoch 27 | loss: 0.17222 | val_0_rmse: 0.31478 | val_1_rmse: 0.32068 |  0:00:45s
epoch 28 | loss: 0.11601 | val_0_rmse: 0.35712 | val_1_rmse: 0.35884 |  0:00:47s
epoch 29 | loss: 0.12645 | val_0_rmse: 0.31021 | val_1_rmse: 0.31277 |  0:00:49s
epoch 30 | loss: 0.16254 | val_0_rmse: 0.37088 | val_1_rmse: 0.37133 |  0:00:50s
epoch 31 | loss: 0.19436 | val_0_rmse: 0.31103 | val_1_rmse: 0.31804 |  0:00:52s
epoch 32 | loss: 0.18293 | val_0_rmse: 0.36655 | val_1_rmse: 0.37029 |  0:00:53s
epoch 33 | loss: 0.19909 | val_0_rmse: 0.32822 | val_1_rmse: 0.33926 |  0:00:55s
epoch 34 | loss: 0.16877 | val_0_rmse: 0.42857 | val_1_rmse: 0.43317 |  0:00:56s
epoch 35 | loss: 0.19821 | val_0_rmse: 0.43987 | val_1_rmse: 0.43872 |  0:00:58s
epoch 36 | loss: 0.14159 | val_0_rmse: 0.33567 | val_1_rmse: 0.33824 |  0:01:00s
epoch 37 | loss: 0.12325 | val_0_rmse: 0.37098 | val_1_rmse: 0.37702 |  0:01:01s
epoch 38 | loss: 0.12319 | val_0_rmse: 0.31237 | val_1_rmse: 0.31632 |  0:01:03s
epoch 39 | loss: 0.11793 | val_0_rmse: 0.30505 | val_1_rmse: 0.3102  |  0:01:04s
epoch 40 | loss: 0.11751 | val_0_rmse: 0.36465 | val_1_rmse: 0.37222 |  0:01:06s
epoch 41 | loss: 0.11964 | val_0_rmse: 0.32444 | val_1_rmse: 0.33004 |  0:01:07s
epoch 42 | loss: 0.11501 | val_0_rmse: 0.30739 | val_1_rmse: 0.31336 |  0:01:09s
epoch 43 | loss: 0.11744 | val_0_rmse: 0.31528 | val_1_rmse: 0.32385 |  0:01:10s
epoch 44 | loss: 0.12312 | val_0_rmse: 0.33055 | val_1_rmse: 0.33478 |  0:01:12s
epoch 45 | loss: 0.12789 | val_0_rmse: 0.30646 | val_1_rmse: 0.3081  |  0:01:14s
epoch 46 | loss: 0.11446 | val_0_rmse: 0.35089 | val_1_rmse: 0.3582  |  0:01:15s
epoch 47 | loss: 0.1276  | val_0_rmse: 0.32958 | val_1_rmse: 0.33274 |  0:01:17s
epoch 48 | loss: 0.12407 | val_0_rmse: 0.31364 | val_1_rmse: 0.31558 |  0:01:18s
epoch 49 | loss: 0.12369 | val_0_rmse: 0.37544 | val_1_rmse: 0.38111 |  0:01:20s
epoch 50 | loss: 0.13169 | val_0_rmse: 0.31508 | val_1_rmse: 0.32238 |  0:01:21s
epoch 51 | loss: 0.12348 | val_0_rmse: 0.31129 | val_1_rmse: 0.31622 |  0:01:23s
epoch 52 | loss: 0.11846 | val_0_rmse: 0.33326 | val_1_rmse: 0.34017 |  0:01:25s
epoch 53 | loss: 0.12021 | val_0_rmse: 0.33271 | val_1_rmse: 0.3354  |  0:01:26s
epoch 54 | loss: 0.12101 | val_0_rmse: 0.31961 | val_1_rmse: 0.32334 |  0:01:28s
epoch 55 | loss: 0.12138 | val_0_rmse: 0.32063 | val_1_rmse: 0.32741 |  0:01:29s
epoch 56 | loss: 0.12077 | val_0_rmse: 0.32037 | val_1_rmse: 0.32418 |  0:01:31s
epoch 57 | loss: 0.11879 | val_0_rmse: 0.31404 | val_1_rmse: 0.32032 |  0:01:32s
epoch 58 | loss: 0.12026 | val_0_rmse: 0.32797 | val_1_rmse: 0.33733 |  0:01:34s
epoch 59 | loss: 0.11879 | val_0_rmse: 0.32682 | val_1_rmse: 0.3319  |  0:01:35s
epoch 60 | loss: 0.11747 | val_0_rmse: 0.30976 | val_1_rmse: 0.31351 |  0:01:37s
epoch 61 | loss: 0.11658 | val_0_rmse: 0.3274  | val_1_rmse: 0.33299 |  0:01:39s
epoch 62 | loss: 0.11281 | val_0_rmse: 0.32665 | val_1_rmse: 0.32977 |  0:01:40s
epoch 63 | loss: 0.11832 | val_0_rmse: 0.29874 | val_1_rmse: 0.30327 |  0:01:42s
epoch 64 | loss: 0.11276 | val_0_rmse: 0.32644 | val_1_rmse: 0.33113 |  0:01:43s
epoch 65 | loss: 0.11401 | val_0_rmse: 0.31228 | val_1_rmse: 0.31548 |  0:01:45s
epoch 66 | loss: 0.11446 | val_0_rmse: 0.30138 | val_1_rmse: 0.30574 |  0:01:46s
epoch 67 | loss: 0.11262 | val_0_rmse: 0.32117 | val_1_rmse: 0.32694 |  0:01:48s
epoch 68 | loss: 0.11247 | val_0_rmse: 0.29985 | val_1_rmse: 0.30143 |  0:01:49s
epoch 69 | loss: 0.13858 | val_0_rmse: 0.33986 | val_1_rmse: 0.34423 |  0:01:51s
epoch 70 | loss: 0.13826 | val_0_rmse: 0.34613 | val_1_rmse: 0.35086 |  0:01:53s
epoch 71 | loss: 0.12303 | val_0_rmse: 0.30955 | val_1_rmse: 0.31247 |  0:01:54s
epoch 72 | loss: 0.11967 | val_0_rmse: 0.31222 | val_1_rmse: 0.31523 |  0:01:56s
epoch 73 | loss: 0.11507 | val_0_rmse: 0.33102 | val_1_rmse: 0.33819 |  0:01:57s
epoch 74 | loss: 0.11433 | val_0_rmse: 0.31541 | val_1_rmse: 0.31891 |  0:01:59s
epoch 75 | loss: 0.11554 | val_0_rmse: 0.31061 | val_1_rmse: 0.31455 |  0:02:00s
epoch 76 | loss: 0.11347 | val_0_rmse: 0.31383 | val_1_rmse: 0.32024 |  0:02:02s
epoch 77 | loss: 0.11476 | val_0_rmse: 0.32027 | val_1_rmse: 0.32249 |  0:02:04s
epoch 78 | loss: 0.11408 | val_0_rmse: 0.31513 | val_1_rmse: 0.31711 |  0:02:05s
epoch 79 | loss: 0.1102  | val_0_rmse: 0.34316 | val_1_rmse: 0.34702 |  0:02:07s
epoch 80 | loss: 0.11144 | val_0_rmse: 0.31784 | val_1_rmse: 0.32102 |  0:02:08s
epoch 81 | loss: 0.11503 | val_0_rmse: 0.31862 | val_1_rmse: 0.31983 |  0:02:10s
epoch 82 | loss: 0.11269 | val_0_rmse: 0.3223  | val_1_rmse: 0.32587 |  0:02:11s
epoch 83 | loss: 0.11136 | val_0_rmse: 0.31743 | val_1_rmse: 0.31902 |  0:02:13s
epoch 84 | loss: 0.10983 | val_0_rmse: 0.30095 | val_1_rmse: 0.30365 |  0:02:15s
epoch 85 | loss: 0.10667 | val_0_rmse: 0.31379 | val_1_rmse: 0.31898 |  0:02:16s
epoch 86 | loss: 0.11041 | val_0_rmse: 0.30844 | val_1_rmse: 0.31097 |  0:02:18s
epoch 87 | loss: 0.10652 | val_0_rmse: 0.29389 | val_1_rmse: 0.29832 |  0:02:19s
epoch 88 | loss: 0.10629 | val_0_rmse: 0.3068  | val_1_rmse: 0.31251 |  0:02:21s
epoch 89 | loss: 0.10919 | val_0_rmse: 0.31038 | val_1_rmse: 0.31369 |  0:02:22s
epoch 90 | loss: 0.10679 | val_0_rmse: 0.29284 | val_1_rmse: 0.29619 |  0:02:24s
epoch 91 | loss: 0.1063  | val_0_rmse: 0.30827 | val_1_rmse: 0.31454 |  0:02:26s
epoch 92 | loss: 0.10713 | val_0_rmse: 0.3292  | val_1_rmse: 0.3317  |  0:02:27s
epoch 93 | loss: 0.10882 | val_0_rmse: 0.30911 | val_1_rmse: 0.31205 |  0:02:29s
epoch 94 | loss: 0.10605 | val_0_rmse: 0.30356 | val_1_rmse: 0.3084  |  0:02:30s
epoch 95 | loss: 0.10626 | val_0_rmse: 0.3134  | val_1_rmse: 0.31569 |  0:02:32s
epoch 96 | loss: 0.10502 | val_0_rmse: 0.29175 | val_1_rmse: 0.29254 |  0:02:33s
epoch 97 | loss: 0.10277 | val_0_rmse: 0.31991 | val_1_rmse: 0.32552 |  0:02:35s
epoch 98 | loss: 0.10513 | val_0_rmse: 0.30086 | val_1_rmse: 0.30351 |  0:02:36s
epoch 99 | loss: 0.104   | val_0_rmse: 0.30801 | val_1_rmse: 0.31167 |  0:02:38s
epoch 100| loss: 0.10754 | val_0_rmse: 0.30569 | val_1_rmse: 0.31284 |  0:02:40s
epoch 101| loss: 0.10752 | val_0_rmse: 0.29919 | val_1_rmse: 0.30298 |  0:02:41s
epoch 102| loss: 0.10838 | val_0_rmse: 0.29885 | val_1_rmse: 0.30157 |  0:02:43s
epoch 103| loss: 0.10917 | val_0_rmse: 0.32615 | val_1_rmse: 0.33278 |  0:02:44s
epoch 104| loss: 0.11432 | val_0_rmse: 0.31381 | val_1_rmse: 0.31563 |  0:02:46s
epoch 105| loss: 0.11161 | val_0_rmse: 0.28728 | val_1_rmse: 0.29316 |  0:02:47s
epoch 106| loss: 0.10526 | val_0_rmse: 0.30739 | val_1_rmse: 0.31316 |  0:02:49s
epoch 107| loss: 0.104   | val_0_rmse: 0.30029 | val_1_rmse: 0.30221 |  0:02:50s
epoch 108| loss: 0.1025  | val_0_rmse: 0.29166 | val_1_rmse: 0.29338 |  0:02:52s
epoch 109| loss: 0.10101 | val_0_rmse: 0.32702 | val_1_rmse: 0.33407 |  0:02:54s
epoch 110| loss: 0.10185 | val_0_rmse: 0.30678 | val_1_rmse: 0.30958 |  0:02:55s
epoch 111| loss: 0.10503 | val_0_rmse: 0.28957 | val_1_rmse: 0.29111 |  0:02:57s
epoch 112| loss: 0.09978 | val_0_rmse: 0.30609 | val_1_rmse: 0.31289 |  0:02:58s
epoch 113| loss: 0.10331 | val_0_rmse: 0.29894 | val_1_rmse: 0.30176 |  0:03:00s
epoch 114| loss: 0.10702 | val_0_rmse: 0.29267 | val_1_rmse: 0.29769 |  0:03:01s
epoch 115| loss: 0.09888 | val_0_rmse: 0.29896 | val_1_rmse: 0.30793 |  0:03:03s
epoch 116| loss: 0.09997 | val_0_rmse: 0.31154 | val_1_rmse: 0.31244 |  0:03:04s
epoch 117| loss: 0.10158 | val_0_rmse: 0.26861 | val_1_rmse: 0.27325 |  0:03:06s
epoch 118| loss: 0.09659 | val_0_rmse: 0.30579 | val_1_rmse: 0.3132  |  0:03:08s
epoch 119| loss: 0.09907 | val_0_rmse: 0.29412 | val_1_rmse: 0.29752 |  0:03:09s
epoch 120| loss: 0.10003 | val_0_rmse: 0.29335 | val_1_rmse: 0.29712 |  0:03:11s
epoch 121| loss: 0.09781 | val_0_rmse: 0.31838 | val_1_rmse: 0.325   |  0:03:12s
epoch 122| loss: 0.10232 | val_0_rmse: 0.28287 | val_1_rmse: 0.28746 |  0:03:14s
epoch 123| loss: 0.09734 | val_0_rmse: 0.28122 | val_1_rmse: 0.28452 |  0:03:15s
epoch 124| loss: 0.09952 | val_0_rmse: 0.30699 | val_1_rmse: 0.31386 |  0:03:17s
epoch 125| loss: 0.0981  | val_0_rmse: 0.29689 | val_1_rmse: 0.29929 |  0:03:19s
epoch 126| loss: 0.10346 | val_0_rmse: 0.2998  | val_1_rmse: 0.30393 |  0:03:20s
epoch 127| loss: 0.09889 | val_0_rmse: 0.32478 | val_1_rmse: 0.33169 |  0:03:22s
epoch 128| loss: 0.09948 | val_0_rmse: 0.29669 | val_1_rmse: 0.30007 |  0:03:23s
epoch 129| loss: 0.09766 | val_0_rmse: 0.28411 | val_1_rmse: 0.28895 |  0:03:25s
epoch 130| loss: 0.09445 | val_0_rmse: 0.28401 | val_1_rmse: 0.29258 |  0:03:26s
epoch 131| loss: 0.10033 | val_0_rmse: 0.3022  | val_1_rmse: 0.30651 |  0:03:28s
epoch 132| loss: 0.10063 | val_0_rmse: 0.27163 | val_1_rmse: 0.27612 |  0:03:30s
epoch 133| loss: 0.0997  | val_0_rmse: 0.32064 | val_1_rmse: 0.3274  |  0:03:31s
epoch 134| loss: 0.09853 | val_0_rmse: 0.29024 | val_1_rmse: 0.29355 |  0:03:33s
epoch 135| loss: 0.09902 | val_0_rmse: 0.28097 | val_1_rmse: 0.28623 |  0:03:34s
epoch 136| loss: 0.09386 | val_0_rmse: 0.30372 | val_1_rmse: 0.31178 |  0:03:36s
epoch 137| loss: 0.09557 | val_0_rmse: 0.29522 | val_1_rmse: 0.2978  |  0:03:37s
epoch 138| loss: 0.10088 | val_0_rmse: 0.29986 | val_1_rmse: 0.30395 |  0:03:39s
epoch 139| loss: 0.09782 | val_0_rmse: 0.28267 | val_1_rmse: 0.29015 |  0:03:41s
epoch 140| loss: 0.09767 | val_0_rmse: 0.28647 | val_1_rmse: 0.28936 |  0:03:42s
epoch 141| loss: 0.09312 | val_0_rmse: 0.26823 | val_1_rmse: 0.2742  |  0:03:44s
epoch 142| loss: 0.09378 | val_0_rmse: 0.30099 | val_1_rmse: 0.31005 |  0:03:45s
epoch 143| loss: 0.09824 | val_0_rmse: 0.30025 | val_1_rmse: 0.30354 |  0:03:47s
epoch 144| loss: 0.10031 | val_0_rmse: 0.27913 | val_1_rmse: 0.28431 |  0:03:48s
epoch 145| loss: 0.09268 | val_0_rmse: 0.30105 | val_1_rmse: 0.30987 |  0:03:50s
epoch 146| loss: 0.09528 | val_0_rmse: 0.28157 | val_1_rmse: 0.28713 |  0:03:51s
epoch 147| loss: 0.09738 | val_0_rmse: 0.27463 | val_1_rmse: 0.27898 |  0:03:53s

Early stopping occured at epoch 147 with best_epoch = 117 and best_val_1_rmse = 0.27325
Best weights from best epoch are automatically used!
ended training at: 03:46:21
Feature importance:
[('Area', 0.35664050806930947), ('Baths', 0.03490157100238294), ('Beds', 0.011052033646406613), ('Latitude', 0.2122926671500181), ('Longitude', 0.35763822118862143), ('Month', 0.02747499894326143), ('Year', 0.0)]
Mean squared error is of 9101935598.833426
Mean absolute error:65775.5655128023
MAPE:0.18575458219961774
R2 score:0.5861078752430707
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:46:21
epoch 0  | loss: 10.45158| val_0_rmse: 0.53397 | val_1_rmse: 0.52936 |  0:00:05s
epoch 1  | loss: 0.27956 | val_0_rmse: 0.48422 | val_1_rmse: 0.47775 |  0:00:11s
epoch 2  | loss: 0.26369 | val_0_rmse: 0.4728  | val_1_rmse: 0.46533 |  0:00:17s
epoch 3  | loss: 0.23393 | val_0_rmse: 0.45899 | val_1_rmse: 0.45407 |  0:00:23s
epoch 4  | loss: 0.23226 | val_0_rmse: 0.42858 | val_1_rmse: 0.42323 |  0:00:29s
epoch 5  | loss: 0.23103 | val_0_rmse: 0.44097 | val_1_rmse: 0.43261 |  0:00:35s
epoch 6  | loss: 0.19383 | val_0_rmse: 0.41306 | val_1_rmse: 0.40718 |  0:00:41s
epoch 7  | loss: 0.2074  | val_0_rmse: 0.46522 | val_1_rmse: 0.45872 |  0:00:47s
epoch 8  | loss: 0.20067 | val_0_rmse: 0.41235 | val_1_rmse: 0.40532 |  0:00:53s
epoch 9  | loss: 0.194   | val_0_rmse: 0.40685 | val_1_rmse: 0.4005  |  0:00:59s
epoch 10 | loss: 0.18293 | val_0_rmse: 0.40789 | val_1_rmse: 0.39978 |  0:01:05s
epoch 11 | loss: 0.18748 | val_0_rmse: 0.53567 | val_1_rmse: 0.53009 |  0:01:11s
epoch 12 | loss: 0.18061 | val_0_rmse: 0.45629 | val_1_rmse: 0.45224 |  0:01:16s
epoch 13 | loss: 0.19416 | val_0_rmse: 0.4311  | val_1_rmse: 0.42177 |  0:01:22s
epoch 14 | loss: 0.19604 | val_0_rmse: 0.50611 | val_1_rmse: 0.50229 |  0:01:28s
epoch 15 | loss: 0.20717 | val_0_rmse: 0.44744 | val_1_rmse: 0.44236 |  0:01:34s
epoch 16 | loss: 0.26672 | val_0_rmse: 0.43061 | val_1_rmse: 0.42273 |  0:01:40s
epoch 17 | loss: 0.23096 | val_0_rmse: 0.44484 | val_1_rmse: 0.43999 |  0:01:46s
epoch 18 | loss: 0.18697 | val_0_rmse: 0.40692 | val_1_rmse: 0.40089 |  0:01:52s
epoch 19 | loss: 0.19315 | val_0_rmse: 0.46856 | val_1_rmse: 0.46548 |  0:01:57s
epoch 20 | loss: 0.19049 | val_0_rmse: 0.42124 | val_1_rmse: 0.41665 |  0:02:03s
epoch 21 | loss: 0.19604 | val_0_rmse: 0.4879  | val_1_rmse: 0.48386 |  0:02:08s
epoch 22 | loss: 0.19559 | val_0_rmse: 0.41162 | val_1_rmse: 0.4064  |  0:02:14s
epoch 23 | loss: 0.18705 | val_0_rmse: 0.41901 | val_1_rmse: 0.41295 |  0:02:20s
epoch 24 | loss: 0.20536 | val_0_rmse: 0.40745 | val_1_rmse: 0.40134 |  0:02:25s
epoch 25 | loss: 0.18015 | val_0_rmse: 0.39908 | val_1_rmse: 0.39227 |  0:02:31s
epoch 26 | loss: 0.19897 | val_0_rmse: 0.42713 | val_1_rmse: 0.42031 |  0:02:37s
epoch 27 | loss: 0.20282 | val_0_rmse: 0.42468 | val_1_rmse: 0.41942 |  0:02:44s
epoch 28 | loss: 0.21192 | val_0_rmse: 0.47409 | val_1_rmse: 0.46802 |  0:02:50s
epoch 29 | loss: 0.16425 | val_0_rmse: 0.40007 | val_1_rmse: 0.39263 |  0:02:56s
epoch 30 | loss: 0.20432 | val_0_rmse: 0.39432 | val_1_rmse: 0.38848 |  0:03:02s
epoch 31 | loss: 0.17456 | val_0_rmse: 0.4004  | val_1_rmse: 0.39319 |  0:03:08s
epoch 32 | loss: 0.1716  | val_0_rmse: 0.43464 | val_1_rmse: 0.42945 |  0:03:14s
epoch 33 | loss: 0.16841 | val_0_rmse: 0.43325 | val_1_rmse: 0.42825 |  0:03:20s
epoch 34 | loss: 0.1616  | val_0_rmse: 0.40756 | val_1_rmse: 0.40073 |  0:03:26s
epoch 35 | loss: 0.16849 | val_0_rmse: 0.38742 | val_1_rmse: 0.38019 |  0:03:32s
epoch 36 | loss: 0.16638 | val_0_rmse: 0.42332 | val_1_rmse: 0.41663 |  0:03:39s
epoch 37 | loss: 0.18298 | val_0_rmse: 0.38818 | val_1_rmse: 0.38326 |  0:03:45s
epoch 38 | loss: 0.18108 | val_0_rmse: 0.49863 | val_1_rmse: 0.49259 |  0:03:51s
epoch 39 | loss: 0.16503 | val_0_rmse: 0.38969 | val_1_rmse: 0.38413 |  0:03:56s
epoch 40 | loss: 0.16397 | val_0_rmse: 0.39266 | val_1_rmse: 0.38525 |  0:04:03s
epoch 41 | loss: 0.1661  | val_0_rmse: 0.38366 | val_1_rmse: 0.37723 |  0:04:09s
epoch 42 | loss: 0.16461 | val_0_rmse: 0.41654 | val_1_rmse: 0.41057 |  0:04:16s
epoch 43 | loss: 0.15793 | val_0_rmse: 0.3853  | val_1_rmse: 0.37965 |  0:04:22s
epoch 44 | loss: 0.16152 | val_0_rmse: 0.39777 | val_1_rmse: 0.39094 |  0:04:29s
epoch 45 | loss: 0.15823 | val_0_rmse: 0.44711 | val_1_rmse: 0.44179 |  0:04:35s
epoch 46 | loss: 0.21113 | val_0_rmse: 0.42255 | val_1_rmse: 0.41692 |  0:04:42s
epoch 47 | loss: 0.18115 | val_0_rmse: 0.41362 | val_1_rmse: 0.40866 |  0:04:48s
epoch 48 | loss: 0.19081 | val_0_rmse: 0.4673  | val_1_rmse: 0.4631  |  0:04:54s
epoch 49 | loss: 0.19822 | val_0_rmse: 0.43643 | val_1_rmse: 0.43192 |  0:05:01s
epoch 50 | loss: 0.17716 | val_0_rmse: 0.39391 | val_1_rmse: 0.38599 |  0:05:07s
epoch 51 | loss: 0.16179 | val_0_rmse: 0.39731 | val_1_rmse: 0.39235 |  0:05:13s
epoch 52 | loss: 0.15907 | val_0_rmse: 0.38031 | val_1_rmse: 0.37564 |  0:05:20s
epoch 53 | loss: 0.15948 | val_0_rmse: 0.4794  | val_1_rmse: 0.47599 |  0:05:26s
epoch 54 | loss: 0.15887 | val_0_rmse: 0.40078 | val_1_rmse: 0.39527 |  0:05:32s
epoch 55 | loss: 0.17052 | val_0_rmse: 0.39107 | val_1_rmse: 0.38515 |  0:05:39s
epoch 56 | loss: 0.16637 | val_0_rmse: 0.41631 | val_1_rmse: 0.41148 |  0:05:45s
epoch 57 | loss: 0.1644  | val_0_rmse: 0.38728 | val_1_rmse: 0.38099 |  0:05:52s
epoch 58 | loss: 0.1729  | val_0_rmse: 0.42298 | val_1_rmse: 0.41942 |  0:05:58s
epoch 59 | loss: 0.18831 | val_0_rmse: 0.41476 | val_1_rmse: 0.40905 |  0:06:04s
epoch 60 | loss: 0.15841 | val_0_rmse: 0.38166 | val_1_rmse: 0.37598 |  0:06:11s
epoch 61 | loss: 0.15701 | val_0_rmse: 0.39665 | val_1_rmse: 0.3905  |  0:06:17s
epoch 62 | loss: 0.16744 | val_0_rmse: 0.37917 | val_1_rmse: 0.37373 |  0:06:24s
epoch 63 | loss: 0.15719 | val_0_rmse: 0.39193 | val_1_rmse: 0.38768 |  0:06:30s
epoch 64 | loss: 0.15172 | val_0_rmse: 0.48203 | val_1_rmse: 0.47835 |  0:06:36s
epoch 65 | loss: 0.16226 | val_0_rmse: 0.38423 | val_1_rmse: 0.37936 |  0:06:43s
epoch 66 | loss: 0.16354 | val_0_rmse: 0.44959 | val_1_rmse: 0.44596 |  0:06:49s
epoch 67 | loss: 0.14969 | val_0_rmse: 0.3961  | val_1_rmse: 0.39006 |  0:06:55s
epoch 68 | loss: 0.15186 | val_0_rmse: 0.41617 | val_1_rmse: 0.41142 |  0:07:02s
epoch 69 | loss: 0.16747 | val_0_rmse: 0.51622 | val_1_rmse: 0.51382 |  0:07:08s
epoch 70 | loss: 0.17938 | val_0_rmse: 0.38365 | val_1_rmse: 0.37806 |  0:07:15s
epoch 71 | loss: 0.1712  | val_0_rmse: 0.47141 | val_1_rmse: 0.46552 |  0:07:21s
epoch 72 | loss: 0.15366 | val_0_rmse: 0.38284 | val_1_rmse: 0.37677 |  0:07:27s
epoch 73 | loss: 0.15301 | val_0_rmse: 0.42687 | val_1_rmse: 0.42295 |  0:07:34s
epoch 74 | loss: 0.1492  | val_0_rmse: 0.3821  | val_1_rmse: 0.37739 |  0:07:40s
epoch 75 | loss: 0.15578 | val_0_rmse: 0.38059 | val_1_rmse: 0.37508 |  0:07:47s
epoch 76 | loss: 0.15668 | val_0_rmse: 0.38232 | val_1_rmse: 0.3779  |  0:07:53s
epoch 77 | loss: 0.15071 | val_0_rmse: 0.38315 | val_1_rmse: 0.37772 |  0:07:59s
epoch 78 | loss: 0.14753 | val_0_rmse: 0.40859 | val_1_rmse: 0.40406 |  0:08:06s
epoch 79 | loss: 0.1483  | val_0_rmse: 0.40609 | val_1_rmse: 0.4015  |  0:08:12s
epoch 80 | loss: 0.15024 | val_0_rmse: 0.39823 | val_1_rmse: 0.3938  |  0:08:19s
epoch 81 | loss: 0.15585 | val_0_rmse: 0.38673 | val_1_rmse: 0.38043 |  0:08:25s
epoch 82 | loss: 0.16132 | val_0_rmse: 0.38246 | val_1_rmse: 0.37772 |  0:08:31s
epoch 83 | loss: 0.16112 | val_0_rmse: 0.43451 | val_1_rmse: 0.43142 |  0:08:38s
epoch 84 | loss: 0.17913 | val_0_rmse: 0.42178 | val_1_rmse: 0.4171  |  0:08:44s
epoch 85 | loss: 0.22821 | val_0_rmse: 0.52476 | val_1_rmse: 0.51908 |  0:08:51s
epoch 86 | loss: 0.21192 | val_0_rmse: 0.48622 | val_1_rmse: 0.48124 |  0:08:57s
epoch 87 | loss: 0.20684 | val_0_rmse: 0.47777 | val_1_rmse: 0.47251 |  0:09:04s
epoch 88 | loss: 0.22473 | val_0_rmse: 0.45714 | val_1_rmse: 0.45161 |  0:09:10s
epoch 89 | loss: 0.20723 | val_0_rmse: 0.43072 | val_1_rmse: 0.42351 |  0:09:16s
epoch 90 | loss: 0.19873 | val_0_rmse: 0.44254 | val_1_rmse: 0.43786 |  0:09:23s
epoch 91 | loss: 0.19545 | val_0_rmse: 0.4708  | val_1_rmse: 0.46373 |  0:09:29s
epoch 92 | loss: 0.21395 | val_0_rmse: 0.44085 | val_1_rmse: 0.43256 |  0:09:35s

Early stopping occured at epoch 92 with best_epoch = 62 and best_val_1_rmse = 0.37373
Best weights from best epoch are automatically used!
ended training at: 03:55:59
Feature importance:
[('Area', 0.0760239833549688), ('Baths', 0.13110162669756845), ('Beds', 0.11149128835804983), ('Latitude', 0.19625037963322142), ('Longitude', 0.26715184324417357), ('Month', 0.21798087871201793), ('Year', 0.0)]
Mean squared error is of 2467023584.1650066
Mean absolute error:35574.211284385376
MAPE:0.32611077165248026
R2 score:0.6390853254993152
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:56:00
epoch 0  | loss: 10.60413| val_0_rmse: 0.5212  | val_1_rmse: 0.52394 |  0:00:06s
epoch 1  | loss: 0.30772 | val_0_rmse: 0.56226 | val_1_rmse: 0.56022 |  0:00:12s
epoch 2  | loss: 0.23533 | val_0_rmse: 0.50806 | val_1_rmse: 0.51047 |  0:00:19s
epoch 3  | loss: 0.23988 | val_0_rmse: 0.47462 | val_1_rmse: 0.47516 |  0:00:25s
epoch 4  | loss: 0.23436 | val_0_rmse: 0.46693 | val_1_rmse: 0.46702 |  0:00:32s
epoch 5  | loss: 0.22963 | val_0_rmse: 0.45062 | val_1_rmse: 0.45191 |  0:00:38s
epoch 6  | loss: 0.20882 | val_0_rmse: 0.44529 | val_1_rmse: 0.44878 |  0:00:44s
epoch 7  | loss: 0.21779 | val_0_rmse: 0.47644 | val_1_rmse: 0.47733 |  0:00:51s
epoch 8  | loss: 0.21498 | val_0_rmse: 0.42174 | val_1_rmse: 0.4237  |  0:00:57s
epoch 9  | loss: 0.19955 | val_0_rmse: 0.41271 | val_1_rmse: 0.41438 |  0:01:04s
epoch 10 | loss: 0.2052  | val_0_rmse: 0.41091 | val_1_rmse: 0.41347 |  0:01:10s
epoch 11 | loss: 0.19689 | val_0_rmse: 0.42541 | val_1_rmse: 0.42648 |  0:01:16s
epoch 12 | loss: 0.19653 | val_0_rmse: 0.40668 | val_1_rmse: 0.40774 |  0:01:23s
epoch 13 | loss: 0.1929  | val_0_rmse: 0.43232 | val_1_rmse: 0.43312 |  0:01:30s
epoch 14 | loss: 0.1918  | val_0_rmse: 0.4338  | val_1_rmse: 0.4339  |  0:01:36s
epoch 15 | loss: 0.19261 | val_0_rmse: 0.42262 | val_1_rmse: 0.42378 |  0:01:43s
epoch 16 | loss: 0.19171 | val_0_rmse: 0.43284 | val_1_rmse: 0.43322 |  0:01:49s
epoch 17 | loss: 0.18146 | val_0_rmse: 0.398   | val_1_rmse: 0.39756 |  0:01:55s
epoch 18 | loss: 0.209   | val_0_rmse: 0.41463 | val_1_rmse: 0.41566 |  0:02:02s
epoch 19 | loss: 0.19642 | val_0_rmse: 0.39409 | val_1_rmse: 0.39464 |  0:02:08s
epoch 20 | loss: 0.18765 | val_0_rmse: 0.40131 | val_1_rmse: 0.40176 |  0:02:15s
epoch 21 | loss: 0.18734 | val_0_rmse: 0.44678 | val_1_rmse: 0.44728 |  0:02:21s
epoch 22 | loss: 0.17087 | val_0_rmse: 0.42085 | val_1_rmse: 0.42104 |  0:02:27s
epoch 23 | loss: 0.19453 | val_0_rmse: 0.39287 | val_1_rmse: 0.39325 |  0:02:34s
epoch 24 | loss: 0.16516 | val_0_rmse: 0.42628 | val_1_rmse: 0.42712 |  0:02:40s
epoch 25 | loss: 0.16564 | val_0_rmse: 0.39135 | val_1_rmse: 0.39136 |  0:02:47s
epoch 26 | loss: 0.16315 | val_0_rmse: 0.42065 | val_1_rmse: 0.42188 |  0:02:53s
epoch 27 | loss: 0.17364 | val_0_rmse: 0.43197 | val_1_rmse: 0.43239 |  0:02:59s
epoch 28 | loss: 0.17837 | val_0_rmse: 0.40695 | val_1_rmse: 0.40883 |  0:03:06s
epoch 29 | loss: 0.1745  | val_0_rmse: 0.40861 | val_1_rmse: 0.41026 |  0:03:12s
epoch 30 | loss: 0.16293 | val_0_rmse: 0.38436 | val_1_rmse: 0.38487 |  0:03:19s
epoch 31 | loss: 0.17082 | val_0_rmse: 0.39733 | val_1_rmse: 0.39668 |  0:03:25s
epoch 32 | loss: 0.17696 | val_0_rmse: 0.50994 | val_1_rmse: 0.51012 |  0:03:32s
epoch 33 | loss: 0.17425 | val_0_rmse: 0.39612 | val_1_rmse: 0.39711 |  0:03:38s
epoch 34 | loss: 0.17238 | val_0_rmse: 0.3996  | val_1_rmse: 0.40064 |  0:03:44s
epoch 35 | loss: 0.15834 | val_0_rmse: 0.38779 | val_1_rmse: 0.38774 |  0:03:51s
epoch 36 | loss: 0.16817 | val_0_rmse: 0.40057 | val_1_rmse: 0.40327 |  0:03:57s
epoch 37 | loss: 0.16077 | val_0_rmse: 0.40678 | val_1_rmse: 0.40736 |  0:04:04s
epoch 38 | loss: 0.17383 | val_0_rmse: 0.41254 | val_1_rmse: 0.41179 |  0:04:10s
epoch 39 | loss: 0.16777 | val_0_rmse: 0.39005 | val_1_rmse: 0.39125 |  0:04:16s
epoch 40 | loss: 0.17277 | val_0_rmse: 0.41458 | val_1_rmse: 0.41538 |  0:04:23s
epoch 41 | loss: 0.17652 | val_0_rmse: 0.68031 | val_1_rmse: 0.67931 |  0:04:29s
epoch 42 | loss: 0.16843 | val_0_rmse: 0.46801 | val_1_rmse: 0.46708 |  0:04:35s
epoch 43 | loss: 0.16979 | val_0_rmse: 0.38253 | val_1_rmse: 0.38466 |  0:04:42s
epoch 44 | loss: 0.16328 | val_0_rmse: 0.44015 | val_1_rmse: 0.44025 |  0:04:48s
epoch 45 | loss: 0.15456 | val_0_rmse: 0.41004 | val_1_rmse: 0.40973 |  0:04:54s
epoch 46 | loss: 0.16759 | val_0_rmse: 0.43093 | val_1_rmse: 0.4314  |  0:05:01s
epoch 47 | loss: 0.16588 | val_0_rmse: 0.43448 | val_1_rmse: 0.43326 |  0:05:07s
epoch 48 | loss: 0.16407 | val_0_rmse: 0.39996 | val_1_rmse: 0.40038 |  0:05:14s
epoch 49 | loss: 0.16094 | val_0_rmse: 0.39601 | val_1_rmse: 0.39608 |  0:05:20s
epoch 50 | loss: 0.16711 | val_0_rmse: 0.38377 | val_1_rmse: 0.38554 |  0:05:26s
epoch 51 | loss: 0.15868 | val_0_rmse: 0.39346 | val_1_rmse: 0.39353 |  0:05:33s
epoch 52 | loss: 0.16251 | val_0_rmse: 0.40397 | val_1_rmse: 0.40399 |  0:05:39s
epoch 53 | loss: 0.16448 | val_0_rmse: 0.40331 | val_1_rmse: 0.40516 |  0:05:46s
epoch 54 | loss: 0.16169 | val_0_rmse: 0.39148 | val_1_rmse: 0.39234 |  0:05:52s
epoch 55 | loss: 0.15224 | val_0_rmse: 0.39072 | val_1_rmse: 0.39244 |  0:05:58s
epoch 56 | loss: 0.15479 | val_0_rmse: 0.39578 | val_1_rmse: 0.39627 |  0:06:05s
epoch 57 | loss: 0.15035 | val_0_rmse: 0.38821 | val_1_rmse: 0.38914 |  0:06:11s
epoch 58 | loss: 0.16283 | val_0_rmse: 0.48859 | val_1_rmse: 0.48902 |  0:06:17s
epoch 59 | loss: 0.17164 | val_0_rmse: 0.3938  | val_1_rmse: 0.39555 |  0:06:24s
epoch 60 | loss: 0.16484 | val_0_rmse: 0.4045  | val_1_rmse: 0.40495 |  0:06:30s
epoch 61 | loss: 0.17654 | val_0_rmse: 0.45869 | val_1_rmse: 0.45982 |  0:06:36s
epoch 62 | loss: 0.15926 | val_0_rmse: 0.37968 | val_1_rmse: 0.3796  |  0:06:43s
epoch 63 | loss: 0.16584 | val_0_rmse: 0.39849 | val_1_rmse: 0.39816 |  0:06:49s
epoch 64 | loss: 0.16908 | val_0_rmse: 0.37859 | val_1_rmse: 0.38036 |  0:06:55s
epoch 65 | loss: 0.16082 | val_0_rmse: 0.42787 | val_1_rmse: 0.42746 |  0:07:02s
epoch 66 | loss: 0.21582 | val_0_rmse: 0.38454 | val_1_rmse: 0.38614 |  0:07:08s
epoch 67 | loss: 0.16054 | val_0_rmse: 0.42779 | val_1_rmse: 0.4291  |  0:07:15s
epoch 68 | loss: 0.16066 | val_0_rmse: 0.39443 | val_1_rmse: 0.39584 |  0:07:21s
epoch 69 | loss: 0.16588 | val_0_rmse: 0.37943 | val_1_rmse: 0.38248 |  0:07:28s
epoch 70 | loss: 0.15377 | val_0_rmse: 0.39682 | val_1_rmse: 0.39849 |  0:07:34s
epoch 71 | loss: 0.14915 | val_0_rmse: 0.48184 | val_1_rmse: 0.48219 |  0:07:40s
epoch 72 | loss: 0.14576 | val_0_rmse: 0.42415 | val_1_rmse: 0.42457 |  0:07:47s
epoch 73 | loss: 0.15929 | val_0_rmse: 0.40725 | val_1_rmse: 0.40902 |  0:07:53s
epoch 74 | loss: 0.16943 | val_0_rmse: 0.38487 | val_1_rmse: 0.38669 |  0:07:59s
epoch 75 | loss: 0.16168 | val_0_rmse: 0.41964 | val_1_rmse: 0.41995 |  0:08:06s
epoch 76 | loss: 0.19978 | val_0_rmse: 0.46167 | val_1_rmse: 0.46087 |  0:08:12s
epoch 77 | loss: 0.15852 | val_0_rmse: 0.39328 | val_1_rmse: 0.39418 |  0:08:19s
epoch 78 | loss: 0.15346 | val_0_rmse: 0.41995 | val_1_rmse: 0.41889 |  0:08:25s
epoch 79 | loss: 0.15861 | val_0_rmse: 0.43217 | val_1_rmse: 0.43242 |  0:08:31s
epoch 80 | loss: 0.16616 | val_0_rmse: 0.44753 | val_1_rmse: 0.44725 |  0:08:38s
epoch 81 | loss: 0.15465 | val_0_rmse: 0.45757 | val_1_rmse: 0.45748 |  0:08:44s
epoch 82 | loss: 0.15645 | val_0_rmse: 0.41069 | val_1_rmse: 0.40995 |  0:08:51s
epoch 83 | loss: 0.15639 | val_0_rmse: 0.41064 | val_1_rmse: 0.41084 |  0:08:57s
epoch 84 | loss: 0.14936 | val_0_rmse: 0.37891 | val_1_rmse: 0.38051 |  0:09:03s
epoch 85 | loss: 0.14914 | val_0_rmse: 0.52789 | val_1_rmse: 0.52673 |  0:09:10s
epoch 86 | loss: 0.15284 | val_0_rmse: 0.41659 | val_1_rmse: 0.41778 |  0:09:16s
epoch 87 | loss: 0.14852 | val_0_rmse: 0.39262 | val_1_rmse: 0.39389 |  0:09:22s
epoch 88 | loss: 0.14555 | val_0_rmse: 0.38366 | val_1_rmse: 0.38537 |  0:09:29s
epoch 89 | loss: 0.14781 | val_0_rmse: 0.3873  | val_1_rmse: 0.3878  |  0:09:35s
epoch 90 | loss: 0.15769 | val_0_rmse: 0.42105 | val_1_rmse: 0.4221  |  0:09:42s
epoch 91 | loss: 0.15136 | val_0_rmse: 0.38093 | val_1_rmse: 0.382   |  0:09:48s
epoch 92 | loss: 0.14392 | val_0_rmse: 0.42579 | val_1_rmse: 0.42643 |  0:09:54s

Early stopping occured at epoch 92 with best_epoch = 62 and best_val_1_rmse = 0.3796
Best weights from best epoch are automatically used!
ended training at: 04:05:57
Feature importance:
[('Area', 0.018996409627873766), ('Baths', 0.07964680436515918), ('Beds', 0.10013744140374552), ('Latitude', 0.3509512458544314), ('Longitude', 0.22747624821223042), ('Month', 0.004488979470259221), ('Year', 0.21830287106630053)]
Mean squared error is of 2435984209.849818
Mean absolute error:35319.18668606705
MAPE:0.3416969312521104
R2 score:0.6409422112342356
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:05:58
epoch 0  | loss: 10.07038| val_0_rmse: 0.52455 | val_1_rmse: 0.52164 |  0:00:06s
epoch 1  | loss: 0.31492 | val_0_rmse: 0.54873 | val_1_rmse: 0.5448  |  0:00:12s
epoch 2  | loss: 0.27495 | val_0_rmse: 0.52713 | val_1_rmse: 0.52597 |  0:00:19s
epoch 3  | loss: 0.2565  | val_0_rmse: 0.48316 | val_1_rmse: 0.47972 |  0:00:25s
epoch 4  | loss: 0.26259 | val_0_rmse: 0.47818 | val_1_rmse: 0.47478 |  0:00:31s
epoch 5  | loss: 0.25863 | val_0_rmse: 0.55471 | val_1_rmse: 0.55294 |  0:00:38s
epoch 6  | loss: 0.27854 | val_0_rmse: 0.48734 | val_1_rmse: 0.4882  |  0:00:44s
epoch 7  | loss: 0.24883 | val_0_rmse: 0.51285 | val_1_rmse: 0.5149  |  0:00:51s
epoch 8  | loss: 0.2486  | val_0_rmse: 0.48449 | val_1_rmse: 0.4843  |  0:00:57s
epoch 9  | loss: 0.24478 | val_0_rmse: 0.47944 | val_1_rmse: 0.47874 |  0:01:03s
epoch 10 | loss: 0.26532 | val_0_rmse: 0.46359 | val_1_rmse: 0.4664  |  0:01:10s
epoch 11 | loss: 0.23308 | val_0_rmse: 0.50027 | val_1_rmse: 0.50381 |  0:01:16s
epoch 12 | loss: 0.2248  | val_0_rmse: 0.44826 | val_1_rmse: 0.45123 |  0:01:23s
epoch 13 | loss: 0.21536 | val_0_rmse: 0.44333 | val_1_rmse: 0.44647 |  0:01:29s
epoch 14 | loss: 0.20803 | val_0_rmse: 0.45608 | val_1_rmse: 0.45629 |  0:01:35s
epoch 15 | loss: 0.22254 | val_0_rmse: 0.50019 | val_1_rmse: 0.50097 |  0:01:42s
epoch 16 | loss: 0.21578 | val_0_rmse: 0.45408 | val_1_rmse: 0.45463 |  0:01:48s
epoch 17 | loss: 0.21838 | val_0_rmse: 0.44771 | val_1_rmse: 0.44862 |  0:01:54s
epoch 18 | loss: 0.21706 | val_0_rmse: 0.45757 | val_1_rmse: 0.46067 |  0:02:01s
epoch 19 | loss: 0.20138 | val_0_rmse: 0.45031 | val_1_rmse: 0.45158 |  0:02:07s
epoch 20 | loss: 0.23228 | val_0_rmse: 0.48231 | val_1_rmse: 0.48339 |  0:02:14s
epoch 21 | loss: 0.19491 | val_0_rmse: 0.44367 | val_1_rmse: 0.44594 |  0:02:20s
epoch 22 | loss: 0.18764 | val_0_rmse: 0.5196  | val_1_rmse: 0.52192 |  0:02:26s
epoch 23 | loss: 0.18206 | val_0_rmse: 0.42038 | val_1_rmse: 0.42062 |  0:02:33s
epoch 24 | loss: 0.18068 | val_0_rmse: 0.43193 | val_1_rmse: 0.43147 |  0:02:39s
epoch 25 | loss: 0.17673 | val_0_rmse: 0.47294 | val_1_rmse: 0.47393 |  0:02:46s
epoch 26 | loss: 0.20479 | val_0_rmse: 0.51632 | val_1_rmse: 0.51461 |  0:02:52s
epoch 27 | loss: 0.19541 | val_0_rmse: 0.43429 | val_1_rmse: 0.43612 |  0:02:58s
epoch 28 | loss: 0.19325 | val_0_rmse: 0.41272 | val_1_rmse: 0.4148  |  0:03:05s
epoch 29 | loss: 0.18525 | val_0_rmse: 0.46576 | val_1_rmse: 0.4656  |  0:03:11s
epoch 30 | loss: 0.19039 | val_0_rmse: 0.41805 | val_1_rmse: 0.41689 |  0:03:17s
epoch 31 | loss: 0.18324 | val_0_rmse: 0.41798 | val_1_rmse: 0.41945 |  0:03:24s
epoch 32 | loss: 0.18484 | val_0_rmse: 0.40987 | val_1_rmse: 0.40906 |  0:03:30s
epoch 33 | loss: 0.1712  | val_0_rmse: 0.40733 | val_1_rmse: 0.40616 |  0:03:37s
epoch 34 | loss: 0.16834 | val_0_rmse: 0.44817 | val_1_rmse: 0.44803 |  0:03:43s
epoch 35 | loss: 0.17615 | val_0_rmse: 0.41139 | val_1_rmse: 0.41084 |  0:03:49s
epoch 36 | loss: 0.16809 | val_0_rmse: 0.40554 | val_1_rmse: 0.40687 |  0:03:56s
epoch 37 | loss: 0.16606 | val_0_rmse: 0.42062 | val_1_rmse: 0.42    |  0:04:02s
epoch 38 | loss: 0.17537 | val_0_rmse: 0.39582 | val_1_rmse: 0.3968  |  0:04:08s
epoch 39 | loss: 0.16927 | val_0_rmse: 0.4844  | val_1_rmse: 0.48474 |  0:04:15s
epoch 40 | loss: 0.16506 | val_0_rmse: 0.39833 | val_1_rmse: 0.39916 |  0:04:21s
epoch 41 | loss: 0.18062 | val_0_rmse: 0.41173 | val_1_rmse: 0.41078 |  0:04:27s
epoch 42 | loss: 0.17187 | val_0_rmse: 0.4323  | val_1_rmse: 0.43098 |  0:04:34s
epoch 43 | loss: 0.16626 | val_0_rmse: 0.39031 | val_1_rmse: 0.39133 |  0:04:40s
epoch 44 | loss: 0.16456 | val_0_rmse: 0.40196 | val_1_rmse: 0.40057 |  0:04:47s
epoch 45 | loss: 0.16859 | val_0_rmse: 0.43128 | val_1_rmse: 0.43224 |  0:04:53s
epoch 46 | loss: 0.16222 | val_0_rmse: 0.40476 | val_1_rmse: 0.40489 |  0:05:00s
epoch 47 | loss: 0.15936 | val_0_rmse: 0.44088 | val_1_rmse: 0.44172 |  0:05:06s
epoch 48 | loss: 0.17483 | val_0_rmse: 0.38949 | val_1_rmse: 0.39076 |  0:05:12s
epoch 49 | loss: 0.16094 | val_0_rmse: 0.41938 | val_1_rmse: 0.42026 |  0:05:19s
epoch 50 | loss: 0.17171 | val_0_rmse: 0.39434 | val_1_rmse: 0.39537 |  0:05:25s
epoch 51 | loss: 0.18183 | val_0_rmse: 0.39655 | val_1_rmse: 0.39689 |  0:05:31s
epoch 52 | loss: 0.16493 | val_0_rmse: 0.41515 | val_1_rmse: 0.41589 |  0:05:38s
epoch 53 | loss: 0.1661  | val_0_rmse: 0.39895 | val_1_rmse: 0.39788 |  0:05:44s
epoch 54 | loss: 0.16203 | val_0_rmse: 0.39197 | val_1_rmse: 0.39379 |  0:05:50s
epoch 55 | loss: 0.1622  | val_0_rmse: 0.43079 | val_1_rmse: 0.43038 |  0:05:57s
epoch 56 | loss: 0.15393 | val_0_rmse: 0.42931 | val_1_rmse: 0.43115 |  0:06:03s
epoch 57 | loss: 0.15857 | val_0_rmse: 0.39478 | val_1_rmse: 0.3952  |  0:06:10s
epoch 58 | loss: 0.16053 | val_0_rmse: 0.43564 | val_1_rmse: 0.4338  |  0:06:16s
epoch 59 | loss: 0.15848 | val_0_rmse: 0.38997 | val_1_rmse: 0.38967 |  0:06:22s
epoch 60 | loss: 0.15946 | val_0_rmse: 0.41986 | val_1_rmse: 0.42221 |  0:06:29s
epoch 61 | loss: 0.16355 | val_0_rmse: 0.37822 | val_1_rmse: 0.37983 |  0:06:35s
epoch 62 | loss: 0.15576 | val_0_rmse: 0.38435 | val_1_rmse: 0.38465 |  0:06:42s
epoch 63 | loss: 0.16996 | val_0_rmse: 0.41506 | val_1_rmse: 0.41609 |  0:06:48s
epoch 64 | loss: 0.15746 | val_0_rmse: 0.39192 | val_1_rmse: 0.3921  |  0:06:54s
epoch 65 | loss: 0.1568  | val_0_rmse: 0.37857 | val_1_rmse: 0.37972 |  0:07:01s
epoch 66 | loss: 0.1564  | val_0_rmse: 0.38965 | val_1_rmse: 0.3922  |  0:07:07s
epoch 67 | loss: 0.15571 | val_0_rmse: 0.39525 | val_1_rmse: 0.3953  |  0:07:14s
epoch 68 | loss: 0.16475 | val_0_rmse: 0.42078 | val_1_rmse: 0.42039 |  0:07:20s
epoch 69 | loss: 0.16678 | val_0_rmse: 0.4015  | val_1_rmse: 0.40186 |  0:07:26s
epoch 70 | loss: 0.15798 | val_0_rmse: 0.46869 | val_1_rmse: 0.46805 |  0:07:33s
epoch 71 | loss: 0.15853 | val_0_rmse: 0.40029 | val_1_rmse: 0.39937 |  0:07:39s
epoch 72 | loss: 0.1532  | val_0_rmse: 0.40392 | val_1_rmse: 0.40386 |  0:07:45s
epoch 73 | loss: 0.1673  | val_0_rmse: 0.40917 | val_1_rmse: 0.41152 |  0:07:52s
epoch 74 | loss: 0.16021 | val_0_rmse: 0.39701 | val_1_rmse: 0.39975 |  0:07:58s
epoch 75 | loss: 0.1546  | val_0_rmse: 0.38584 | val_1_rmse: 0.3863  |  0:08:04s
epoch 76 | loss: 0.15084 | val_0_rmse: 0.41654 | val_1_rmse: 0.41714 |  0:08:11s
epoch 77 | loss: 0.15261 | val_0_rmse: 0.50616 | val_1_rmse: 0.5072  |  0:08:17s
epoch 78 | loss: 0.16584 | val_0_rmse: 0.38374 | val_1_rmse: 0.38474 |  0:08:24s
epoch 79 | loss: 0.15059 | val_0_rmse: 0.38547 | val_1_rmse: 0.38708 |  0:08:30s
epoch 80 | loss: 0.14892 | val_0_rmse: 0.38399 | val_1_rmse: 0.38594 |  0:08:36s
epoch 81 | loss: 0.15116 | val_0_rmse: 0.42676 | val_1_rmse: 0.42463 |  0:08:43s
epoch 82 | loss: 0.14898 | val_0_rmse: 0.40049 | val_1_rmse: 0.40029 |  0:08:49s
epoch 83 | loss: 0.15687 | val_0_rmse: 0.38289 | val_1_rmse: 0.3828  |  0:08:56s
epoch 84 | loss: 0.16099 | val_0_rmse: 0.39847 | val_1_rmse: 0.39728 |  0:09:02s
epoch 85 | loss: 0.15956 | val_0_rmse: 0.38974 | val_1_rmse: 0.38955 |  0:09:08s
epoch 86 | loss: 0.14873 | val_0_rmse: 0.46417 | val_1_rmse: 0.46664 |  0:09:15s
epoch 87 | loss: 0.15661 | val_0_rmse: 0.39062 | val_1_rmse: 0.391   |  0:09:21s
epoch 88 | loss: 0.15572 | val_0_rmse: 0.40812 | val_1_rmse: 0.41009 |  0:09:27s
epoch 89 | loss: 0.16542 | val_0_rmse: 0.38701 | val_1_rmse: 0.38852 |  0:09:34s
epoch 90 | loss: 0.154   | val_0_rmse: 0.38083 | val_1_rmse: 0.38292 |  0:09:40s
epoch 91 | loss: 0.15238 | val_0_rmse: 0.38349 | val_1_rmse: 0.38446 |  0:09:47s
epoch 92 | loss: 0.15352 | val_0_rmse: 0.41993 | val_1_rmse: 0.42221 |  0:09:53s
epoch 93 | loss: 0.15605 | val_0_rmse: 0.41279 | val_1_rmse: 0.41349 |  0:09:59s
epoch 94 | loss: 0.15676 | val_0_rmse: 0.40116 | val_1_rmse: 0.40246 |  0:10:06s
epoch 95 | loss: 0.15355 | val_0_rmse: 0.391   | val_1_rmse: 0.3917  |  0:10:12s

Early stopping occured at epoch 95 with best_epoch = 65 and best_val_1_rmse = 0.37972
Best weights from best epoch are automatically used!
ended training at: 04:16:12
Feature importance:
[('Area', 0.280029905818383), ('Baths', 0.2433427241052543), ('Beds', 0.05604017700151923), ('Latitude', 0.05494549117252498), ('Longitude', 0.05768830616898501), ('Month', 0.06239884394268986), ('Year', 0.24555455179064367)]
Mean squared error is of 2591006501.502152
Mean absolute error:35829.89588159715
MAPE:0.30481857261601086
R2 score:0.6234386472920539
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:16:13
epoch 0  | loss: 10.59894| val_0_rmse: 0.67752 | val_1_rmse: 0.68242 |  0:00:06s
epoch 1  | loss: 0.30467 | val_0_rmse: 0.47826 | val_1_rmse: 0.48476 |  0:00:12s
epoch 2  | loss: 0.24315 | val_0_rmse: 0.48699 | val_1_rmse: 0.49343 |  0:00:19s
epoch 3  | loss: 0.26115 | val_0_rmse: 0.50575 | val_1_rmse: 0.51215 |  0:00:25s
epoch 4  | loss: 0.24457 | val_0_rmse: 0.47095 | val_1_rmse: 0.47734 |  0:00:31s
epoch 5  | loss: 0.2333  | val_0_rmse: 0.48785 | val_1_rmse: 0.49538 |  0:00:38s
epoch 6  | loss: 0.22848 | val_0_rmse: 0.44516 | val_1_rmse: 0.45061 |  0:00:44s
epoch 7  | loss: 0.21786 | val_0_rmse: 0.47479 | val_1_rmse: 0.47987 |  0:00:50s
epoch 8  | loss: 0.21461 | val_0_rmse: 0.43495 | val_1_rmse: 0.43993 |  0:00:57s
epoch 9  | loss: 0.21104 | val_0_rmse: 0.42582 | val_1_rmse: 0.43308 |  0:01:03s
epoch 10 | loss: 0.19373 | val_0_rmse: 0.4768  | val_1_rmse: 0.48399 |  0:01:10s
epoch 11 | loss: 0.20126 | val_0_rmse: 0.41081 | val_1_rmse: 0.42008 |  0:01:16s
epoch 12 | loss: 0.18238 | val_0_rmse: 0.40554 | val_1_rmse: 0.41392 |  0:01:22s
epoch 13 | loss: 0.19602 | val_0_rmse: 0.45522 | val_1_rmse: 0.46217 |  0:01:29s
epoch 14 | loss: 0.1951  | val_0_rmse: 0.44991 | val_1_rmse: 0.45511 |  0:01:35s
epoch 15 | loss: 0.18093 | val_0_rmse: 0.41451 | val_1_rmse: 0.42201 |  0:01:41s
epoch 16 | loss: 0.18027 | val_0_rmse: 0.42639 | val_1_rmse: 0.43418 |  0:01:48s
epoch 17 | loss: 0.17661 | val_0_rmse: 0.40647 | val_1_rmse: 0.41372 |  0:01:54s
epoch 18 | loss: 0.18815 | val_0_rmse: 0.49638 | val_1_rmse: 0.50155 |  0:02:01s
epoch 19 | loss: 0.17456 | val_0_rmse: 0.39463 | val_1_rmse: 0.40202 |  0:02:07s
epoch 20 | loss: 0.1743  | val_0_rmse: 0.39475 | val_1_rmse: 0.40049 |  0:02:13s
epoch 21 | loss: 0.17025 | val_0_rmse: 0.42368 | val_1_rmse: 0.4306  |  0:02:20s
epoch 22 | loss: 0.18565 | val_0_rmse: 0.39452 | val_1_rmse: 0.40168 |  0:02:26s
epoch 23 | loss: 0.17433 | val_0_rmse: 0.39492 | val_1_rmse: 0.40274 |  0:02:32s
epoch 24 | loss: 0.17838 | val_0_rmse: 0.41285 | val_1_rmse: 0.41972 |  0:02:39s
epoch 25 | loss: 0.16891 | val_0_rmse: 0.44189 | val_1_rmse: 0.44952 |  0:02:45s
epoch 26 | loss: 0.16502 | val_0_rmse: 0.39989 | val_1_rmse: 0.40742 |  0:02:51s
epoch 27 | loss: 0.17112 | val_0_rmse: 0.38851 | val_1_rmse: 0.39539 |  0:02:58s
epoch 28 | loss: 0.17991 | val_0_rmse: 0.41398 | val_1_rmse: 0.42133 |  0:03:04s
epoch 29 | loss: 0.1778  | val_0_rmse: 0.4142  | val_1_rmse: 0.42005 |  0:03:10s
epoch 30 | loss: 0.17923 | val_0_rmse: 0.43749 | val_1_rmse: 0.44376 |  0:03:17s
epoch 31 | loss: 0.18002 | val_0_rmse: 0.3879  | val_1_rmse: 0.39497 |  0:03:23s
epoch 32 | loss: 0.16854 | val_0_rmse: 0.39986 | val_1_rmse: 0.40624 |  0:03:30s
epoch 33 | loss: 0.1727  | val_0_rmse: 0.39814 | val_1_rmse: 0.40515 |  0:03:36s
epoch 34 | loss: 0.16348 | val_0_rmse: 0.45461 | val_1_rmse: 0.46227 |  0:03:42s
epoch 35 | loss: 0.17282 | val_0_rmse: 0.38427 | val_1_rmse: 0.39197 |  0:03:49s
epoch 36 | loss: 0.16347 | val_0_rmse: 0.39106 | val_1_rmse: 0.39933 |  0:03:55s
epoch 37 | loss: 0.17165 | val_0_rmse: 0.40881 | val_1_rmse: 0.41608 |  0:04:01s
epoch 38 | loss: 0.16887 | val_0_rmse: 0.45411 | val_1_rmse: 0.46464 |  0:04:08s
epoch 39 | loss: 0.18932 | val_0_rmse: 0.51267 | val_1_rmse: 0.51889 |  0:04:14s
epoch 40 | loss: 0.18588 | val_0_rmse: 0.42246 | val_1_rmse: 0.42987 |  0:04:20s
epoch 41 | loss: 0.18973 | val_0_rmse: 0.48582 | val_1_rmse: 0.49279 |  0:04:27s
epoch 42 | loss: 0.18547 | val_0_rmse: 0.40526 | val_1_rmse: 0.4128  |  0:04:33s
epoch 43 | loss: 0.18158 | val_0_rmse: 0.39487 | val_1_rmse: 0.40243 |  0:04:39s
epoch 44 | loss: 0.17872 | val_0_rmse: 0.40446 | val_1_rmse: 0.41236 |  0:04:46s
epoch 45 | loss: 0.17801 | val_0_rmse: 0.39337 | val_1_rmse: 0.40101 |  0:04:52s
epoch 46 | loss: 0.16288 | val_0_rmse: 0.40185 | val_1_rmse: 0.40857 |  0:04:59s
epoch 47 | loss: 0.17471 | val_0_rmse: 0.39913 | val_1_rmse: 0.40761 |  0:05:05s
epoch 48 | loss: 0.16359 | val_0_rmse: 0.39032 | val_1_rmse: 0.39877 |  0:05:11s
epoch 49 | loss: 0.15504 | val_0_rmse: 0.3826  | val_1_rmse: 0.39078 |  0:05:18s
epoch 50 | loss: 0.15353 | val_0_rmse: 0.39123 | val_1_rmse: 0.39885 |  0:05:24s
epoch 51 | loss: 0.16432 | val_0_rmse: 0.38449 | val_1_rmse: 0.39235 |  0:05:30s
epoch 52 | loss: 0.17121 | val_0_rmse: 0.3993  | val_1_rmse: 0.40805 |  0:05:37s
epoch 53 | loss: 0.16835 | val_0_rmse: 0.44707 | val_1_rmse: 0.45443 |  0:05:43s
epoch 54 | loss: 0.16186 | val_0_rmse: 0.38424 | val_1_rmse: 0.39275 |  0:05:50s
epoch 55 | loss: 0.15872 | val_0_rmse: 0.41162 | val_1_rmse: 0.42002 |  0:05:56s
epoch 56 | loss: 0.16333 | val_0_rmse: 0.39621 | val_1_rmse: 0.40401 |  0:06:02s
epoch 57 | loss: 0.15568 | val_0_rmse: 0.3771  | val_1_rmse: 0.38504 |  0:06:09s
epoch 58 | loss: 0.15951 | val_0_rmse: 0.38195 | val_1_rmse: 0.38991 |  0:06:15s
epoch 59 | loss: 0.17133 | val_0_rmse: 0.43013 | val_1_rmse: 0.43801 |  0:06:21s
epoch 60 | loss: 0.15981 | val_0_rmse: 0.40836 | val_1_rmse: 0.41638 |  0:06:28s
epoch 61 | loss: 0.16683 | val_0_rmse: 0.38678 | val_1_rmse: 0.39404 |  0:06:34s
epoch 62 | loss: 0.16302 | val_0_rmse: 0.39106 | val_1_rmse: 0.39795 |  0:06:40s
epoch 63 | loss: 0.16122 | val_0_rmse: 0.39653 | val_1_rmse: 0.40398 |  0:06:47s
epoch 64 | loss: 0.16188 | val_0_rmse: 0.41231 | val_1_rmse: 0.41942 |  0:06:53s
epoch 65 | loss: 0.15312 | val_0_rmse: 0.38862 | val_1_rmse: 0.39471 |  0:07:00s
epoch 66 | loss: 0.15721 | val_0_rmse: 0.40928 | val_1_rmse: 0.41744 |  0:07:06s
epoch 67 | loss: 0.14844 | val_0_rmse: 0.38204 | val_1_rmse: 0.38846 |  0:07:12s
epoch 68 | loss: 0.15611 | val_0_rmse: 0.42897 | val_1_rmse: 0.43482 |  0:07:19s
epoch 69 | loss: 0.15599 | val_0_rmse: 0.40286 | val_1_rmse: 0.41028 |  0:07:25s
epoch 70 | loss: 0.15709 | val_0_rmse: 0.37564 | val_1_rmse: 0.38365 |  0:07:31s
epoch 71 | loss: 0.15368 | val_0_rmse: 0.40669 | val_1_rmse: 0.41411 |  0:07:38s
epoch 72 | loss: 0.14642 | val_0_rmse: 0.39476 | val_1_rmse: 0.40251 |  0:07:44s
epoch 73 | loss: 0.16005 | val_0_rmse: 0.37679 | val_1_rmse: 0.38357 |  0:07:51s
epoch 74 | loss: 0.16641 | val_0_rmse: 0.38655 | val_1_rmse: 0.39348 |  0:07:57s
epoch 75 | loss: 0.14789 | val_0_rmse: 0.41536 | val_1_rmse: 0.42186 |  0:08:03s
epoch 76 | loss: 0.15351 | val_0_rmse: 0.39508 | val_1_rmse: 0.40251 |  0:08:10s
epoch 77 | loss: 0.15118 | val_0_rmse: 0.40134 | val_1_rmse: 0.40812 |  0:08:16s
epoch 78 | loss: 0.15437 | val_0_rmse: 0.40963 | val_1_rmse: 0.41696 |  0:08:22s
epoch 79 | loss: 0.14612 | val_0_rmse: 0.37563 | val_1_rmse: 0.38252 |  0:08:29s
epoch 80 | loss: 0.1554  | val_0_rmse: 0.39724 | val_1_rmse: 0.4035  |  0:08:35s
epoch 81 | loss: 0.14771 | val_0_rmse: 0.39709 | val_1_rmse: 0.40392 |  0:08:41s
epoch 82 | loss: 0.14552 | val_0_rmse: 0.37131 | val_1_rmse: 0.37894 |  0:08:48s
epoch 83 | loss: 0.1492  | val_0_rmse: 0.41152 | val_1_rmse: 0.41821 |  0:08:54s
epoch 84 | loss: 0.15639 | val_0_rmse: 0.37439 | val_1_rmse: 0.38144 |  0:09:01s
epoch 85 | loss: 0.15622 | val_0_rmse: 0.38321 | val_1_rmse: 0.39049 |  0:09:06s
epoch 86 | loss: 0.14413 | val_0_rmse: 0.37231 | val_1_rmse: 0.38032 |  0:09:12s
epoch 87 | loss: 0.15089 | val_0_rmse: 0.37811 | val_1_rmse: 0.38493 |  0:09:17s
epoch 88 | loss: 0.15641 | val_0_rmse: 0.40641 | val_1_rmse: 0.41469 |  0:09:23s
epoch 89 | loss: 0.15971 | val_0_rmse: 0.40026 | val_1_rmse: 0.40735 |  0:09:28s
epoch 90 | loss: 0.14621 | val_0_rmse: 0.41353 | val_1_rmse: 0.42112 |  0:09:33s
epoch 91 | loss: 0.15229 | val_0_rmse: 0.43316 | val_1_rmse: 0.43937 |  0:09:39s
epoch 92 | loss: 0.14825 | val_0_rmse: 0.37777 | val_1_rmse: 0.38475 |  0:09:44s
epoch 93 | loss: 0.15128 | val_0_rmse: 0.38765 | val_1_rmse: 0.3937  |  0:09:50s
epoch 94 | loss: 0.14626 | val_0_rmse: 0.40162 | val_1_rmse: 0.40741 |  0:09:55s
epoch 95 | loss: 0.14355 | val_0_rmse: 0.37463 | val_1_rmse: 0.38269 |  0:10:01s
epoch 96 | loss: 0.14631 | val_0_rmse: 0.41256 | val_1_rmse: 0.41968 |  0:10:06s
epoch 97 | loss: 0.16387 | val_0_rmse: 0.41929 | val_1_rmse: 0.42636 |  0:10:11s
epoch 98 | loss: 0.14646 | val_0_rmse: 0.43044 | val_1_rmse: 0.43599 |  0:10:17s
epoch 99 | loss: 0.14401 | val_0_rmse: 0.40209 | val_1_rmse: 0.40909 |  0:10:22s
epoch 100| loss: 0.14593 | val_0_rmse: 0.38681 | val_1_rmse: 0.39478 |  0:10:28s
epoch 101| loss: 0.14608 | val_0_rmse: 0.40417 | val_1_rmse: 0.41047 |  0:10:33s
epoch 102| loss: 0.15504 | val_0_rmse: 0.38142 | val_1_rmse: 0.3891  |  0:10:39s
epoch 103| loss: 0.15955 | val_0_rmse: 0.37848 | val_1_rmse: 0.38576 |  0:10:44s
epoch 104| loss: 0.16169 | val_0_rmse: 0.40564 | val_1_rmse: 0.41245 |  0:10:50s
epoch 105| loss: 0.14772 | val_0_rmse: 0.41195 | val_1_rmse: 0.41863 |  0:10:55s
epoch 106| loss: 0.15102 | val_0_rmse: 0.39489 | val_1_rmse: 0.40173 |  0:11:00s
epoch 107| loss: 0.15177 | val_0_rmse: 0.3956  | val_1_rmse: 0.40239 |  0:11:06s
epoch 108| loss: 0.16373 | val_0_rmse: 0.42081 | val_1_rmse: 0.4282  |  0:11:11s
epoch 109| loss: 0.15872 | val_0_rmse: 0.37943 | val_1_rmse: 0.38673 |  0:11:17s
epoch 110| loss: 0.14593 | val_0_rmse: 0.3781  | val_1_rmse: 0.38567 |  0:11:22s
epoch 111| loss: 0.14465 | val_0_rmse: 0.47345 | val_1_rmse: 0.47956 |  0:11:28s
epoch 112| loss: 0.14404 | val_0_rmse: 0.4302  | val_1_rmse: 0.43667 |  0:11:33s

Early stopping occured at epoch 112 with best_epoch = 82 and best_val_1_rmse = 0.37894
Best weights from best epoch are automatically used!
ended training at: 04:27:48
Feature importance:
[('Area', 0.33449821298432303), ('Baths', 0.25455809639672233), ('Beds', 0.0), ('Latitude', 0.016977420272454198), ('Longitude', 0.33863106104825824), ('Month', 0.0), ('Year', 0.05533520929824225)]
Mean squared error is of 2379618090.4914794
Mean absolute error:34567.621279339066
MAPE:0.31020328065673736
R2 score:0.6526971971910067
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:27:49
epoch 0  | loss: 10.48221| val_0_rmse: 0.50593 | val_1_rmse: 0.50531 |  0:00:05s
epoch 1  | loss: 0.24233 | val_0_rmse: 0.46189 | val_1_rmse: 0.46095 |  0:00:10s
epoch 2  | loss: 0.236   | val_0_rmse: 0.45493 | val_1_rmse: 0.45204 |  0:00:16s
epoch 3  | loss: 0.21746 | val_0_rmse: 0.45531 | val_1_rmse: 0.4538  |  0:00:21s
epoch 4  | loss: 0.22599 | val_0_rmse: 0.4411  | val_1_rmse: 0.43919 |  0:00:27s
epoch 5  | loss: 0.20815 | val_0_rmse: 0.43493 | val_1_rmse: 0.43298 |  0:00:32s
epoch 6  | loss: 0.21362 | val_0_rmse: 0.46592 | val_1_rmse: 0.46317 |  0:00:38s
epoch 7  | loss: 0.22029 | val_0_rmse: 0.42776 | val_1_rmse: 0.4258  |  0:00:43s
epoch 8  | loss: 0.21388 | val_0_rmse: 0.44527 | val_1_rmse: 0.44497 |  0:00:48s
epoch 9  | loss: 0.20077 | val_0_rmse: 0.42399 | val_1_rmse: 0.42221 |  0:00:54s
epoch 10 | loss: 0.1849  | val_0_rmse: 0.50825 | val_1_rmse: 0.50655 |  0:00:59s
epoch 11 | loss: 0.20693 | val_0_rmse: 0.4124  | val_1_rmse: 0.41004 |  0:01:05s
epoch 12 | loss: 0.18316 | val_0_rmse: 0.41256 | val_1_rmse: 0.41112 |  0:01:10s
epoch 13 | loss: 0.20755 | val_0_rmse: 0.46851 | val_1_rmse: 0.46739 |  0:01:16s
epoch 14 | loss: 0.2438  | val_0_rmse: 0.44405 | val_1_rmse: 0.44434 |  0:01:21s
epoch 15 | loss: 0.17739 | val_0_rmse: 0.43791 | val_1_rmse: 0.43543 |  0:01:27s
epoch 16 | loss: 0.19171 | val_0_rmse: 0.41681 | val_1_rmse: 0.41602 |  0:01:32s
epoch 17 | loss: 0.18925 | val_0_rmse: 0.41476 | val_1_rmse: 0.41386 |  0:01:37s
epoch 18 | loss: 0.18441 | val_0_rmse: 0.41171 | val_1_rmse: 0.41082 |  0:01:43s
epoch 19 | loss: 0.18456 | val_0_rmse: 0.39066 | val_1_rmse: 0.38904 |  0:01:48s
epoch 20 | loss: 0.16847 | val_0_rmse: 0.41634 | val_1_rmse: 0.41529 |  0:01:54s
epoch 21 | loss: 0.16582 | val_0_rmse: 0.39786 | val_1_rmse: 0.39726 |  0:01:59s
epoch 22 | loss: 0.16241 | val_0_rmse: 0.41174 | val_1_rmse: 0.41079 |  0:02:05s
epoch 23 | loss: 0.16618 | val_0_rmse: 0.4102  | val_1_rmse: 0.40869 |  0:02:10s
epoch 24 | loss: 0.16137 | val_0_rmse: 0.40963 | val_1_rmse: 0.40993 |  0:02:16s
epoch 25 | loss: 0.16475 | val_0_rmse: 0.39448 | val_1_rmse: 0.39373 |  0:02:21s
epoch 26 | loss: 0.1755  | val_0_rmse: 0.3933  | val_1_rmse: 0.39288 |  0:02:26s
epoch 27 | loss: 0.19272 | val_0_rmse: 0.39685 | val_1_rmse: 0.39666 |  0:02:32s
epoch 28 | loss: 0.17994 | val_0_rmse: 0.4004  | val_1_rmse: 0.40012 |  0:02:37s
epoch 29 | loss: 0.1772  | val_0_rmse: 0.42724 | val_1_rmse: 0.42646 |  0:02:43s
epoch 30 | loss: 0.16919 | val_0_rmse: 0.38466 | val_1_rmse: 0.38389 |  0:02:48s
epoch 31 | loss: 0.18457 | val_0_rmse: 0.4118  | val_1_rmse: 0.41116 |  0:02:54s
epoch 32 | loss: 0.22031 | val_0_rmse: 0.45381 | val_1_rmse: 0.45175 |  0:02:59s
epoch 33 | loss: 0.22707 | val_0_rmse: 0.5233  | val_1_rmse: 0.52103 |  0:03:04s
epoch 34 | loss: 0.22661 | val_0_rmse: 0.41505 | val_1_rmse: 0.41512 |  0:03:10s
epoch 35 | loss: 0.21493 | val_0_rmse: 0.40558 | val_1_rmse: 0.40537 |  0:03:15s
epoch 36 | loss: 0.17666 | val_0_rmse: 0.46669 | val_1_rmse: 0.4668  |  0:03:21s
epoch 37 | loss: 0.17686 | val_0_rmse: 0.49072 | val_1_rmse: 0.48967 |  0:03:26s
epoch 38 | loss: 0.17607 | val_0_rmse: 0.42871 | val_1_rmse: 0.42802 |  0:03:32s
epoch 39 | loss: 0.17552 | val_0_rmse: 0.38977 | val_1_rmse: 0.3903  |  0:03:37s
epoch 40 | loss: 0.17817 | val_0_rmse: 0.4446  | val_1_rmse: 0.4424  |  0:03:42s
epoch 41 | loss: 0.17597 | val_0_rmse: 0.42777 | val_1_rmse: 0.42767 |  0:03:48s
epoch 42 | loss: 0.16987 | val_0_rmse: 0.43464 | val_1_rmse: 0.43306 |  0:03:53s
epoch 43 | loss: 0.17191 | val_0_rmse: 0.40368 | val_1_rmse: 0.40265 |  0:03:59s
epoch 44 | loss: 0.17186 | val_0_rmse: 0.409   | val_1_rmse: 0.40798 |  0:04:04s
epoch 45 | loss: 0.16309 | val_0_rmse: 0.40835 | val_1_rmse: 0.40515 |  0:04:10s
epoch 46 | loss: 0.15307 | val_0_rmse: 0.38847 | val_1_rmse: 0.38796 |  0:04:15s
epoch 47 | loss: 0.15367 | val_0_rmse: 0.41159 | val_1_rmse: 0.41098 |  0:04:21s
epoch 48 | loss: 0.17318 | val_0_rmse: 0.51451 | val_1_rmse: 0.51047 |  0:04:26s
epoch 49 | loss: 0.16897 | val_0_rmse: 0.42035 | val_1_rmse: 0.41633 |  0:04:31s
epoch 50 | loss: 0.16773 | val_0_rmse: 0.41885 | val_1_rmse: 0.41529 |  0:04:37s
epoch 51 | loss: 0.16919 | val_0_rmse: 0.38641 | val_1_rmse: 0.38676 |  0:04:42s
epoch 52 | loss: 0.17088 | val_0_rmse: 0.42329 | val_1_rmse: 0.42351 |  0:04:48s
epoch 53 | loss: 0.15483 | val_0_rmse: 0.43249 | val_1_rmse: 0.43232 |  0:04:53s
epoch 54 | loss: 0.15089 | val_0_rmse: 0.39324 | val_1_rmse: 0.39335 |  0:04:59s
epoch 55 | loss: 0.15207 | val_0_rmse: 0.41235 | val_1_rmse: 0.41328 |  0:05:04s
epoch 56 | loss: 0.15273 | val_0_rmse: 0.38653 | val_1_rmse: 0.38584 |  0:05:09s
epoch 57 | loss: 0.15226 | val_0_rmse: 0.38296 | val_1_rmse: 0.38321 |  0:05:15s
epoch 58 | loss: 0.1517  | val_0_rmse: 0.40077 | val_1_rmse: 0.40093 |  0:05:21s
epoch 59 | loss: 0.15711 | val_0_rmse: 0.42425 | val_1_rmse: 0.42364 |  0:05:26s
epoch 60 | loss: 0.14795 | val_0_rmse: 0.39767 | val_1_rmse: 0.39698 |  0:05:32s
epoch 61 | loss: 0.1492  | val_0_rmse: 0.42267 | val_1_rmse: 0.42204 |  0:05:37s
epoch 62 | loss: 0.15023 | val_0_rmse: 0.4042  | val_1_rmse: 0.4032  |  0:05:42s
epoch 63 | loss: 0.14523 | val_0_rmse: 0.41751 | val_1_rmse: 0.41632 |  0:05:48s
epoch 64 | loss: 0.15209 | val_0_rmse: 0.39157 | val_1_rmse: 0.39184 |  0:05:53s
epoch 65 | loss: 0.16376 | val_0_rmse: 0.39481 | val_1_rmse: 0.3948  |  0:05:59s
epoch 66 | loss: 0.15981 | val_0_rmse: 0.3745  | val_1_rmse: 0.3747  |  0:06:04s
epoch 67 | loss: 0.16629 | val_0_rmse: 0.47308 | val_1_rmse: 0.47319 |  0:06:10s
epoch 68 | loss: 0.1627  | val_0_rmse: 0.44198 | val_1_rmse: 0.43947 |  0:06:15s
epoch 69 | loss: 0.15769 | val_0_rmse: 0.39451 | val_1_rmse: 0.39317 |  0:06:21s
epoch 70 | loss: 0.17281 | val_0_rmse: 0.42189 | val_1_rmse: 0.42304 |  0:06:26s
epoch 71 | loss: 0.16297 | val_0_rmse: 0.39285 | val_1_rmse: 0.39503 |  0:06:31s
epoch 72 | loss: 0.16205 | val_0_rmse: 0.41844 | val_1_rmse: 0.41762 |  0:06:37s
epoch 73 | loss: 0.15003 | val_0_rmse: 0.40711 | val_1_rmse: 0.4057  |  0:06:42s
epoch 74 | loss: 0.14718 | val_0_rmse: 0.39085 | val_1_rmse: 0.40329 |  0:06:48s
epoch 75 | loss: 0.14394 | val_0_rmse: 0.39653 | val_1_rmse: 0.39839 |  0:06:53s
epoch 76 | loss: 0.14517 | val_0_rmse: 0.41568 | val_1_rmse: 0.41344 |  0:06:59s
epoch 77 | loss: 0.14813 | val_0_rmse: 0.42988 | val_1_rmse: 0.42853 |  0:07:04s
epoch 78 | loss: 0.14587 | val_0_rmse: 0.39273 | val_1_rmse: 0.39292 |  0:07:09s
epoch 79 | loss: 0.14397 | val_0_rmse: 0.36439 | val_1_rmse: 0.36362 |  0:07:15s
epoch 80 | loss: 0.14302 | val_0_rmse: 0.37897 | val_1_rmse: 0.37854 |  0:07:20s
epoch 81 | loss: 0.15378 | val_0_rmse: 0.37825 | val_1_rmse: 0.37902 |  0:07:26s
epoch 82 | loss: 0.15903 | val_0_rmse: 0.38355 | val_1_rmse: 0.38536 |  0:07:31s
epoch 83 | loss: 0.14513 | val_0_rmse: 0.42709 | val_1_rmse: 0.42595 |  0:07:37s
epoch 84 | loss: 0.1436  | val_0_rmse: 0.39765 | val_1_rmse: 0.39733 |  0:07:42s
epoch 85 | loss: 0.14389 | val_0_rmse: 0.43293 | val_1_rmse: 0.43135 |  0:07:48s
epoch 86 | loss: 0.14651 | val_0_rmse: 0.47381 | val_1_rmse: 0.4754  |  0:07:53s
epoch 87 | loss: 0.14412 | val_0_rmse: 0.44727 | val_1_rmse: 0.44638 |  0:07:58s
epoch 88 | loss: 0.14157 | val_0_rmse: 0.43383 | val_1_rmse: 0.43402 |  0:08:04s
epoch 89 | loss: 0.14085 | val_0_rmse: 0.40531 | val_1_rmse: 0.40587 |  0:08:09s
epoch 90 | loss: 0.15884 | val_0_rmse: 0.3848  | val_1_rmse: 0.38547 |  0:08:15s
epoch 91 | loss: 0.15985 | val_0_rmse: 0.46083 | val_1_rmse: 0.45923 |  0:08:20s
epoch 92 | loss: 0.15918 | val_0_rmse: 0.44993 | val_1_rmse: 0.45051 |  0:08:26s
epoch 93 | loss: 0.16276 | val_0_rmse: 0.37951 | val_1_rmse: 0.38143 |  0:08:31s
epoch 94 | loss: 0.15722 | val_0_rmse: 0.45275 | val_1_rmse: 0.45412 |  0:08:36s
epoch 95 | loss: 0.15567 | val_0_rmse: 0.38837 | val_1_rmse: 0.38827 |  0:08:42s
epoch 96 | loss: 0.14317 | val_0_rmse: 0.37549 | val_1_rmse: 0.37498 |  0:08:47s
epoch 97 | loss: 0.16139 | val_0_rmse: 0.41314 | val_1_rmse: 0.41201 |  0:08:53s
epoch 98 | loss: 0.14045 | val_0_rmse: 0.38299 | val_1_rmse: 0.3853  |  0:08:58s
epoch 99 | loss: 0.13938 | val_0_rmse: 0.40789 | val_1_rmse: 0.40768 |  0:09:04s
epoch 100| loss: 0.15792 | val_0_rmse: 0.42527 | val_1_rmse: 0.42443 |  0:09:09s
epoch 101| loss: 0.14704 | val_0_rmse: 0.42171 | val_1_rmse: 0.42161 |  0:09:14s
epoch 102| loss: 0.14164 | val_0_rmse: 0.39162 | val_1_rmse: 0.39106 |  0:09:20s
epoch 103| loss: 0.13842 | val_0_rmse: 0.39348 | val_1_rmse: 0.38949 |  0:09:25s
epoch 104| loss: 0.15545 | val_0_rmse: 0.41058 | val_1_rmse: 0.41031 |  0:09:31s
epoch 105| loss: 0.14689 | val_0_rmse: 0.38414 | val_1_rmse: 0.38489 |  0:09:36s
epoch 106| loss: 0.15655 | val_0_rmse: 0.42954 | val_1_rmse: 0.43014 |  0:09:42s
epoch 107| loss: 0.13695 | val_0_rmse: 0.36775 | val_1_rmse: 0.36851 |  0:09:47s
epoch 108| loss: 0.14425 | val_0_rmse: 0.37538 | val_1_rmse: 0.37417 |  0:09:52s
epoch 109| loss: 0.1404  | val_0_rmse: 0.3857  | val_1_rmse: 0.38553 |  0:09:58s

Early stopping occured at epoch 109 with best_epoch = 79 and best_val_1_rmse = 0.36362
Best weights from best epoch are automatically used!
ended training at: 04:37:49
Feature importance:
[('Area', 0.0), ('Baths', 0.5650909340949355), ('Beds', 0.07248456697498555), ('Latitude', 0.09269091068674552), ('Longitude', 0.020405093702161467), ('Month', 0.17298688329133197), ('Year', 0.07634161124984004)]
Mean squared error is of 2329020180.4861755
Mean absolute error:33860.294017000866
MAPE:0.29319855333738215
R2 score:0.6556959942258633
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:37:49
epoch 0  | loss: 25.92062| val_0_rmse: 1.87846 | val_1_rmse: 1.89333 |  0:00:02s
epoch 1  | loss: 0.30753 | val_0_rmse: 0.5704  | val_1_rmse: 0.57827 |  0:00:03s
epoch 2  | loss: 0.20834 | val_0_rmse: 0.42223 | val_1_rmse: 0.426   |  0:00:05s
epoch 3  | loss: 0.34874 | val_0_rmse: 0.48758 | val_1_rmse: 0.49396 |  0:00:07s
epoch 4  | loss: 0.17946 | val_0_rmse: 0.37678 | val_1_rmse: 0.37769 |  0:00:09s
epoch 5  | loss: 0.15027 | val_0_rmse: 0.36788 | val_1_rmse: 0.36879 |  0:00:11s
epoch 6  | loss: 0.14775 | val_0_rmse: 0.37242 | val_1_rmse: 0.37676 |  0:00:13s
epoch 7  | loss: 0.16835 | val_0_rmse: 0.42014 | val_1_rmse: 0.41474 |  0:00:15s
epoch 8  | loss: 0.16351 | val_0_rmse: 0.39556 | val_1_rmse: 0.39218 |  0:00:17s
epoch 9  | loss: 0.16256 | val_0_rmse: 0.35642 | val_1_rmse: 0.35841 |  0:00:19s
epoch 10 | loss: 0.17458 | val_0_rmse: 0.38722 | val_1_rmse: 0.38308 |  0:00:21s
epoch 11 | loss: 0.15393 | val_0_rmse: 0.46873 | val_1_rmse: 0.4572  |  0:00:23s
epoch 12 | loss: 0.17617 | val_0_rmse: 0.39738 | val_1_rmse: 0.40632 |  0:00:25s
epoch 13 | loss: 0.13891 | val_0_rmse: 0.34988 | val_1_rmse: 0.35124 |  0:00:27s
epoch 14 | loss: 0.13099 | val_0_rmse: 0.3476  | val_1_rmse: 0.3505  |  0:00:29s
epoch 15 | loss: 0.14594 | val_0_rmse: 0.33646 | val_1_rmse: 0.33676 |  0:00:31s
epoch 16 | loss: 0.15423 | val_0_rmse: 0.35757 | val_1_rmse: 0.36241 |  0:00:33s
epoch 17 | loss: 0.13795 | val_0_rmse: 0.34753 | val_1_rmse: 0.35304 |  0:00:35s
epoch 18 | loss: 0.16619 | val_0_rmse: 0.36698 | val_1_rmse: 0.37574 |  0:00:37s
epoch 19 | loss: 0.13126 | val_0_rmse: 0.33356 | val_1_rmse: 0.33305 |  0:00:39s
epoch 20 | loss: 0.12309 | val_0_rmse: 0.38653 | val_1_rmse: 0.39627 |  0:00:41s
epoch 21 | loss: 0.12773 | val_0_rmse: 0.3455  | val_1_rmse: 0.33979 |  0:00:43s
epoch 22 | loss: 0.1407  | val_0_rmse: 0.35139 | val_1_rmse: 0.35753 |  0:00:45s
epoch 23 | loss: 0.12917 | val_0_rmse: 0.34902 | val_1_rmse: 0.34784 |  0:00:47s
epoch 24 | loss: 0.15471 | val_0_rmse: 0.36804 | val_1_rmse: 0.37605 |  0:00:49s
epoch 25 | loss: 0.13727 | val_0_rmse: 0.33922 | val_1_rmse: 0.34298 |  0:00:51s
epoch 26 | loss: 0.12458 | val_0_rmse: 0.33531 | val_1_rmse: 0.33979 |  0:00:53s
epoch 27 | loss: 0.13249 | val_0_rmse: 0.37752 | val_1_rmse: 0.38686 |  0:00:55s
epoch 28 | loss: 0.12424 | val_0_rmse: 0.33845 | val_1_rmse: 0.33473 |  0:00:57s
epoch 29 | loss: 0.12337 | val_0_rmse: 0.33273 | val_1_rmse: 0.33386 |  0:00:59s
epoch 30 | loss: 0.1438  | val_0_rmse: 0.34244 | val_1_rmse: 0.34123 |  0:01:01s
epoch 31 | loss: 0.12456 | val_0_rmse: 0.42122 | val_1_rmse: 0.41359 |  0:01:03s
epoch 32 | loss: 0.15583 | val_0_rmse: 0.33618 | val_1_rmse: 0.34151 |  0:01:05s
epoch 33 | loss: 0.12648 | val_0_rmse: 0.34616 | val_1_rmse: 0.34114 |  0:01:07s
epoch 34 | loss: 0.11745 | val_0_rmse: 0.36785 | val_1_rmse: 0.37521 |  0:01:09s
epoch 35 | loss: 0.12069 | val_0_rmse: 0.33497 | val_1_rmse: 0.33347 |  0:01:11s
epoch 36 | loss: 0.11828 | val_0_rmse: 0.32989 | val_1_rmse: 0.33094 |  0:01:13s
epoch 37 | loss: 0.12625 | val_0_rmse: 0.35074 | val_1_rmse: 0.34562 |  0:01:15s
epoch 38 | loss: 0.11431 | val_0_rmse: 0.32715 | val_1_rmse: 0.32618 |  0:01:17s
epoch 39 | loss: 0.17164 | val_0_rmse: 0.33941 | val_1_rmse: 0.34465 |  0:01:19s
epoch 40 | loss: 0.20364 | val_0_rmse: 0.42315 | val_1_rmse: 0.41441 |  0:01:21s
epoch 41 | loss: 0.19716 | val_0_rmse: 0.52622 | val_1_rmse: 0.53973 |  0:01:23s
epoch 42 | loss: 0.20153 | val_0_rmse: 0.37305 | val_1_rmse: 0.3658  |  0:01:25s
epoch 43 | loss: 0.20003 | val_0_rmse: 0.3317  | val_1_rmse: 0.32816 |  0:01:27s
epoch 44 | loss: 0.1936  | val_0_rmse: 0.44699 | val_1_rmse: 0.4591  |  0:01:29s
epoch 45 | loss: 0.19779 | val_0_rmse: 0.39662 | val_1_rmse: 0.38817 |  0:01:31s
epoch 46 | loss: 0.14139 | val_0_rmse: 0.32709 | val_1_rmse: 0.32545 |  0:01:33s
epoch 47 | loss: 0.1167  | val_0_rmse: 0.3241  | val_1_rmse: 0.32365 |  0:01:35s
epoch 48 | loss: 0.11681 | val_0_rmse: 0.32694 | val_1_rmse: 0.32877 |  0:01:36s
epoch 49 | loss: 0.12235 | val_0_rmse: 0.37237 | val_1_rmse: 0.38226 |  0:01:38s
epoch 50 | loss: 0.1388  | val_0_rmse: 0.36112 | val_1_rmse: 0.36677 |  0:01:40s
epoch 51 | loss: 0.11815 | val_0_rmse: 0.35963 | val_1_rmse: 0.36614 |  0:01:42s
epoch 52 | loss: 0.16097 | val_0_rmse: 0.41973 | val_1_rmse: 0.4143  |  0:01:44s
epoch 53 | loss: 0.14136 | val_0_rmse: 0.35635 | val_1_rmse: 0.36751 |  0:01:46s
epoch 54 | loss: 0.14812 | val_0_rmse: 0.40444 | val_1_rmse: 0.41554 |  0:01:48s
epoch 55 | loss: 0.13755 | val_0_rmse: 0.37003 | val_1_rmse: 0.369   |  0:01:50s
epoch 56 | loss: 0.13944 | val_0_rmse: 0.37138 | val_1_rmse: 0.38076 |  0:01:52s
epoch 57 | loss: 0.12776 | val_0_rmse: 0.3487  | val_1_rmse: 0.34816 |  0:01:54s
epoch 58 | loss: 0.12297 | val_0_rmse: 0.3855  | val_1_rmse: 0.37955 |  0:01:56s
epoch 59 | loss: 0.13614 | val_0_rmse: 0.36058 | val_1_rmse: 0.37123 |  0:01:58s
epoch 60 | loss: 0.13523 | val_0_rmse: 0.38954 | val_1_rmse: 0.38438 |  0:02:00s
epoch 61 | loss: 0.13701 | val_0_rmse: 0.37982 | val_1_rmse: 0.3919  |  0:02:02s
epoch 62 | loss: 0.14523 | val_0_rmse: 0.34626 | val_1_rmse: 0.34197 |  0:02:04s
epoch 63 | loss: 0.13124 | val_0_rmse: 0.34141 | val_1_rmse: 0.34722 |  0:02:06s
epoch 64 | loss: 0.11537 | val_0_rmse: 0.34706 | val_1_rmse: 0.35735 |  0:02:08s
epoch 65 | loss: 0.13446 | val_0_rmse: 0.33251 | val_1_rmse: 0.334   |  0:02:10s
epoch 66 | loss: 0.13001 | val_0_rmse: 0.31657 | val_1_rmse: 0.32365 |  0:02:12s
epoch 67 | loss: 0.1205  | val_0_rmse: 0.32774 | val_1_rmse: 0.33413 |  0:02:14s
epoch 68 | loss: 0.11871 | val_0_rmse: 0.34816 | val_1_rmse: 0.34681 |  0:02:16s
epoch 69 | loss: 0.13339 | val_0_rmse: 0.35846 | val_1_rmse: 0.37059 |  0:02:18s
epoch 70 | loss: 0.143   | val_0_rmse: 0.32815 | val_1_rmse: 0.32942 |  0:02:20s
epoch 71 | loss: 0.10874 | val_0_rmse: 0.32869 | val_1_rmse: 0.33687 |  0:02:22s
epoch 72 | loss: 0.11169 | val_0_rmse: 0.32787 | val_1_rmse: 0.33084 |  0:02:24s
epoch 73 | loss: 0.11013 | val_0_rmse: 0.31803 | val_1_rmse: 0.32477 |  0:02:26s
epoch 74 | loss: 0.11081 | val_0_rmse: 0.32938 | val_1_rmse: 0.33748 |  0:02:28s
epoch 75 | loss: 0.11298 | val_0_rmse: 0.31378 | val_1_rmse: 0.31834 |  0:02:30s
epoch 76 | loss: 0.11608 | val_0_rmse: 0.3852  | val_1_rmse: 0.39712 |  0:02:32s
epoch 77 | loss: 0.13737 | val_0_rmse: 0.37226 | val_1_rmse: 0.36696 |  0:02:34s
epoch 78 | loss: 0.13181 | val_0_rmse: 0.35407 | val_1_rmse: 0.36335 |  0:02:36s
epoch 79 | loss: 0.11152 | val_0_rmse: 0.33687 | val_1_rmse: 0.34519 |  0:02:38s
epoch 80 | loss: 0.12828 | val_0_rmse: 0.35935 | val_1_rmse: 0.37151 |  0:02:40s
epoch 81 | loss: 0.11181 | val_0_rmse: 0.32109 | val_1_rmse: 0.32061 |  0:02:42s
epoch 82 | loss: 0.11587 | val_0_rmse: 0.32559 | val_1_rmse: 0.33304 |  0:02:44s
epoch 83 | loss: 0.10689 | val_0_rmse: 0.3377  | val_1_rmse: 0.34874 |  0:02:46s
epoch 84 | loss: 0.16952 | val_0_rmse: 0.38166 | val_1_rmse: 0.37552 |  0:02:48s
epoch 85 | loss: 0.10815 | val_0_rmse: 0.31928 | val_1_rmse: 0.3209  |  0:02:50s
epoch 86 | loss: 0.11515 | val_0_rmse: 0.36962 | val_1_rmse: 0.36664 |  0:02:52s
epoch 87 | loss: 0.13185 | val_0_rmse: 0.33233 | val_1_rmse: 0.34181 |  0:02:54s
epoch 88 | loss: 0.12736 | val_0_rmse: 0.33667 | val_1_rmse: 0.33476 |  0:02:56s
epoch 89 | loss: 0.12569 | val_0_rmse: 0.33216 | val_1_rmse: 0.34093 |  0:02:58s
epoch 90 | loss: 0.12941 | val_0_rmse: 0.32672 | val_1_rmse: 0.32634 |  0:02:59s
epoch 91 | loss: 0.12838 | val_0_rmse: 0.32412 | val_1_rmse: 0.33361 |  0:03:01s
epoch 92 | loss: 0.12733 | val_0_rmse: 0.31951 | val_1_rmse: 0.31956 |  0:03:03s
epoch 93 | loss: 0.11849 | val_0_rmse: 0.31528 | val_1_rmse: 0.32427 |  0:03:05s
epoch 94 | loss: 0.10274 | val_0_rmse: 0.31431 | val_1_rmse: 0.31949 |  0:03:07s
epoch 95 | loss: 0.10712 | val_0_rmse: 0.3104  | val_1_rmse: 0.31266 |  0:03:09s
epoch 96 | loss: 0.10365 | val_0_rmse: 0.3083  | val_1_rmse: 0.31577 |  0:03:11s
epoch 97 | loss: 0.10593 | val_0_rmse: 0.45167 | val_1_rmse: 0.44517 |  0:03:13s
epoch 98 | loss: 0.10786 | val_0_rmse: 0.3264  | val_1_rmse: 0.33497 |  0:03:15s
epoch 99 | loss: 0.10235 | val_0_rmse: 0.32647 | val_1_rmse: 0.32627 |  0:03:17s
epoch 100| loss: 0.15279 | val_0_rmse: 0.3231  | val_1_rmse: 0.33085 |  0:03:19s
epoch 101| loss: 0.12079 | val_0_rmse: 0.32527 | val_1_rmse: 0.32455 |  0:03:21s
epoch 102| loss: 0.12692 | val_0_rmse: 0.32726 | val_1_rmse: 0.33803 |  0:03:23s
epoch 103| loss: 0.12583 | val_0_rmse: 0.32073 | val_1_rmse: 0.32046 |  0:03:25s
epoch 104| loss: 0.12638 | val_0_rmse: 0.35925 | val_1_rmse: 0.37163 |  0:03:27s
epoch 105| loss: 0.12635 | val_0_rmse: 0.3238  | val_1_rmse: 0.32326 |  0:03:29s
epoch 106| loss: 0.12383 | val_0_rmse: 0.35902 | val_1_rmse: 0.3718  |  0:03:31s
epoch 107| loss: 0.12437 | val_0_rmse: 0.32714 | val_1_rmse: 0.32711 |  0:03:33s
epoch 108| loss: 0.11271 | val_0_rmse: 0.43385 | val_1_rmse: 0.42801 |  0:03:35s
epoch 109| loss: 0.11881 | val_0_rmse: 0.34791 | val_1_rmse: 0.34585 |  0:03:37s
epoch 110| loss: 0.12336 | val_0_rmse: 0.31936 | val_1_rmse: 0.33043 |  0:03:39s
epoch 111| loss: 0.12105 | val_0_rmse: 0.32619 | val_1_rmse: 0.32636 |  0:03:41s
epoch 112| loss: 0.12434 | val_0_rmse: 0.31978 | val_1_rmse: 0.33019 |  0:03:43s
epoch 113| loss: 0.12196 | val_0_rmse: 0.30937 | val_1_rmse: 0.31364 |  0:03:45s
epoch 114| loss: 0.10496 | val_0_rmse: 0.32587 | val_1_rmse: 0.32647 |  0:03:47s
epoch 115| loss: 0.1016  | val_0_rmse: 0.34627 | val_1_rmse: 0.35799 |  0:03:49s
epoch 116| loss: 0.11516 | val_0_rmse: 0.41673 | val_1_rmse: 0.4305  |  0:03:51s
epoch 117| loss: 0.16402 | val_0_rmse: 0.31569 | val_1_rmse: 0.32515 |  0:03:53s
epoch 118| loss: 0.12628 | val_0_rmse: 0.31606 | val_1_rmse: 0.31949 |  0:03:55s
epoch 119| loss: 0.12424 | val_0_rmse: 0.32646 | val_1_rmse: 0.33928 |  0:03:57s
epoch 120| loss: 0.12461 | val_0_rmse: 0.32459 | val_1_rmse: 0.32547 |  0:03:59s
epoch 121| loss: 0.12091 | val_0_rmse: 0.33187 | val_1_rmse: 0.34346 |  0:04:01s
epoch 122| loss: 0.11534 | val_0_rmse: 0.41811 | val_1_rmse: 0.43386 |  0:04:03s
epoch 123| loss: 0.10804 | val_0_rmse: 0.35443 | val_1_rmse: 0.35287 |  0:04:05s
epoch 124| loss: 0.10379 | val_0_rmse: 0.31245 | val_1_rmse: 0.31968 |  0:04:07s
epoch 125| loss: 0.10771 | val_0_rmse: 0.3155  | val_1_rmse: 0.32362 |  0:04:09s

Early stopping occured at epoch 125 with best_epoch = 95 and best_val_1_rmse = 0.31266
Best weights from best epoch are automatically used!
ended training at: 04:41:59
Feature importance:
[('Area', 0.20449220517050132), ('Baths', 0.10679502711356231), ('Beds', 0.0), ('Latitude', 0.30269439712091284), ('Longitude', 0.3158204991027571), ('Month', 0.07019787149226642), ('Year', 0.0)]
Mean squared error is of 1093014844.0294123
Mean absolute error:22215.0503934941
MAPE:0.2601624696852929
R2 score:0.7372528862225497
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:41:59
epoch 0  | loss: 25.65915| val_0_rmse: 1.47117 | val_1_rmse: 1.4683  |  0:00:01s
epoch 1  | loss: 0.44219 | val_0_rmse: 0.56831 | val_1_rmse: 0.55857 |  0:00:03s
epoch 2  | loss: 0.25923 | val_0_rmse: 0.46346 | val_1_rmse: 0.46461 |  0:00:05s
epoch 3  | loss: 0.23576 | val_0_rmse: 0.4632  | val_1_rmse: 0.4653  |  0:00:07s
epoch 4  | loss: 0.19964 | val_0_rmse: 0.44266 | val_1_rmse: 0.44702 |  0:00:09s
epoch 5  | loss: 0.22114 | val_0_rmse: 0.45067 | val_1_rmse: 0.45127 |  0:00:11s
epoch 6  | loss: 0.18576 | val_0_rmse: 0.39224 | val_1_rmse: 0.39076 |  0:00:13s
epoch 7  | loss: 0.17356 | val_0_rmse: 0.38893 | val_1_rmse: 0.39353 |  0:00:15s
epoch 8  | loss: 0.17354 | val_0_rmse: 0.37245 | val_1_rmse: 0.37411 |  0:00:17s
epoch 9  | loss: 0.15514 | val_0_rmse: 0.38132 | val_1_rmse: 0.38213 |  0:00:19s
epoch 10 | loss: 0.1516  | val_0_rmse: 0.37226 | val_1_rmse: 0.37817 |  0:00:21s
epoch 11 | loss: 0.18294 | val_0_rmse: 0.37871 | val_1_rmse: 0.38188 |  0:00:23s
epoch 12 | loss: 0.17784 | val_0_rmse: 0.40913 | val_1_rmse: 0.41028 |  0:00:25s
epoch 13 | loss: 0.16387 | val_0_rmse: 0.37711 | val_1_rmse: 0.38148 |  0:00:27s
epoch 14 | loss: 0.16432 | val_0_rmse: 0.3703  | val_1_rmse: 0.37055 |  0:00:29s
epoch 15 | loss: 0.1593  | val_0_rmse: 0.3661  | val_1_rmse: 0.36993 |  0:00:31s
epoch 16 | loss: 0.1613  | val_0_rmse: 0.37893 | val_1_rmse: 0.37735 |  0:00:33s
epoch 17 | loss: 0.15694 | val_0_rmse: 0.35464 | val_1_rmse: 0.35875 |  0:00:35s
epoch 18 | loss: 0.1577  | val_0_rmse: 0.38148 | val_1_rmse: 0.38112 |  0:00:37s
epoch 19 | loss: 0.16261 | val_0_rmse: 0.38289 | val_1_rmse: 0.38608 |  0:00:39s
epoch 20 | loss: 0.16506 | val_0_rmse: 0.39178 | val_1_rmse: 0.39133 |  0:00:41s
epoch 21 | loss: 0.15761 | val_0_rmse: 0.35768 | val_1_rmse: 0.36274 |  0:00:43s
epoch 22 | loss: 0.15886 | val_0_rmse: 0.34704 | val_1_rmse: 0.34996 |  0:00:45s
epoch 23 | loss: 0.1347  | val_0_rmse: 0.35667 | val_1_rmse: 0.35795 |  0:00:47s
epoch 24 | loss: 0.1522  | val_0_rmse: 0.37407 | val_1_rmse: 0.37842 |  0:00:49s
epoch 25 | loss: 0.15633 | val_0_rmse: 0.34473 | val_1_rmse: 0.34549 |  0:00:51s
epoch 26 | loss: 0.14896 | val_0_rmse: 0.35063 | val_1_rmse: 0.35481 |  0:00:53s
epoch 27 | loss: 0.14907 | val_0_rmse: 0.43148 | val_1_rmse: 0.43438 |  0:00:55s
epoch 28 | loss: 0.1676  | val_0_rmse: 0.43579 | val_1_rmse: 0.43279 |  0:00:57s
epoch 29 | loss: 0.17838 | val_0_rmse: 0.44784 | val_1_rmse: 0.44524 |  0:00:59s
epoch 30 | loss: 0.14738 | val_0_rmse: 0.34195 | val_1_rmse: 0.34513 |  0:01:01s
epoch 31 | loss: 0.16705 | val_0_rmse: 0.37359 | val_1_rmse: 0.37983 |  0:01:03s
epoch 32 | loss: 0.14012 | val_0_rmse: 0.34558 | val_1_rmse: 0.35026 |  0:01:05s
epoch 33 | loss: 0.12315 | val_0_rmse: 0.34252 | val_1_rmse: 0.34457 |  0:01:07s
epoch 34 | loss: 0.13718 | val_0_rmse: 0.34849 | val_1_rmse: 0.35378 |  0:01:09s
epoch 35 | loss: 0.14092 | val_0_rmse: 0.36912 | val_1_rmse: 0.36821 |  0:01:11s
epoch 36 | loss: 0.15045 | val_0_rmse: 0.32947 | val_1_rmse: 0.33238 |  0:01:13s
epoch 37 | loss: 0.1866  | val_0_rmse: 0.33649 | val_1_rmse: 0.34011 |  0:01:15s
epoch 38 | loss: 0.18836 | val_0_rmse: 0.44191 | val_1_rmse: 0.43903 |  0:01:17s
epoch 39 | loss: 0.19202 | val_0_rmse: 0.4198  | val_1_rmse: 0.43028 |  0:01:19s
epoch 40 | loss: 0.19125 | val_0_rmse: 0.39915 | val_1_rmse: 0.39737 |  0:01:21s
epoch 41 | loss: 0.19795 | val_0_rmse: 0.35314 | val_1_rmse: 0.35364 |  0:01:23s
epoch 42 | loss: 0.18932 | val_0_rmse: 0.4074  | val_1_rmse: 0.41464 |  0:01:25s
epoch 43 | loss: 0.16661 | val_0_rmse: 0.37088 | val_1_rmse: 0.37473 |  0:01:27s
epoch 44 | loss: 0.13773 | val_0_rmse: 0.36679 | val_1_rmse: 0.36879 |  0:01:29s
epoch 45 | loss: 0.11905 | val_0_rmse: 0.35754 | val_1_rmse: 0.36456 |  0:01:31s
epoch 46 | loss: 0.14148 | val_0_rmse: 0.34927 | val_1_rmse: 0.34932 |  0:01:33s
epoch 47 | loss: 0.12525 | val_0_rmse: 0.34675 | val_1_rmse: 0.35013 |  0:01:35s
epoch 48 | loss: 0.12052 | val_0_rmse: 0.35012 | val_1_rmse: 0.35797 |  0:01:37s
epoch 49 | loss: 0.11988 | val_0_rmse: 0.33477 | val_1_rmse: 0.33807 |  0:01:39s
epoch 50 | loss: 0.13888 | val_0_rmse: 0.34316 | val_1_rmse: 0.34692 |  0:01:41s
epoch 51 | loss: 0.12676 | val_0_rmse: 0.33798 | val_1_rmse: 0.34088 |  0:01:43s
epoch 52 | loss: 0.11602 | val_0_rmse: 0.32861 | val_1_rmse: 0.33041 |  0:01:45s
epoch 53 | loss: 0.13159 | val_0_rmse: 0.33712 | val_1_rmse: 0.34287 |  0:01:47s
epoch 54 | loss: 0.11765 | val_0_rmse: 0.33037 | val_1_rmse: 0.33192 |  0:01:49s
epoch 55 | loss: 0.12878 | val_0_rmse: 0.34898 | val_1_rmse: 0.35537 |  0:01:51s
epoch 56 | loss: 0.12704 | val_0_rmse: 0.32306 | val_1_rmse: 0.32714 |  0:01:53s
epoch 57 | loss: 0.1173  | val_0_rmse: 0.32993 | val_1_rmse: 0.33572 |  0:01:55s
epoch 58 | loss: 0.12014 | val_0_rmse: 0.33656 | val_1_rmse: 0.3425  |  0:01:57s
epoch 59 | loss: 0.13385 | val_0_rmse: 0.34518 | val_1_rmse: 0.34591 |  0:01:59s
epoch 60 | loss: 0.1226  | val_0_rmse: 0.32301 | val_1_rmse: 0.32621 |  0:02:01s
epoch 61 | loss: 0.11906 | val_0_rmse: 0.32387 | val_1_rmse: 0.32601 |  0:02:03s
epoch 62 | loss: 0.12681 | val_0_rmse: 0.34462 | val_1_rmse: 0.34649 |  0:02:05s
epoch 63 | loss: 0.12595 | val_0_rmse: 0.32566 | val_1_rmse: 0.33002 |  0:02:07s
epoch 64 | loss: 0.13812 | val_0_rmse: 0.33247 | val_1_rmse: 0.33422 |  0:02:09s
epoch 65 | loss: 0.1401  | val_0_rmse: 0.33201 | val_1_rmse: 0.33707 |  0:02:11s
epoch 66 | loss: 0.13786 | val_0_rmse: 0.33185 | val_1_rmse: 0.33308 |  0:02:13s
epoch 67 | loss: 0.13935 | val_0_rmse: 0.32733 | val_1_rmse: 0.33015 |  0:02:15s
epoch 68 | loss: 0.13591 | val_0_rmse: 0.33086 | val_1_rmse: 0.33261 |  0:02:17s
epoch 69 | loss: 0.13452 | val_0_rmse: 0.31937 | val_1_rmse: 0.32283 |  0:02:19s
epoch 70 | loss: 0.13133 | val_0_rmse: 0.33017 | val_1_rmse: 0.3296  |  0:02:21s
epoch 71 | loss: 0.12764 | val_0_rmse: 0.34873 | val_1_rmse: 0.35426 |  0:02:23s
epoch 72 | loss: 0.11854 | val_0_rmse: 0.33284 | val_1_rmse: 0.33244 |  0:02:25s
epoch 73 | loss: 0.11028 | val_0_rmse: 0.32118 | val_1_rmse: 0.32253 |  0:02:27s
epoch 74 | loss: 0.11399 | val_0_rmse: 0.33851 | val_1_rmse: 0.34333 |  0:02:29s
epoch 75 | loss: 0.11559 | val_0_rmse: 0.31932 | val_1_rmse: 0.32244 |  0:02:31s
epoch 76 | loss: 0.11319 | val_0_rmse: 0.34446 | val_1_rmse: 0.35122 |  0:02:32s
epoch 77 | loss: 0.12173 | val_0_rmse: 0.31904 | val_1_rmse: 0.3214  |  0:02:35s
epoch 78 | loss: 0.11015 | val_0_rmse: 0.31929 | val_1_rmse: 0.31996 |  0:02:36s
epoch 79 | loss: 0.1085  | val_0_rmse: 0.33727 | val_1_rmse: 0.33684 |  0:02:38s
epoch 80 | loss: 0.12009 | val_0_rmse: 0.32077 | val_1_rmse: 0.32335 |  0:02:40s
epoch 81 | loss: 0.10779 | val_0_rmse: 0.33718 | val_1_rmse: 0.33957 |  0:02:42s
epoch 82 | loss: 0.11169 | val_0_rmse: 0.36576 | val_1_rmse: 0.36919 |  0:02:44s
epoch 83 | loss: 0.11115 | val_0_rmse: 0.3461  | val_1_rmse: 0.34549 |  0:02:46s
epoch 84 | loss: 0.13121 | val_0_rmse: 0.38681 | val_1_rmse: 0.39081 |  0:02:48s
epoch 85 | loss: 0.11304 | val_0_rmse: 0.32835 | val_1_rmse: 0.33158 |  0:02:50s
epoch 86 | loss: 0.12206 | val_0_rmse: 0.34847 | val_1_rmse: 0.35152 |  0:02:52s
epoch 87 | loss: 0.11917 | val_0_rmse: 0.33655 | val_1_rmse: 0.3384  |  0:02:54s
epoch 88 | loss: 0.12156 | val_0_rmse: 0.32598 | val_1_rmse: 0.3291  |  0:02:56s
epoch 89 | loss: 0.12871 | val_0_rmse: 0.33026 | val_1_rmse: 0.33414 |  0:02:58s
epoch 90 | loss: 0.12444 | val_0_rmse: 0.34928 | val_1_rmse: 0.35487 |  0:03:00s
epoch 91 | loss: 0.15182 | val_0_rmse: 0.32453 | val_1_rmse: 0.33051 |  0:03:02s
epoch 92 | loss: 0.10815 | val_0_rmse: 0.31761 | val_1_rmse: 0.31838 |  0:03:04s
epoch 93 | loss: 0.10711 | val_0_rmse: 0.32406 | val_1_rmse: 0.32784 |  0:03:06s
epoch 94 | loss: 0.11214 | val_0_rmse: 0.31995 | val_1_rmse: 0.32181 |  0:03:08s
epoch 95 | loss: 0.1084  | val_0_rmse: 0.32176 | val_1_rmse: 0.32425 |  0:03:10s
epoch 96 | loss: 0.12347 | val_0_rmse: 0.31058 | val_1_rmse: 0.31009 |  0:03:12s
epoch 97 | loss: 0.10517 | val_0_rmse: 0.3225  | val_1_rmse: 0.32307 |  0:03:14s
epoch 98 | loss: 0.10477 | val_0_rmse: 0.3154  | val_1_rmse: 0.31376 |  0:03:16s
epoch 99 | loss: 0.10652 | val_0_rmse: 0.33369 | val_1_rmse: 0.3398  |  0:03:18s
epoch 100| loss: 0.10579 | val_0_rmse: 0.31447 | val_1_rmse: 0.31447 |  0:03:20s
epoch 101| loss: 0.11029 | val_0_rmse: 0.309   | val_1_rmse: 0.31201 |  0:03:22s
epoch 102| loss: 0.10487 | val_0_rmse: 0.31931 | val_1_rmse: 0.32274 |  0:03:24s
epoch 103| loss: 0.10646 | val_0_rmse: 0.3141  | val_1_rmse: 0.31368 |  0:03:26s
epoch 104| loss: 0.11346 | val_0_rmse: 0.33518 | val_1_rmse: 0.33306 |  0:03:28s
epoch 105| loss: 0.113   | val_0_rmse: 0.33861 | val_1_rmse: 0.34367 |  0:03:30s
epoch 106| loss: 0.10092 | val_0_rmse: 0.32137 | val_1_rmse: 0.32229 |  0:03:32s
epoch 107| loss: 0.12027 | val_0_rmse: 0.38228 | val_1_rmse: 0.3815  |  0:03:34s
epoch 108| loss: 0.1282  | val_0_rmse: 0.36727 | val_1_rmse: 0.37235 |  0:03:36s
epoch 109| loss: 0.12585 | val_0_rmse: 0.37716 | val_1_rmse: 0.37556 |  0:03:38s
epoch 110| loss: 0.12475 | val_0_rmse: 0.40485 | val_1_rmse: 0.40722 |  0:03:40s
epoch 111| loss: 0.12967 | val_0_rmse: 0.37196 | val_1_rmse: 0.37032 |  0:03:42s
epoch 112| loss: 0.125   | val_0_rmse: 0.32169 | val_1_rmse: 0.32568 |  0:03:44s
epoch 113| loss: 0.11239 | val_0_rmse: 0.32158 | val_1_rmse: 0.32121 |  0:03:46s
epoch 114| loss: 0.10224 | val_0_rmse: 0.31683 | val_1_rmse: 0.31819 |  0:03:48s
epoch 115| loss: 0.10208 | val_0_rmse: 0.34556 | val_1_rmse: 0.34749 |  0:03:50s
epoch 116| loss: 0.11227 | val_0_rmse: 0.38868 | val_1_rmse: 0.3864  |  0:03:52s
epoch 117| loss: 0.1254  | val_0_rmse: 0.38536 | val_1_rmse: 0.38958 |  0:03:54s
epoch 118| loss: 0.12065 | val_0_rmse: 0.34405 | val_1_rmse: 0.34278 |  0:03:56s
epoch 119| loss: 0.11645 | val_0_rmse: 0.34351 | val_1_rmse: 0.3462  |  0:03:58s
epoch 120| loss: 0.10521 | val_0_rmse: 0.31709 | val_1_rmse: 0.32079 |  0:04:00s
epoch 121| loss: 0.12465 | val_0_rmse: 0.30717 | val_1_rmse: 0.30965 |  0:04:02s
epoch 122| loss: 0.10856 | val_0_rmse: 0.35078 | val_1_rmse: 0.35128 |  0:04:04s
epoch 123| loss: 0.11234 | val_0_rmse: 0.30606 | val_1_rmse: 0.31001 |  0:04:06s
epoch 124| loss: 0.15361 | val_0_rmse: 0.32288 | val_1_rmse: 0.3287  |  0:04:08s
epoch 125| loss: 0.13739 | val_0_rmse: 0.3898  | val_1_rmse: 0.39431 |  0:04:10s
epoch 126| loss: 0.13162 | val_0_rmse: 0.40378 | val_1_rmse: 0.40189 |  0:04:12s
epoch 127| loss: 0.1263  | val_0_rmse: 0.34241 | val_1_rmse: 0.34847 |  0:04:14s
epoch 128| loss: 0.12343 | val_0_rmse: 0.34414 | val_1_rmse: 0.34522 |  0:04:16s
epoch 129| loss: 0.11003 | val_0_rmse: 0.30641 | val_1_rmse: 0.30993 |  0:04:17s
epoch 130| loss: 0.10455 | val_0_rmse: 0.30293 | val_1_rmse: 0.30646 |  0:04:20s
epoch 131| loss: 0.1034  | val_0_rmse: 0.31813 | val_1_rmse: 0.31869 |  0:04:21s
epoch 132| loss: 0.10236 | val_0_rmse: 0.3184  | val_1_rmse: 0.31812 |  0:04:23s
epoch 133| loss: 0.11237 | val_0_rmse: 0.31846 | val_1_rmse: 0.31908 |  0:04:25s
epoch 134| loss: 0.12495 | val_0_rmse: 0.31586 | val_1_rmse: 0.32066 |  0:04:27s
epoch 135| loss: 0.10859 | val_0_rmse: 0.32839 | val_1_rmse: 0.32985 |  0:04:29s
epoch 136| loss: 0.10185 | val_0_rmse: 0.33043 | val_1_rmse: 0.3353  |  0:04:31s
epoch 137| loss: 0.11294 | val_0_rmse: 0.33311 | val_1_rmse: 0.33541 |  0:04:33s
epoch 138| loss: 0.10832 | val_0_rmse: 0.33311 | val_1_rmse: 0.33186 |  0:04:35s
epoch 139| loss: 0.09985 | val_0_rmse: 0.32617 | val_1_rmse: 0.32681 |  0:04:37s
epoch 140| loss: 0.1017  | val_0_rmse: 0.32805 | val_1_rmse: 0.33393 |  0:04:39s
epoch 141| loss: 0.09944 | val_0_rmse: 0.30728 | val_1_rmse: 0.31061 |  0:04:41s
epoch 142| loss: 0.10299 | val_0_rmse: 0.32387 | val_1_rmse: 0.32524 |  0:04:43s
epoch 143| loss: 0.10191 | val_0_rmse: 0.31572 | val_1_rmse: 0.31896 |  0:04:45s
epoch 144| loss: 0.10441 | val_0_rmse: 0.31293 | val_1_rmse: 0.3148  |  0:04:47s
epoch 145| loss: 0.09658 | val_0_rmse: 0.30133 | val_1_rmse: 0.30465 |  0:04:49s
epoch 146| loss: 0.09773 | val_0_rmse: 0.32844 | val_1_rmse: 0.32914 |  0:04:51s
epoch 147| loss: 0.10083 | val_0_rmse: 0.30912 | val_1_rmse: 0.31363 |  0:04:53s
epoch 148| loss: 0.10342 | val_0_rmse: 0.3308  | val_1_rmse: 0.32853 |  0:04:55s
epoch 149| loss: 0.10417 | val_0_rmse: 0.32824 | val_1_rmse: 0.32621 |  0:04:57s
Stop training because you reached max_epochs = 150 with best_epoch = 145 and best_val_1_rmse = 0.30465
Best weights from best epoch are automatically used!
ended training at: 04:46:58
Feature importance:
[('Area', 0.20413994748080486), ('Baths', 0.3198900171228788), ('Beds', 0.2743616408591581), ('Latitude', 0.03465513560741238), ('Longitude', 0.05695465919249445), ('Month', 0.10999859973725147), ('Year', 0.0)]
Mean squared error is of 1031572286.131941
Mean absolute error:21048.962710092186
MAPE:0.23475689557435628
R2 score:0.7541161775350197
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:46:58
epoch 0  | loss: 25.99492| val_0_rmse: 1.30189 | val_1_rmse: 1.30643 |  0:00:01s
epoch 1  | loss: 0.43177 | val_0_rmse: 0.69909 | val_1_rmse: 0.69892 |  0:00:03s
epoch 2  | loss: 0.32049 | val_0_rmse: 0.66679 | val_1_rmse: 0.66964 |  0:00:05s
epoch 3  | loss: 0.24847 | val_0_rmse: 0.43471 | val_1_rmse: 0.43763 |  0:00:07s
epoch 4  | loss: 0.22814 | val_0_rmse: 0.45458 | val_1_rmse: 0.45697 |  0:00:09s
epoch 5  | loss: 0.22799 | val_0_rmse: 0.44626 | val_1_rmse: 0.44651 |  0:00:11s
epoch 6  | loss: 0.18154 | val_0_rmse: 0.42525 | val_1_rmse: 0.42619 |  0:00:13s
epoch 7  | loss: 0.19676 | val_0_rmse: 0.38356 | val_1_rmse: 0.38787 |  0:00:15s
epoch 8  | loss: 0.16747 | val_0_rmse: 0.38252 | val_1_rmse: 0.38593 |  0:00:17s
epoch 9  | loss: 0.15145 | val_0_rmse: 0.37813 | val_1_rmse: 0.38147 |  0:00:19s
epoch 10 | loss: 0.15313 | val_0_rmse: 0.3734  | val_1_rmse: 0.37451 |  0:00:21s
epoch 11 | loss: 0.14546 | val_0_rmse: 0.38167 | val_1_rmse: 0.38228 |  0:00:23s
epoch 12 | loss: 0.14394 | val_0_rmse: 0.3565  | val_1_rmse: 0.35663 |  0:00:25s
epoch 13 | loss: 0.14336 | val_0_rmse: 0.45606 | val_1_rmse: 0.45864 |  0:00:27s
epoch 14 | loss: 0.16625 | val_0_rmse: 0.39029 | val_1_rmse: 0.39096 |  0:00:29s
epoch 15 | loss: 0.16219 | val_0_rmse: 0.3949  | val_1_rmse: 0.39736 |  0:00:31s
epoch 16 | loss: 0.15897 | val_0_rmse: 0.4165  | val_1_rmse: 0.41812 |  0:00:33s
epoch 17 | loss: 0.16004 | val_0_rmse: 0.39415 | val_1_rmse: 0.39745 |  0:00:35s
epoch 18 | loss: 0.15995 | val_0_rmse: 0.40887 | val_1_rmse: 0.40995 |  0:00:37s
epoch 19 | loss: 0.15614 | val_0_rmse: 0.39478 | val_1_rmse: 0.39688 |  0:00:39s
epoch 20 | loss: 0.15381 | val_0_rmse: 0.39225 | val_1_rmse: 0.39348 |  0:00:41s
epoch 21 | loss: 0.15346 | val_0_rmse: 0.41115 | val_1_rmse: 0.41451 |  0:00:43s
epoch 22 | loss: 0.15051 | val_0_rmse: 0.40407 | val_1_rmse: 0.40513 |  0:00:45s
epoch 23 | loss: 0.1363  | val_0_rmse: 0.38881 | val_1_rmse: 0.39119 |  0:00:47s
epoch 24 | loss: 0.13843 | val_0_rmse: 0.36192 | val_1_rmse: 0.36442 |  0:00:49s
epoch 25 | loss: 0.12702 | val_0_rmse: 0.39559 | val_1_rmse: 0.39636 |  0:00:51s
epoch 26 | loss: 0.16265 | val_0_rmse: 0.40539 | val_1_rmse: 0.41104 |  0:00:53s
epoch 27 | loss: 0.1256  | val_0_rmse: 0.33478 | val_1_rmse: 0.33695 |  0:00:55s
epoch 28 | loss: 0.12553 | val_0_rmse: 0.36404 | val_1_rmse: 0.36841 |  0:00:57s
epoch 29 | loss: 0.12263 | val_0_rmse: 0.34109 | val_1_rmse: 0.34415 |  0:00:59s
epoch 30 | loss: 0.12111 | val_0_rmse: 0.36223 | val_1_rmse: 0.3652  |  0:01:01s
epoch 31 | loss: 0.1418  | val_0_rmse: 0.35516 | val_1_rmse: 0.35514 |  0:01:03s
epoch 32 | loss: 0.16149 | val_0_rmse: 0.55314 | val_1_rmse: 0.55602 |  0:01:05s
epoch 33 | loss: 0.135   | val_0_rmse: 0.36771 | val_1_rmse: 0.37143 |  0:01:07s
epoch 34 | loss: 0.1265  | val_0_rmse: 0.3736  | val_1_rmse: 0.37755 |  0:01:09s
epoch 35 | loss: 0.14624 | val_0_rmse: 0.33548 | val_1_rmse: 0.3379  |  0:01:11s
epoch 36 | loss: 0.1389  | val_0_rmse: 0.35941 | val_1_rmse: 0.36166 |  0:01:13s
epoch 37 | loss: 0.12865 | val_0_rmse: 0.34026 | val_1_rmse: 0.3406  |  0:01:15s
epoch 38 | loss: 0.12323 | val_0_rmse: 0.33559 | val_1_rmse: 0.33704 |  0:01:17s
epoch 39 | loss: 0.12293 | val_0_rmse: 0.33416 | val_1_rmse: 0.33605 |  0:01:19s
epoch 40 | loss: 0.1176  | val_0_rmse: 0.35757 | val_1_rmse: 0.35964 |  0:01:21s
epoch 41 | loss: 0.1165  | val_0_rmse: 0.33287 | val_1_rmse: 0.33528 |  0:01:23s
epoch 42 | loss: 0.11742 | val_0_rmse: 0.35711 | val_1_rmse: 0.35986 |  0:01:25s
epoch 43 | loss: 0.12691 | val_0_rmse: 0.34187 | val_1_rmse: 0.34339 |  0:01:27s
epoch 44 | loss: 0.12074 | val_0_rmse: 0.32941 | val_1_rmse: 0.33166 |  0:01:29s
epoch 45 | loss: 0.12377 | val_0_rmse: 0.35372 | val_1_rmse: 0.35432 |  0:01:31s
epoch 46 | loss: 0.12352 | val_0_rmse: 0.33514 | val_1_rmse: 0.33767 |  0:01:33s
epoch 47 | loss: 0.12002 | val_0_rmse: 0.34368 | val_1_rmse: 0.34526 |  0:01:35s
epoch 48 | loss: 0.12661 | val_0_rmse: 0.32902 | val_1_rmse: 0.33008 |  0:01:37s
epoch 49 | loss: 0.11501 | val_0_rmse: 0.34645 | val_1_rmse: 0.34695 |  0:01:39s
epoch 50 | loss: 0.13148 | val_0_rmse: 0.38517 | val_1_rmse: 0.38598 |  0:01:41s
epoch 51 | loss: 0.11658 | val_0_rmse: 0.36046 | val_1_rmse: 0.36229 |  0:01:43s
epoch 52 | loss: 0.1251  | val_0_rmse: 0.3606  | val_1_rmse: 0.36053 |  0:01:45s
epoch 53 | loss: 0.16237 | val_0_rmse: 0.36636 | val_1_rmse: 0.36635 |  0:01:47s
epoch 54 | loss: 0.14381 | val_0_rmse: 0.34551 | val_1_rmse: 0.34687 |  0:01:49s
epoch 55 | loss: 0.14122 | val_0_rmse: 0.34958 | val_1_rmse: 0.35278 |  0:01:51s
epoch 56 | loss: 0.13957 | val_0_rmse: 0.33319 | val_1_rmse: 0.3339  |  0:01:53s
epoch 57 | loss: 0.13219 | val_0_rmse: 0.37475 | val_1_rmse: 0.37712 |  0:01:55s
epoch 58 | loss: 0.14076 | val_0_rmse: 0.39126 | val_1_rmse: 0.3919  |  0:01:57s
epoch 59 | loss: 0.13919 | val_0_rmse: 0.38591 | val_1_rmse: 0.38756 |  0:01:59s
epoch 60 | loss: 0.13476 | val_0_rmse: 0.41679 | val_1_rmse: 0.41814 |  0:02:01s
epoch 61 | loss: 0.1188  | val_0_rmse: 0.32019 | val_1_rmse: 0.32147 |  0:02:03s
epoch 62 | loss: 0.11978 | val_0_rmse: 0.33888 | val_1_rmse: 0.34227 |  0:02:05s
epoch 63 | loss: 0.14433 | val_0_rmse: 0.36912 | val_1_rmse: 0.37291 |  0:02:06s
epoch 64 | loss: 0.14206 | val_0_rmse: 0.34537 | val_1_rmse: 0.34791 |  0:02:08s
epoch 65 | loss: 0.1319  | val_0_rmse: 0.34933 | val_1_rmse: 0.35378 |  0:02:10s
epoch 66 | loss: 0.1186  | val_0_rmse: 0.32549 | val_1_rmse: 0.32742 |  0:02:12s
epoch 67 | loss: 0.13411 | val_0_rmse: 0.37653 | val_1_rmse: 0.37823 |  0:02:14s
epoch 68 | loss: 0.12134 | val_0_rmse: 0.32361 | val_1_rmse: 0.32632 |  0:02:16s
epoch 69 | loss: 0.13346 | val_0_rmse: 0.34652 | val_1_rmse: 0.35005 |  0:02:18s
epoch 70 | loss: 0.13994 | val_0_rmse: 0.36328 | val_1_rmse: 0.3655  |  0:02:20s
epoch 71 | loss: 0.13419 | val_0_rmse: 0.33353 | val_1_rmse: 0.33614 |  0:02:22s
epoch 72 | loss: 0.13385 | val_0_rmse: 0.32484 | val_1_rmse: 0.32837 |  0:02:24s
epoch 73 | loss: 0.13591 | val_0_rmse: 0.33512 | val_1_rmse: 0.33677 |  0:02:26s
epoch 74 | loss: 0.1349  | val_0_rmse: 0.31886 | val_1_rmse: 0.31953 |  0:02:28s
epoch 75 | loss: 0.12463 | val_0_rmse: 0.34269 | val_1_rmse: 0.34394 |  0:02:30s
epoch 76 | loss: 0.11998 | val_0_rmse: 0.33367 | val_1_rmse: 0.33241 |  0:02:32s
epoch 77 | loss: 0.11344 | val_0_rmse: 0.35033 | val_1_rmse: 0.35067 |  0:02:34s
epoch 78 | loss: 0.13574 | val_0_rmse: 0.40225 | val_1_rmse: 0.40664 |  0:02:36s
epoch 79 | loss: 0.13399 | val_0_rmse: 0.393   | val_1_rmse: 0.39477 |  0:02:38s
epoch 80 | loss: 0.1375  | val_0_rmse: 0.35155 | val_1_rmse: 0.35379 |  0:02:40s
epoch 81 | loss: 0.12501 | val_0_rmse: 0.3287  | val_1_rmse: 0.33143 |  0:02:42s
epoch 82 | loss: 0.11688 | val_0_rmse: 0.32521 | val_1_rmse: 0.32788 |  0:02:44s
epoch 83 | loss: 0.13774 | val_0_rmse: 0.333   | val_1_rmse: 0.3335  |  0:02:46s
epoch 84 | loss: 0.1372  | val_0_rmse: 0.35576 | val_1_rmse: 0.35977 |  0:02:48s
epoch 85 | loss: 0.13619 | val_0_rmse: 0.34111 | val_1_rmse: 0.34269 |  0:02:50s
epoch 86 | loss: 0.13181 | val_0_rmse: 0.40976 | val_1_rmse: 0.41417 |  0:02:53s
epoch 87 | loss: 0.12648 | val_0_rmse: 0.3182  | val_1_rmse: 0.31802 |  0:02:55s
epoch 88 | loss: 0.11524 | val_0_rmse: 0.31383 | val_1_rmse: 0.31534 |  0:02:57s
epoch 89 | loss: 0.11047 | val_0_rmse: 0.37046 | val_1_rmse: 0.37403 |  0:02:59s
epoch 90 | loss: 0.16371 | val_0_rmse: 0.31449 | val_1_rmse: 0.31626 |  0:03:01s
epoch 91 | loss: 0.12485 | val_0_rmse: 0.32876 | val_1_rmse: 0.32998 |  0:03:03s
epoch 92 | loss: 0.1693  | val_0_rmse: 0.373   | val_1_rmse: 0.37522 |  0:03:05s
epoch 93 | loss: 0.12131 | val_0_rmse: 0.31415 | val_1_rmse: 0.31664 |  0:03:07s
epoch 94 | loss: 0.11471 | val_0_rmse: 0.34758 | val_1_rmse: 0.34781 |  0:03:09s
epoch 95 | loss: 0.11065 | val_0_rmse: 0.33712 | val_1_rmse: 0.3384  |  0:03:11s
epoch 96 | loss: 0.10815 | val_0_rmse: 0.31881 | val_1_rmse: 0.32052 |  0:03:13s
epoch 97 | loss: 0.10991 | val_0_rmse: 0.32851 | val_1_rmse: 0.33117 |  0:03:15s
epoch 98 | loss: 0.11489 | val_0_rmse: 0.35938 | val_1_rmse: 0.36377 |  0:03:17s
epoch 99 | loss: 0.11157 | val_0_rmse: 0.31348 | val_1_rmse: 0.31627 |  0:03:19s
epoch 100| loss: 0.10508 | val_0_rmse: 0.33071 | val_1_rmse: 0.33126 |  0:03:21s
epoch 101| loss: 0.1091  | val_0_rmse: 0.31427 | val_1_rmse: 0.31835 |  0:03:23s
epoch 102| loss: 0.12321 | val_0_rmse: 0.34759 | val_1_rmse: 0.34885 |  0:03:25s
epoch 103| loss: 0.10846 | val_0_rmse: 0.32915 | val_1_rmse: 0.33027 |  0:03:27s
epoch 104| loss: 0.13865 | val_0_rmse: 0.40483 | val_1_rmse: 0.40812 |  0:03:29s
epoch 105| loss: 0.18313 | val_0_rmse: 0.34135 | val_1_rmse: 0.34339 |  0:03:31s
epoch 106| loss: 0.17512 | val_0_rmse: 0.3274  | val_1_rmse: 0.32789 |  0:03:33s
epoch 107| loss: 0.16803 | val_0_rmse: 0.44524 | val_1_rmse: 0.44651 |  0:03:35s
epoch 108| loss: 0.16716 | val_0_rmse: 0.36273 | val_1_rmse: 0.36488 |  0:03:37s
epoch 109| loss: 0.12878 | val_0_rmse: 0.33442 | val_1_rmse: 0.33753 |  0:03:39s
epoch 110| loss: 0.12704 | val_0_rmse: 0.35013 | val_1_rmse: 0.35191 |  0:03:40s
epoch 111| loss: 0.10751 | val_0_rmse: 0.31795 | val_1_rmse: 0.32058 |  0:03:42s
epoch 112| loss: 0.1054  | val_0_rmse: 0.31536 | val_1_rmse: 0.31815 |  0:03:44s
epoch 113| loss: 0.10395 | val_0_rmse: 0.32084 | val_1_rmse: 0.32362 |  0:03:46s
epoch 114| loss: 0.09973 | val_0_rmse: 0.31969 | val_1_rmse: 0.32039 |  0:03:48s
epoch 115| loss: 0.10478 | val_0_rmse: 0.31889 | val_1_rmse: 0.3215  |  0:03:50s
epoch 116| loss: 0.1033  | val_0_rmse: 0.35941 | val_1_rmse: 0.36118 |  0:03:52s
epoch 117| loss: 0.10843 | val_0_rmse: 0.31818 | val_1_rmse: 0.31952 |  0:03:54s
epoch 118| loss: 0.11075 | val_0_rmse: 0.38834 | val_1_rmse: 0.3909  |  0:03:56s

Early stopping occured at epoch 118 with best_epoch = 88 and best_val_1_rmse = 0.31534
Best weights from best epoch are automatically used!
ended training at: 04:50:55
Feature importance:
[('Area', 0.24742233208043898), ('Baths', 0.13636564721900402), ('Beds', 0.039257605132860834), ('Latitude', 0.1101163931380332), ('Longitude', 0.1434602035260371), ('Month', 0.25645042046766664), ('Year', 0.06692739843595925)]
Mean squared error is of 1077295726.7791038
Mean absolute error:21496.043924585574
MAPE:0.24310663486103953
R2 score:0.7233144055936143
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:50:55
epoch 0  | loss: 25.42362| val_0_rmse: 1.71051 | val_1_rmse: 1.71754 |  0:00:01s
epoch 1  | loss: 0.37346 | val_0_rmse: 0.57207 | val_1_rmse: 0.57471 |  0:00:03s
epoch 2  | loss: 0.24688 | val_0_rmse: 0.44152 | val_1_rmse: 0.44719 |  0:00:05s
epoch 3  | loss: 0.33068 | val_0_rmse: 0.44961 | val_1_rmse: 0.44591 |  0:00:08s
epoch 4  | loss: 0.22202 | val_0_rmse: 0.42724 | val_1_rmse: 0.43345 |  0:00:09s
epoch 5  | loss: 0.20368 | val_0_rmse: 0.42131 | val_1_rmse: 0.43    |  0:00:11s
epoch 6  | loss: 0.1961  | val_0_rmse: 0.38533 | val_1_rmse: 0.3892  |  0:00:13s
epoch 7  | loss: 0.18254 | val_0_rmse: 0.37865 | val_1_rmse: 0.38434 |  0:00:15s
epoch 8  | loss: 0.15065 | val_0_rmse: 0.36495 | val_1_rmse: 0.36671 |  0:00:17s
epoch 9  | loss: 0.15649 | val_0_rmse: 0.36041 | val_1_rmse: 0.36451 |  0:00:19s
epoch 10 | loss: 0.16682 | val_0_rmse: 0.38766 | val_1_rmse: 0.3888  |  0:00:21s
epoch 11 | loss: 0.16809 | val_0_rmse: 0.37372 | val_1_rmse: 0.3766  |  0:00:23s
epoch 12 | loss: 0.16521 | val_0_rmse: 0.37571 | val_1_rmse: 0.38331 |  0:00:25s
epoch 13 | loss: 0.15905 | val_0_rmse: 0.3612  | val_1_rmse: 0.36232 |  0:00:27s
epoch 14 | loss: 0.15551 | val_0_rmse: 0.3845  | val_1_rmse: 0.3887  |  0:00:29s
epoch 15 | loss: 0.15781 | val_0_rmse: 0.36247 | val_1_rmse: 0.36388 |  0:00:31s
epoch 16 | loss: 0.1524  | val_0_rmse: 0.34741 | val_1_rmse: 0.35016 |  0:00:33s
epoch 17 | loss: 0.12897 | val_0_rmse: 0.34603 | val_1_rmse: 0.34903 |  0:00:35s
epoch 18 | loss: 0.19741 | val_0_rmse: 0.38682 | val_1_rmse: 0.39324 |  0:00:37s
epoch 19 | loss: 0.20024 | val_0_rmse: 0.3677  | val_1_rmse: 0.37439 |  0:00:39s
epoch 20 | loss: 0.20137 | val_0_rmse: 0.38495 | val_1_rmse: 0.38511 |  0:00:41s
epoch 21 | loss: 0.19714 | val_0_rmse: 0.47334 | val_1_rmse: 0.47977 |  0:00:43s
epoch 22 | loss: 0.15053 | val_0_rmse: 0.3323  | val_1_rmse: 0.33483 |  0:00:45s
epoch 23 | loss: 0.12505 | val_0_rmse: 0.33472 | val_1_rmse: 0.33877 |  0:00:47s
epoch 24 | loss: 0.12539 | val_0_rmse: 0.34513 | val_1_rmse: 0.34554 |  0:00:49s
epoch 25 | loss: 0.12256 | val_0_rmse: 0.34283 | val_1_rmse: 0.34426 |  0:00:51s
epoch 26 | loss: 0.12022 | val_0_rmse: 0.32922 | val_1_rmse: 0.33256 |  0:00:53s
epoch 27 | loss: 0.12363 | val_0_rmse: 0.34216 | val_1_rmse: 0.34351 |  0:00:55s
epoch 28 | loss: 0.18312 | val_0_rmse: 0.37742 | val_1_rmse: 0.37895 |  0:00:57s
epoch 29 | loss: 0.12635 | val_0_rmse: 0.33754 | val_1_rmse: 0.34027 |  0:00:59s
epoch 30 | loss: 0.12059 | val_0_rmse: 0.36313 | val_1_rmse: 0.36861 |  0:01:01s
epoch 31 | loss: 0.12257 | val_0_rmse: 0.33138 | val_1_rmse: 0.33426 |  0:01:03s
epoch 32 | loss: 0.11798 | val_0_rmse: 0.3252  | val_1_rmse: 0.32956 |  0:01:05s
epoch 33 | loss: 0.12802 | val_0_rmse: 0.38206 | val_1_rmse: 0.38379 |  0:01:07s
epoch 34 | loss: 0.14219 | val_0_rmse: 0.33207 | val_1_rmse: 0.33364 |  0:01:09s
epoch 35 | loss: 0.13804 | val_0_rmse: 0.33112 | val_1_rmse: 0.3345  |  0:01:11s
epoch 36 | loss: 0.13394 | val_0_rmse: 0.35532 | val_1_rmse: 0.35682 |  0:01:13s
epoch 37 | loss: 0.12143 | val_0_rmse: 0.31897 | val_1_rmse: 0.32166 |  0:01:15s
epoch 38 | loss: 0.11753 | val_0_rmse: 0.33914 | val_1_rmse: 0.33697 |  0:01:17s
epoch 39 | loss: 0.18316 | val_0_rmse: 0.38153 | val_1_rmse: 0.38526 |  0:01:19s
epoch 40 | loss: 0.13878 | val_0_rmse: 0.33495 | val_1_rmse: 0.33501 |  0:01:21s
epoch 41 | loss: 0.13458 | val_0_rmse: 0.34867 | val_1_rmse: 0.353   |  0:01:23s
epoch 42 | loss: 0.16158 | val_0_rmse: 0.51605 | val_1_rmse: 0.52086 |  0:01:25s
epoch 43 | loss: 0.18839 | val_0_rmse: 0.41152 | val_1_rmse: 0.40982 |  0:01:27s
epoch 44 | loss: 0.19418 | val_0_rmse: 0.33406 | val_1_rmse: 0.33605 |  0:01:29s
epoch 45 | loss: 0.1677  | val_0_rmse: 0.34292 | val_1_rmse: 0.34307 |  0:01:31s
epoch 46 | loss: 0.14362 | val_0_rmse: 0.36423 | val_1_rmse: 0.36606 |  0:01:33s
epoch 47 | loss: 0.11713 | val_0_rmse: 0.32023 | val_1_rmse: 0.32186 |  0:01:35s
epoch 48 | loss: 0.11384 | val_0_rmse: 0.32244 | val_1_rmse: 0.32407 |  0:01:37s
epoch 49 | loss: 0.11563 | val_0_rmse: 0.36077 | val_1_rmse: 0.36591 |  0:01:39s
epoch 50 | loss: 0.12814 | val_0_rmse: 0.32496 | val_1_rmse: 0.32753 |  0:01:41s
epoch 51 | loss: 0.13174 | val_0_rmse: 0.36198 | val_1_rmse: 0.3634  |  0:01:43s
epoch 52 | loss: 0.15181 | val_0_rmse: 0.32476 | val_1_rmse: 0.32468 |  0:01:45s
epoch 53 | loss: 0.176   | val_0_rmse: 0.32892 | val_1_rmse: 0.33039 |  0:01:47s
epoch 54 | loss: 0.18994 | val_0_rmse: 0.41875 | val_1_rmse: 0.41891 |  0:01:49s
epoch 55 | loss: 0.18096 | val_0_rmse: 0.48294 | val_1_rmse: 0.4875  |  0:01:51s
epoch 56 | loss: 0.18664 | val_0_rmse: 0.36382 | val_1_rmse: 0.36425 |  0:01:53s
epoch 57 | loss: 0.18521 | val_0_rmse: 0.34054 | val_1_rmse: 0.3407  |  0:01:55s
epoch 58 | loss: 0.17934 | val_0_rmse: 0.44987 | val_1_rmse: 0.45418 |  0:01:57s
epoch 59 | loss: 0.18287 | val_0_rmse: 0.38882 | val_1_rmse: 0.3888  |  0:01:59s
epoch 60 | loss: 0.16219 | val_0_rmse: 0.33699 | val_1_rmse: 0.33853 |  0:02:01s
epoch 61 | loss: 0.11787 | val_0_rmse: 0.31836 | val_1_rmse: 0.32228 |  0:02:03s
epoch 62 | loss: 0.17489 | val_0_rmse: 0.36244 | val_1_rmse: 0.36549 |  0:02:05s
epoch 63 | loss: 0.18274 | val_0_rmse: 0.41433 | val_1_rmse: 0.41457 |  0:02:07s
epoch 64 | loss: 0.17765 | val_0_rmse: 0.44662 | val_1_rmse: 0.45044 |  0:02:09s
epoch 65 | loss: 0.18079 | val_0_rmse: 0.3679  | val_1_rmse: 0.36683 |  0:02:11s
epoch 66 | loss: 0.14136 | val_0_rmse: 0.3254  | val_1_rmse: 0.32978 |  0:02:13s
epoch 67 | loss: 0.11691 | val_0_rmse: 0.42233 | val_1_rmse: 0.42274 |  0:02:14s

Early stopping occured at epoch 67 with best_epoch = 37 and best_val_1_rmse = 0.32166
Best weights from best epoch are automatically used!
ended training at: 04:53:11
Feature importance:
[('Area', 0.35317746767055685), ('Baths', 0.1610991700447042), ('Beds', 0.02655207828058505), ('Latitude', 0.035860986320209724), ('Longitude', 0.32391221024386413), ('Month', 0.09939805634671492), ('Year', 3.109336506040737e-08)]
Mean squared error is of 1202244494.0412457
Mean absolute error:23075.628629426476
MAPE:0.25343275826228656
R2 score:0.6965463648573816
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:53:11
epoch 0  | loss: 25.5395 | val_0_rmse: 1.07505 | val_1_rmse: 1.06708 |  0:00:02s
epoch 1  | loss: 0.46268 | val_0_rmse: 0.71455 | val_1_rmse: 0.71213 |  0:00:03s
epoch 2  | loss: 0.2581  | val_0_rmse: 0.45067 | val_1_rmse: 0.4548  |  0:00:06s
epoch 3  | loss: 0.22123 | val_0_rmse: 0.44534 | val_1_rmse: 0.45058 |  0:00:07s
epoch 4  | loss: 0.1849  | val_0_rmse: 0.41983 | val_1_rmse: 0.42619 |  0:00:09s
epoch 5  | loss: 0.17112 | val_0_rmse: 0.41357 | val_1_rmse: 0.41778 |  0:00:11s
epoch 6  | loss: 0.17145 | val_0_rmse: 0.41465 | val_1_rmse: 0.41832 |  0:00:13s
epoch 7  | loss: 0.18606 | val_0_rmse: 0.43519 | val_1_rmse: 0.44507 |  0:00:15s
epoch 8  | loss: 0.19243 | val_0_rmse: 0.38768 | val_1_rmse: 0.38846 |  0:00:17s
epoch 9  | loss: 0.16904 | val_0_rmse: 0.41146 | val_1_rmse: 0.42036 |  0:00:19s
epoch 10 | loss: 0.16792 | val_0_rmse: 0.39292 | val_1_rmse: 0.39657 |  0:00:21s
epoch 11 | loss: 0.15864 | val_0_rmse: 0.37088 | val_1_rmse: 0.37717 |  0:00:23s
epoch 12 | loss: 0.15668 | val_0_rmse: 0.387   | val_1_rmse: 0.38915 |  0:00:25s
epoch 13 | loss: 0.15834 | val_0_rmse: 0.37176 | val_1_rmse: 0.3799  |  0:00:27s
epoch 14 | loss: 0.15369 | val_0_rmse: 0.39794 | val_1_rmse: 0.4024  |  0:00:29s
epoch 15 | loss: 0.15121 | val_0_rmse: 0.38315 | val_1_rmse: 0.38758 |  0:00:31s
epoch 16 | loss: 0.14855 | val_0_rmse: 0.37733 | val_1_rmse: 0.3804  |  0:00:33s
epoch 17 | loss: 0.14523 | val_0_rmse: 0.36647 | val_1_rmse: 0.37113 |  0:00:35s
epoch 18 | loss: 0.14405 | val_0_rmse: 0.36255 | val_1_rmse: 0.36568 |  0:00:37s
epoch 19 | loss: 0.14449 | val_0_rmse: 0.39224 | val_1_rmse: 0.39779 |  0:00:39s
epoch 20 | loss: 0.14673 | val_0_rmse: 0.44089 | val_1_rmse: 0.44176 |  0:00:41s
epoch 21 | loss: 0.13358 | val_0_rmse: 0.3326  | val_1_rmse: 0.33944 |  0:00:43s
epoch 22 | loss: 0.15634 | val_0_rmse: 0.47258 | val_1_rmse: 0.46987 |  0:00:45s
epoch 23 | loss: 0.14238 | val_0_rmse: 0.34716 | val_1_rmse: 0.35207 |  0:00:47s
epoch 24 | loss: 0.12221 | val_0_rmse: 0.34395 | val_1_rmse: 0.34815 |  0:00:49s
epoch 25 | loss: 0.13832 | val_0_rmse: 0.3927  | val_1_rmse: 0.39223 |  0:00:51s
epoch 26 | loss: 0.14634 | val_0_rmse: 0.40718 | val_1_rmse: 0.41362 |  0:00:53s
epoch 27 | loss: 0.14438 | val_0_rmse: 0.32544 | val_1_rmse: 0.33148 |  0:00:55s
epoch 28 | loss: 0.13955 | val_0_rmse: 0.34119 | val_1_rmse: 0.34671 |  0:00:57s
epoch 29 | loss: 0.14033 | val_0_rmse: 0.34505 | val_1_rmse: 0.35332 |  0:00:59s
epoch 30 | loss: 0.13425 | val_0_rmse: 0.3418  | val_1_rmse: 0.34941 |  0:01:01s
epoch 31 | loss: 0.11599 | val_0_rmse: 0.32168 | val_1_rmse: 0.32872 |  0:01:03s
epoch 32 | loss: 0.13278 | val_0_rmse: 0.3455  | val_1_rmse: 0.35336 |  0:01:05s
epoch 33 | loss: 0.12458 | val_0_rmse: 0.33851 | val_1_rmse: 0.34728 |  0:01:07s
epoch 34 | loss: 0.13058 | val_0_rmse: 0.36144 | val_1_rmse: 0.36824 |  0:01:09s
epoch 35 | loss: 0.12279 | val_0_rmse: 0.3992  | val_1_rmse: 0.40356 |  0:01:11s
epoch 36 | loss: 0.14199 | val_0_rmse: 0.31841 | val_1_rmse: 0.32489 |  0:01:13s
epoch 37 | loss: 0.13327 | val_0_rmse: 0.34079 | val_1_rmse: 0.34666 |  0:01:15s
epoch 38 | loss: 0.13212 | val_0_rmse: 0.34369 | val_1_rmse: 0.35096 |  0:01:17s
epoch 39 | loss: 0.13482 | val_0_rmse: 0.34079 | val_1_rmse: 0.34483 |  0:01:19s
epoch 40 | loss: 0.12123 | val_0_rmse: 0.31383 | val_1_rmse: 0.32161 |  0:01:21s
epoch 41 | loss: 0.12751 | val_0_rmse: 0.34016 | val_1_rmse: 0.34403 |  0:01:23s
epoch 42 | loss: 0.11736 | val_0_rmse: 0.32162 | val_1_rmse: 0.33115 |  0:01:25s
epoch 43 | loss: 0.11343 | val_0_rmse: 0.34908 | val_1_rmse: 0.35233 |  0:01:27s
epoch 44 | loss: 0.1196  | val_0_rmse: 0.33124 | val_1_rmse: 0.33979 |  0:01:29s
epoch 45 | loss: 0.12177 | val_0_rmse: 0.33947 | val_1_rmse: 0.34714 |  0:01:31s
epoch 46 | loss: 0.11909 | val_0_rmse: 0.48645 | val_1_rmse: 0.49526 |  0:01:33s
epoch 47 | loss: 0.20867 | val_0_rmse: 0.36804 | val_1_rmse: 0.3748  |  0:01:35s
epoch 48 | loss: 0.16571 | val_0_rmse: 0.32763 | val_1_rmse: 0.33476 |  0:01:37s
epoch 49 | loss: 0.18299 | val_0_rmse: 0.38797 | val_1_rmse: 0.39855 |  0:01:39s
epoch 50 | loss: 0.13386 | val_0_rmse: 0.377   | val_1_rmse: 0.38412 |  0:01:41s
epoch 51 | loss: 0.10855 | val_0_rmse: 0.31158 | val_1_rmse: 0.31851 |  0:01:43s
epoch 52 | loss: 0.10706 | val_0_rmse: 0.31847 | val_1_rmse: 0.32644 |  0:01:45s
epoch 53 | loss: 0.12168 | val_0_rmse: 0.36463 | val_1_rmse: 0.37249 |  0:01:47s
epoch 54 | loss: 0.11796 | val_0_rmse: 0.3182  | val_1_rmse: 0.32526 |  0:01:49s
epoch 55 | loss: 0.10876 | val_0_rmse: 0.32695 | val_1_rmse: 0.33481 |  0:01:51s
epoch 56 | loss: 0.10436 | val_0_rmse: 0.32075 | val_1_rmse: 0.32852 |  0:01:52s
epoch 57 | loss: 0.12908 | val_0_rmse: 0.33256 | val_1_rmse: 0.33973 |  0:01:54s
epoch 58 | loss: 0.12542 | val_0_rmse: 0.31037 | val_1_rmse: 0.31724 |  0:01:56s
epoch 59 | loss: 0.12841 | val_0_rmse: 0.34753 | val_1_rmse: 0.35413 |  0:01:58s
epoch 60 | loss: 0.13029 | val_0_rmse: 0.32603 | val_1_rmse: 0.33257 |  0:02:00s
epoch 61 | loss: 0.12447 | val_0_rmse: 0.31251 | val_1_rmse: 0.31904 |  0:02:02s
epoch 62 | loss: 0.10445 | val_0_rmse: 0.32529 | val_1_rmse: 0.33245 |  0:02:04s
epoch 63 | loss: 0.10737 | val_0_rmse: 0.38794 | val_1_rmse: 0.39214 |  0:02:06s
epoch 64 | loss: 0.11275 | val_0_rmse: 0.36523 | val_1_rmse: 0.37244 |  0:02:08s
epoch 65 | loss: 0.11105 | val_0_rmse: 0.38086 | val_1_rmse: 0.38536 |  0:02:10s
epoch 66 | loss: 0.16989 | val_0_rmse: 0.31368 | val_1_rmse: 0.32242 |  0:02:12s
epoch 67 | loss: 0.15012 | val_0_rmse: 0.31951 | val_1_rmse: 0.32618 |  0:02:14s
epoch 68 | loss: 0.10398 | val_0_rmse: 0.30425 | val_1_rmse: 0.31271 |  0:02:16s
epoch 69 | loss: 0.11027 | val_0_rmse: 0.35286 | val_1_rmse: 0.35974 |  0:02:18s
epoch 70 | loss: 0.1158  | val_0_rmse: 0.3224  | val_1_rmse: 0.32886 |  0:02:20s
epoch 71 | loss: 0.10477 | val_0_rmse: 0.32406 | val_1_rmse: 0.33087 |  0:02:22s
epoch 72 | loss: 0.10076 | val_0_rmse: 0.34016 | val_1_rmse: 0.34634 |  0:02:24s
epoch 73 | loss: 0.10707 | val_0_rmse: 0.32591 | val_1_rmse: 0.33136 |  0:02:26s
epoch 74 | loss: 0.14552 | val_0_rmse: 0.35739 | val_1_rmse: 0.36426 |  0:02:28s
epoch 75 | loss: 0.15503 | val_0_rmse: 0.37477 | val_1_rmse: 0.38305 |  0:02:30s
epoch 76 | loss: 0.11314 | val_0_rmse: 0.3225  | val_1_rmse: 0.33058 |  0:02:32s
epoch 77 | loss: 0.10648 | val_0_rmse: 0.31871 | val_1_rmse: 0.32756 |  0:02:34s
epoch 78 | loss: 0.10321 | val_0_rmse: 0.30548 | val_1_rmse: 0.31302 |  0:02:36s
epoch 79 | loss: 0.10747 | val_0_rmse: 0.32144 | val_1_rmse: 0.32846 |  0:02:38s
epoch 80 | loss: 0.11452 | val_0_rmse: 0.31968 | val_1_rmse: 0.32592 |  0:02:40s
epoch 81 | loss: 0.10193 | val_0_rmse: 0.31095 | val_1_rmse: 0.31812 |  0:02:42s
epoch 82 | loss: 0.13823 | val_0_rmse: 0.46423 | val_1_rmse: 0.46891 |  0:02:44s
epoch 83 | loss: 0.11652 | val_0_rmse: 0.37728 | val_1_rmse: 0.38593 |  0:02:46s
epoch 84 | loss: 0.11501 | val_0_rmse: 0.38526 | val_1_rmse: 0.39228 |  0:02:48s
epoch 85 | loss: 0.14149 | val_0_rmse: 0.31208 | val_1_rmse: 0.3203  |  0:02:50s
epoch 86 | loss: 0.10107 | val_0_rmse: 0.31009 | val_1_rmse: 0.31668 |  0:02:52s
epoch 87 | loss: 0.10804 | val_0_rmse: 0.31018 | val_1_rmse: 0.31709 |  0:02:54s
epoch 88 | loss: 0.10134 | val_0_rmse: 0.3096  | val_1_rmse: 0.31641 |  0:02:56s
epoch 89 | loss: 0.10433 | val_0_rmse: 0.30952 | val_1_rmse: 0.31621 |  0:02:58s
epoch 90 | loss: 0.11604 | val_0_rmse: 0.31342 | val_1_rmse: 0.32127 |  0:03:00s
epoch 91 | loss: 0.15507 | val_0_rmse: 0.37    | val_1_rmse: 0.37755 |  0:03:02s
epoch 92 | loss: 0.12392 | val_0_rmse: 0.33665 | val_1_rmse: 0.34255 |  0:03:04s
epoch 93 | loss: 0.12554 | val_0_rmse: 0.32757 | val_1_rmse: 0.33616 |  0:03:06s
epoch 94 | loss: 0.12477 | val_0_rmse: 0.31907 | val_1_rmse: 0.32734 |  0:03:08s
epoch 95 | loss: 0.12268 | val_0_rmse: 0.31458 | val_1_rmse: 0.32148 |  0:03:10s
epoch 96 | loss: 0.1082  | val_0_rmse: 0.32446 | val_1_rmse: 0.33067 |  0:03:12s
epoch 97 | loss: 0.1039  | val_0_rmse: 0.31569 | val_1_rmse: 0.32362 |  0:03:14s
epoch 98 | loss: 0.10188 | val_0_rmse: 0.36502 | val_1_rmse: 0.37001 |  0:03:16s

Early stopping occured at epoch 98 with best_epoch = 68 and best_val_1_rmse = 0.31271
Best weights from best epoch are automatically used!
ended training at: 04:56:28
Feature importance:
[('Area', 0.15777872618976221), ('Baths', 0.27587424884239065), ('Beds', 0.013926271277600422), ('Latitude', 0.27849198647953094), ('Longitude', 0.20512018408462732), ('Month', 0.06880858312608844), ('Year', 0.0)]
Mean squared error is of 983643001.0868919
Mean absolute error:20874.176775596738
MAPE:0.2484037442597481
R2 score:0.7479653980453839
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:56:28
epoch 0  | loss: 21.5959 | val_0_rmse: 0.88938 | val_1_rmse: 0.89389 |  0:00:03s
epoch 1  | loss: 0.34164 | val_0_rmse: 0.57011 | val_1_rmse: 0.56269 |  0:00:06s
epoch 2  | loss: 0.26432 | val_0_rmse: 0.51044 | val_1_rmse: 0.50178 |  0:00:09s
epoch 3  | loss: 0.25155 | val_0_rmse: 0.45382 | val_1_rmse: 0.44774 |  0:00:12s
epoch 4  | loss: 0.24081 | val_0_rmse: 0.44305 | val_1_rmse: 0.43458 |  0:00:15s
epoch 5  | loss: 0.23224 | val_0_rmse: 0.48907 | val_1_rmse: 0.48038 |  0:00:19s
epoch 6  | loss: 0.20815 | val_0_rmse: 0.433   | val_1_rmse: 0.42572 |  0:00:22s
epoch 7  | loss: 0.20411 | val_0_rmse: 0.43384 | val_1_rmse: 0.42692 |  0:00:25s
epoch 8  | loss: 0.2277  | val_0_rmse: 0.43632 | val_1_rmse: 0.42861 |  0:00:28s
epoch 9  | loss: 0.20027 | val_0_rmse: 0.43386 | val_1_rmse: 0.42722 |  0:00:31s
epoch 10 | loss: 0.20157 | val_0_rmse: 0.45578 | val_1_rmse: 0.44735 |  0:00:35s
epoch 11 | loss: 0.19263 | val_0_rmse: 0.39313 | val_1_rmse: 0.38593 |  0:00:38s
epoch 12 | loss: 0.18612 | val_0_rmse: 0.38895 | val_1_rmse: 0.38287 |  0:00:41s
epoch 13 | loss: 0.18827 | val_0_rmse: 0.43216 | val_1_rmse: 0.42802 |  0:00:44s
epoch 14 | loss: 0.1844  | val_0_rmse: 0.39958 | val_1_rmse: 0.39085 |  0:00:47s
epoch 15 | loss: 0.19538 | val_0_rmse: 0.40568 | val_1_rmse: 0.39764 |  0:00:51s
epoch 16 | loss: 0.16843 | val_0_rmse: 0.39098 | val_1_rmse: 0.38049 |  0:00:54s
epoch 17 | loss: 0.1597  | val_0_rmse: 0.38404 | val_1_rmse: 0.3763  |  0:00:57s
epoch 18 | loss: 0.15925 | val_0_rmse: 0.37632 | val_1_rmse: 0.36474 |  0:01:00s
epoch 19 | loss: 0.15287 | val_0_rmse: 0.37092 | val_1_rmse: 0.3616  |  0:01:03s
epoch 20 | loss: 0.17885 | val_0_rmse: 0.41678 | val_1_rmse: 0.4103  |  0:01:07s
epoch 21 | loss: 0.18085 | val_0_rmse: 0.37144 | val_1_rmse: 0.36342 |  0:01:10s
epoch 22 | loss: 0.17431 | val_0_rmse: 0.40596 | val_1_rmse: 0.39789 |  0:01:13s
epoch 23 | loss: 0.17231 | val_0_rmse: 0.37744 | val_1_rmse: 0.37021 |  0:01:16s
epoch 24 | loss: 0.15537 | val_0_rmse: 0.39749 | val_1_rmse: 0.38931 |  0:01:19s
epoch 25 | loss: 0.15341 | val_0_rmse: 0.38736 | val_1_rmse: 0.3778  |  0:01:22s
epoch 26 | loss: 0.16696 | val_0_rmse: 0.38741 | val_1_rmse: 0.38125 |  0:01:26s
epoch 27 | loss: 0.15894 | val_0_rmse: 0.44994 | val_1_rmse: 0.44368 |  0:01:29s
epoch 28 | loss: 0.17426 | val_0_rmse: 0.38797 | val_1_rmse: 0.38045 |  0:01:32s
epoch 29 | loss: 0.16771 | val_0_rmse: 0.38536 | val_1_rmse: 0.37874 |  0:01:35s
epoch 30 | loss: 0.18658 | val_0_rmse: 0.38057 | val_1_rmse: 0.3756  |  0:01:38s
epoch 31 | loss: 0.18015 | val_0_rmse: 0.38219 | val_1_rmse: 0.37213 |  0:01:42s
epoch 32 | loss: 0.16965 | val_0_rmse: 0.40903 | val_1_rmse: 0.40162 |  0:01:45s
epoch 33 | loss: 0.16989 | val_0_rmse: 0.38695 | val_1_rmse: 0.37701 |  0:01:48s
epoch 34 | loss: 0.16339 | val_0_rmse: 0.3587  | val_1_rmse: 0.35281 |  0:01:51s
epoch 35 | loss: 0.16573 | val_0_rmse: 0.36508 | val_1_rmse: 0.35847 |  0:01:54s
epoch 36 | loss: 0.16632 | val_0_rmse: 0.39405 | val_1_rmse: 0.38776 |  0:01:57s
epoch 37 | loss: 0.14834 | val_0_rmse: 0.37715 | val_1_rmse: 0.3686  |  0:02:01s
epoch 38 | loss: 0.14454 | val_0_rmse: 0.38387 | val_1_rmse: 0.37544 |  0:02:04s
epoch 39 | loss: 0.1471  | val_0_rmse: 0.36245 | val_1_rmse: 0.35721 |  0:02:07s
epoch 40 | loss: 0.14311 | val_0_rmse: 0.34058 | val_1_rmse: 0.33351 |  0:02:10s
epoch 41 | loss: 0.142   | val_0_rmse: 0.35829 | val_1_rmse: 0.34964 |  0:02:13s
epoch 42 | loss: 0.13633 | val_0_rmse: 0.44675 | val_1_rmse: 0.4386  |  0:02:16s
epoch 43 | loss: 0.20541 | val_0_rmse: 0.35084 | val_1_rmse: 0.3428  |  0:02:20s
epoch 44 | loss: 0.14435 | val_0_rmse: 0.41213 | val_1_rmse: 0.40437 |  0:02:23s
epoch 45 | loss: 0.16176 | val_0_rmse: 0.41671 | val_1_rmse: 0.40853 |  0:02:26s
epoch 46 | loss: 0.15985 | val_0_rmse: 0.34843 | val_1_rmse: 0.34051 |  0:02:29s
epoch 47 | loss: 0.1575  | val_0_rmse: 0.34218 | val_1_rmse: 0.33391 |  0:02:32s
epoch 48 | loss: 0.15882 | val_0_rmse: 0.39698 | val_1_rmse: 0.39128 |  0:02:36s
epoch 49 | loss: 0.17841 | val_0_rmse: 0.42459 | val_1_rmse: 0.41579 |  0:02:39s
epoch 50 | loss: 0.14234 | val_0_rmse: 0.36165 | val_1_rmse: 0.35775 |  0:02:42s
epoch 51 | loss: 0.19219 | val_0_rmse: 0.38774 | val_1_rmse: 0.38454 |  0:02:45s
epoch 52 | loss: 0.14514 | val_0_rmse: 0.371   | val_1_rmse: 0.36525 |  0:02:48s
epoch 53 | loss: 0.15485 | val_0_rmse: 0.41382 | val_1_rmse: 0.40865 |  0:02:51s
epoch 54 | loss: 0.16027 | val_0_rmse: 0.38823 | val_1_rmse: 0.38308 |  0:02:55s
epoch 55 | loss: 0.15344 | val_0_rmse: 0.34502 | val_1_rmse: 0.33868 |  0:02:58s
epoch 56 | loss: 0.15147 | val_0_rmse: 0.37022 | val_1_rmse: 0.36319 |  0:03:01s
epoch 57 | loss: 0.15311 | val_0_rmse: 0.40008 | val_1_rmse: 0.39381 |  0:03:04s
epoch 58 | loss: 0.15363 | val_0_rmse: 0.35163 | val_1_rmse: 0.34589 |  0:03:07s
epoch 59 | loss: 0.15097 | val_0_rmse: 0.35167 | val_1_rmse: 0.34734 |  0:03:11s
epoch 60 | loss: 0.15129 | val_0_rmse: 0.39834 | val_1_rmse: 0.39076 |  0:03:14s
epoch 61 | loss: 0.15415 | val_0_rmse: 0.3513  | val_1_rmse: 0.346   |  0:03:17s
epoch 62 | loss: 0.15505 | val_0_rmse: 0.35025 | val_1_rmse: 0.34214 |  0:03:20s
epoch 63 | loss: 0.15198 | val_0_rmse: 0.34685 | val_1_rmse: 0.34021 |  0:03:23s
epoch 64 | loss: 0.16084 | val_0_rmse: 0.43314 | val_1_rmse: 0.42622 |  0:03:26s
epoch 65 | loss: 0.16329 | val_0_rmse: 0.33936 | val_1_rmse: 0.33485 |  0:03:30s
epoch 66 | loss: 0.15124 | val_0_rmse: 0.36084 | val_1_rmse: 0.35682 |  0:03:33s
epoch 67 | loss: 0.15253 | val_0_rmse: 0.38033 | val_1_rmse: 0.3781  |  0:03:36s
epoch 68 | loss: 0.15423 | val_0_rmse: 0.34691 | val_1_rmse: 0.34186 |  0:03:39s
epoch 69 | loss: 0.14874 | val_0_rmse: 0.35692 | val_1_rmse: 0.3518  |  0:03:42s
epoch 70 | loss: 0.15179 | val_0_rmse: 0.40857 | val_1_rmse: 0.40346 |  0:03:45s

Early stopping occured at epoch 70 with best_epoch = 40 and best_val_1_rmse = 0.33351
Best weights from best epoch are automatically used!
ended training at: 05:00:15
Feature importance:
[('Area', 0.11548813490623225), ('Baths', 0.16606238187147115), ('Beds', 0.00030724462605015673), ('Latitude', 0.147656562065161), ('Longitude', 0.2954893613046086), ('Month', 0.17642660928227308), ('Year', 0.09856970594420372)]
Mean squared error is of 11289657985.616459
Mean absolute error:73490.44773766323
MAPE:0.28417262429372947
R2 score:0.7977430919165598
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:00:16
epoch 0  | loss: 21.40241| val_0_rmse: 0.80591 | val_1_rmse: 0.8125  |  0:00:03s
epoch 1  | loss: 0.32451 | val_0_rmse: 0.51527 | val_1_rmse: 0.52992 |  0:00:06s
epoch 2  | loss: 0.23101 | val_0_rmse: 0.41556 | val_1_rmse: 0.42284 |  0:00:09s
epoch 3  | loss: 0.21411 | val_0_rmse: 0.41939 | val_1_rmse: 0.42772 |  0:00:12s
epoch 4  | loss: 0.2118  | val_0_rmse: 0.40014 | val_1_rmse: 0.40576 |  0:00:15s
epoch 5  | loss: 0.20027 | val_0_rmse: 0.42541 | val_1_rmse: 0.43259 |  0:00:19s
epoch 6  | loss: 0.19079 | val_0_rmse: 0.43718 | val_1_rmse: 0.44283 |  0:00:22s
epoch 7  | loss: 0.18797 | val_0_rmse: 0.37674 | val_1_rmse: 0.38245 |  0:00:25s
epoch 8  | loss: 0.19774 | val_0_rmse: 0.38491 | val_1_rmse: 0.38985 |  0:00:28s
epoch 9  | loss: 0.18717 | val_0_rmse: 0.419   | val_1_rmse: 0.42442 |  0:00:31s
epoch 10 | loss: 0.17218 | val_0_rmse: 0.39744 | val_1_rmse: 0.40259 |  0:00:35s
epoch 11 | loss: 0.19705 | val_0_rmse: 0.37215 | val_1_rmse: 0.37709 |  0:00:38s
epoch 12 | loss: 0.16367 | val_0_rmse: 0.41037 | val_1_rmse: 0.41742 |  0:00:41s
epoch 13 | loss: 0.16057 | val_0_rmse: 0.36049 | val_1_rmse: 0.36647 |  0:00:44s
epoch 14 | loss: 0.1621  | val_0_rmse: 0.40096 | val_1_rmse: 0.40603 |  0:00:47s
epoch 15 | loss: 0.17257 | val_0_rmse: 0.3642  | val_1_rmse: 0.37128 |  0:00:51s
epoch 16 | loss: 0.1604  | val_0_rmse: 0.36864 | val_1_rmse: 0.37596 |  0:00:54s
epoch 17 | loss: 0.16852 | val_0_rmse: 0.42935 | val_1_rmse: 0.43489 |  0:00:57s
epoch 18 | loss: 0.17741 | val_0_rmse: 0.35537 | val_1_rmse: 0.36151 |  0:01:00s
epoch 19 | loss: 0.16638 | val_0_rmse: 0.39662 | val_1_rmse: 0.40323 |  0:01:03s
epoch 20 | loss: 0.16256 | val_0_rmse: 0.35138 | val_1_rmse: 0.35761 |  0:01:07s
epoch 21 | loss: 0.15793 | val_0_rmse: 0.40377 | val_1_rmse: 0.40954 |  0:01:10s
epoch 22 | loss: 0.18633 | val_0_rmse: 0.36067 | val_1_rmse: 0.36717 |  0:01:13s
epoch 23 | loss: 0.16666 | val_0_rmse: 0.3936  | val_1_rmse: 0.39958 |  0:01:16s
epoch 24 | loss: 0.1678  | val_0_rmse: 0.40549 | val_1_rmse: 0.41144 |  0:01:19s
epoch 25 | loss: 0.14795 | val_0_rmse: 0.34767 | val_1_rmse: 0.35456 |  0:01:22s
epoch 26 | loss: 0.15476 | val_0_rmse: 0.38497 | val_1_rmse: 0.39062 |  0:01:26s
epoch 27 | loss: 0.14846 | val_0_rmse: 0.35626 | val_1_rmse: 0.36029 |  0:01:29s
epoch 28 | loss: 0.18224 | val_0_rmse: 0.35127 | val_1_rmse: 0.35924 |  0:01:32s
epoch 29 | loss: 0.15454 | val_0_rmse: 0.3688  | val_1_rmse: 0.37661 |  0:01:35s
epoch 30 | loss: 0.13993 | val_0_rmse: 0.34385 | val_1_rmse: 0.35033 |  0:01:38s
epoch 31 | loss: 0.13956 | val_0_rmse: 0.37181 | val_1_rmse: 0.37879 |  0:01:42s
epoch 32 | loss: 0.19543 | val_0_rmse: 0.35743 | val_1_rmse: 0.36382 |  0:01:45s
epoch 33 | loss: 0.16372 | val_0_rmse: 0.34725 | val_1_rmse: 0.35402 |  0:01:48s
epoch 34 | loss: 0.14132 | val_0_rmse: 0.34183 | val_1_rmse: 0.34888 |  0:01:51s
epoch 35 | loss: 0.14386 | val_0_rmse: 0.34478 | val_1_rmse: 0.35127 |  0:01:54s
epoch 36 | loss: 0.17565 | val_0_rmse: 0.36382 | val_1_rmse: 0.36827 |  0:01:57s
epoch 37 | loss: 0.1646  | val_0_rmse: 0.33908 | val_1_rmse: 0.3449  |  0:02:01s
epoch 38 | loss: 0.1732  | val_0_rmse: 0.33935 | val_1_rmse: 0.34902 |  0:02:04s
epoch 39 | loss: 0.13463 | val_0_rmse: 0.34536 | val_1_rmse: 0.35351 |  0:02:07s
epoch 40 | loss: 0.15466 | val_0_rmse: 0.35275 | val_1_rmse: 0.36132 |  0:02:10s
epoch 41 | loss: 0.14751 | val_0_rmse: 0.33843 | val_1_rmse: 0.34623 |  0:02:13s
epoch 42 | loss: 0.13733 | val_0_rmse: 0.37312 | val_1_rmse: 0.37963 |  0:02:17s
epoch 43 | loss: 0.16728 | val_0_rmse: 0.4082  | val_1_rmse: 0.41357 |  0:02:20s
epoch 44 | loss: 0.14653 | val_0_rmse: 0.3361  | val_1_rmse: 0.34434 |  0:02:23s
epoch 45 | loss: 0.13615 | val_0_rmse: 0.34585 | val_1_rmse: 0.35311 |  0:02:26s
epoch 46 | loss: 0.14078 | val_0_rmse: 0.34079 | val_1_rmse: 0.35005 |  0:02:29s
epoch 47 | loss: 0.1494  | val_0_rmse: 0.36682 | val_1_rmse: 0.3746  |  0:02:32s
epoch 48 | loss: 0.16074 | val_0_rmse: 0.39646 | val_1_rmse: 0.40331 |  0:02:36s
epoch 49 | loss: 0.17078 | val_0_rmse: 0.34353 | val_1_rmse: 0.35166 |  0:02:39s
epoch 50 | loss: 0.15278 | val_0_rmse: 0.37237 | val_1_rmse: 0.37936 |  0:02:42s
epoch 51 | loss: 0.15141 | val_0_rmse: 0.3583  | val_1_rmse: 0.36763 |  0:02:45s
epoch 52 | loss: 0.15194 | val_0_rmse: 0.39049 | val_1_rmse: 0.39728 |  0:02:48s
epoch 53 | loss: 0.15138 | val_0_rmse: 0.34023 | val_1_rmse: 0.34633 |  0:02:52s
epoch 54 | loss: 0.15467 | val_0_rmse: 0.34208 | val_1_rmse: 0.35054 |  0:02:55s
epoch 55 | loss: 0.15465 | val_0_rmse: 0.39983 | val_1_rmse: 0.4058  |  0:02:58s
epoch 56 | loss: 0.16196 | val_0_rmse: 0.34142 | val_1_rmse: 0.34923 |  0:03:01s
epoch 57 | loss: 0.14331 | val_0_rmse: 0.3578  | val_1_rmse: 0.36434 |  0:03:04s
epoch 58 | loss: 0.13287 | val_0_rmse: 0.34598 | val_1_rmse: 0.35526 |  0:03:07s
epoch 59 | loss: 0.15713 | val_0_rmse: 0.35759 | val_1_rmse: 0.36627 |  0:03:11s
epoch 60 | loss: 0.13254 | val_0_rmse: 0.33093 | val_1_rmse: 0.34081 |  0:03:14s
epoch 61 | loss: 0.12688 | val_0_rmse: 0.33893 | val_1_rmse: 0.34795 |  0:03:17s
epoch 62 | loss: 0.16793 | val_0_rmse: 0.34151 | val_1_rmse: 0.34987 |  0:03:20s
epoch 63 | loss: 0.14383 | val_0_rmse: 0.34669 | val_1_rmse: 0.35526 |  0:03:23s
epoch 64 | loss: 0.1685  | val_0_rmse: 0.36173 | val_1_rmse: 0.37099 |  0:03:27s
epoch 65 | loss: 0.15292 | val_0_rmse: 0.37397 | val_1_rmse: 0.38347 |  0:03:30s
epoch 66 | loss: 0.1486  | val_0_rmse: 0.33062 | val_1_rmse: 0.34152 |  0:03:33s
epoch 67 | loss: 0.16547 | val_0_rmse: 0.33751 | val_1_rmse: 0.34389 |  0:03:36s
epoch 68 | loss: 0.14654 | val_0_rmse: 0.39182 | val_1_rmse: 0.39827 |  0:03:39s
epoch 69 | loss: 0.14725 | val_0_rmse: 0.33699 | val_1_rmse: 0.34751 |  0:03:43s
epoch 70 | loss: 0.14762 | val_0_rmse: 0.35143 | val_1_rmse: 0.36105 |  0:03:46s
epoch 71 | loss: 0.13805 | val_0_rmse: 0.35929 | val_1_rmse: 0.36753 |  0:03:49s
epoch 72 | loss: 0.1323  | val_0_rmse: 0.33474 | val_1_rmse: 0.34539 |  0:03:52s
epoch 73 | loss: 0.13096 | val_0_rmse: 0.33314 | val_1_rmse: 0.34039 |  0:03:55s
epoch 74 | loss: 0.14083 | val_0_rmse: 0.34877 | val_1_rmse: 0.35938 |  0:03:59s
epoch 75 | loss: 0.13445 | val_0_rmse: 0.35836 | val_1_rmse: 0.3668  |  0:04:02s
epoch 76 | loss: 0.15932 | val_0_rmse: 0.4417  | val_1_rmse: 0.44922 |  0:04:05s
epoch 77 | loss: 0.20405 | val_0_rmse: 0.52712 | val_1_rmse: 0.53539 |  0:04:08s
epoch 78 | loss: 0.17386 | val_0_rmse: 0.32412 | val_1_rmse: 0.33436 |  0:04:11s
epoch 79 | loss: 0.12846 | val_0_rmse: 0.34585 | val_1_rmse: 0.35625 |  0:04:14s
epoch 80 | loss: 0.12998 | val_0_rmse: 0.34153 | val_1_rmse: 0.34959 |  0:04:18s
epoch 81 | loss: 0.13308 | val_0_rmse: 0.33932 | val_1_rmse: 0.34765 |  0:04:21s
epoch 82 | loss: 0.15404 | val_0_rmse: 0.3545  | val_1_rmse: 0.3615  |  0:04:24s
epoch 83 | loss: 0.14881 | val_0_rmse: 0.32427 | val_1_rmse: 0.33381 |  0:04:27s
epoch 84 | loss: 0.1723  | val_0_rmse: 0.54403 | val_1_rmse: 0.54903 |  0:04:30s
epoch 85 | loss: 0.19141 | val_0_rmse: 0.34967 | val_1_rmse: 0.35804 |  0:04:34s
epoch 86 | loss: 0.13435 | val_0_rmse: 0.33162 | val_1_rmse: 0.34052 |  0:04:37s
epoch 87 | loss: 0.12917 | val_0_rmse: 0.33619 | val_1_rmse: 0.3466  |  0:04:40s
epoch 88 | loss: 0.14834 | val_0_rmse: 0.33046 | val_1_rmse: 0.33886 |  0:04:43s
epoch 89 | loss: 0.14285 | val_0_rmse: 0.3701  | val_1_rmse: 0.37818 |  0:04:46s
epoch 90 | loss: 0.14104 | val_0_rmse: 0.42084 | val_1_rmse: 0.42931 |  0:04:49s
epoch 91 | loss: 0.15946 | val_0_rmse: 0.34749 | val_1_rmse: 0.35735 |  0:04:53s
epoch 92 | loss: 0.14016 | val_0_rmse: 0.35493 | val_1_rmse: 0.36277 |  0:04:56s
epoch 93 | loss: 0.14603 | val_0_rmse: 0.34915 | val_1_rmse: 0.35794 |  0:04:59s
epoch 94 | loss: 0.1431  | val_0_rmse: 0.34699 | val_1_rmse: 0.35584 |  0:05:02s
epoch 95 | loss: 0.14231 | val_0_rmse: 0.32452 | val_1_rmse: 0.3355  |  0:05:05s
epoch 96 | loss: 0.12354 | val_0_rmse: 0.33241 | val_1_rmse: 0.34195 |  0:05:08s
epoch 97 | loss: 0.13975 | val_0_rmse: 0.33379 | val_1_rmse: 0.34476 |  0:05:12s
epoch 98 | loss: 0.13519 | val_0_rmse: 0.34401 | val_1_rmse: 0.35291 |  0:05:15s
epoch 99 | loss: 0.13079 | val_0_rmse: 0.33862 | val_1_rmse: 0.35045 |  0:05:18s
epoch 100| loss: 0.14287 | val_0_rmse: 0.35539 | val_1_rmse: 0.36522 |  0:05:21s
epoch 101| loss: 0.14392 | val_0_rmse: 0.33442 | val_1_rmse: 0.34368 |  0:05:24s
epoch 102| loss: 0.13826 | val_0_rmse: 0.33385 | val_1_rmse: 0.34505 |  0:05:28s
epoch 103| loss: 0.12349 | val_0_rmse: 0.32837 | val_1_rmse: 0.33805 |  0:05:31s
epoch 104| loss: 0.14394 | val_0_rmse: 0.34457 | val_1_rmse: 0.35459 |  0:05:34s
epoch 105| loss: 0.14228 | val_0_rmse: 0.39057 | val_1_rmse: 0.3995  |  0:05:37s
epoch 106| loss: 0.1349  | val_0_rmse: 0.33947 | val_1_rmse: 0.34865 |  0:05:40s
epoch 107| loss: 0.1275  | val_0_rmse: 0.33437 | val_1_rmse: 0.34448 |  0:05:44s
epoch 108| loss: 0.14626 | val_0_rmse: 0.33782 | val_1_rmse: 0.34713 |  0:05:47s
epoch 109| loss: 0.12794 | val_0_rmse: 0.32315 | val_1_rmse: 0.33318 |  0:05:50s
epoch 110| loss: 0.1225  | val_0_rmse: 0.31862 | val_1_rmse: 0.32995 |  0:05:53s
epoch 111| loss: 0.13461 | val_0_rmse: 0.32429 | val_1_rmse: 0.33673 |  0:05:56s
epoch 112| loss: 0.13407 | val_0_rmse: 0.38479 | val_1_rmse: 0.39525 |  0:05:59s
epoch 113| loss: 0.14515 | val_0_rmse: 0.32663 | val_1_rmse: 0.33785 |  0:06:03s
epoch 114| loss: 0.13599 | val_0_rmse: 0.38684 | val_1_rmse: 0.39638 |  0:06:06s
epoch 115| loss: 0.13249 | val_0_rmse: 0.32635 | val_1_rmse: 0.33811 |  0:06:09s
epoch 116| loss: 0.14582 | val_0_rmse: 0.32264 | val_1_rmse: 0.33388 |  0:06:12s
epoch 117| loss: 0.1662  | val_0_rmse: 0.39069 | val_1_rmse: 0.40203 |  0:06:15s
epoch 118| loss: 0.1432  | val_0_rmse: 0.3621  | val_1_rmse: 0.37347 |  0:06:19s
epoch 119| loss: 0.12347 | val_0_rmse: 0.32314 | val_1_rmse: 0.33473 |  0:06:22s
epoch 120| loss: 0.15969 | val_0_rmse: 0.33481 | val_1_rmse: 0.34587 |  0:06:25s
epoch 121| loss: 0.1276  | val_0_rmse: 0.32577 | val_1_rmse: 0.33863 |  0:06:28s
epoch 122| loss: 0.12196 | val_0_rmse: 0.33775 | val_1_rmse: 0.34984 |  0:06:31s
epoch 123| loss: 0.16134 | val_0_rmse: 0.32196 | val_1_rmse: 0.33345 |  0:06:35s
epoch 124| loss: 0.11935 | val_0_rmse: 0.32545 | val_1_rmse: 0.33699 |  0:06:38s
epoch 125| loss: 0.12289 | val_0_rmse: 0.32567 | val_1_rmse: 0.33811 |  0:06:41s
epoch 126| loss: 0.15047 | val_0_rmse: 0.33735 | val_1_rmse: 0.3494  |  0:06:44s
epoch 127| loss: 0.13798 | val_0_rmse: 0.34196 | val_1_rmse: 0.35325 |  0:06:47s
epoch 128| loss: 0.15116 | val_0_rmse: 0.34437 | val_1_rmse: 0.35612 |  0:06:50s
epoch 129| loss: 0.11694 | val_0_rmse: 0.33962 | val_1_rmse: 0.3512  |  0:06:54s
epoch 130| loss: 0.12368 | val_0_rmse: 0.32484 | val_1_rmse: 0.33507 |  0:06:57s
epoch 131| loss: 0.12285 | val_0_rmse: 0.32368 | val_1_rmse: 0.33625 |  0:07:00s
epoch 132| loss: 0.13254 | val_0_rmse: 0.32728 | val_1_rmse: 0.33918 |  0:07:03s
epoch 133| loss: 0.11874 | val_0_rmse: 0.33907 | val_1_rmse: 0.34994 |  0:07:06s
epoch 134| loss: 0.12923 | val_0_rmse: 0.3759  | val_1_rmse: 0.38767 |  0:07:09s
epoch 135| loss: 0.12944 | val_0_rmse: 0.31939 | val_1_rmse: 0.33108 |  0:07:13s
epoch 136| loss: 0.12321 | val_0_rmse: 0.33018 | val_1_rmse: 0.34163 |  0:07:16s
epoch 137| loss: 0.12876 | val_0_rmse: 0.32113 | val_1_rmse: 0.33429 |  0:07:19s
epoch 138| loss: 0.12059 | val_0_rmse: 0.34394 | val_1_rmse: 0.35575 |  0:07:22s
epoch 139| loss: 0.12232 | val_0_rmse: 0.33781 | val_1_rmse: 0.35065 |  0:07:25s
epoch 140| loss: 0.13207 | val_0_rmse: 0.32724 | val_1_rmse: 0.33955 |  0:07:29s

Early stopping occured at epoch 140 with best_epoch = 110 and best_val_1_rmse = 0.32995
Best weights from best epoch are automatically used!
ended training at: 05:07:46
Feature importance:
[('Area', 0.10548531031573342), ('Baths', 0.13216832677342846), ('Beds', 0.04600834292019461), ('Latitude', 0.1730492005503412), ('Longitude', 0.318939002479217), ('Month', 0.0), ('Year', 0.22434981696108527)]
Mean squared error is of 10683973309.036682
Mean absolute error:70428.95557264275
MAPE:0.26000178268313234
R2 score:0.8149175967379051
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:07:46
epoch 0  | loss: 21.66371| val_0_rmse: 0.98124 | val_1_rmse: 0.9782  |  0:00:03s
epoch 1  | loss: 0.45445 | val_0_rmse: 0.6821  | val_1_rmse: 0.67709 |  0:00:06s
epoch 2  | loss: 0.31146 | val_0_rmse: 0.44983 | val_1_rmse: 0.44884 |  0:00:09s
epoch 3  | loss: 0.23164 | val_0_rmse: 0.44871 | val_1_rmse: 0.44326 |  0:00:12s
epoch 4  | loss: 0.21596 | val_0_rmse: 0.42288 | val_1_rmse: 0.42251 |  0:00:15s
epoch 5  | loss: 0.2295  | val_0_rmse: 0.42066 | val_1_rmse: 0.417   |  0:00:19s
epoch 6  | loss: 0.20913 | val_0_rmse: 0.42916 | val_1_rmse: 0.42768 |  0:00:22s
epoch 7  | loss: 0.20534 | val_0_rmse: 0.44491 | val_1_rmse: 0.4417  |  0:00:25s
epoch 8  | loss: 0.20204 | val_0_rmse: 0.40865 | val_1_rmse: 0.40766 |  0:00:28s
epoch 9  | loss: 0.19589 | val_0_rmse: 0.38952 | val_1_rmse: 0.3882  |  0:00:31s
epoch 10 | loss: 0.19459 | val_0_rmse: 0.44618 | val_1_rmse: 0.44652 |  0:00:35s
epoch 11 | loss: 0.19835 | val_0_rmse: 0.40354 | val_1_rmse: 0.40408 |  0:00:38s
epoch 12 | loss: 0.18362 | val_0_rmse: 0.39193 | val_1_rmse: 0.38974 |  0:00:41s
epoch 13 | loss: 0.17153 | val_0_rmse: 0.39575 | val_1_rmse: 0.39385 |  0:00:44s
epoch 14 | loss: 0.17236 | val_0_rmse: 0.37896 | val_1_rmse: 0.37851 |  0:00:47s
epoch 15 | loss: 0.16079 | val_0_rmse: 0.38145 | val_1_rmse: 0.37994 |  0:00:50s
epoch 16 | loss: 0.19227 | val_0_rmse: 0.40556 | val_1_rmse: 0.40716 |  0:00:54s
epoch 17 | loss: 0.21592 | val_0_rmse: 0.36591 | val_1_rmse: 0.36559 |  0:00:57s
epoch 18 | loss: 0.16326 | val_0_rmse: 0.39303 | val_1_rmse: 0.39207 |  0:01:00s
epoch 19 | loss: 0.15593 | val_0_rmse: 0.38479 | val_1_rmse: 0.3843  |  0:01:03s
epoch 20 | loss: 0.15895 | val_0_rmse: 0.37402 | val_1_rmse: 0.3752  |  0:01:06s
epoch 21 | loss: 0.18643 | val_0_rmse: 0.45843 | val_1_rmse: 0.4583  |  0:01:10s
epoch 22 | loss: 0.18802 | val_0_rmse: 0.38647 | val_1_rmse: 0.38602 |  0:01:13s
epoch 23 | loss: 0.15842 | val_0_rmse: 0.37042 | val_1_rmse: 0.37018 |  0:01:16s
epoch 24 | loss: 0.21325 | val_0_rmse: 0.39679 | val_1_rmse: 0.39707 |  0:01:19s
epoch 25 | loss: 0.16621 | val_0_rmse: 0.42123 | val_1_rmse: 0.42291 |  0:01:22s
epoch 26 | loss: 0.21248 | val_0_rmse: 0.37093 | val_1_rmse: 0.37144 |  0:01:26s
epoch 27 | loss: 0.1447  | val_0_rmse: 0.36268 | val_1_rmse: 0.36397 |  0:01:29s
epoch 28 | loss: 0.14594 | val_0_rmse: 0.38881 | val_1_rmse: 0.38807 |  0:01:32s
epoch 29 | loss: 0.14818 | val_0_rmse: 0.36055 | val_1_rmse: 0.35793 |  0:01:35s
epoch 30 | loss: 0.19102 | val_0_rmse: 0.36202 | val_1_rmse: 0.3625  |  0:01:38s
epoch 31 | loss: 0.16966 | val_0_rmse: 0.37095 | val_1_rmse: 0.37349 |  0:01:42s
epoch 32 | loss: 0.16572 | val_0_rmse: 0.34492 | val_1_rmse: 0.34447 |  0:01:45s
epoch 33 | loss: 0.17514 | val_0_rmse: 0.34688 | val_1_rmse: 0.34726 |  0:01:48s
epoch 34 | loss: 0.16684 | val_0_rmse: 0.36074 | val_1_rmse: 0.35887 |  0:01:51s
epoch 35 | loss: 0.17067 | val_0_rmse: 0.39115 | val_1_rmse: 0.39254 |  0:01:54s
epoch 36 | loss: 0.17207 | val_0_rmse: 0.3629  | val_1_rmse: 0.3609  |  0:01:57s
epoch 37 | loss: 0.143   | val_0_rmse: 0.41298 | val_1_rmse: 0.41199 |  0:02:01s
epoch 38 | loss: 0.1647  | val_0_rmse: 0.4267  | val_1_rmse: 0.42478 |  0:02:04s
epoch 39 | loss: 0.16937 | val_0_rmse: 0.36867 | val_1_rmse: 0.36761 |  0:02:07s
epoch 40 | loss: 0.15971 | val_0_rmse: 0.37697 | val_1_rmse: 0.38032 |  0:02:10s
epoch 41 | loss: 0.15926 | val_0_rmse: 0.37486 | val_1_rmse: 0.37466 |  0:02:13s
epoch 42 | loss: 0.1594  | val_0_rmse: 0.34426 | val_1_rmse: 0.34456 |  0:02:17s
epoch 43 | loss: 0.1573  | val_0_rmse: 0.3455  | val_1_rmse: 0.34655 |  0:02:20s
epoch 44 | loss: 0.15567 | val_0_rmse: 0.40645 | val_1_rmse: 0.40445 |  0:02:23s
epoch 45 | loss: 0.16453 | val_0_rmse: 0.36179 | val_1_rmse: 0.36037 |  0:02:26s
epoch 46 | loss: 0.14003 | val_0_rmse: 0.35239 | val_1_rmse: 0.35435 |  0:02:29s
epoch 47 | loss: 0.14905 | val_0_rmse: 0.34025 | val_1_rmse: 0.34193 |  0:02:32s
epoch 48 | loss: 0.13293 | val_0_rmse: 0.39875 | val_1_rmse: 0.40209 |  0:02:36s
epoch 49 | loss: 0.13272 | val_0_rmse: 0.34589 | val_1_rmse: 0.34637 |  0:02:39s
epoch 50 | loss: 0.15221 | val_0_rmse: 0.41949 | val_1_rmse: 0.42295 |  0:02:42s
epoch 51 | loss: 0.18553 | val_0_rmse: 0.43262 | val_1_rmse: 0.43197 |  0:02:45s
epoch 52 | loss: 0.15859 | val_0_rmse: 0.36222 | val_1_rmse: 0.36154 |  0:02:48s
epoch 53 | loss: 0.15464 | val_0_rmse: 0.33668 | val_1_rmse: 0.33712 |  0:02:52s
epoch 54 | loss: 0.18038 | val_0_rmse: 0.34048 | val_1_rmse: 0.33955 |  0:02:55s
epoch 55 | loss: 0.15449 | val_0_rmse: 0.35325 | val_1_rmse: 0.35346 |  0:02:58s
epoch 56 | loss: 0.15359 | val_0_rmse: 0.35742 | val_1_rmse: 0.35827 |  0:03:01s
epoch 57 | loss: 0.15751 | val_0_rmse: 0.37111 | val_1_rmse: 0.37044 |  0:03:04s
epoch 58 | loss: 0.14453 | val_0_rmse: 0.33626 | val_1_rmse: 0.33823 |  0:03:07s
epoch 59 | loss: 0.17123 | val_0_rmse: 0.34016 | val_1_rmse: 0.33882 |  0:03:11s
epoch 60 | loss: 0.20607 | val_0_rmse: 0.42652 | val_1_rmse: 0.42905 |  0:03:14s
epoch 61 | loss: 0.15963 | val_0_rmse: 0.34042 | val_1_rmse: 0.34234 |  0:03:17s
epoch 62 | loss: 0.14198 | val_0_rmse: 0.37494 | val_1_rmse: 0.37607 |  0:03:20s
epoch 63 | loss: 0.18294 | val_0_rmse: 0.36739 | val_1_rmse: 0.36594 |  0:03:23s
epoch 64 | loss: 0.13578 | val_0_rmse: 0.34202 | val_1_rmse: 0.3435  |  0:03:26s
epoch 65 | loss: 0.13237 | val_0_rmse: 0.36928 | val_1_rmse: 0.36979 |  0:03:30s
epoch 66 | loss: 0.12984 | val_0_rmse: 0.35222 | val_1_rmse: 0.35162 |  0:03:33s
epoch 67 | loss: 0.13552 | val_0_rmse: 0.34514 | val_1_rmse: 0.34516 |  0:03:36s
epoch 68 | loss: 0.14999 | val_0_rmse: 0.35431 | val_1_rmse: 0.35623 |  0:03:39s
epoch 69 | loss: 0.15008 | val_0_rmse: 0.41079 | val_1_rmse: 0.41288 |  0:03:42s
epoch 70 | loss: 0.15299 | val_0_rmse: 0.3472  | val_1_rmse: 0.34861 |  0:03:46s
epoch 71 | loss: 0.14972 | val_0_rmse: 0.36009 | val_1_rmse: 0.36228 |  0:03:49s
epoch 72 | loss: 0.15545 | val_0_rmse: 0.4129  | val_1_rmse: 0.41187 |  0:03:52s
epoch 73 | loss: 0.15792 | val_0_rmse: 0.38371 | val_1_rmse: 0.38792 |  0:03:55s
epoch 74 | loss: 0.144   | val_0_rmse: 0.33061 | val_1_rmse: 0.33372 |  0:03:58s
epoch 75 | loss: 0.13489 | val_0_rmse: 0.33617 | val_1_rmse: 0.33857 |  0:04:01s
epoch 76 | loss: 0.12718 | val_0_rmse: 0.35001 | val_1_rmse: 0.35298 |  0:04:05s
epoch 77 | loss: 0.12989 | val_0_rmse: 0.34056 | val_1_rmse: 0.34358 |  0:04:08s
epoch 78 | loss: 0.16191 | val_0_rmse: 0.40603 | val_1_rmse: 0.40846 |  0:04:11s
epoch 79 | loss: 0.17289 | val_0_rmse: 0.39604 | val_1_rmse: 0.39823 |  0:04:14s
epoch 80 | loss: 0.14646 | val_0_rmse: 0.33397 | val_1_rmse: 0.33709 |  0:04:17s
epoch 81 | loss: 0.1387  | val_0_rmse: 0.35498 | val_1_rmse: 0.35814 |  0:04:20s
epoch 82 | loss: 0.14361 | val_0_rmse: 0.34354 | val_1_rmse: 0.34537 |  0:04:24s
epoch 83 | loss: 0.15304 | val_0_rmse: 0.33374 | val_1_rmse: 0.33703 |  0:04:27s
epoch 84 | loss: 0.12819 | val_0_rmse: 0.34637 | val_1_rmse: 0.3474  |  0:04:30s
epoch 85 | loss: 0.1495  | val_0_rmse: 0.3391  | val_1_rmse: 0.34057 |  0:04:33s
epoch 86 | loss: 0.14429 | val_0_rmse: 0.32571 | val_1_rmse: 0.32909 |  0:04:36s
epoch 87 | loss: 0.15248 | val_0_rmse: 0.3349  | val_1_rmse: 0.33768 |  0:04:40s
epoch 88 | loss: 0.17841 | val_0_rmse: 0.3548  | val_1_rmse: 0.35787 |  0:04:43s
epoch 89 | loss: 0.15199 | val_0_rmse: 0.41701 | val_1_rmse: 0.4186  |  0:04:46s
epoch 90 | loss: 0.14005 | val_0_rmse: 0.39496 | val_1_rmse: 0.39625 |  0:04:49s
epoch 91 | loss: 0.16111 | val_0_rmse: 0.33752 | val_1_rmse: 0.33842 |  0:04:52s
epoch 92 | loss: 0.13674 | val_0_rmse: 0.34938 | val_1_rmse: 0.35206 |  0:04:56s
epoch 93 | loss: 0.12805 | val_0_rmse: 0.33887 | val_1_rmse: 0.34266 |  0:04:59s
epoch 94 | loss: 0.14687 | val_0_rmse: 0.33229 | val_1_rmse: 0.33239 |  0:05:02s
epoch 95 | loss: 0.14584 | val_0_rmse: 0.34036 | val_1_rmse: 0.34326 |  0:05:05s
epoch 96 | loss: 0.14865 | val_0_rmse: 0.33156 | val_1_rmse: 0.33536 |  0:05:08s
epoch 97 | loss: 0.14944 | val_0_rmse: 0.386   | val_1_rmse: 0.38982 |  0:05:11s
epoch 98 | loss: 0.20138 | val_0_rmse: 0.32473 | val_1_rmse: 0.32887 |  0:05:15s
epoch 99 | loss: 0.14508 | val_0_rmse: 0.32955 | val_1_rmse: 0.332   |  0:05:18s
epoch 100| loss: 0.12575 | val_0_rmse: 0.34296 | val_1_rmse: 0.34665 |  0:05:21s
epoch 101| loss: 0.13877 | val_0_rmse: 0.3721  | val_1_rmse: 0.37475 |  0:05:24s
epoch 102| loss: 0.13157 | val_0_rmse: 0.38948 | val_1_rmse: 0.39418 |  0:05:27s
epoch 103| loss: 0.16674 | val_0_rmse: 0.32488 | val_1_rmse: 0.32812 |  0:05:30s
epoch 104| loss: 0.15302 | val_0_rmse: 0.34862 | val_1_rmse: 0.34919 |  0:05:34s
epoch 105| loss: 0.15005 | val_0_rmse: 0.37931 | val_1_rmse: 0.38286 |  0:05:37s
epoch 106| loss: 0.13333 | val_0_rmse: 0.35171 | val_1_rmse: 0.35434 |  0:05:40s
epoch 107| loss: 0.15532 | val_0_rmse: 0.33918 | val_1_rmse: 0.34485 |  0:05:43s
epoch 108| loss: 0.12069 | val_0_rmse: 0.33525 | val_1_rmse: 0.34005 |  0:05:46s
epoch 109| loss: 0.12693 | val_0_rmse: 0.34302 | val_1_rmse: 0.34584 |  0:05:50s
epoch 110| loss: 0.13925 | val_0_rmse: 0.34904 | val_1_rmse: 0.35494 |  0:05:53s
epoch 111| loss: 0.14591 | val_0_rmse: 0.37291 | val_1_rmse: 0.37372 |  0:05:56s
epoch 112| loss: 0.12659 | val_0_rmse: 0.33503 | val_1_rmse: 0.34006 |  0:05:59s
epoch 113| loss: 0.16577 | val_0_rmse: 0.33092 | val_1_rmse: 0.33369 |  0:06:02s
epoch 114| loss: 0.13746 | val_0_rmse: 0.37937 | val_1_rmse: 0.3802  |  0:06:05s
epoch 115| loss: 0.13261 | val_0_rmse: 0.33234 | val_1_rmse: 0.33587 |  0:06:09s
epoch 116| loss: 0.12268 | val_0_rmse: 0.33437 | val_1_rmse: 0.33776 |  0:06:12s
epoch 117| loss: 0.12324 | val_0_rmse: 0.32775 | val_1_rmse: 0.33429 |  0:06:15s
epoch 118| loss: 0.12258 | val_0_rmse: 0.35187 | val_1_rmse: 0.3531  |  0:06:18s
epoch 119| loss: 0.1249  | val_0_rmse: 0.32821 | val_1_rmse: 0.33114 |  0:06:21s
epoch 120| loss: 0.12144 | val_0_rmse: 0.36851 | val_1_rmse: 0.37378 |  0:06:25s
epoch 121| loss: 0.14996 | val_0_rmse: 0.33043 | val_1_rmse: 0.33406 |  0:06:28s
epoch 122| loss: 0.14525 | val_0_rmse: 0.34114 | val_1_rmse: 0.34349 |  0:06:31s
epoch 123| loss: 0.11883 | val_0_rmse: 0.34932 | val_1_rmse: 0.35279 |  0:06:34s
epoch 124| loss: 0.11928 | val_0_rmse: 0.34139 | val_1_rmse: 0.34449 |  0:06:37s
epoch 125| loss: 0.1256  | val_0_rmse: 0.3394  | val_1_rmse: 0.3425  |  0:06:40s
epoch 126| loss: 0.13157 | val_0_rmse: 0.38306 | val_1_rmse: 0.38668 |  0:06:44s
epoch 127| loss: 0.14879 | val_0_rmse: 0.34971 | val_1_rmse: 0.35385 |  0:06:47s
epoch 128| loss: 0.13157 | val_0_rmse: 0.43658 | val_1_rmse: 0.44014 |  0:06:50s
epoch 129| loss: 0.13442 | val_0_rmse: 0.32478 | val_1_rmse: 0.32961 |  0:06:53s
epoch 130| loss: 0.1681  | val_0_rmse: 0.49857 | val_1_rmse: 0.50124 |  0:06:56s
epoch 131| loss: 0.13652 | val_0_rmse: 0.32147 | val_1_rmse: 0.3261  |  0:06:59s
epoch 132| loss: 0.12185 | val_0_rmse: 0.35206 | val_1_rmse: 0.35568 |  0:07:03s
epoch 133| loss: 0.13122 | val_0_rmse: 0.37487 | val_1_rmse: 0.37833 |  0:07:06s
epoch 134| loss: 0.14402 | val_0_rmse: 0.4159  | val_1_rmse: 0.41914 |  0:07:09s
epoch 135| loss: 0.14321 | val_0_rmse: 0.3238  | val_1_rmse: 0.32807 |  0:07:12s
epoch 136| loss: 0.12544 | val_0_rmse: 0.32485 | val_1_rmse: 0.32724 |  0:07:15s
epoch 137| loss: 0.12275 | val_0_rmse: 0.39184 | val_1_rmse: 0.39614 |  0:07:19s
epoch 138| loss: 0.12125 | val_0_rmse: 0.35043 | val_1_rmse: 0.35389 |  0:07:22s
epoch 139| loss: 0.13246 | val_0_rmse: 0.3479  | val_1_rmse: 0.34988 |  0:07:25s
epoch 140| loss: 0.11738 | val_0_rmse: 0.33947 | val_1_rmse: 0.34495 |  0:07:28s
epoch 141| loss: 0.12268 | val_0_rmse: 0.32394 | val_1_rmse: 0.32997 |  0:07:31s
epoch 142| loss: 0.12628 | val_0_rmse: 0.32333 | val_1_rmse: 0.3268  |  0:07:34s
epoch 143| loss: 0.12007 | val_0_rmse: 0.33117 | val_1_rmse: 0.33712 |  0:07:38s
epoch 144| loss: 0.11496 | val_0_rmse: 0.35272 | val_1_rmse: 0.35936 |  0:07:41s
epoch 145| loss: 0.12265 | val_0_rmse: 0.34111 | val_1_rmse: 0.34654 |  0:07:44s
epoch 146| loss: 0.13934 | val_0_rmse: 0.32858 | val_1_rmse: 0.33438 |  0:07:47s
epoch 147| loss: 0.13723 | val_0_rmse: 0.32465 | val_1_rmse: 0.32849 |  0:07:50s
epoch 148| loss: 0.12587 | val_0_rmse: 0.32038 | val_1_rmse: 0.32596 |  0:07:53s
epoch 149| loss: 0.13864 | val_0_rmse: 0.3194  | val_1_rmse: 0.32374 |  0:07:57s
Stop training because you reached max_epochs = 150 with best_epoch = 149 and best_val_1_rmse = 0.32374
Best weights from best epoch are automatically used!
ended training at: 05:15:44
Feature importance:
[('Area', 0.07710441030819937), ('Baths', 0.08260325886330458), ('Beds', 0.3426178301033267), ('Latitude', 0.038276611461301346), ('Longitude', 0.11488769332523538), ('Month', 0.17806574983504617), ('Year', 0.16644444610358647)]
Mean squared error is of 10760807229.54616
Mean absolute error:70184.15555266841
MAPE:0.2562030278333554
R2 score:0.8137241557831035
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:15:44
epoch 0  | loss: 21.58594| val_0_rmse: 0.69981 | val_1_rmse: 0.69674 |  0:00:03s
epoch 1  | loss: 0.41884 | val_0_rmse: 0.45261 | val_1_rmse: 0.44935 |  0:00:06s
epoch 2  | loss: 0.23873 | val_0_rmse: 0.42001 | val_1_rmse: 0.41415 |  0:00:09s
epoch 3  | loss: 0.19886 | val_0_rmse: 0.403   | val_1_rmse: 0.39862 |  0:00:12s
epoch 4  | loss: 0.1889  | val_0_rmse: 0.38254 | val_1_rmse: 0.38053 |  0:00:15s
epoch 5  | loss: 0.18137 | val_0_rmse: 0.42024 | val_1_rmse: 0.41539 |  0:00:19s
epoch 6  | loss: 0.19001 | val_0_rmse: 0.38306 | val_1_rmse: 0.37947 |  0:00:22s
epoch 7  | loss: 0.19375 | val_0_rmse: 0.38404 | val_1_rmse: 0.38077 |  0:00:25s
epoch 8  | loss: 0.1816  | val_0_rmse: 0.39045 | val_1_rmse: 0.38945 |  0:00:28s
epoch 9  | loss: 0.15881 | val_0_rmse: 0.37019 | val_1_rmse: 0.36817 |  0:00:31s
epoch 10 | loss: 0.1856  | val_0_rmse: 0.43295 | val_1_rmse: 0.43264 |  0:00:35s
epoch 11 | loss: 0.24892 | val_0_rmse: 0.37583 | val_1_rmse: 0.3746  |  0:00:38s
epoch 12 | loss: 0.23784 | val_0_rmse: 0.44613 | val_1_rmse: 0.44479 |  0:00:41s
epoch 13 | loss: 0.23882 | val_0_rmse: 0.5086  | val_1_rmse: 0.50723 |  0:00:44s
epoch 14 | loss: 0.1895  | val_0_rmse: 0.37155 | val_1_rmse: 0.37125 |  0:00:47s
epoch 15 | loss: 0.15261 | val_0_rmse: 0.36371 | val_1_rmse: 0.36137 |  0:00:51s
epoch 16 | loss: 0.19556 | val_0_rmse: 0.42338 | val_1_rmse: 0.42144 |  0:00:54s
epoch 17 | loss: 0.17303 | val_0_rmse: 0.36335 | val_1_rmse: 0.36123 |  0:00:57s
epoch 18 | loss: 0.16256 | val_0_rmse: 0.36202 | val_1_rmse: 0.36127 |  0:01:00s
epoch 19 | loss: 0.1606  | val_0_rmse: 0.38423 | val_1_rmse: 0.38226 |  0:01:03s
epoch 20 | loss: 0.16632 | val_0_rmse: 0.36914 | val_1_rmse: 0.36677 |  0:01:06s
epoch 21 | loss: 0.20219 | val_0_rmse: 0.36224 | val_1_rmse: 0.36126 |  0:01:10s
epoch 22 | loss: 0.17255 | val_0_rmse: 0.41097 | val_1_rmse: 0.40769 |  0:01:13s
epoch 23 | loss: 0.17464 | val_0_rmse: 0.39753 | val_1_rmse: 0.39752 |  0:01:16s
epoch 24 | loss: 0.17059 | val_0_rmse: 0.40961 | val_1_rmse: 0.40961 |  0:01:19s
epoch 25 | loss: 0.16724 | val_0_rmse: 0.36906 | val_1_rmse: 0.3685  |  0:01:22s
epoch 26 | loss: 0.16317 | val_0_rmse: 0.34864 | val_1_rmse: 0.34899 |  0:01:26s
epoch 27 | loss: 0.14682 | val_0_rmse: 0.36565 | val_1_rmse: 0.36419 |  0:01:29s
epoch 28 | loss: 0.16762 | val_0_rmse: 0.38665 | val_1_rmse: 0.38622 |  0:01:32s
epoch 29 | loss: 0.16218 | val_0_rmse: 0.38682 | val_1_rmse: 0.38534 |  0:01:35s
epoch 30 | loss: 0.16235 | val_0_rmse: 0.37795 | val_1_rmse: 0.37494 |  0:01:38s
epoch 31 | loss: 0.16263 | val_0_rmse: 0.34969 | val_1_rmse: 0.35052 |  0:01:41s
epoch 32 | loss: 0.1445  | val_0_rmse: 0.35815 | val_1_rmse: 0.35921 |  0:01:45s
epoch 33 | loss: 0.13631 | val_0_rmse: 0.37751 | val_1_rmse: 0.37865 |  0:01:48s
epoch 34 | loss: 0.14127 | val_0_rmse: 0.40818 | val_1_rmse: 0.40828 |  0:01:51s
epoch 35 | loss: 0.16222 | val_0_rmse: 0.35697 | val_1_rmse: 0.35822 |  0:01:54s
epoch 36 | loss: 0.15816 | val_0_rmse: 0.35996 | val_1_rmse: 0.36054 |  0:01:57s
epoch 37 | loss: 0.15737 | val_0_rmse: 0.37307 | val_1_rmse: 0.37346 |  0:02:01s
epoch 38 | loss: 0.15771 | val_0_rmse: 0.35246 | val_1_rmse: 0.35254 |  0:02:04s
epoch 39 | loss: 0.15216 | val_0_rmse: 0.38169 | val_1_rmse: 0.38385 |  0:02:07s
epoch 40 | loss: 0.17052 | val_0_rmse: 0.34754 | val_1_rmse: 0.34797 |  0:02:10s
epoch 41 | loss: 0.15988 | val_0_rmse: 0.34832 | val_1_rmse: 0.3499  |  0:02:13s
epoch 42 | loss: 0.1509  | val_0_rmse: 0.41602 | val_1_rmse: 0.41826 |  0:02:16s
epoch 43 | loss: 0.1542  | val_0_rmse: 0.34305 | val_1_rmse: 0.34434 |  0:02:20s
epoch 44 | loss: 0.1519  | val_0_rmse: 0.36305 | val_1_rmse: 0.36348 |  0:02:23s
epoch 45 | loss: 0.14957 | val_0_rmse: 0.38405 | val_1_rmse: 0.38404 |  0:02:26s
epoch 46 | loss: 0.14888 | val_0_rmse: 0.34686 | val_1_rmse: 0.34704 |  0:02:29s
epoch 47 | loss: 0.15096 | val_0_rmse: 0.37951 | val_1_rmse: 0.3825  |  0:02:32s
epoch 48 | loss: 0.13279 | val_0_rmse: 0.3463  | val_1_rmse: 0.34772 |  0:02:36s
epoch 49 | loss: 0.13746 | val_0_rmse: 0.35527 | val_1_rmse: 0.3545  |  0:02:39s
epoch 50 | loss: 0.14305 | val_0_rmse: 0.34398 | val_1_rmse: 0.34439 |  0:02:42s
epoch 51 | loss: 0.19457 | val_0_rmse: 0.38073 | val_1_rmse: 0.38186 |  0:02:45s
epoch 52 | loss: 0.1999  | val_0_rmse: 0.33148 | val_1_rmse: 0.33202 |  0:02:48s
epoch 53 | loss: 0.13305 | val_0_rmse: 0.38769 | val_1_rmse: 0.38615 |  0:02:51s
epoch 54 | loss: 0.14971 | val_0_rmse: 0.33146 | val_1_rmse: 0.33269 |  0:02:55s
epoch 55 | loss: 0.12907 | val_0_rmse: 0.33325 | val_1_rmse: 0.33515 |  0:02:58s
epoch 56 | loss: 0.14581 | val_0_rmse: 0.43136 | val_1_rmse: 0.43123 |  0:03:01s
epoch 57 | loss: 0.14402 | val_0_rmse: 0.34782 | val_1_rmse: 0.34728 |  0:03:04s
epoch 58 | loss: 0.1546  | val_0_rmse: 0.3634  | val_1_rmse: 0.36424 |  0:03:07s
epoch 59 | loss: 0.14818 | val_0_rmse: 0.36379 | val_1_rmse: 0.36336 |  0:03:11s
epoch 60 | loss: 0.14678 | val_0_rmse: 0.33347 | val_1_rmse: 0.33414 |  0:03:14s
epoch 61 | loss: 0.15545 | val_0_rmse: 0.52549 | val_1_rmse: 0.52706 |  0:03:17s
epoch 62 | loss: 0.20437 | val_0_rmse: 0.36421 | val_1_rmse: 0.36486 |  0:03:20s
epoch 63 | loss: 0.16142 | val_0_rmse: 0.34541 | val_1_rmse: 0.34723 |  0:03:23s
epoch 64 | loss: 0.12743 | val_0_rmse: 0.34228 | val_1_rmse: 0.34388 |  0:03:26s
epoch 65 | loss: 0.13341 | val_0_rmse: 0.33108 | val_1_rmse: 0.33189 |  0:03:30s
epoch 66 | loss: 0.13284 | val_0_rmse: 0.43598 | val_1_rmse: 0.4366  |  0:03:33s
epoch 67 | loss: 0.13265 | val_0_rmse: 0.32925 | val_1_rmse: 0.33177 |  0:03:36s
epoch 68 | loss: 0.12941 | val_0_rmse: 0.3812  | val_1_rmse: 0.38297 |  0:03:39s
epoch 69 | loss: 0.15143 | val_0_rmse: 0.37208 | val_1_rmse: 0.37213 |  0:03:42s
epoch 70 | loss: 0.14722 | val_0_rmse: 0.44847 | val_1_rmse: 0.4499  |  0:03:46s
epoch 71 | loss: 0.14724 | val_0_rmse: 0.33571 | val_1_rmse: 0.33714 |  0:03:49s
epoch 72 | loss: 0.13654 | val_0_rmse: 0.37553 | val_1_rmse: 0.37685 |  0:03:52s
epoch 73 | loss: 0.12564 | val_0_rmse: 0.33146 | val_1_rmse: 0.33299 |  0:03:55s
epoch 74 | loss: 0.12294 | val_0_rmse: 0.3582  | val_1_rmse: 0.361   |  0:03:58s
epoch 75 | loss: 0.13395 | val_0_rmse: 0.35404 | val_1_rmse: 0.3576  |  0:04:01s
epoch 76 | loss: 0.15404 | val_0_rmse: 0.3782  | val_1_rmse: 0.38401 |  0:04:05s
epoch 77 | loss: 0.13127 | val_0_rmse: 0.34079 | val_1_rmse: 0.34404 |  0:04:08s
epoch 78 | loss: 0.12979 | val_0_rmse: 0.34634 | val_1_rmse: 0.34868 |  0:04:11s
epoch 79 | loss: 0.13166 | val_0_rmse: 0.3528  | val_1_rmse: 0.35568 |  0:04:14s
epoch 80 | loss: 0.12354 | val_0_rmse: 0.3304  | val_1_rmse: 0.33465 |  0:04:17s
epoch 81 | loss: 0.1235  | val_0_rmse: 0.33549 | val_1_rmse: 0.33817 |  0:04:21s
epoch 82 | loss: 0.13124 | val_0_rmse: 0.34295 | val_1_rmse: 0.34624 |  0:04:24s
epoch 83 | loss: 0.17953 | val_0_rmse: 0.36014 | val_1_rmse: 0.36272 |  0:04:27s
epoch 84 | loss: 0.12852 | val_0_rmse: 0.37087 | val_1_rmse: 0.3739  |  0:04:30s
epoch 85 | loss: 0.14246 | val_0_rmse: 0.36737 | val_1_rmse: 0.36954 |  0:04:33s
epoch 86 | loss: 0.14185 | val_0_rmse: 0.33373 | val_1_rmse: 0.33766 |  0:04:36s
epoch 87 | loss: 0.14327 | val_0_rmse: 0.3636  | val_1_rmse: 0.36592 |  0:04:40s
epoch 88 | loss: 0.1435  | val_0_rmse: 0.37576 | val_1_rmse: 0.37851 |  0:04:43s
epoch 89 | loss: 0.14087 | val_0_rmse: 0.3478  | val_1_rmse: 0.35197 |  0:04:46s
epoch 90 | loss: 0.1399  | val_0_rmse: 0.34132 | val_1_rmse: 0.34675 |  0:04:49s
epoch 91 | loss: 0.13828 | val_0_rmse: 0.38129 | val_1_rmse: 0.3868  |  0:04:52s
epoch 92 | loss: 0.14303 | val_0_rmse: 0.35206 | val_1_rmse: 0.35636 |  0:04:56s
epoch 93 | loss: 0.14071 | val_0_rmse: 0.34931 | val_1_rmse: 0.35241 |  0:04:59s
epoch 94 | loss: 0.13056 | val_0_rmse: 0.3361  | val_1_rmse: 0.34168 |  0:05:02s
epoch 95 | loss: 0.12448 | val_0_rmse: 0.3516  | val_1_rmse: 0.35451 |  0:05:05s
epoch 96 | loss: 0.14124 | val_0_rmse: 0.32565 | val_1_rmse: 0.33025 |  0:05:08s
epoch 97 | loss: 0.12217 | val_0_rmse: 0.33346 | val_1_rmse: 0.33603 |  0:05:12s
epoch 98 | loss: 0.12428 | val_0_rmse: 0.32826 | val_1_rmse: 0.33141 |  0:05:15s
epoch 99 | loss: 0.1242  | val_0_rmse: 0.34875 | val_1_rmse: 0.35323 |  0:05:18s
epoch 100| loss: 0.14614 | val_0_rmse: 0.38669 | val_1_rmse: 0.38907 |  0:05:21s
epoch 101| loss: 0.12451 | val_0_rmse: 0.34    | val_1_rmse: 0.34529 |  0:05:24s
epoch 102| loss: 0.12281 | val_0_rmse: 0.3767  | val_1_rmse: 0.37957 |  0:05:27s
epoch 103| loss: 0.15775 | val_0_rmse: 0.51765 | val_1_rmse: 0.51986 |  0:05:31s
epoch 104| loss: 0.1306  | val_0_rmse: 0.37542 | val_1_rmse: 0.37936 |  0:05:34s
epoch 105| loss: 0.12828 | val_0_rmse: 0.3327  | val_1_rmse: 0.33609 |  0:05:37s
epoch 106| loss: 0.14103 | val_0_rmse: 0.34837 | val_1_rmse: 0.35242 |  0:05:40s
epoch 107| loss: 0.13284 | val_0_rmse: 0.32015 | val_1_rmse: 0.32448 |  0:05:43s
epoch 108| loss: 0.12009 | val_0_rmse: 0.34254 | val_1_rmse: 0.34553 |  0:05:47s
epoch 109| loss: 0.132   | val_0_rmse: 0.31814 | val_1_rmse: 0.32515 |  0:05:50s
epoch 110| loss: 0.11796 | val_0_rmse: 0.32732 | val_1_rmse: 0.3326  |  0:05:53s
epoch 111| loss: 0.13669 | val_0_rmse: 0.38566 | val_1_rmse: 0.38946 |  0:05:56s
epoch 112| loss: 0.1383  | val_0_rmse: 0.35529 | val_1_rmse: 0.35998 |  0:05:59s
epoch 113| loss: 0.11883 | val_0_rmse: 0.32045 | val_1_rmse: 0.32419 |  0:06:02s
epoch 114| loss: 0.13522 | val_0_rmse: 0.32035 | val_1_rmse: 0.32404 |  0:06:06s
epoch 115| loss: 0.15194 | val_0_rmse: 0.35007 | val_1_rmse: 0.35459 |  0:06:09s
epoch 116| loss: 0.12034 | val_0_rmse: 0.32506 | val_1_rmse: 0.32845 |  0:06:12s
epoch 117| loss: 0.1354  | val_0_rmse: 0.36588 | val_1_rmse: 0.36993 |  0:06:15s
epoch 118| loss: 0.13921 | val_0_rmse: 0.33067 | val_1_rmse: 0.33431 |  0:06:18s
epoch 119| loss: 0.13422 | val_0_rmse: 0.3472  | val_1_rmse: 0.35252 |  0:06:22s
epoch 120| loss: 0.13783 | val_0_rmse: 0.38026 | val_1_rmse: 0.38692 |  0:06:25s
epoch 121| loss: 0.15374 | val_0_rmse: 0.3758  | val_1_rmse: 0.37985 |  0:06:28s
epoch 122| loss: 0.14637 | val_0_rmse: 0.31716 | val_1_rmse: 0.32184 |  0:06:31s
epoch 123| loss: 0.13625 | val_0_rmse: 0.3384  | val_1_rmse: 0.345   |  0:06:34s
epoch 124| loss: 0.1391  | val_0_rmse: 0.36721 | val_1_rmse: 0.37415 |  0:06:38s
epoch 125| loss: 0.14847 | val_0_rmse: 0.32814 | val_1_rmse: 0.33333 |  0:06:41s
epoch 126| loss: 0.12955 | val_0_rmse: 0.36251 | val_1_rmse: 0.36892 |  0:06:44s
epoch 127| loss: 0.13607 | val_0_rmse: 0.34095 | val_1_rmse: 0.34653 |  0:06:47s
epoch 128| loss: 0.13014 | val_0_rmse: 0.32551 | val_1_rmse: 0.32985 |  0:06:50s
epoch 129| loss: 0.11665 | val_0_rmse: 0.36907 | val_1_rmse: 0.37417 |  0:06:54s
epoch 130| loss: 0.13682 | val_0_rmse: 0.3783  | val_1_rmse: 0.3835  |  0:06:57s
epoch 131| loss: 0.13544 | val_0_rmse: 0.33178 | val_1_rmse: 0.33654 |  0:07:00s
epoch 132| loss: 0.13287 | val_0_rmse: 0.33899 | val_1_rmse: 0.34683 |  0:07:03s
epoch 133| loss: 0.12806 | val_0_rmse: 0.33277 | val_1_rmse: 0.33827 |  0:07:06s
epoch 134| loss: 0.13226 | val_0_rmse: 0.32757 | val_1_rmse: 0.33458 |  0:07:09s
epoch 135| loss: 0.14769 | val_0_rmse: 0.33464 | val_1_rmse: 0.34051 |  0:07:13s
epoch 136| loss: 0.12375 | val_0_rmse: 0.31954 | val_1_rmse: 0.32482 |  0:07:16s
epoch 137| loss: 0.11669 | val_0_rmse: 0.3284  | val_1_rmse: 0.33496 |  0:07:19s
epoch 138| loss: 0.11598 | val_0_rmse: 0.33053 | val_1_rmse: 0.33749 |  0:07:22s
epoch 139| loss: 0.1187  | val_0_rmse: 0.32635 | val_1_rmse: 0.33202 |  0:07:25s
epoch 140| loss: 0.1189  | val_0_rmse: 0.33704 | val_1_rmse: 0.34239 |  0:07:28s
epoch 141| loss: 0.13687 | val_0_rmse: 0.31979 | val_1_rmse: 0.32612 |  0:07:32s
epoch 142| loss: 0.11975 | val_0_rmse: 0.31466 | val_1_rmse: 0.32405 |  0:07:35s
epoch 143| loss: 0.12447 | val_0_rmse: 0.33632 | val_1_rmse: 0.34211 |  0:07:38s
epoch 144| loss: 0.12994 | val_0_rmse: 0.33605 | val_1_rmse: 0.3418  |  0:07:41s
epoch 145| loss: 0.12128 | val_0_rmse: 0.32583 | val_1_rmse: 0.33029 |  0:07:44s
epoch 146| loss: 0.12144 | val_0_rmse: 0.34923 | val_1_rmse: 0.35702 |  0:07:48s
epoch 147| loss: 0.11714 | val_0_rmse: 0.3238  | val_1_rmse: 0.3301  |  0:07:51s
epoch 148| loss: 0.11408 | val_0_rmse: 0.31549 | val_1_rmse: 0.32556 |  0:07:54s
epoch 149| loss: 0.13639 | val_0_rmse: 0.36609 | val_1_rmse: 0.37197 |  0:07:57s
Stop training because you reached max_epochs = 150 with best_epoch = 122 and best_val_1_rmse = 0.32184
Best weights from best epoch are automatically used!
ended training at: 05:23:43
Feature importance:
[('Area', 0.0), ('Baths', 0.13642370461419653), ('Beds', 0.2348170687247384), ('Latitude', 0.07514587751111507), ('Longitude', 0.09102737621874647), ('Month', 0.18753460776860684), ('Year', 0.2750513651625967)]
Mean squared error is of 10545147120.820303
Mean absolute error:70831.9619454578
MAPE:0.24621632147388484
R2 score:0.8182218844028915
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:23:43
epoch 0  | loss: 21.54344| val_0_rmse: 0.70863 | val_1_rmse: 0.71153 |  0:00:03s
epoch 1  | loss: 0.35582 | val_0_rmse: 0.51688 | val_1_rmse: 0.52095 |  0:00:06s
epoch 2  | loss: 0.27256 | val_0_rmse: 0.47824 | val_1_rmse: 0.47814 |  0:00:09s
epoch 3  | loss: 0.24352 | val_0_rmse: 0.4882  | val_1_rmse: 0.48697 |  0:00:12s
epoch 4  | loss: 0.27078 | val_0_rmse: 0.44683 | val_1_rmse: 0.44275 |  0:00:15s
epoch 5  | loss: 0.23572 | val_0_rmse: 0.44894 | val_1_rmse: 0.44529 |  0:00:19s
epoch 6  | loss: 0.2245  | val_0_rmse: 0.43252 | val_1_rmse: 0.42884 |  0:00:22s
epoch 7  | loss: 0.21821 | val_0_rmse: 0.42811 | val_1_rmse: 0.42281 |  0:00:25s
epoch 8  | loss: 0.22323 | val_0_rmse: 0.48889 | val_1_rmse: 0.48451 |  0:00:28s
epoch 9  | loss: 0.24044 | val_0_rmse: 0.53348 | val_1_rmse: 0.52958 |  0:00:31s
epoch 10 | loss: 0.21742 | val_0_rmse: 0.44222 | val_1_rmse: 0.43829 |  0:00:35s
epoch 11 | loss: 0.2031  | val_0_rmse: 0.4168  | val_1_rmse: 0.41239 |  0:00:38s
epoch 12 | loss: 0.19344 | val_0_rmse: 0.43389 | val_1_rmse: 0.42983 |  0:00:41s
epoch 13 | loss: 0.19638 | val_0_rmse: 0.39923 | val_1_rmse: 0.39517 |  0:00:44s
epoch 14 | loss: 0.19193 | val_0_rmse: 0.39427 | val_1_rmse: 0.38856 |  0:00:47s
epoch 15 | loss: 0.19246 | val_0_rmse: 0.44669 | val_1_rmse: 0.44391 |  0:00:51s
epoch 16 | loss: 0.18904 | val_0_rmse: 0.41015 | val_1_rmse: 0.40449 |  0:00:54s
epoch 17 | loss: 0.18407 | val_0_rmse: 0.38079 | val_1_rmse: 0.37771 |  0:00:57s
epoch 18 | loss: 0.18208 | val_0_rmse: 0.4301  | val_1_rmse: 0.42734 |  0:01:00s
epoch 19 | loss: 0.18553 | val_0_rmse: 0.40923 | val_1_rmse: 0.40668 |  0:01:03s
epoch 20 | loss: 0.18281 | val_0_rmse: 0.39012 | val_1_rmse: 0.38478 |  0:01:06s
epoch 21 | loss: 0.17994 | val_0_rmse: 0.40532 | val_1_rmse: 0.40197 |  0:01:10s
epoch 22 | loss: 0.18209 | val_0_rmse: 0.43756 | val_1_rmse: 0.43457 |  0:01:13s
epoch 23 | loss: 0.18082 | val_0_rmse: 0.42594 | val_1_rmse: 0.42104 |  0:01:16s
epoch 24 | loss: 0.17227 | val_0_rmse: 0.37818 | val_1_rmse: 0.3725  |  0:01:19s
epoch 25 | loss: 0.19257 | val_0_rmse: 0.36897 | val_1_rmse: 0.36273 |  0:01:22s
epoch 26 | loss: 0.17925 | val_0_rmse: 0.38415 | val_1_rmse: 0.37883 |  0:01:26s
epoch 27 | loss: 0.15995 | val_0_rmse: 0.41585 | val_1_rmse: 0.41156 |  0:01:29s
epoch 28 | loss: 0.17836 | val_0_rmse: 0.41394 | val_1_rmse: 0.41255 |  0:01:32s
epoch 29 | loss: 0.17147 | val_0_rmse: 0.37525 | val_1_rmse: 0.37111 |  0:01:35s
epoch 30 | loss: 0.16645 | val_0_rmse: 0.36989 | val_1_rmse: 0.36627 |  0:01:38s
epoch 31 | loss: 0.17063 | val_0_rmse: 0.395   | val_1_rmse: 0.39108 |  0:01:42s
epoch 32 | loss: 0.16754 | val_0_rmse: 0.34953 | val_1_rmse: 0.34411 |  0:01:45s
epoch 33 | loss: 0.16245 | val_0_rmse: 0.35824 | val_1_rmse: 0.35474 |  0:01:48s
epoch 34 | loss: 0.16481 | val_0_rmse: 0.39323 | val_1_rmse: 0.39195 |  0:01:51s
epoch 35 | loss: 0.15986 | val_0_rmse: 0.34772 | val_1_rmse: 0.34232 |  0:01:54s
epoch 36 | loss: 0.18563 | val_0_rmse: 0.37835 | val_1_rmse: 0.37682 |  0:01:57s
epoch 37 | loss: 0.16526 | val_0_rmse: 0.35465 | val_1_rmse: 0.35011 |  0:02:01s
epoch 38 | loss: 0.19066 | val_0_rmse: 0.36628 | val_1_rmse: 0.3604  |  0:02:04s
epoch 39 | loss: 0.15828 | val_0_rmse: 0.35947 | val_1_rmse: 0.35567 |  0:02:07s
epoch 40 | loss: 0.16065 | val_0_rmse: 0.38575 | val_1_rmse: 0.38277 |  0:02:10s
epoch 41 | loss: 0.16117 | val_0_rmse: 0.37066 | val_1_rmse: 0.36695 |  0:02:13s
epoch 42 | loss: 0.15639 | val_0_rmse: 0.37208 | val_1_rmse: 0.36723 |  0:02:17s
epoch 43 | loss: 0.15593 | val_0_rmse: 0.36502 | val_1_rmse: 0.36051 |  0:02:20s
epoch 44 | loss: 0.15782 | val_0_rmse: 0.3646  | val_1_rmse: 0.3626  |  0:02:23s
epoch 45 | loss: 0.16195 | val_0_rmse: 0.37322 | val_1_rmse: 0.36777 |  0:02:26s
epoch 46 | loss: 0.16341 | val_0_rmse: 0.38248 | val_1_rmse: 0.38001 |  0:02:29s
epoch 47 | loss: 0.15782 | val_0_rmse: 0.3404  | val_1_rmse: 0.335   |  0:02:32s
epoch 48 | loss: 0.15474 | val_0_rmse: 0.36089 | val_1_rmse: 0.3581  |  0:02:36s
epoch 49 | loss: 0.16308 | val_0_rmse: 0.4209  | val_1_rmse: 0.41879 |  0:02:39s
epoch 50 | loss: 0.15894 | val_0_rmse: 0.36281 | val_1_rmse: 0.36012 |  0:02:42s
epoch 51 | loss: 0.17127 | val_0_rmse: 0.37216 | val_1_rmse: 0.3675  |  0:02:45s
epoch 52 | loss: 0.16091 | val_0_rmse: 0.35905 | val_1_rmse: 0.35397 |  0:02:48s
epoch 53 | loss: 0.1531  | val_0_rmse: 0.37968 | val_1_rmse: 0.37642 |  0:02:52s
epoch 54 | loss: 0.15063 | val_0_rmse: 0.3899  | val_1_rmse: 0.38534 |  0:02:55s
epoch 55 | loss: 0.14448 | val_0_rmse: 0.37835 | val_1_rmse: 0.375   |  0:02:58s
epoch 56 | loss: 0.16033 | val_0_rmse: 0.36111 | val_1_rmse: 0.35842 |  0:03:01s
epoch 57 | loss: 0.16618 | val_0_rmse: 0.35983 | val_1_rmse: 0.35635 |  0:03:04s
epoch 58 | loss: 0.15498 | val_0_rmse: 0.37347 | val_1_rmse: 0.36679 |  0:03:08s
epoch 59 | loss: 0.16884 | val_0_rmse: 0.35113 | val_1_rmse: 0.34715 |  0:03:11s
epoch 60 | loss: 0.13803 | val_0_rmse: 0.34851 | val_1_rmse: 0.34289 |  0:03:14s
epoch 61 | loss: 0.13713 | val_0_rmse: 0.3591  | val_1_rmse: 0.35709 |  0:03:17s
epoch 62 | loss: 0.16573 | val_0_rmse: 0.36941 | val_1_rmse: 0.36439 |  0:03:20s
epoch 63 | loss: 0.15319 | val_0_rmse: 0.35031 | val_1_rmse: 0.34381 |  0:03:24s
epoch 64 | loss: 0.15155 | val_0_rmse: 0.35197 | val_1_rmse: 0.34609 |  0:03:27s
epoch 65 | loss: 0.15722 | val_0_rmse: 0.3905  | val_1_rmse: 0.38762 |  0:03:30s
epoch 66 | loss: 0.15319 | val_0_rmse: 0.35601 | val_1_rmse: 0.35268 |  0:03:33s
epoch 67 | loss: 0.14839 | val_0_rmse: 0.37186 | val_1_rmse: 0.36685 |  0:03:36s
epoch 68 | loss: 0.14778 | val_0_rmse: 0.40609 | val_1_rmse: 0.4026  |  0:03:39s
epoch 69 | loss: 0.15885 | val_0_rmse: 0.39079 | val_1_rmse: 0.38793 |  0:03:43s
epoch 70 | loss: 0.1347  | val_0_rmse: 0.34123 | val_1_rmse: 0.336   |  0:03:46s
epoch 71 | loss: 0.14164 | val_0_rmse: 0.35995 | val_1_rmse: 0.35577 |  0:03:49s
epoch 72 | loss: 0.14366 | val_0_rmse: 0.34406 | val_1_rmse: 0.33899 |  0:03:52s
epoch 73 | loss: 0.14789 | val_0_rmse: 0.37594 | val_1_rmse: 0.37157 |  0:03:56s
epoch 74 | loss: 0.14577 | val_0_rmse: 0.35352 | val_1_rmse: 0.34907 |  0:03:59s
epoch 75 | loss: 0.14698 | val_0_rmse: 0.3359  | val_1_rmse: 0.33174 |  0:04:02s
epoch 76 | loss: 0.14353 | val_0_rmse: 0.34726 | val_1_rmse: 0.34422 |  0:04:05s
epoch 77 | loss: 0.14059 | val_0_rmse: 0.33303 | val_1_rmse: 0.32661 |  0:04:08s
epoch 78 | loss: 0.14459 | val_0_rmse: 0.41617 | val_1_rmse: 0.41218 |  0:04:11s
epoch 79 | loss: 0.1456  | val_0_rmse: 0.34022 | val_1_rmse: 0.33517 |  0:04:15s
epoch 80 | loss: 0.15586 | val_0_rmse: 0.37883 | val_1_rmse: 0.37558 |  0:04:18s
epoch 81 | loss: 0.14545 | val_0_rmse: 0.33789 | val_1_rmse: 0.33314 |  0:04:21s
epoch 82 | loss: 0.13681 | val_0_rmse: 0.34717 | val_1_rmse: 0.34518 |  0:04:24s
epoch 83 | loss: 0.14665 | val_0_rmse: 0.38675 | val_1_rmse: 0.38332 |  0:04:27s
epoch 84 | loss: 0.13993 | val_0_rmse: 0.33661 | val_1_rmse: 0.33156 |  0:04:31s
epoch 85 | loss: 0.14592 | val_0_rmse: 0.44126 | val_1_rmse: 0.43827 |  0:04:34s
epoch 86 | loss: 0.15009 | val_0_rmse: 0.34573 | val_1_rmse: 0.34168 |  0:04:37s
epoch 87 | loss: 0.15073 | val_0_rmse: 0.36119 | val_1_rmse: 0.35639 |  0:04:40s
epoch 88 | loss: 0.14838 | val_0_rmse: 0.37307 | val_1_rmse: 0.36928 |  0:04:43s
epoch 89 | loss: 0.14067 | val_0_rmse: 0.37614 | val_1_rmse: 0.37194 |  0:04:46s
epoch 90 | loss: 0.12888 | val_0_rmse: 0.34528 | val_1_rmse: 0.34039 |  0:04:50s
epoch 91 | loss: 0.14606 | val_0_rmse: 0.39119 | val_1_rmse: 0.38919 |  0:04:53s
epoch 92 | loss: 0.14862 | val_0_rmse: 0.39513 | val_1_rmse: 0.39299 |  0:04:56s
epoch 93 | loss: 0.14582 | val_0_rmse: 0.3411  | val_1_rmse: 0.33871 |  0:04:59s
epoch 94 | loss: 0.14346 | val_0_rmse: 0.35893 | val_1_rmse: 0.35362 |  0:05:02s
epoch 95 | loss: 0.13097 | val_0_rmse: 0.34106 | val_1_rmse: 0.34018 |  0:05:06s
epoch 96 | loss: 0.14339 | val_0_rmse: 0.3416  | val_1_rmse: 0.33874 |  0:05:09s
epoch 97 | loss: 0.17371 | val_0_rmse: 0.34244 | val_1_rmse: 0.33877 |  0:05:12s
epoch 98 | loss: 0.13248 | val_0_rmse: 0.33018 | val_1_rmse: 0.3282  |  0:05:15s
epoch 99 | loss: 0.12815 | val_0_rmse: 0.32892 | val_1_rmse: 0.32703 |  0:05:18s
epoch 100| loss: 0.12864 | val_0_rmse: 0.33836 | val_1_rmse: 0.33535 |  0:05:22s
epoch 101| loss: 0.12496 | val_0_rmse: 0.33205 | val_1_rmse: 0.33011 |  0:05:25s
epoch 102| loss: 0.12183 | val_0_rmse: 0.36029 | val_1_rmse: 0.35735 |  0:05:28s
epoch 103| loss: 0.13405 | val_0_rmse: 0.33128 | val_1_rmse: 0.32691 |  0:05:31s
epoch 104| loss: 0.13506 | val_0_rmse: 0.33579 | val_1_rmse: 0.33256 |  0:05:34s
epoch 105| loss: 0.12711 | val_0_rmse: 0.35644 | val_1_rmse: 0.35447 |  0:05:37s
epoch 106| loss: 0.17478 | val_0_rmse: 0.3742  | val_1_rmse: 0.37306 |  0:05:41s
epoch 107| loss: 0.12901 | val_0_rmse: 0.32669 | val_1_rmse: 0.32404 |  0:05:44s
epoch 108| loss: 0.12694 | val_0_rmse: 0.33945 | val_1_rmse: 0.3357  |  0:05:47s
epoch 109| loss: 0.15513 | val_0_rmse: 0.33299 | val_1_rmse: 0.32942 |  0:05:50s
epoch 110| loss: 0.13534 | val_0_rmse: 0.3344  | val_1_rmse: 0.33202 |  0:05:53s
epoch 111| loss: 0.12249 | val_0_rmse: 0.36241 | val_1_rmse: 0.36035 |  0:05:57s
epoch 112| loss: 0.13774 | val_0_rmse: 0.33944 | val_1_rmse: 0.33641 |  0:06:00s
epoch 113| loss: 0.13941 | val_0_rmse: 0.41856 | val_1_rmse: 0.41754 |  0:06:03s
epoch 114| loss: 0.13497 | val_0_rmse: 0.33207 | val_1_rmse: 0.32934 |  0:06:06s
epoch 115| loss: 0.1266  | val_0_rmse: 0.3328  | val_1_rmse: 0.32951 |  0:06:09s
epoch 116| loss: 0.13746 | val_0_rmse: 0.33885 | val_1_rmse: 0.33463 |  0:06:13s
epoch 117| loss: 0.13046 | val_0_rmse: 0.35412 | val_1_rmse: 0.35095 |  0:06:16s
epoch 118| loss: 0.13039 | val_0_rmse: 0.35196 | val_1_rmse: 0.34895 |  0:06:19s
epoch 119| loss: 0.14408 | val_0_rmse: 0.37995 | val_1_rmse: 0.37648 |  0:06:22s
epoch 120| loss: 0.13087 | val_0_rmse: 0.36952 | val_1_rmse: 0.36661 |  0:06:25s
epoch 121| loss: 0.1366  | val_0_rmse: 0.33054 | val_1_rmse: 0.32794 |  0:06:28s
epoch 122| loss: 0.14219 | val_0_rmse: 0.37697 | val_1_rmse: 0.37444 |  0:06:32s
epoch 123| loss: 0.14068 | val_0_rmse: 0.34227 | val_1_rmse: 0.34119 |  0:06:35s
epoch 124| loss: 0.13557 | val_0_rmse: 0.33027 | val_1_rmse: 0.32879 |  0:06:38s
epoch 125| loss: 0.12683 | val_0_rmse: 0.36823 | val_1_rmse: 0.36683 |  0:06:41s
epoch 126| loss: 0.14673 | val_0_rmse: 0.45164 | val_1_rmse: 0.45106 |  0:06:44s
epoch 127| loss: 0.14668 | val_0_rmse: 0.37646 | val_1_rmse: 0.37486 |  0:06:48s
epoch 128| loss: 0.22893 | val_0_rmse: 0.41204 | val_1_rmse: 0.40866 |  0:06:51s
epoch 129| loss: 0.19369 | val_0_rmse: 0.44635 | val_1_rmse: 0.4461  |  0:06:54s
epoch 130| loss: 0.16511 | val_0_rmse: 0.32754 | val_1_rmse: 0.32537 |  0:06:57s
epoch 131| loss: 0.15064 | val_0_rmse: 0.34963 | val_1_rmse: 0.34794 |  0:07:00s
epoch 132| loss: 0.1514  | val_0_rmse: 0.34677 | val_1_rmse: 0.34438 |  0:07:03s
epoch 133| loss: 0.13293 | val_0_rmse: 0.33494 | val_1_rmse: 0.33442 |  0:07:07s
epoch 134| loss: 0.13632 | val_0_rmse: 0.33457 | val_1_rmse: 0.33242 |  0:07:10s
epoch 135| loss: 0.13834 | val_0_rmse: 0.39908 | val_1_rmse: 0.39733 |  0:07:13s
epoch 136| loss: 0.12112 | val_0_rmse: 0.33513 | val_1_rmse: 0.33366 |  0:07:16s
epoch 137| loss: 0.12689 | val_0_rmse: 0.33895 | val_1_rmse: 0.33786 |  0:07:19s

Early stopping occured at epoch 137 with best_epoch = 107 and best_val_1_rmse = 0.32404
Best weights from best epoch are automatically used!
ended training at: 05:31:04
Feature importance:
[('Area', 0.04237993435196614), ('Baths', 0.0), ('Beds', 0.16149804976289298), ('Latitude', 0.22969120941769847), ('Longitude', 0.3192522693177872), ('Month', 0.0), ('Year', 0.2471785371496552)]
Mean squared error is of 10908671925.832838
Mean absolute error:72225.68321950224
MAPE:0.2717861291163247
R2 score:0.8129550008930998
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:31:04
epoch 0  | loss: 69.69241| val_0_rmse: 4.26886 | val_1_rmse: 4.36343 |  0:00:00s
epoch 1  | loss: 3.87487 | val_0_rmse: 1.4579  | val_1_rmse: 1.45293 |  0:00:01s
epoch 2  | loss: 0.36055 | val_0_rmse: 1.08163 | val_1_rmse: 1.09261 |  0:00:02s
epoch 3  | loss: 0.20952 | val_0_rmse: 0.41638 | val_1_rmse: 0.41545 |  0:00:03s
epoch 4  | loss: 0.22031 | val_0_rmse: 0.524   | val_1_rmse: 0.5321  |  0:00:04s
epoch 5  | loss: 0.20672 | val_0_rmse: 0.35145 | val_1_rmse: 0.35273 |  0:00:05s
epoch 6  | loss: 0.17933 | val_0_rmse: 0.33905 | val_1_rmse: 0.33526 |  0:00:06s
epoch 7  | loss: 0.13454 | val_0_rmse: 0.35747 | val_1_rmse: 0.35024 |  0:00:07s
epoch 8  | loss: 0.19956 | val_0_rmse: 0.36047 | val_1_rmse: 0.35994 |  0:00:08s
epoch 9  | loss: 0.22356 | val_0_rmse: 0.43879 | val_1_rmse: 0.44493 |  0:00:09s
epoch 10 | loss: 0.22799 | val_0_rmse: 0.41314 | val_1_rmse: 0.41082 |  0:00:10s
epoch 11 | loss: 0.21273 | val_0_rmse: 0.48899 | val_1_rmse: 0.49706 |  0:00:11s
epoch 12 | loss: 0.20128 | val_0_rmse: 0.29857 | val_1_rmse: 0.29636 |  0:00:12s
epoch 13 | loss: 0.21367 | val_0_rmse: 0.31546 | val_1_rmse: 0.3105  |  0:00:13s
epoch 14 | loss: 0.19425 | val_0_rmse: 0.50771 | val_1_rmse: 0.51262 |  0:00:14s
epoch 15 | loss: 0.20316 | val_0_rmse: 0.37942 | val_1_rmse: 0.36994 |  0:00:15s
epoch 16 | loss: 0.19776 | val_0_rmse: 0.44107 | val_1_rmse: 0.44749 |  0:00:16s
epoch 17 | loss: 0.17276 | val_0_rmse: 0.43245 | val_1_rmse: 0.43399 |  0:00:16s
epoch 18 | loss: 0.12305 | val_0_rmse: 0.28854 | val_1_rmse: 0.29055 |  0:00:17s
epoch 19 | loss: 0.08872 | val_0_rmse: 0.30991 | val_1_rmse: 0.30859 |  0:00:18s
epoch 20 | loss: 0.09437 | val_0_rmse: 0.28181 | val_1_rmse: 0.28161 |  0:00:19s
epoch 21 | loss: 0.0831  | val_0_rmse: 0.25693 | val_1_rmse: 0.2597  |  0:00:20s
epoch 22 | loss: 0.08299 | val_0_rmse: 0.25395 | val_1_rmse: 0.25417 |  0:00:21s
epoch 23 | loss: 0.07711 | val_0_rmse: 0.28009 | val_1_rmse: 0.2825  |  0:00:22s
epoch 24 | loss: 0.07122 | val_0_rmse: 0.25489 | val_1_rmse: 0.25329 |  0:00:23s
epoch 25 | loss: 0.0988  | val_0_rmse: 0.32183 | val_1_rmse: 0.32127 |  0:00:24s
epoch 26 | loss: 0.14147 | val_0_rmse: 0.33659 | val_1_rmse: 0.3362  |  0:00:25s
epoch 27 | loss: 0.11317 | val_0_rmse: 0.29942 | val_1_rmse: 0.30154 |  0:00:26s
epoch 28 | loss: 0.09154 | val_0_rmse: 0.23766 | val_1_rmse: 0.23626 |  0:00:27s
epoch 29 | loss: 0.09253 | val_0_rmse: 0.23836 | val_1_rmse: 0.2352  |  0:00:28s
epoch 30 | loss: 0.0982  | val_0_rmse: 0.29367 | val_1_rmse: 0.28927 |  0:00:29s
epoch 31 | loss: 0.1011  | val_0_rmse: 0.2257  | val_1_rmse: 0.22209 |  0:00:30s
epoch 32 | loss: 0.09328 | val_0_rmse: 0.30219 | val_1_rmse: 0.30359 |  0:00:31s
epoch 33 | loss: 0.09193 | val_0_rmse: 0.33007 | val_1_rmse: 0.33248 |  0:00:32s
epoch 34 | loss: 0.09396 | val_0_rmse: 0.25085 | val_1_rmse: 0.2513  |  0:00:33s
epoch 35 | loss: 0.09785 | val_0_rmse: 0.22896 | val_1_rmse: 0.22586 |  0:00:34s
epoch 36 | loss: 0.07696 | val_0_rmse: 0.23946 | val_1_rmse: 0.2375  |  0:00:34s
epoch 37 | loss: 0.08714 | val_0_rmse: 0.31631 | val_1_rmse: 0.31762 |  0:00:35s
epoch 38 | loss: 0.10423 | val_0_rmse: 0.34731 | val_1_rmse: 0.35113 |  0:00:36s
epoch 39 | loss: 0.10679 | val_0_rmse: 0.28708 | val_1_rmse: 0.28917 |  0:00:37s
epoch 40 | loss: 0.09288 | val_0_rmse: 0.26286 | val_1_rmse: 0.25831 |  0:00:38s
epoch 41 | loss: 0.09185 | val_0_rmse: 0.28563 | val_1_rmse: 0.28179 |  0:00:39s
epoch 42 | loss: 0.08766 | val_0_rmse: 0.23771 | val_1_rmse: 0.23342 |  0:00:40s
epoch 43 | loss: 0.08368 | val_0_rmse: 0.2744  | val_1_rmse: 0.27575 |  0:00:41s
epoch 44 | loss: 0.0869  | val_0_rmse: 0.33408 | val_1_rmse: 0.3382  |  0:00:42s
epoch 45 | loss: 0.08998 | val_0_rmse: 0.28376 | val_1_rmse: 0.28589 |  0:00:43s
epoch 46 | loss: 0.08603 | val_0_rmse: 0.2286  | val_1_rmse: 0.22683 |  0:00:44s
epoch 47 | loss: 0.08466 | val_0_rmse: 0.26342 | val_1_rmse: 0.25932 |  0:00:45s
epoch 48 | loss: 0.08683 | val_0_rmse: 0.23811 | val_1_rmse: 0.23364 |  0:00:46s
epoch 49 | loss: 0.08427 | val_0_rmse: 0.26219 | val_1_rmse: 0.26405 |  0:00:47s
epoch 50 | loss: 0.08789 | val_0_rmse: 0.3314  | val_1_rmse: 0.33356 |  0:00:48s
epoch 51 | loss: 0.08849 | val_0_rmse: 0.26263 | val_1_rmse: 0.26303 |  0:00:49s
epoch 52 | loss: 0.08853 | val_0_rmse: 0.22954 | val_1_rmse: 0.22478 |  0:00:49s
epoch 53 | loss: 0.08667 | val_0_rmse: 0.24682 | val_1_rmse: 0.24529 |  0:00:50s
epoch 54 | loss: 0.08671 | val_0_rmse: 0.22475 | val_1_rmse: 0.22129 |  0:00:51s
epoch 55 | loss: 0.08505 | val_0_rmse: 0.2652  | val_1_rmse: 0.26517 |  0:00:52s
epoch 56 | loss: 0.08535 | val_0_rmse: 0.30175 | val_1_rmse: 0.30306 |  0:00:53s
epoch 57 | loss: 0.08551 | val_0_rmse: 0.24685 | val_1_rmse: 0.24623 |  0:00:54s
epoch 58 | loss: 0.08182 | val_0_rmse: 0.25596 | val_1_rmse: 0.25191 |  0:00:55s
epoch 59 | loss: 0.08559 | val_0_rmse: 0.29126 | val_1_rmse: 0.2878  |  0:00:56s
epoch 60 | loss: 0.0864  | val_0_rmse: 0.22218 | val_1_rmse: 0.21979 |  0:00:57s
epoch 61 | loss: 0.08253 | val_0_rmse: 0.28548 | val_1_rmse: 0.28586 |  0:00:58s
epoch 62 | loss: 0.0841  | val_0_rmse: 0.30722 | val_1_rmse: 0.3082  |  0:00:59s
epoch 63 | loss: 0.08074 | val_0_rmse: 0.27142 | val_1_rmse: 0.27068 |  0:01:00s
epoch 64 | loss: 0.08385 | val_0_rmse: 0.22363 | val_1_rmse: 0.22147 |  0:01:01s
epoch 65 | loss: 0.08168 | val_0_rmse: 0.24111 | val_1_rmse: 0.23876 |  0:01:02s
epoch 66 | loss: 0.08466 | val_0_rmse: 0.21531 | val_1_rmse: 0.2146  |  0:01:03s
epoch 67 | loss: 0.08023 | val_0_rmse: 0.26446 | val_1_rmse: 0.26623 |  0:01:04s
epoch 68 | loss: 0.07867 | val_0_rmse: 0.26816 | val_1_rmse: 0.26939 |  0:01:05s
epoch 69 | loss: 0.08176 | val_0_rmse: 0.24681 | val_1_rmse: 0.24794 |  0:01:06s
epoch 70 | loss: 0.08039 | val_0_rmse: 0.22078 | val_1_rmse: 0.21929 |  0:01:06s
epoch 71 | loss: 0.08047 | val_0_rmse: 0.28747 | val_1_rmse: 0.2857  |  0:01:07s
epoch 72 | loss: 0.07928 | val_0_rmse: 0.24331 | val_1_rmse: 0.24048 |  0:01:08s
epoch 73 | loss: 0.0783  | val_0_rmse: 0.28483 | val_1_rmse: 0.28678 |  0:01:09s
epoch 74 | loss: 0.08221 | val_0_rmse: 0.32498 | val_1_rmse: 0.32742 |  0:01:10s
epoch 75 | loss: 0.08406 | val_0_rmse: 0.2529  | val_1_rmse: 0.25214 |  0:01:11s
epoch 76 | loss: 0.08514 | val_0_rmse: 0.24121 | val_1_rmse: 0.23751 |  0:01:12s
epoch 77 | loss: 0.08309 | val_0_rmse: 0.28315 | val_1_rmse: 0.27992 |  0:01:13s
epoch 78 | loss: 0.08014 | val_0_rmse: 0.22067 | val_1_rmse: 0.21782 |  0:01:14s
epoch 79 | loss: 0.07603 | val_0_rmse: 0.25583 | val_1_rmse: 0.25729 |  0:01:15s
epoch 80 | loss: 0.07455 | val_0_rmse: 0.2242  | val_1_rmse: 0.22471 |  0:01:16s
epoch 81 | loss: 0.09302 | val_0_rmse: 0.27379 | val_1_rmse: 0.27332 |  0:01:17s
epoch 82 | loss: 0.08057 | val_0_rmse: 0.25929 | val_1_rmse: 0.2605  |  0:01:18s
epoch 83 | loss: 0.0627  | val_0_rmse: 0.22783 | val_1_rmse: 0.22982 |  0:01:19s
epoch 84 | loss: 0.05657 | val_0_rmse: 0.23347 | val_1_rmse: 0.23161 |  0:01:20s
epoch 85 | loss: 0.05514 | val_0_rmse: 0.21414 | val_1_rmse: 0.21182 |  0:01:21s
epoch 86 | loss: 0.07925 | val_0_rmse: 0.25232 | val_1_rmse: 0.25243 |  0:01:21s
epoch 87 | loss: 0.06175 | val_0_rmse: 0.23576 | val_1_rmse: 0.23383 |  0:01:22s
epoch 88 | loss: 0.08419 | val_0_rmse: 0.31044 | val_1_rmse: 0.30665 |  0:01:23s
epoch 89 | loss: 0.13501 | val_0_rmse: 0.28531 | val_1_rmse: 0.28449 |  0:01:24s
epoch 90 | loss: 0.06047 | val_0_rmse: 0.22476 | val_1_rmse: 0.22407 |  0:01:25s
epoch 91 | loss: 0.05786 | val_0_rmse: 0.23849 | val_1_rmse: 0.23428 |  0:01:26s
epoch 92 | loss: 0.05542 | val_0_rmse: 0.21946 | val_1_rmse: 0.2148  |  0:01:27s
epoch 93 | loss: 0.07442 | val_0_rmse: 0.21812 | val_1_rmse: 0.21633 |  0:01:28s
epoch 94 | loss: 0.05149 | val_0_rmse: 0.24052 | val_1_rmse: 0.23491 |  0:01:29s
epoch 95 | loss: 0.06794 | val_0_rmse: 0.23308 | val_1_rmse: 0.23374 |  0:01:30s
epoch 96 | loss: 0.05658 | val_0_rmse: 0.24753 | val_1_rmse: 0.24863 |  0:01:31s
epoch 97 | loss: 0.05564 | val_0_rmse: 0.23007 | val_1_rmse: 0.23028 |  0:01:32s
epoch 98 | loss: 0.05459 | val_0_rmse: 0.20972 | val_1_rmse: 0.20672 |  0:01:33s
epoch 99 | loss: 0.05525 | val_0_rmse: 0.22712 | val_1_rmse: 0.22354 |  0:01:34s
epoch 100| loss: 0.05587 | val_0_rmse: 0.21972 | val_1_rmse: 0.21591 |  0:01:35s
epoch 101| loss: 0.05476 | val_0_rmse: 0.23304 | val_1_rmse: 0.23348 |  0:01:36s
epoch 102| loss: 0.05723 | val_0_rmse: 0.23636 | val_1_rmse: 0.23491 |  0:01:37s
epoch 103| loss: 0.05592 | val_0_rmse: 0.22509 | val_1_rmse: 0.2216  |  0:01:38s
epoch 104| loss: 0.05674 | val_0_rmse: 0.20642 | val_1_rmse: 0.20486 |  0:01:38s
epoch 105| loss: 0.05413 | val_0_rmse: 0.2257  | val_1_rmse: 0.2274  |  0:01:39s
epoch 106| loss: 0.05425 | val_0_rmse: 0.21492 | val_1_rmse: 0.21458 |  0:01:40s
epoch 107| loss: 0.05288 | val_0_rmse: 0.23877 | val_1_rmse: 0.23715 |  0:01:41s
epoch 108| loss: 0.05734 | val_0_rmse: 0.20605 | val_1_rmse: 0.20499 |  0:01:42s
epoch 109| loss: 0.07132 | val_0_rmse: 0.23789 | val_1_rmse: 0.23916 |  0:01:43s
epoch 110| loss: 0.05042 | val_0_rmse: 0.21242 | val_1_rmse: 0.2113  |  0:01:44s
epoch 111| loss: 0.04733 | val_0_rmse: 0.21677 | val_1_rmse: 0.2154  |  0:01:45s
epoch 112| loss: 0.04764 | val_0_rmse: 0.21184 | val_1_rmse: 0.21119 |  0:01:46s
epoch 113| loss: 0.05885 | val_0_rmse: 0.22934 | val_1_rmse: 0.22622 |  0:01:47s
epoch 114| loss: 0.05203 | val_0_rmse: 0.21403 | val_1_rmse: 0.21307 |  0:01:48s
epoch 115| loss: 0.05288 | val_0_rmse: 0.21721 | val_1_rmse: 0.21599 |  0:01:49s
epoch 116| loss: 0.05967 | val_0_rmse: 0.23678 | val_1_rmse: 0.23655 |  0:01:50s
epoch 117| loss: 0.06417 | val_0_rmse: 0.23799 | val_1_rmse: 0.23846 |  0:01:51s
epoch 118| loss: 0.05839 | val_0_rmse: 0.20483 | val_1_rmse: 0.20342 |  0:01:52s
epoch 119| loss: 0.05072 | val_0_rmse: 0.20605 | val_1_rmse: 0.20554 |  0:01:53s
epoch 120| loss: 0.06043 | val_0_rmse: 0.24632 | val_1_rmse: 0.24278 |  0:01:54s
epoch 121| loss: 0.09141 | val_0_rmse: 0.37325 | val_1_rmse: 0.37664 |  0:01:54s
epoch 122| loss: 0.09698 | val_0_rmse: 0.31058 | val_1_rmse: 0.30477 |  0:01:55s
epoch 123| loss: 0.06876 | val_0_rmse: 0.23946 | val_1_rmse: 0.23608 |  0:01:56s
epoch 124| loss: 0.05998 | val_0_rmse: 0.22384 | val_1_rmse: 0.22242 |  0:01:57s
epoch 125| loss: 0.05749 | val_0_rmse: 0.21586 | val_1_rmse: 0.21641 |  0:01:58s
epoch 126| loss: 0.0562  | val_0_rmse: 0.22069 | val_1_rmse: 0.22006 |  0:01:59s
epoch 127| loss: 0.05405 | val_0_rmse: 0.21183 | val_1_rmse: 0.20965 |  0:02:00s
epoch 128| loss: 0.05385 | val_0_rmse: 0.28244 | val_1_rmse: 0.28449 |  0:02:01s
epoch 129| loss: 0.06074 | val_0_rmse: 0.2086  | val_1_rmse: 0.20583 |  0:02:02s
epoch 130| loss: 0.06715 | val_0_rmse: 0.31857 | val_1_rmse: 0.31866 |  0:02:03s
epoch 131| loss: 0.06208 | val_0_rmse: 0.24782 | val_1_rmse: 0.24694 |  0:02:04s
epoch 132| loss: 0.06125 | val_0_rmse: 0.2156  | val_1_rmse: 0.2102  |  0:02:05s
epoch 133| loss: 0.05454 | val_0_rmse: 0.23168 | val_1_rmse: 0.23227 |  0:02:06s
epoch 134| loss: 0.05227 | val_0_rmse: 0.21021 | val_1_rmse: 0.20731 |  0:02:07s
epoch 135| loss: 0.04795 | val_0_rmse: 0.22259 | val_1_rmse: 0.2208  |  0:02:08s
epoch 136| loss: 0.04889 | val_0_rmse: 0.21594 | val_1_rmse: 0.21465 |  0:02:09s
epoch 137| loss: 0.05327 | val_0_rmse: 0.20289 | val_1_rmse: 0.20161 |  0:02:10s
epoch 138| loss: 0.05322 | val_0_rmse: 0.2375  | val_1_rmse: 0.2371  |  0:02:10s
epoch 139| loss: 0.05418 | val_0_rmse: 0.25309 | val_1_rmse: 0.25358 |  0:02:11s
epoch 140| loss: 0.05877 | val_0_rmse: 0.31648 | val_1_rmse: 0.3208  |  0:02:12s
epoch 141| loss: 0.08981 | val_0_rmse: 0.2309  | val_1_rmse: 0.23003 |  0:02:13s
epoch 142| loss: 0.08162 | val_0_rmse: 0.24968 | val_1_rmse: 0.24459 |  0:02:14s
epoch 143| loss: 0.08088 | val_0_rmse: 0.28368 | val_1_rmse: 0.27757 |  0:02:15s
epoch 144| loss: 0.07952 | val_0_rmse: 0.21063 | val_1_rmse: 0.20615 |  0:02:16s
epoch 145| loss: 0.07471 | val_0_rmse: 0.27933 | val_1_rmse: 0.28109 |  0:02:17s
epoch 146| loss: 0.08416 | val_0_rmse: 0.3258  | val_1_rmse: 0.32888 |  0:02:18s
epoch 147| loss: 0.08084 | val_0_rmse: 0.24159 | val_1_rmse: 0.24349 |  0:02:19s
epoch 148| loss: 0.07604 | val_0_rmse: 0.21626 | val_1_rmse: 0.21026 |  0:02:20s
epoch 149| loss: 0.07865 | val_0_rmse: 0.27598 | val_1_rmse: 0.2697  |  0:02:21s
Stop training because you reached max_epochs = 150 with best_epoch = 137 and best_val_1_rmse = 0.20161
Best weights from best epoch are automatically used!
ended training at: 05:33:26
Feature importance:
[('Area', 0.0), ('Baths', 0.19602911464935932), ('Beds', 0.0005883014306056204), ('Latitude', 0.38288830520918776), ('Longitude', 0.3659770567489005), ('Month', 0.007432671742147102), ('Year', 0.0470845502197997)]
Mean squared error is of 7615805974.844456
Mean absolute error:60327.48550830397
MAPE:0.15243765096688827
R2 score:0.750040664188601
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:33:27
epoch 0  | loss: 69.77853| val_0_rmse: 5.09353 | val_1_rmse: 5.09694 |  0:00:00s
epoch 1  | loss: 4.52866 | val_0_rmse: 1.77759 | val_1_rmse: 1.7494  |  0:00:01s
epoch 2  | loss: 0.56948 | val_0_rmse: 1.06498 | val_1_rmse: 1.05582 |  0:00:02s
epoch 3  | loss: 0.6722  | val_0_rmse: 0.83917 | val_1_rmse: 0.8464  |  0:00:03s
epoch 4  | loss: 0.2377  | val_0_rmse: 0.54893 | val_1_rmse: 0.54756 |  0:00:04s
epoch 5  | loss: 0.16361 | val_0_rmse: 0.53281 | val_1_rmse: 0.53018 |  0:00:05s
epoch 6  | loss: 0.17615 | val_0_rmse: 0.42588 | val_1_rmse: 0.42658 |  0:00:06s
epoch 7  | loss: 0.15049 | val_0_rmse: 0.33578 | val_1_rmse: 0.33459 |  0:00:07s
epoch 8  | loss: 0.1334  | val_0_rmse: 0.30811 | val_1_rmse: 0.30713 |  0:00:08s
epoch 9  | loss: 0.13424 | val_0_rmse: 0.34675 | val_1_rmse: 0.34598 |  0:00:09s
epoch 10 | loss: 0.13397 | val_0_rmse: 0.29646 | val_1_rmse: 0.29498 |  0:00:10s
epoch 11 | loss: 0.12514 | val_0_rmse: 0.32351 | val_1_rmse: 0.32133 |  0:00:11s
epoch 12 | loss: 0.12329 | val_0_rmse: 0.36721 | val_1_rmse: 0.36213 |  0:00:12s
epoch 13 | loss: 0.12232 | val_0_rmse: 0.30424 | val_1_rmse: 0.30388 |  0:00:13s
epoch 14 | loss: 0.11813 | val_0_rmse: 0.28299 | val_1_rmse: 0.28264 |  0:00:14s
epoch 15 | loss: 0.11264 | val_0_rmse: 0.31486 | val_1_rmse: 0.31662 |  0:00:15s
epoch 16 | loss: 0.11054 | val_0_rmse: 0.26857 | val_1_rmse: 0.26888 |  0:00:16s
epoch 17 | loss: 0.10602 | val_0_rmse: 0.3061  | val_1_rmse: 0.30334 |  0:00:17s
epoch 18 | loss: 0.1121  | val_0_rmse: 0.36105 | val_1_rmse: 0.35393 |  0:00:17s
epoch 19 | loss: 0.10903 | val_0_rmse: 0.29075 | val_1_rmse: 0.29177 |  0:00:18s
epoch 20 | loss: 0.11155 | val_0_rmse: 0.26567 | val_1_rmse: 0.2675  |  0:00:19s
epoch 21 | loss: 0.11045 | val_0_rmse: 0.32726 | val_1_rmse: 0.32731 |  0:00:20s
epoch 22 | loss: 0.09177 | val_0_rmse: 0.37222 | val_1_rmse: 0.36918 |  0:00:21s
epoch 23 | loss: 0.11938 | val_0_rmse: 0.29418 | val_1_rmse: 0.29041 |  0:00:22s
epoch 24 | loss: 0.10582 | val_0_rmse: 0.31447 | val_1_rmse: 0.3079  |  0:00:23s
epoch 25 | loss: 0.08793 | val_0_rmse: 0.29956 | val_1_rmse: 0.29773 |  0:00:24s
epoch 26 | loss: 0.08098 | val_0_rmse: 0.24388 | val_1_rmse: 0.24306 |  0:00:25s
epoch 27 | loss: 0.06771 | val_0_rmse: 0.24496 | val_1_rmse: 0.2458  |  0:00:26s
epoch 28 | loss: 0.07275 | val_0_rmse: 0.24814 | val_1_rmse: 0.2475  |  0:00:27s
epoch 29 | loss: 0.07095 | val_0_rmse: 0.25054 | val_1_rmse: 0.24844 |  0:00:28s
epoch 30 | loss: 0.06932 | val_0_rmse: 0.31703 | val_1_rmse: 0.3219  |  0:00:29s
epoch 31 | loss: 0.13761 | val_0_rmse: 0.25781 | val_1_rmse: 0.26048 |  0:00:30s
epoch 32 | loss: 0.07398 | val_0_rmse: 0.2943  | val_1_rmse: 0.29662 |  0:00:31s
epoch 33 | loss: 0.08657 | val_0_rmse: 0.25819 | val_1_rmse: 0.26117 |  0:00:32s
epoch 34 | loss: 0.0635  | val_0_rmse: 0.23126 | val_1_rmse: 0.22961 |  0:00:33s
epoch 35 | loss: 0.07318 | val_0_rmse: 0.2408  | val_1_rmse: 0.24082 |  0:00:34s
epoch 36 | loss: 0.07454 | val_0_rmse: 0.2601  | val_1_rmse: 0.25878 |  0:00:35s
epoch 37 | loss: 0.0849  | val_0_rmse: 0.32405 | val_1_rmse: 0.32766 |  0:00:35s
epoch 38 | loss: 0.09679 | val_0_rmse: 0.24103 | val_1_rmse: 0.23982 |  0:00:36s
epoch 39 | loss: 0.11009 | val_0_rmse: 0.31905 | val_1_rmse: 0.32019 |  0:00:37s
epoch 40 | loss: 0.08865 | val_0_rmse: 0.28646 | val_1_rmse: 0.2827  |  0:00:38s
epoch 41 | loss: 0.07058 | val_0_rmse: 0.24087 | val_1_rmse: 0.23988 |  0:00:39s
epoch 42 | loss: 0.06761 | val_0_rmse: 0.22958 | val_1_rmse: 0.22563 |  0:00:40s
epoch 43 | loss: 0.06308 | val_0_rmse: 0.22451 | val_1_rmse: 0.224   |  0:00:41s
epoch 44 | loss: 0.10239 | val_0_rmse: 0.23896 | val_1_rmse: 0.23713 |  0:00:42s
epoch 45 | loss: 0.06909 | val_0_rmse: 0.23591 | val_1_rmse: 0.23742 |  0:00:43s
epoch 46 | loss: 0.07262 | val_0_rmse: 0.22901 | val_1_rmse: 0.23024 |  0:00:44s
epoch 47 | loss: 0.06702 | val_0_rmse: 0.25689 | val_1_rmse: 0.25472 |  0:00:45s
epoch 48 | loss: 0.06472 | val_0_rmse: 0.22786 | val_1_rmse: 0.22659 |  0:00:46s
epoch 49 | loss: 0.06406 | val_0_rmse: 0.23125 | val_1_rmse: 0.23196 |  0:00:47s
epoch 50 | loss: 0.06417 | val_0_rmse: 0.24425 | val_1_rmse: 0.24917 |  0:00:48s
epoch 51 | loss: 0.06342 | val_0_rmse: 0.22738 | val_1_rmse: 0.22703 |  0:00:49s
epoch 52 | loss: 0.06192 | val_0_rmse: 0.23184 | val_1_rmse: 0.23044 |  0:00:50s
epoch 53 | loss: 0.06029 | val_0_rmse: 0.23581 | val_1_rmse: 0.23903 |  0:00:51s
epoch 54 | loss: 0.06134 | val_0_rmse: 0.23633 | val_1_rmse: 0.2405  |  0:00:51s
epoch 55 | loss: 0.06105 | val_0_rmse: 0.23302 | val_1_rmse: 0.23108 |  0:00:52s
epoch 56 | loss: 0.0618  | val_0_rmse: 0.22795 | val_1_rmse: 0.22647 |  0:00:53s
epoch 57 | loss: 0.062   | val_0_rmse: 0.22266 | val_1_rmse: 0.22724 |  0:00:54s
epoch 58 | loss: 0.06213 | val_0_rmse: 0.22236 | val_1_rmse: 0.22453 |  0:00:55s
epoch 59 | loss: 0.065   | val_0_rmse: 0.22273 | val_1_rmse: 0.2222  |  0:00:56s
epoch 60 | loss: 0.0599  | val_0_rmse: 0.22657 | val_1_rmse: 0.22847 |  0:00:57s
epoch 61 | loss: 0.06754 | val_0_rmse: 0.24466 | val_1_rmse: 0.24238 |  0:00:58s
epoch 62 | loss: 0.05376 | val_0_rmse: 0.22942 | val_1_rmse: 0.2298  |  0:00:59s
epoch 63 | loss: 0.05606 | val_0_rmse: 0.22795 | val_1_rmse: 0.2281  |  0:01:00s
epoch 64 | loss: 0.05759 | val_0_rmse: 0.22706 | val_1_rmse: 0.2285  |  0:01:01s
epoch 65 | loss: 0.0622  | val_0_rmse: 0.21592 | val_1_rmse: 0.21518 |  0:01:02s
epoch 66 | loss: 0.06006 | val_0_rmse: 0.23638 | val_1_rmse: 0.2377  |  0:01:03s
epoch 67 | loss: 0.11706 | val_0_rmse: 0.34194 | val_1_rmse: 0.3378  |  0:01:04s
epoch 68 | loss: 0.11417 | val_0_rmse: 0.28978 | val_1_rmse: 0.28547 |  0:01:05s
epoch 69 | loss: 0.06377 | val_0_rmse: 0.21563 | val_1_rmse: 0.21524 |  0:01:06s
epoch 70 | loss: 0.06095 | val_0_rmse: 0.21905 | val_1_rmse: 0.21806 |  0:01:07s
epoch 71 | loss: 0.0552  | val_0_rmse: 0.21556 | val_1_rmse: 0.21392 |  0:01:08s
epoch 72 | loss: 0.06161 | val_0_rmse: 0.21849 | val_1_rmse: 0.21822 |  0:01:09s
epoch 73 | loss: 0.05316 | val_0_rmse: 0.21528 | val_1_rmse: 0.2149  |  0:01:09s
epoch 74 | loss: 0.08116 | val_0_rmse: 0.26102 | val_1_rmse: 0.25875 |  0:01:10s
epoch 75 | loss: 0.06181 | val_0_rmse: 0.23992 | val_1_rmse: 0.23866 |  0:01:11s
epoch 76 | loss: 0.05739 | val_0_rmse: 0.21645 | val_1_rmse: 0.21475 |  0:01:12s
epoch 77 | loss: 0.0515  | val_0_rmse: 0.21816 | val_1_rmse: 0.21897 |  0:01:13s
epoch 78 | loss: 0.0538  | val_0_rmse: 0.21808 | val_1_rmse: 0.21847 |  0:01:14s
epoch 79 | loss: 0.05166 | val_0_rmse: 0.26142 | val_1_rmse: 0.2596  |  0:01:15s
epoch 80 | loss: 0.06389 | val_0_rmse: 0.25886 | val_1_rmse: 0.25553 |  0:01:16s
epoch 81 | loss: 0.05534 | val_0_rmse: 0.21188 | val_1_rmse: 0.21039 |  0:01:17s
epoch 82 | loss: 0.05782 | val_0_rmse: 0.2489  | val_1_rmse: 0.24687 |  0:01:18s
epoch 83 | loss: 0.05802 | val_0_rmse: 0.22715 | val_1_rmse: 0.23047 |  0:01:19s
epoch 84 | loss: 0.05574 | val_0_rmse: 0.22883 | val_1_rmse: 0.23344 |  0:01:20s
epoch 85 | loss: 0.057   | val_0_rmse: 0.22843 | val_1_rmse: 0.22782 |  0:01:21s
epoch 86 | loss: 0.05995 | val_0_rmse: 0.22925 | val_1_rmse: 0.22998 |  0:01:22s
epoch 87 | loss: 0.0571  | val_0_rmse: 0.22537 | val_1_rmse: 0.22793 |  0:01:23s
epoch 88 | loss: 0.05696 | val_0_rmse: 0.22597 | val_1_rmse: 0.22919 |  0:01:24s
epoch 89 | loss: 0.05683 | val_0_rmse: 0.23602 | val_1_rmse: 0.23477 |  0:01:25s
epoch 90 | loss: 0.05788 | val_0_rmse: 0.23437 | val_1_rmse: 0.23305 |  0:01:25s
epoch 91 | loss: 0.05756 | val_0_rmse: 0.21377 | val_1_rmse: 0.21484 |  0:01:26s
epoch 92 | loss: 0.05704 | val_0_rmse: 0.21898 | val_1_rmse: 0.22301 |  0:01:27s
epoch 93 | loss: 0.05543 | val_0_rmse: 0.21984 | val_1_rmse: 0.22064 |  0:01:28s
epoch 94 | loss: 0.05621 | val_0_rmse: 0.22237 | val_1_rmse: 0.2235  |  0:01:29s
epoch 95 | loss: 0.05779 | val_0_rmse: 0.20871 | val_1_rmse: 0.21111 |  0:01:30s
epoch 96 | loss: 0.05691 | val_0_rmse: 0.20846 | val_1_rmse: 0.21111 |  0:01:31s
epoch 97 | loss: 0.05905 | val_0_rmse: 0.22276 | val_1_rmse: 0.222   |  0:01:32s
epoch 98 | loss: 0.05684 | val_0_rmse: 0.2105  | val_1_rmse: 0.21108 |  0:01:33s
epoch 99 | loss: 0.05772 | val_0_rmse: 0.23982 | val_1_rmse: 0.24457 |  0:01:34s
epoch 100| loss: 0.05372 | val_0_rmse: 0.22269 | val_1_rmse: 0.2274  |  0:01:35s
epoch 101| loss: 0.05468 | val_0_rmse: 0.21823 | val_1_rmse: 0.21971 |  0:01:36s
epoch 102| loss: 0.05647 | val_0_rmse: 0.22529 | val_1_rmse: 0.22308 |  0:01:37s
epoch 103| loss: 0.05608 | val_0_rmse: 0.2144  | val_1_rmse: 0.21587 |  0:01:38s
epoch 104| loss: 0.05789 | val_0_rmse: 0.20835 | val_1_rmse: 0.21015 |  0:01:39s
epoch 105| loss: 0.05612 | val_0_rmse: 0.2121  | val_1_rmse: 0.21154 |  0:01:40s
epoch 106| loss: 0.05444 | val_0_rmse: 0.20709 | val_1_rmse: 0.20992 |  0:01:41s
epoch 107| loss: 0.06545 | val_0_rmse: 0.37109 | val_1_rmse: 0.36816 |  0:01:42s
epoch 108| loss: 0.19637 | val_0_rmse: 0.23437 | val_1_rmse: 0.23333 |  0:01:43s
epoch 109| loss: 0.11948 | val_0_rmse: 0.3573  | val_1_rmse: 0.35547 |  0:01:43s
epoch 110| loss: 0.09464 | val_0_rmse: 0.31426 | val_1_rmse: 0.31193 |  0:01:44s
epoch 111| loss: 0.08397 | val_0_rmse: 0.23438 | val_1_rmse: 0.23054 |  0:01:45s
epoch 112| loss: 0.08196 | val_0_rmse: 0.25065 | val_1_rmse: 0.25478 |  0:01:46s
epoch 113| loss: 0.08058 | val_0_rmse: 0.26283 | val_1_rmse: 0.26727 |  0:01:47s
epoch 114| loss: 0.07843 | val_0_rmse: 0.20974 | val_1_rmse: 0.21198 |  0:01:48s
epoch 115| loss: 0.0782  | val_0_rmse: 0.2802  | val_1_rmse: 0.27708 |  0:01:49s
epoch 116| loss: 0.08468 | val_0_rmse: 0.30299 | val_1_rmse: 0.29983 |  0:01:50s
epoch 117| loss: 0.0833  | val_0_rmse: 0.27206 | val_1_rmse: 0.26944 |  0:01:51s
epoch 118| loss: 0.07876 | val_0_rmse: 0.22307 | val_1_rmse: 0.22751 |  0:01:52s
epoch 119| loss: 0.07913 | val_0_rmse: 0.25608 | val_1_rmse: 0.26191 |  0:01:53s
epoch 120| loss: 0.07826 | val_0_rmse: 0.20488 | val_1_rmse: 0.20818 |  0:01:54s
epoch 121| loss: 0.07867 | val_0_rmse: 0.23737 | val_1_rmse: 0.23694 |  0:01:55s
epoch 122| loss: 0.08052 | val_0_rmse: 0.34615 | val_1_rmse: 0.34245 |  0:01:56s
epoch 123| loss: 0.09804 | val_0_rmse: 0.25606 | val_1_rmse: 0.25407 |  0:01:57s
epoch 124| loss: 0.07841 | val_0_rmse: 0.21945 | val_1_rmse: 0.22264 |  0:01:58s
epoch 125| loss: 0.05143 | val_0_rmse: 0.25317 | val_1_rmse: 0.24971 |  0:01:59s
epoch 126| loss: 0.05471 | val_0_rmse: 0.20928 | val_1_rmse: 0.21133 |  0:01:59s
epoch 127| loss: 0.04959 | val_0_rmse: 0.20921 | val_1_rmse: 0.20895 |  0:02:00s
epoch 128| loss: 0.06087 | val_0_rmse: 0.30641 | val_1_rmse: 0.31008 |  0:02:01s
epoch 129| loss: 0.19743 | val_0_rmse: 0.32429 | val_1_rmse: 0.31888 |  0:02:02s
epoch 130| loss: 0.15369 | val_0_rmse: 0.35211 | val_1_rmse: 0.35564 |  0:02:03s
epoch 131| loss: 0.14039 | val_0_rmse: 0.35812 | val_1_rmse: 0.35463 |  0:02:04s
epoch 132| loss: 0.13652 | val_0_rmse: 0.23027 | val_1_rmse: 0.23386 |  0:02:05s
epoch 133| loss: 0.13417 | val_0_rmse: 0.24943 | val_1_rmse: 0.25232 |  0:02:06s
epoch 134| loss: 0.12774 | val_0_rmse: 0.40967 | val_1_rmse: 0.40576 |  0:02:07s
epoch 135| loss: 0.14091 | val_0_rmse: 0.31543 | val_1_rmse: 0.32093 |  0:02:08s
epoch 136| loss: 0.14311 | val_0_rmse: 0.28399 | val_1_rmse: 0.28115 |  0:02:09s
epoch 137| loss: 0.12855 | val_0_rmse: 0.32102 | val_1_rmse: 0.31816 |  0:02:10s
epoch 138| loss: 0.13397 | val_0_rmse: 0.41    | val_1_rmse: 0.41694 |  0:02:11s
epoch 139| loss: 0.12317 | val_0_rmse: 0.22495 | val_1_rmse: 0.22432 |  0:02:12s
epoch 140| loss: 0.06802 | val_0_rmse: 0.20861 | val_1_rmse: 0.21006 |  0:02:13s
epoch 141| loss: 0.05285 | val_0_rmse: 0.20623 | val_1_rmse: 0.20713 |  0:02:14s
epoch 142| loss: 0.05134 | val_0_rmse: 0.19892 | val_1_rmse: 0.2005  |  0:02:15s
epoch 143| loss: 0.05298 | val_0_rmse: 0.21749 | val_1_rmse: 0.22104 |  0:02:16s
epoch 144| loss: 0.09777 | val_0_rmse: 0.25387 | val_1_rmse: 0.25771 |  0:02:17s
epoch 145| loss: 0.0743  | val_0_rmse: 0.25369 | val_1_rmse: 0.25274 |  0:02:17s
epoch 146| loss: 0.07902 | val_0_rmse: 0.30411 | val_1_rmse: 0.30177 |  0:02:18s
epoch 147| loss: 0.07879 | val_0_rmse: 0.22209 | val_1_rmse: 0.22244 |  0:02:19s
epoch 148| loss: 0.06898 | val_0_rmse: 0.20642 | val_1_rmse: 0.20779 |  0:02:20s
epoch 149| loss: 0.0532  | val_0_rmse: 0.20761 | val_1_rmse: 0.20798 |  0:02:21s
Stop training because you reached max_epochs = 150 with best_epoch = 142 and best_val_1_rmse = 0.2005
Best weights from best epoch are automatically used!
ended training at: 05:35:49
Feature importance:
[('Area', 0.04317462149600398), ('Baths', 0.14524015111459512), ('Beds', 0.0583165369061933), ('Latitude', 0.20091192275991282), ('Longitude', 0.3041519580349589), ('Month', 0.051677102529823375), ('Year', 0.19652770715851245)]
Mean squared error is of 7021806218.553521
Mean absolute error:59128.25695023905
MAPE:0.15543295122466044
R2 score:0.7736189693259823
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:35:49
epoch 0  | loss: 69.55906| val_0_rmse: 6.56412 | val_1_rmse: 6.63175 |  0:00:00s
epoch 1  | loss: 4.00641 | val_0_rmse: 1.97967 | val_1_rmse: 1.98319 |  0:00:01s
epoch 2  | loss: 0.61816 | val_0_rmse: 0.69719 | val_1_rmse: 0.70977 |  0:00:02s
epoch 3  | loss: 0.28595 | val_0_rmse: 0.60324 | val_1_rmse: 0.59748 |  0:00:03s
epoch 4  | loss: 0.23294 | val_0_rmse: 0.36186 | val_1_rmse: 0.36227 |  0:00:04s
epoch 5  | loss: 0.16647 | val_0_rmse: 0.38609 | val_1_rmse: 0.38885 |  0:00:05s
epoch 6  | loss: 0.13736 | val_0_rmse: 0.34673 | val_1_rmse: 0.34858 |  0:00:06s
epoch 7  | loss: 0.12799 | val_0_rmse: 0.3008  | val_1_rmse: 0.30166 |  0:00:07s
epoch 8  | loss: 0.10403 | val_0_rmse: 0.3055  | val_1_rmse: 0.30814 |  0:00:08s
epoch 9  | loss: 0.1353  | val_0_rmse: 0.27176 | val_1_rmse: 0.27785 |  0:00:09s
epoch 10 | loss: 0.11715 | val_0_rmse: 0.29127 | val_1_rmse: 0.29147 |  0:00:10s
epoch 11 | loss: 0.1099  | val_0_rmse: 0.32268 | val_1_rmse: 0.32005 |  0:00:11s
epoch 12 | loss: 0.11132 | val_0_rmse: 0.25973 | val_1_rmse: 0.25835 |  0:00:12s
epoch 13 | loss: 0.10944 | val_0_rmse: 0.27545 | val_1_rmse: 0.27686 |  0:00:13s
epoch 14 | loss: 0.10213 | val_0_rmse: 0.28657 | val_1_rmse: 0.28973 |  0:00:14s
epoch 15 | loss: 0.10125 | val_0_rmse: 0.26847 | val_1_rmse: 0.26996 |  0:00:15s
epoch 16 | loss: 0.10342 | val_0_rmse: 0.25674 | val_1_rmse: 0.25534 |  0:00:16s
epoch 17 | loss: 0.10194 | val_0_rmse: 0.35647 | val_1_rmse: 0.35294 |  0:00:16s
epoch 18 | loss: 0.1067  | val_0_rmse: 0.30568 | val_1_rmse: 0.30605 |  0:00:17s
epoch 19 | loss: 0.08497 | val_0_rmse: 0.26312 | val_1_rmse: 0.26313 |  0:00:18s
epoch 20 | loss: 0.07953 | val_0_rmse: 0.27423 | val_1_rmse: 0.27338 |  0:00:19s
epoch 21 | loss: 0.06995 | val_0_rmse: 0.24989 | val_1_rmse: 0.24892 |  0:00:20s
epoch 22 | loss: 0.07243 | val_0_rmse: 0.2374  | val_1_rmse: 0.23304 |  0:00:21s
epoch 23 | loss: 0.0683  | val_0_rmse: 0.23362 | val_1_rmse: 0.23207 |  0:00:22s
epoch 24 | loss: 0.06945 | val_0_rmse: 0.25432 | val_1_rmse: 0.25306 |  0:00:23s
epoch 25 | loss: 0.06638 | val_0_rmse: 0.266   | val_1_rmse: 0.26356 |  0:00:24s
epoch 26 | loss: 0.06305 | val_0_rmse: 0.2274  | val_1_rmse: 0.22677 |  0:00:25s
epoch 27 | loss: 0.06548 | val_0_rmse: 0.22896 | val_1_rmse: 0.22983 |  0:00:26s
epoch 28 | loss: 0.06259 | val_0_rmse: 0.22268 | val_1_rmse: 0.22301 |  0:00:27s
epoch 29 | loss: 0.06447 | val_0_rmse: 0.22902 | val_1_rmse: 0.23215 |  0:00:28s
epoch 30 | loss: 0.05792 | val_0_rmse: 0.2569  | val_1_rmse: 0.25628 |  0:00:29s
epoch 31 | loss: 0.07266 | val_0_rmse: 0.2372  | val_1_rmse: 0.23829 |  0:00:30s
epoch 32 | loss: 0.0662  | val_0_rmse: 0.25136 | val_1_rmse: 0.25081 |  0:00:31s
epoch 33 | loss: 0.06313 | val_0_rmse: 0.30341 | val_1_rmse: 0.30081 |  0:00:32s
epoch 34 | loss: 0.0731  | val_0_rmse: 0.295   | val_1_rmse: 0.29222 |  0:00:33s
epoch 35 | loss: 0.07299 | val_0_rmse: 0.22424 | val_1_rmse: 0.22462 |  0:00:33s
epoch 36 | loss: 0.06708 | val_0_rmse: 0.28219 | val_1_rmse: 0.27796 |  0:00:34s
epoch 37 | loss: 0.07304 | val_0_rmse: 0.2687  | val_1_rmse: 0.26876 |  0:00:35s
epoch 38 | loss: 0.06089 | val_0_rmse: 0.21909 | val_1_rmse: 0.22134 |  0:00:36s
epoch 39 | loss: 0.06186 | val_0_rmse: 0.21848 | val_1_rmse: 0.22425 |  0:00:37s
epoch 40 | loss: 0.06328 | val_0_rmse: 0.22375 | val_1_rmse: 0.22297 |  0:00:38s
epoch 41 | loss: 0.05846 | val_0_rmse: 0.23894 | val_1_rmse: 0.23703 |  0:00:39s
epoch 42 | loss: 0.05875 | val_0_rmse: 0.21699 | val_1_rmse: 0.21737 |  0:00:40s
epoch 43 | loss: 0.07643 | val_0_rmse: 0.3042  | val_1_rmse: 0.30295 |  0:00:41s
epoch 44 | loss: 0.08868 | val_0_rmse: 0.25073 | val_1_rmse: 0.25721 |  0:00:42s
epoch 45 | loss: 0.08538 | val_0_rmse: 0.30664 | val_1_rmse: 0.31109 |  0:00:43s
epoch 46 | loss: 0.08938 | val_0_rmse: 0.24094 | val_1_rmse: 0.24364 |  0:00:44s
epoch 47 | loss: 0.08455 | val_0_rmse: 0.24    | val_1_rmse: 0.23994 |  0:00:45s
epoch 48 | loss: 0.08294 | val_0_rmse: 0.30287 | val_1_rmse: 0.3006  |  0:00:46s
epoch 49 | loss: 0.08389 | val_0_rmse: 0.24511 | val_1_rmse: 0.24197 |  0:00:47s
epoch 50 | loss: 0.08523 | val_0_rmse: 0.23995 | val_1_rmse: 0.24332 |  0:00:48s
epoch 51 | loss: 0.08314 | val_0_rmse: 0.26193 | val_1_rmse: 0.26627 |  0:00:49s
epoch 52 | loss: 0.07934 | val_0_rmse: 0.22933 | val_1_rmse: 0.23339 |  0:00:50s
epoch 53 | loss: 0.08124 | val_0_rmse: 0.24316 | val_1_rmse: 0.24214 |  0:00:51s
epoch 54 | loss: 0.07835 | val_0_rmse: 0.32133 | val_1_rmse: 0.31818 |  0:00:51s
epoch 55 | loss: 0.08099 | val_0_rmse: 0.24105 | val_1_rmse: 0.23953 |  0:00:52s
epoch 56 | loss: 0.0801  | val_0_rmse: 0.22945 | val_1_rmse: 0.23268 |  0:00:53s
epoch 57 | loss: 0.08043 | val_0_rmse: 0.29282 | val_1_rmse: 0.29381 |  0:00:54s
epoch 58 | loss: 0.08108 | val_0_rmse: 0.23253 | val_1_rmse: 0.23358 |  0:00:55s
epoch 59 | loss: 0.0828  | val_0_rmse: 0.26128 | val_1_rmse: 0.25903 |  0:00:56s
epoch 60 | loss: 0.08187 | val_0_rmse: 0.29002 | val_1_rmse: 0.28681 |  0:00:57s
epoch 61 | loss: 0.08435 | val_0_rmse: 0.24528 | val_1_rmse: 0.24003 |  0:00:58s
epoch 62 | loss: 0.08101 | val_0_rmse: 0.22427 | val_1_rmse: 0.22607 |  0:00:59s
epoch 63 | loss: 0.08514 | val_0_rmse: 0.2678  | val_1_rmse: 0.27394 |  0:01:00s
epoch 64 | loss: 0.08325 | val_0_rmse: 0.23501 | val_1_rmse: 0.23976 |  0:01:01s
epoch 65 | loss: 0.08358 | val_0_rmse: 0.28008 | val_1_rmse: 0.27939 |  0:01:02s
epoch 66 | loss: 0.08424 | val_0_rmse: 0.30775 | val_1_rmse: 0.30463 |  0:01:03s
epoch 67 | loss: 0.08228 | val_0_rmse: 0.23994 | val_1_rmse: 0.23982 |  0:01:04s
epoch 68 | loss: 0.08054 | val_0_rmse: 0.232   | val_1_rmse: 0.23655 |  0:01:05s
epoch 69 | loss: 0.08304 | val_0_rmse: 0.27093 | val_1_rmse: 0.27346 |  0:01:06s
epoch 70 | loss: 0.08203 | val_0_rmse: 0.22264 | val_1_rmse: 0.22681 |  0:01:07s
epoch 71 | loss: 0.08214 | val_0_rmse: 0.2834  | val_1_rmse: 0.28148 |  0:01:07s
epoch 72 | loss: 0.09002 | val_0_rmse: 0.29673 | val_1_rmse: 0.29377 |  0:01:08s

Early stopping occured at epoch 72 with best_epoch = 42 and best_val_1_rmse = 0.21737
Best weights from best epoch are automatically used!
ended training at: 05:36:58
Feature importance:
[('Area', 0.2047848204586491), ('Baths', 0.07052369912332393), ('Beds', 0.023789064233159102), ('Latitude', 0.11348090156507767), ('Longitude', 0.1830029418454973), ('Month', 0.07370078710434382), ('Year', 0.3307177856699491)]
Mean squared error is of 8254100436.886925
Mean absolute error:65322.93288459361
MAPE:0.1733357946051076
R2 score:0.7361373036152116
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:36:58
epoch 0  | loss: 71.4558 | val_0_rmse: 6.03071 | val_1_rmse: 6.07846 |  0:00:00s
epoch 1  | loss: 4.61707 | val_0_rmse: 1.5259  | val_1_rmse: 1.50151 |  0:00:01s
epoch 2  | loss: 0.33905 | val_0_rmse: 0.66757 | val_1_rmse: 0.67286 |  0:00:02s
epoch 3  | loss: 0.23162 | val_0_rmse: 0.55148 | val_1_rmse: 0.54332 |  0:00:03s
epoch 4  | loss: 0.17938 | val_0_rmse: 0.49799 | val_1_rmse: 0.49893 |  0:00:04s
epoch 5  | loss: 0.19421 | val_0_rmse: 0.39329 | val_1_rmse: 0.39557 |  0:00:05s
epoch 6  | loss: 0.14548 | val_0_rmse: 0.38203 | val_1_rmse: 0.38536 |  0:00:06s
epoch 7  | loss: 0.16655 | val_0_rmse: 0.31586 | val_1_rmse: 0.31996 |  0:00:07s
epoch 8  | loss: 0.1094  | val_0_rmse: 0.31956 | val_1_rmse: 0.32081 |  0:00:08s
epoch 9  | loss: 0.10602 | val_0_rmse: 0.29558 | val_1_rmse: 0.30015 |  0:00:09s
epoch 10 | loss: 0.08946 | val_0_rmse: 0.26526 | val_1_rmse: 0.27055 |  0:00:10s
epoch 11 | loss: 0.0927  | val_0_rmse: 0.32714 | val_1_rmse: 0.33322 |  0:00:11s
epoch 12 | loss: 0.08532 | val_0_rmse: 0.25024 | val_1_rmse: 0.25185 |  0:00:12s
epoch 13 | loss: 0.13721 | val_0_rmse: 0.61547 | val_1_rmse: 0.61373 |  0:00:13s
epoch 14 | loss: 0.32727 | val_0_rmse: 0.44666 | val_1_rmse: 0.44989 |  0:00:14s
epoch 15 | loss: 0.20276 | val_0_rmse: 0.45008 | val_1_rmse: 0.45187 |  0:00:15s
epoch 16 | loss: 0.17953 | val_0_rmse: 0.26821 | val_1_rmse: 0.27264 |  0:00:16s
epoch 17 | loss: 0.1685  | val_0_rmse: 0.27664 | val_1_rmse: 0.27804 |  0:00:17s
epoch 18 | loss: 0.15375 | val_0_rmse: 0.47514 | val_1_rmse: 0.47959 |  0:00:17s
epoch 19 | loss: 0.16594 | val_0_rmse: 0.31116 | val_1_rmse: 0.31148 |  0:00:18s
epoch 20 | loss: 0.16597 | val_0_rmse: 0.30234 | val_1_rmse: 0.30586 |  0:00:19s
epoch 21 | loss: 0.14219 | val_0_rmse: 0.35393 | val_1_rmse: 0.35717 |  0:00:20s
epoch 22 | loss: 0.1623  | val_0_rmse: 0.33471 | val_1_rmse: 0.33412 |  0:00:21s
epoch 23 | loss: 0.12838 | val_0_rmse: 0.27546 | val_1_rmse: 0.27586 |  0:00:22s
epoch 24 | loss: 0.12072 | val_0_rmse: 0.27479 | val_1_rmse: 0.27515 |  0:00:23s
epoch 25 | loss: 0.08092 | val_0_rmse: 0.29979 | val_1_rmse: 0.30817 |  0:00:24s
epoch 26 | loss: 0.07947 | val_0_rmse: 0.25479 | val_1_rmse: 0.25396 |  0:00:25s
epoch 27 | loss: 0.0675  | val_0_rmse: 0.34668 | val_1_rmse: 0.34996 |  0:00:26s
epoch 28 | loss: 0.10362 | val_0_rmse: 0.30265 | val_1_rmse: 0.30658 |  0:00:27s
epoch 29 | loss: 0.09208 | val_0_rmse: 0.25864 | val_1_rmse: 0.26357 |  0:00:28s
epoch 30 | loss: 0.0902  | val_0_rmse: 0.26048 | val_1_rmse: 0.25704 |  0:00:29s
epoch 31 | loss: 0.08989 | val_0_rmse: 0.28818 | val_1_rmse: 0.28746 |  0:00:30s
epoch 32 | loss: 0.08823 | val_0_rmse: 0.22886 | val_1_rmse: 0.22967 |  0:00:31s
epoch 33 | loss: 0.08587 | val_0_rmse: 0.26925 | val_1_rmse: 0.27495 |  0:00:32s
epoch 34 | loss: 0.08405 | val_0_rmse: 0.33121 | val_1_rmse: 0.33502 |  0:00:33s
epoch 35 | loss: 0.08634 | val_0_rmse: 0.27438 | val_1_rmse: 0.27794 |  0:00:34s
epoch 36 | loss: 0.08663 | val_0_rmse: 0.23369 | val_1_rmse: 0.23683 |  0:00:34s
epoch 37 | loss: 0.08461 | val_0_rmse: 0.26972 | val_1_rmse: 0.27029 |  0:00:35s
epoch 38 | loss: 0.08442 | val_0_rmse: 0.21483 | val_1_rmse: 0.21785 |  0:00:36s
epoch 39 | loss: 0.08436 | val_0_rmse: 0.28716 | val_1_rmse: 0.29135 |  0:00:37s
epoch 40 | loss: 0.08532 | val_0_rmse: 0.27202 | val_1_rmse: 0.27793 |  0:00:38s
epoch 41 | loss: 0.08531 | val_0_rmse: 0.26914 | val_1_rmse: 0.27417 |  0:00:39s
epoch 42 | loss: 0.0944  | val_0_rmse: 0.30318 | val_1_rmse: 0.30162 |  0:00:40s
epoch 43 | loss: 0.13082 | val_0_rmse: 0.28708 | val_1_rmse: 0.28734 |  0:00:41s
epoch 44 | loss: 0.1125  | val_0_rmse: 0.24881 | val_1_rmse: 0.25395 |  0:00:42s
epoch 45 | loss: 0.09997 | val_0_rmse: 0.29655 | val_1_rmse: 0.30326 |  0:00:43s
epoch 46 | loss: 0.0903  | val_0_rmse: 0.31839 | val_1_rmse: 0.32514 |  0:00:44s
epoch 47 | loss: 0.08384 | val_0_rmse: 0.25415 | val_1_rmse: 0.25973 |  0:00:45s
epoch 48 | loss: 0.08124 | val_0_rmse: 0.23721 | val_1_rmse: 0.23817 |  0:00:46s
epoch 49 | loss: 0.08374 | val_0_rmse: 0.25777 | val_1_rmse: 0.25779 |  0:00:47s
epoch 50 | loss: 0.08384 | val_0_rmse: 0.2213  | val_1_rmse: 0.22204 |  0:00:48s
epoch 51 | loss: 0.0811  | val_0_rmse: 0.27113 | val_1_rmse: 0.27567 |  0:00:49s
epoch 52 | loss: 0.0838  | val_0_rmse: 0.30507 | val_1_rmse: 0.31055 |  0:00:50s
epoch 53 | loss: 0.07986 | val_0_rmse: 0.24483 | val_1_rmse: 0.25002 |  0:00:50s
epoch 54 | loss: 0.07946 | val_0_rmse: 0.23926 | val_1_rmse: 0.24046 |  0:00:51s
epoch 55 | loss: 0.07793 | val_0_rmse: 0.27707 | val_1_rmse: 0.2771  |  0:00:52s
epoch 56 | loss: 0.07767 | val_0_rmse: 0.21948 | val_1_rmse: 0.22148 |  0:00:53s
epoch 57 | loss: 0.0777  | val_0_rmse: 0.27873 | val_1_rmse: 0.2827  |  0:00:54s
epoch 58 | loss: 0.08159 | val_0_rmse: 0.30935 | val_1_rmse: 0.31293 |  0:00:55s
epoch 59 | loss: 0.08044 | val_0_rmse: 0.24608 | val_1_rmse: 0.2499  |  0:00:56s
epoch 60 | loss: 0.08071 | val_0_rmse: 0.22809 | val_1_rmse: 0.23085 |  0:00:57s
epoch 61 | loss: 0.0767  | val_0_rmse: 0.25186 | val_1_rmse: 0.25421 |  0:00:58s
epoch 62 | loss: 0.082   | val_0_rmse: 0.21173 | val_1_rmse: 0.2156  |  0:00:59s
epoch 63 | loss: 0.07983 | val_0_rmse: 0.26456 | val_1_rmse: 0.26999 |  0:01:00s
epoch 64 | loss: 0.08397 | val_0_rmse: 0.31104 | val_1_rmse: 0.31701 |  0:01:01s
epoch 65 | loss: 0.08498 | val_0_rmse: 0.21457 | val_1_rmse: 0.21959 |  0:01:02s
epoch 66 | loss: 0.0571  | val_0_rmse: 0.23696 | val_1_rmse: 0.24001 |  0:01:03s
epoch 67 | loss: 0.05769 | val_0_rmse: 0.21904 | val_1_rmse: 0.22382 |  0:01:04s
epoch 68 | loss: 0.05379 | val_0_rmse: 0.22122 | val_1_rmse: 0.22488 |  0:01:05s
epoch 69 | loss: 0.06468 | val_0_rmse: 0.24299 | val_1_rmse: 0.2498  |  0:01:06s
epoch 70 | loss: 0.05794 | val_0_rmse: 0.21211 | val_1_rmse: 0.21465 |  0:01:07s
epoch 71 | loss: 0.057   | val_0_rmse: 0.21189 | val_1_rmse: 0.21443 |  0:01:08s
epoch 72 | loss: 0.05573 | val_0_rmse: 0.2209  | val_1_rmse: 0.22571 |  0:01:08s
epoch 73 | loss: 0.0565  | val_0_rmse: 0.21106 | val_1_rmse: 0.21547 |  0:01:09s
epoch 74 | loss: 0.0553  | val_0_rmse: 0.21567 | val_1_rmse: 0.22039 |  0:01:10s
epoch 75 | loss: 0.05399 | val_0_rmse: 0.20633 | val_1_rmse: 0.21059 |  0:01:11s
epoch 76 | loss: 0.05551 | val_0_rmse: 0.21703 | val_1_rmse: 0.22184 |  0:01:12s
epoch 77 | loss: 0.04931 | val_0_rmse: 0.21939 | val_1_rmse: 0.22358 |  0:01:13s
epoch 78 | loss: 0.05047 | val_0_rmse: 0.22971 | val_1_rmse: 0.23558 |  0:01:14s
epoch 79 | loss: 0.05441 | val_0_rmse: 0.24672 | val_1_rmse: 0.2526  |  0:01:15s
epoch 80 | loss: 0.05773 | val_0_rmse: 0.25651 | val_1_rmse: 0.2621  |  0:01:16s
epoch 81 | loss: 0.0503  | val_0_rmse: 0.22273 | val_1_rmse: 0.2279  |  0:01:17s
epoch 82 | loss: 0.05288 | val_0_rmse: 0.20541 | val_1_rmse: 0.21058 |  0:01:18s
epoch 83 | loss: 0.04732 | val_0_rmse: 0.21581 | val_1_rmse: 0.22093 |  0:01:19s
epoch 84 | loss: 0.05312 | val_0_rmse: 0.2227  | val_1_rmse: 0.22447 |  0:01:20s
epoch 85 | loss: 0.06506 | val_0_rmse: 0.29331 | val_1_rmse: 0.29954 |  0:01:21s
epoch 86 | loss: 0.0994  | val_0_rmse: 0.29061 | val_1_rmse: 0.29398 |  0:01:22s
epoch 87 | loss: 0.12473 | val_0_rmse: 0.32176 | val_1_rmse: 0.32976 |  0:01:23s
epoch 88 | loss: 0.08455 | val_0_rmse: 0.23801 | val_1_rmse: 0.24403 |  0:01:24s
epoch 89 | loss: 0.07081 | val_0_rmse: 0.20983 | val_1_rmse: 0.21525 |  0:01:24s
epoch 90 | loss: 0.06096 | val_0_rmse: 0.21097 | val_1_rmse: 0.21576 |  0:01:25s
epoch 91 | loss: 0.05956 | val_0_rmse: 0.21461 | val_1_rmse: 0.21926 |  0:01:26s
epoch 92 | loss: 0.07767 | val_0_rmse: 0.26888 | val_1_rmse: 0.27506 |  0:01:27s
epoch 93 | loss: 0.07485 | val_0_rmse: 0.33622 | val_1_rmse: 0.342   |  0:01:28s
epoch 94 | loss: 0.08151 | val_0_rmse: 0.20361 | val_1_rmse: 0.20791 |  0:01:29s
epoch 95 | loss: 0.07368 | val_0_rmse: 0.30081 | val_1_rmse: 0.3011  |  0:01:30s
epoch 96 | loss: 0.12981 | val_0_rmse: 0.19788 | val_1_rmse: 0.20208 |  0:01:31s
epoch 97 | loss: 0.04681 | val_0_rmse: 0.20243 | val_1_rmse: 0.20724 |  0:01:32s
epoch 98 | loss: 0.07683 | val_0_rmse: 0.20479 | val_1_rmse: 0.20888 |  0:01:33s
epoch 99 | loss: 0.05131 | val_0_rmse: 0.20247 | val_1_rmse: 0.20687 |  0:01:34s
epoch 100| loss: 0.04573 | val_0_rmse: 0.2317  | val_1_rmse: 0.23283 |  0:01:35s
epoch 101| loss: 0.06375 | val_0_rmse: 0.23988 | val_1_rmse: 0.24686 |  0:01:36s
epoch 102| loss: 0.07869 | val_0_rmse: 0.21804 | val_1_rmse: 0.22233 |  0:01:37s
epoch 103| loss: 0.10702 | val_0_rmse: 0.23312 | val_1_rmse: 0.24011 |  0:01:38s
epoch 104| loss: 0.05693 | val_0_rmse: 0.26883 | val_1_rmse: 0.27028 |  0:01:39s
epoch 105| loss: 0.07979 | val_0_rmse: 0.20188 | val_1_rmse: 0.20579 |  0:01:40s
epoch 106| loss: 0.04626 | val_0_rmse: 0.20702 | val_1_rmse: 0.21183 |  0:01:41s
epoch 107| loss: 0.0475  | val_0_rmse: 0.21729 | val_1_rmse: 0.22067 |  0:01:41s
epoch 108| loss: 0.05469 | val_0_rmse: 0.21699 | val_1_rmse: 0.22165 |  0:01:42s
epoch 109| loss: 0.05332 | val_0_rmse: 0.23083 | val_1_rmse: 0.23198 |  0:01:43s
epoch 110| loss: 0.05181 | val_0_rmse: 0.20534 | val_1_rmse: 0.21002 |  0:01:44s
epoch 111| loss: 0.04699 | val_0_rmse: 0.20898 | val_1_rmse: 0.21227 |  0:01:45s
epoch 112| loss: 0.05182 | val_0_rmse: 0.19855 | val_1_rmse: 0.20059 |  0:01:46s
epoch 113| loss: 0.05287 | val_0_rmse: 0.22554 | val_1_rmse: 0.23    |  0:01:47s
epoch 114| loss: 0.05354 | val_0_rmse: 0.21884 | val_1_rmse: 0.22314 |  0:01:48s
epoch 115| loss: 0.05212 | val_0_rmse: 0.21064 | val_1_rmse: 0.21299 |  0:01:49s
epoch 116| loss: 0.05327 | val_0_rmse: 0.20744 | val_1_rmse: 0.20937 |  0:01:50s
epoch 117| loss: 0.05167 | val_0_rmse: 0.22165 | val_1_rmse: 0.22696 |  0:01:51s
epoch 118| loss: 0.05732 | val_0_rmse: 0.21624 | val_1_rmse: 0.21839 |  0:01:52s
epoch 119| loss: 0.0564  | val_0_rmse: 0.20443 | val_1_rmse: 0.20721 |  0:01:53s
epoch 120| loss: 0.05615 | val_0_rmse: 0.22167 | val_1_rmse: 0.22153 |  0:01:54s
epoch 121| loss: 0.05408 | val_0_rmse: 0.21507 | val_1_rmse: 0.21825 |  0:01:55s
epoch 122| loss: 0.05413 | val_0_rmse: 0.21675 | val_1_rmse: 0.21904 |  0:01:56s
epoch 123| loss: 0.05406 | val_0_rmse: 0.20926 | val_1_rmse: 0.21043 |  0:01:57s
epoch 124| loss: 0.05252 | val_0_rmse: 0.21078 | val_1_rmse: 0.21163 |  0:01:58s
epoch 125| loss: 0.05199 | val_0_rmse: 0.23064 | val_1_rmse: 0.23441 |  0:01:58s
epoch 126| loss: 0.05447 | val_0_rmse: 0.21201 | val_1_rmse: 0.21615 |  0:01:59s
epoch 127| loss: 0.05395 | val_0_rmse: 0.21845 | val_1_rmse: 0.2204  |  0:02:00s
epoch 128| loss: 0.05256 | val_0_rmse: 0.21563 | val_1_rmse: 0.21722 |  0:02:01s
epoch 129| loss: 0.05215 | val_0_rmse: 0.21596 | val_1_rmse: 0.22052 |  0:02:02s
epoch 130| loss: 0.05145 | val_0_rmse: 0.20767 | val_1_rmse: 0.21225 |  0:02:03s
epoch 131| loss: 0.05079 | val_0_rmse: 0.20976 | val_1_rmse: 0.21241 |  0:02:04s
epoch 132| loss: 0.051   | val_0_rmse: 0.20448 | val_1_rmse: 0.20582 |  0:02:05s
epoch 133| loss: 0.05131 | val_0_rmse: 0.20783 | val_1_rmse: 0.21166 |  0:02:06s
epoch 134| loss: 0.05371 | val_0_rmse: 0.23511 | val_1_rmse: 0.23814 |  0:02:07s
epoch 135| loss: 0.06231 | val_0_rmse: 0.2488  | val_1_rmse: 0.24336 |  0:02:08s
epoch 136| loss: 0.05913 | val_0_rmse: 0.23593 | val_1_rmse: 0.23402 |  0:02:09s
epoch 137| loss: 0.06035 | val_0_rmse: 0.23692 | val_1_rmse: 0.24087 |  0:02:10s
epoch 138| loss: 0.05233 | val_0_rmse: 0.20973 | val_1_rmse: 0.21184 |  0:02:11s
epoch 139| loss: 0.04705 | val_0_rmse: 0.20791 | val_1_rmse: 0.21065 |  0:02:12s
epoch 140| loss: 0.05285 | val_0_rmse: 0.21182 | val_1_rmse: 0.21439 |  0:02:13s
epoch 141| loss: 0.05616 | val_0_rmse: 0.20638 | val_1_rmse: 0.20665 |  0:02:14s
epoch 142| loss: 0.05532 | val_0_rmse: 0.20547 | val_1_rmse: 0.20575 |  0:02:14s

Early stopping occured at epoch 142 with best_epoch = 112 and best_val_1_rmse = 0.20059
Best weights from best epoch are automatically used!
ended training at: 05:39:13
Feature importance:
[('Area', 0.2704876039864217), ('Baths', 0.09751796678898778), ('Beds', 0.004384176310519349), ('Latitude', 0.1715460399714364), ('Longitude', 0.3884499010722036), ('Month', 0.05358531711556802), ('Year', 0.014028994754863168)]
Mean squared error is of 7313656581.801739
Mean absolute error:59343.960974081536
MAPE:0.15510098249208193
R2 score:0.7635925997345394
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:39:13
epoch 0  | loss: 68.50503| val_0_rmse: 5.79881 | val_1_rmse: 5.75031 |  0:00:00s
epoch 1  | loss: 3.35762 | val_0_rmse: 1.65572 | val_1_rmse: 1.67611 |  0:00:01s
epoch 2  | loss: 0.52296 | val_0_rmse: 1.17091 | val_1_rmse: 1.17073 |  0:00:02s
epoch 3  | loss: 0.2006  | val_0_rmse: 0.74814 | val_1_rmse: 0.74416 |  0:00:03s
epoch 4  | loss: 0.14861 | val_0_rmse: 0.43177 | val_1_rmse: 0.42813 |  0:00:04s
epoch 5  | loss: 0.1274  | val_0_rmse: 0.30541 | val_1_rmse: 0.30256 |  0:00:05s
epoch 6  | loss: 0.1263  | val_0_rmse: 0.35959 | val_1_rmse: 0.35864 |  0:00:06s
epoch 7  | loss: 0.11391 | val_0_rmse: 0.3159  | val_1_rmse: 0.31686 |  0:00:07s
epoch 8  | loss: 0.13359 | val_0_rmse: 0.4406  | val_1_rmse: 0.44409 |  0:00:08s
epoch 9  | loss: 0.12182 | val_0_rmse: 0.28376 | val_1_rmse: 0.28086 |  0:00:09s
epoch 10 | loss: 0.10297 | val_0_rmse: 0.29478 | val_1_rmse: 0.29166 |  0:00:10s
epoch 11 | loss: 0.10166 | val_0_rmse: 0.25733 | val_1_rmse: 0.25338 |  0:00:11s
epoch 12 | loss: 0.09293 | val_0_rmse: 0.27193 | val_1_rmse: 0.26487 |  0:00:12s
epoch 13 | loss: 0.08497 | val_0_rmse: 0.29385 | val_1_rmse: 0.2951  |  0:00:13s
epoch 14 | loss: 0.08203 | val_0_rmse: 0.26645 | val_1_rmse: 0.26514 |  0:00:14s
epoch 15 | loss: 0.07738 | val_0_rmse: 0.27855 | val_1_rmse: 0.27165 |  0:00:15s
epoch 16 | loss: 0.11165 | val_0_rmse: 0.34746 | val_1_rmse: 0.34173 |  0:00:16s
epoch 17 | loss: 0.11038 | val_0_rmse: 0.31448 | val_1_rmse: 0.31071 |  0:00:17s
epoch 18 | loss: 0.08456 | val_0_rmse: 0.23186 | val_1_rmse: 0.22771 |  0:00:17s
epoch 19 | loss: 0.07287 | val_0_rmse: 0.23779 | val_1_rmse: 0.23692 |  0:00:18s
epoch 20 | loss: 0.06719 | val_0_rmse: 0.27825 | val_1_rmse: 0.2764  |  0:00:19s
epoch 21 | loss: 0.07573 | val_0_rmse: 0.2598  | val_1_rmse: 0.2601  |  0:00:20s
epoch 22 | loss: 0.07188 | val_0_rmse: 0.27369 | val_1_rmse: 0.27578 |  0:00:21s
epoch 23 | loss: 0.07235 | val_0_rmse: 0.22129 | val_1_rmse: 0.21698 |  0:00:22s
epoch 24 | loss: 0.06776 | val_0_rmse: 0.21602 | val_1_rmse: 0.21246 |  0:00:23s
epoch 25 | loss: 0.07622 | val_0_rmse: 0.24174 | val_1_rmse: 0.2383  |  0:00:24s
epoch 26 | loss: 0.06724 | val_0_rmse: 0.24907 | val_1_rmse: 0.2468  |  0:00:25s
epoch 27 | loss: 0.06791 | val_0_rmse: 0.23066 | val_1_rmse: 0.22626 |  0:00:26s
epoch 28 | loss: 0.06268 | val_0_rmse: 0.21953 | val_1_rmse: 0.21629 |  0:00:27s
epoch 29 | loss: 0.06074 | val_0_rmse: 0.21911 | val_1_rmse: 0.21577 |  0:00:28s
epoch 30 | loss: 0.06879 | val_0_rmse: 0.23257 | val_1_rmse: 0.22969 |  0:00:29s
epoch 31 | loss: 0.06208 | val_0_rmse: 0.23417 | val_1_rmse: 0.23434 |  0:00:30s
epoch 32 | loss: 0.05884 | val_0_rmse: 0.23483 | val_1_rmse: 0.23318 |  0:00:31s
epoch 33 | loss: 0.06233 | val_0_rmse: 0.22977 | val_1_rmse: 0.23075 |  0:00:32s
epoch 34 | loss: 0.06277 | val_0_rmse: 0.23392 | val_1_rmse: 0.23337 |  0:00:33s
epoch 35 | loss: 0.06105 | val_0_rmse: 0.23692 | val_1_rmse: 0.23723 |  0:00:33s
epoch 36 | loss: 0.06177 | val_0_rmse: 0.22529 | val_1_rmse: 0.22287 |  0:00:34s
epoch 37 | loss: 0.0622  | val_0_rmse: 0.22168 | val_1_rmse: 0.2208  |  0:00:35s
epoch 38 | loss: 0.06057 | val_0_rmse: 0.21695 | val_1_rmse: 0.21434 |  0:00:36s
epoch 39 | loss: 0.05758 | val_0_rmse: 0.23517 | val_1_rmse: 0.23472 |  0:00:37s
epoch 40 | loss: 0.05749 | val_0_rmse: 0.21983 | val_1_rmse: 0.22064 |  0:00:38s
epoch 41 | loss: 0.05786 | val_0_rmse: 0.20685 | val_1_rmse: 0.20632 |  0:00:39s
epoch 42 | loss: 0.05723 | val_0_rmse: 0.22217 | val_1_rmse: 0.22085 |  0:00:40s
epoch 43 | loss: 0.05968 | val_0_rmse: 0.22596 | val_1_rmse: 0.22732 |  0:00:41s
epoch 44 | loss: 0.06028 | val_0_rmse: 0.25168 | val_1_rmse: 0.25051 |  0:00:42s
epoch 45 | loss: 0.06609 | val_0_rmse: 0.24038 | val_1_rmse: 0.2416  |  0:00:43s
epoch 46 | loss: 0.0558  | val_0_rmse: 0.22221 | val_1_rmse: 0.22118 |  0:00:44s
epoch 47 | loss: 0.06302 | val_0_rmse: 0.26325 | val_1_rmse: 0.25874 |  0:00:45s
epoch 48 | loss: 0.16384 | val_0_rmse: 0.41591 | val_1_rmse: 0.41787 |  0:00:46s
epoch 49 | loss: 0.1751  | val_0_rmse: 0.35941 | val_1_rmse: 0.35687 |  0:00:47s
epoch 50 | loss: 0.19149 | val_0_rmse: 0.24515 | val_1_rmse: 0.24    |  0:00:48s
epoch 51 | loss: 0.18355 | val_0_rmse: 0.29473 | val_1_rmse: 0.29591 |  0:00:49s
epoch 52 | loss: 0.10945 | val_0_rmse: 0.27927 | val_1_rmse: 0.2786  |  0:00:50s
epoch 53 | loss: 0.09621 | val_0_rmse: 0.31178 | val_1_rmse: 0.31266 |  0:00:50s
epoch 54 | loss: 0.06464 | val_0_rmse: 0.21616 | val_1_rmse: 0.21369 |  0:00:51s
epoch 55 | loss: 0.05298 | val_0_rmse: 0.20937 | val_1_rmse: 0.20773 |  0:00:52s
epoch 56 | loss: 0.05294 | val_0_rmse: 0.20535 | val_1_rmse: 0.2039  |  0:00:53s
epoch 57 | loss: 0.05311 | val_0_rmse: 0.23053 | val_1_rmse: 0.22961 |  0:00:54s
epoch 58 | loss: 0.0692  | val_0_rmse: 0.25693 | val_1_rmse: 0.2579  |  0:00:55s
epoch 59 | loss: 0.05303 | val_0_rmse: 0.23231 | val_1_rmse: 0.23242 |  0:00:56s
epoch 60 | loss: 0.08468 | val_0_rmse: 0.2167  | val_1_rmse: 0.21452 |  0:00:57s
epoch 61 | loss: 0.06001 | val_0_rmse: 0.20416 | val_1_rmse: 0.20309 |  0:00:58s
epoch 62 | loss: 0.05171 | val_0_rmse: 0.20946 | val_1_rmse: 0.20757 |  0:00:59s
epoch 63 | loss: 0.06054 | val_0_rmse: 0.22205 | val_1_rmse: 0.21943 |  0:01:00s
epoch 64 | loss: 0.07306 | val_0_rmse: 0.21739 | val_1_rmse: 0.21516 |  0:01:01s
epoch 65 | loss: 0.05468 | val_0_rmse: 0.21703 | val_1_rmse: 0.21518 |  0:01:02s
epoch 66 | loss: 0.14011 | val_0_rmse: 0.24135 | val_1_rmse: 0.236   |  0:01:03s
epoch 67 | loss: 0.08525 | val_0_rmse: 0.25775 | val_1_rmse: 0.25647 |  0:01:04s
epoch 68 | loss: 0.06158 | val_0_rmse: 0.24047 | val_1_rmse: 0.23649 |  0:01:05s
epoch 69 | loss: 0.06274 | val_0_rmse: 0.20042 | val_1_rmse: 0.19952 |  0:01:06s
epoch 70 | loss: 0.04683 | val_0_rmse: 0.24901 | val_1_rmse: 0.24793 |  0:01:07s
epoch 71 | loss: 0.10439 | val_0_rmse: 0.21651 | val_1_rmse: 0.21363 |  0:01:07s
epoch 72 | loss: 0.08616 | val_0_rmse: 0.26414 | val_1_rmse: 0.26168 |  0:01:08s
epoch 73 | loss: 0.08682 | val_0_rmse: 0.21478 | val_1_rmse: 0.21366 |  0:01:09s
epoch 74 | loss: 0.08103 | val_0_rmse: 0.2919  | val_1_rmse: 0.29243 |  0:01:10s
epoch 75 | loss: 0.07172 | val_0_rmse: 0.2031  | val_1_rmse: 0.20061 |  0:01:11s
epoch 76 | loss: 0.0463  | val_0_rmse: 0.25539 | val_1_rmse: 0.25673 |  0:01:12s
epoch 77 | loss: 0.06548 | val_0_rmse: 0.24401 | val_1_rmse: 0.24359 |  0:01:13s
epoch 78 | loss: 0.0613  | val_0_rmse: 0.20764 | val_1_rmse: 0.20555 |  0:01:14s
epoch 79 | loss: 0.05238 | val_0_rmse: 0.22533 | val_1_rmse: 0.22253 |  0:01:15s
epoch 80 | loss: 0.05603 | val_0_rmse: 0.32556 | val_1_rmse: 0.32295 |  0:01:16s
epoch 81 | loss: 0.08902 | val_0_rmse: 0.20109 | val_1_rmse: 0.19845 |  0:01:17s
epoch 82 | loss: 0.0821  | val_0_rmse: 0.30429 | val_1_rmse: 0.30613 |  0:01:18s
epoch 83 | loss: 0.15452 | val_0_rmse: 0.45163 | val_1_rmse: 0.44888 |  0:01:19s
epoch 84 | loss: 0.15328 | val_0_rmse: 0.38643 | val_1_rmse: 0.38879 |  0:01:20s
epoch 85 | loss: 0.14763 | val_0_rmse: 0.20537 | val_1_rmse: 0.2032  |  0:01:21s
epoch 86 | loss: 0.14926 | val_0_rmse: 0.25216 | val_1_rmse: 0.24784 |  0:01:22s
epoch 87 | loss: 0.13919 | val_0_rmse: 0.42511 | val_1_rmse: 0.42616 |  0:01:23s
epoch 88 | loss: 0.15213 | val_0_rmse: 0.31709 | val_1_rmse: 0.31372 |  0:01:24s
epoch 89 | loss: 0.14342 | val_0_rmse: 0.23217 | val_1_rmse: 0.23421 |  0:01:24s
epoch 90 | loss: 0.12309 | val_0_rmse: 0.22923 | val_1_rmse: 0.2296  |  0:01:25s
epoch 91 | loss: 0.1045  | val_0_rmse: 0.25837 | val_1_rmse: 0.26007 |  0:01:26s
epoch 92 | loss: 0.07373 | val_0_rmse: 0.24696 | val_1_rmse: 0.2458  |  0:01:27s
epoch 93 | loss: 0.0551  | val_0_rmse: 0.2185  | val_1_rmse: 0.21768 |  0:01:28s
epoch 94 | loss: 0.0494  | val_0_rmse: 0.20447 | val_1_rmse: 0.2024  |  0:01:29s
epoch 95 | loss: 0.05229 | val_0_rmse: 0.21591 | val_1_rmse: 0.21605 |  0:01:30s
epoch 96 | loss: 0.05284 | val_0_rmse: 0.21722 | val_1_rmse: 0.21371 |  0:01:31s
epoch 97 | loss: 0.05383 | val_0_rmse: 0.20639 | val_1_rmse: 0.2028  |  0:01:32s
epoch 98 | loss: 0.0538  | val_0_rmse: 0.23098 | val_1_rmse: 0.23013 |  0:01:33s
epoch 99 | loss: 0.05423 | val_0_rmse: 0.20384 | val_1_rmse: 0.20203 |  0:01:34s
epoch 100| loss: 0.05563 | val_0_rmse: 0.21905 | val_1_rmse: 0.21566 |  0:01:35s
epoch 101| loss: 0.05534 | val_0_rmse: 0.21122 | val_1_rmse: 0.20846 |  0:01:36s
epoch 102| loss: 0.04619 | val_0_rmse: 0.27862 | val_1_rmse: 0.27621 |  0:01:37s
epoch 103| loss: 0.07396 | val_0_rmse: 0.22644 | val_1_rmse: 0.22578 |  0:01:38s
epoch 104| loss: 0.0941  | val_0_rmse: 0.20954 | val_1_rmse: 0.20631 |  0:01:39s
epoch 105| loss: 0.06471 | val_0_rmse: 0.20067 | val_1_rmse: 0.1972  |  0:01:40s
epoch 106| loss: 0.05574 | val_0_rmse: 0.19664 | val_1_rmse: 0.19462 |  0:01:41s
epoch 107| loss: 0.05973 | val_0_rmse: 0.25653 | val_1_rmse: 0.25314 |  0:01:41s
epoch 108| loss: 0.08908 | val_0_rmse: 0.33903 | val_1_rmse: 0.33443 |  0:01:42s
epoch 109| loss: 0.0701  | val_0_rmse: 0.21632 | val_1_rmse: 0.21379 |  0:01:43s
epoch 110| loss: 0.05503 | val_0_rmse: 0.19856 | val_1_rmse: 0.19567 |  0:01:44s
epoch 111| loss: 0.05076 | val_0_rmse: 0.22731 | val_1_rmse: 0.22428 |  0:01:45s
epoch 112| loss: 0.08273 | val_0_rmse: 0.25538 | val_1_rmse: 0.25192 |  0:01:46s
epoch 113| loss: 0.07929 | val_0_rmse: 0.30467 | val_1_rmse: 0.30516 |  0:01:47s
epoch 114| loss: 0.06486 | val_0_rmse: 0.23224 | val_1_rmse: 0.23265 |  0:01:48s
epoch 115| loss: 0.0611  | val_0_rmse: 0.202   | val_1_rmse: 0.20127 |  0:01:49s
epoch 116| loss: 0.04581 | val_0_rmse: 0.21183 | val_1_rmse: 0.21015 |  0:01:50s
epoch 117| loss: 0.04307 | val_0_rmse: 0.20156 | val_1_rmse: 0.19995 |  0:01:51s
epoch 118| loss: 0.04507 | val_0_rmse: 0.19852 | val_1_rmse: 0.19578 |  0:01:52s
epoch 119| loss: 0.04999 | val_0_rmse: 0.21215 | val_1_rmse: 0.21113 |  0:01:53s
epoch 120| loss: 0.05395 | val_0_rmse: 0.19812 | val_1_rmse: 0.19564 |  0:01:54s
epoch 121| loss: 0.04399 | val_0_rmse: 0.20089 | val_1_rmse: 0.19911 |  0:01:55s
epoch 122| loss: 0.04733 | val_0_rmse: 0.22171 | val_1_rmse: 0.22096 |  0:01:56s
epoch 123| loss: 0.05468 | val_0_rmse: 0.20784 | val_1_rmse: 0.2062  |  0:01:57s
epoch 124| loss: 0.04356 | val_0_rmse: 0.19867 | val_1_rmse: 0.19664 |  0:01:58s
epoch 125| loss: 0.04687 | val_0_rmse: 0.19657 | val_1_rmse: 0.19599 |  0:01:58s
epoch 126| loss: 0.04701 | val_0_rmse: 0.20227 | val_1_rmse: 0.20155 |  0:01:59s
epoch 127| loss: 0.04566 | val_0_rmse: 0.20086 | val_1_rmse: 0.2003  |  0:02:00s
epoch 128| loss: 0.04454 | val_0_rmse: 0.20348 | val_1_rmse: 0.20192 |  0:02:01s
epoch 129| loss: 0.04539 | val_0_rmse: 0.19991 | val_1_rmse: 0.19791 |  0:02:02s
epoch 130| loss: 0.05125 | val_0_rmse: 0.19828 | val_1_rmse: 0.19769 |  0:02:03s
epoch 131| loss: 0.05376 | val_0_rmse: 0.24739 | val_1_rmse: 0.24789 |  0:02:04s
epoch 132| loss: 0.04903 | val_0_rmse: 0.21537 | val_1_rmse: 0.21352 |  0:02:05s
epoch 133| loss: 0.06286 | val_0_rmse: 0.22479 | val_1_rmse: 0.22287 |  0:02:06s
epoch 134| loss: 0.05806 | val_0_rmse: 0.21101 | val_1_rmse: 0.21056 |  0:02:07s
epoch 135| loss: 0.05451 | val_0_rmse: 0.20698 | val_1_rmse: 0.2033  |  0:02:08s
epoch 136| loss: 0.05612 | val_0_rmse: 0.20885 | val_1_rmse: 0.20645 |  0:02:09s

Early stopping occured at epoch 136 with best_epoch = 106 and best_val_1_rmse = 0.19462
Best weights from best epoch are automatically used!
ended training at: 05:41:23
Feature importance:
[('Area', 0.05495955768854974), ('Baths', 0.036283919450532484), ('Beds', 0.03003005904376533), ('Latitude', 0.39032818977753375), ('Longitude', 0.35844382261401014), ('Month', 0.060163941590129444), ('Year', 0.06979050983547909)]
Mean squared error is of 6528924797.65191
Mean absolute error:57169.53971272647
MAPE:0.14642497968448628
R2 score:0.7870076887062318
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:41:23
epoch 0  | loss: 130.23386| val_0_rmse: 6.34812 | val_1_rmse: 6.36755 |  0:00:00s
epoch 1  | loss: 24.69224| val_0_rmse: 7.11968 | val_1_rmse: 7.06623 |  0:00:00s
epoch 2  | loss: 7.63411 | val_0_rmse: 4.70974 | val_1_rmse: 4.68423 |  0:00:01s
epoch 3  | loss: 1.63827 | val_0_rmse: 1.12146 | val_1_rmse: 1.10588 |  0:00:01s
epoch 4  | loss: 0.67703 | val_0_rmse: 1.05502 | val_1_rmse: 1.05791 |  0:00:02s
epoch 5  | loss: 0.39348 | val_0_rmse: 0.93647 | val_1_rmse: 0.92594 |  0:00:02s
epoch 6  | loss: 0.39073 | val_0_rmse: 0.73676 | val_1_rmse: 0.74485 |  0:00:03s
epoch 7  | loss: 0.3107  | val_0_rmse: 0.48318 | val_1_rmse: 0.48007 |  0:00:03s
epoch 8  | loss: 0.26701 | val_0_rmse: 0.46359 | val_1_rmse: 0.45064 |  0:00:04s
epoch 9  | loss: 0.21181 | val_0_rmse: 0.52996 | val_1_rmse: 0.51812 |  0:00:04s
epoch 10 | loss: 0.26145 | val_0_rmse: 0.49685 | val_1_rmse: 0.49192 |  0:00:04s
epoch 11 | loss: 0.23709 | val_0_rmse: 0.467   | val_1_rmse: 0.45544 |  0:00:05s
epoch 12 | loss: 0.25931 | val_0_rmse: 0.46539 | val_1_rmse: 0.46056 |  0:00:05s
epoch 13 | loss: 0.23238 | val_0_rmse: 0.43185 | val_1_rmse: 0.41355 |  0:00:06s
epoch 14 | loss: 0.21354 | val_0_rmse: 0.36791 | val_1_rmse: 0.3583  |  0:00:06s
epoch 15 | loss: 0.18138 | val_0_rmse: 0.41611 | val_1_rmse: 0.40273 |  0:00:07s
epoch 16 | loss: 0.18626 | val_0_rmse: 0.39271 | val_1_rmse: 0.38634 |  0:00:07s
epoch 17 | loss: 0.17143 | val_0_rmse: 0.38278 | val_1_rmse: 0.37071 |  0:00:08s
epoch 18 | loss: 0.16207 | val_0_rmse: 0.35089 | val_1_rmse: 0.34074 |  0:00:08s
epoch 19 | loss: 0.1539  | val_0_rmse: 0.36425 | val_1_rmse: 0.35182 |  0:00:09s
epoch 20 | loss: 0.16297 | val_0_rmse: 0.34813 | val_1_rmse: 0.33531 |  0:00:09s
epoch 21 | loss: 0.15166 | val_0_rmse: 0.34718 | val_1_rmse: 0.33359 |  0:00:09s
epoch 22 | loss: 0.1548  | val_0_rmse: 0.37575 | val_1_rmse: 0.36303 |  0:00:10s
epoch 23 | loss: 0.1574  | val_0_rmse: 0.35994 | val_1_rmse: 0.34644 |  0:00:10s
epoch 24 | loss: 0.1569  | val_0_rmse: 0.34254 | val_1_rmse: 0.33066 |  0:00:11s
epoch 25 | loss: 0.15553 | val_0_rmse: 0.35213 | val_1_rmse: 0.34159 |  0:00:11s
epoch 26 | loss: 0.15542 | val_0_rmse: 0.3492  | val_1_rmse: 0.33822 |  0:00:12s
epoch 27 | loss: 0.15749 | val_0_rmse: 0.34655 | val_1_rmse: 0.33724 |  0:00:12s
epoch 28 | loss: 0.15235 | val_0_rmse: 0.35604 | val_1_rmse: 0.343   |  0:00:13s
epoch 29 | loss: 0.15357 | val_0_rmse: 0.34392 | val_1_rmse: 0.33184 |  0:00:13s
epoch 30 | loss: 0.15817 | val_0_rmse: 0.36239 | val_1_rmse: 0.34908 |  0:00:13s
epoch 31 | loss: 0.14222 | val_0_rmse: 0.34421 | val_1_rmse: 0.33071 |  0:00:14s
epoch 32 | loss: 0.14402 | val_0_rmse: 0.35277 | val_1_rmse: 0.33977 |  0:00:14s
epoch 33 | loss: 0.15049 | val_0_rmse: 0.34554 | val_1_rmse: 0.33212 |  0:00:15s
epoch 34 | loss: 0.14919 | val_0_rmse: 0.35992 | val_1_rmse: 0.34764 |  0:00:15s
epoch 35 | loss: 0.15588 | val_0_rmse: 0.35675 | val_1_rmse: 0.34407 |  0:00:16s
epoch 36 | loss: 0.14578 | val_0_rmse: 0.35932 | val_1_rmse: 0.34614 |  0:00:16s
epoch 37 | loss: 0.14444 | val_0_rmse: 0.34826 | val_1_rmse: 0.33466 |  0:00:17s
epoch 38 | loss: 0.14623 | val_0_rmse: 0.35216 | val_1_rmse: 0.3381  |  0:00:17s
epoch 39 | loss: 0.14399 | val_0_rmse: 0.35299 | val_1_rmse: 0.33902 |  0:00:18s
epoch 40 | loss: 0.1489  | val_0_rmse: 0.35284 | val_1_rmse: 0.33934 |  0:00:18s
epoch 41 | loss: 0.14243 | val_0_rmse: 0.35101 | val_1_rmse: 0.33663 |  0:00:18s
epoch 42 | loss: 0.14281 | val_0_rmse: 0.34413 | val_1_rmse: 0.3319  |  0:00:19s
epoch 43 | loss: 0.14559 | val_0_rmse: 0.34787 | val_1_rmse: 0.33459 |  0:00:19s
epoch 44 | loss: 0.14371 | val_0_rmse: 0.35313 | val_1_rmse: 0.33958 |  0:00:20s
epoch 45 | loss: 0.14098 | val_0_rmse: 0.34158 | val_1_rmse: 0.33016 |  0:00:20s
epoch 46 | loss: 0.14229 | val_0_rmse: 0.34399 | val_1_rmse: 0.33302 |  0:00:21s
epoch 47 | loss: 0.14178 | val_0_rmse: 0.36026 | val_1_rmse: 0.35089 |  0:00:21s
epoch 48 | loss: 0.1443  | val_0_rmse: 0.35071 | val_1_rmse: 0.33977 |  0:00:21s
epoch 49 | loss: 0.14651 | val_0_rmse: 0.35311 | val_1_rmse: 0.34077 |  0:00:22s
epoch 50 | loss: 0.14563 | val_0_rmse: 0.34961 | val_1_rmse: 0.33639 |  0:00:22s
epoch 51 | loss: 0.14333 | val_0_rmse: 0.34498 | val_1_rmse: 0.33298 |  0:00:23s
epoch 52 | loss: 0.14167 | val_0_rmse: 0.34486 | val_1_rmse: 0.33244 |  0:00:23s
epoch 53 | loss: 0.14494 | val_0_rmse: 0.35535 | val_1_rmse: 0.34329 |  0:00:24s
epoch 54 | loss: 0.13958 | val_0_rmse: 0.34403 | val_1_rmse: 0.33429 |  0:00:24s
epoch 55 | loss: 0.14417 | val_0_rmse: 0.35655 | val_1_rmse: 0.34568 |  0:00:25s
epoch 56 | loss: 0.14145 | val_0_rmse: 0.3606  | val_1_rmse: 0.35024 |  0:00:25s
epoch 57 | loss: 0.14583 | val_0_rmse: 0.36995 | val_1_rmse: 0.35831 |  0:00:26s
epoch 58 | loss: 0.13873 | val_0_rmse: 0.35394 | val_1_rmse: 0.34139 |  0:00:26s
epoch 59 | loss: 0.13801 | val_0_rmse: 0.33006 | val_1_rmse: 0.31801 |  0:00:26s
epoch 60 | loss: 0.14163 | val_0_rmse: 0.31947 | val_1_rmse: 0.30849 |  0:00:27s
epoch 61 | loss: 0.14129 | val_0_rmse: 0.39141 | val_1_rmse: 0.38078 |  0:00:27s
epoch 62 | loss: 0.13251 | val_0_rmse: 0.35521 | val_1_rmse: 0.35192 |  0:00:28s
epoch 63 | loss: 0.13737 | val_0_rmse: 0.39362 | val_1_rmse: 0.38859 |  0:00:28s
epoch 64 | loss: 0.12844 | val_0_rmse: 0.3575  | val_1_rmse: 0.34965 |  0:00:29s
epoch 65 | loss: 0.12431 | val_0_rmse: 0.3415  | val_1_rmse: 0.33517 |  0:00:29s
epoch 66 | loss: 0.12577 | val_0_rmse: 0.36084 | val_1_rmse: 0.35461 |  0:00:30s
epoch 67 | loss: 0.13503 | val_0_rmse: 0.33659 | val_1_rmse: 0.32725 |  0:00:30s
epoch 68 | loss: 0.11343 | val_0_rmse: 0.32804 | val_1_rmse: 0.32046 |  0:00:30s
epoch 69 | loss: 0.12714 | val_0_rmse: 0.34317 | val_1_rmse: 0.332   |  0:00:31s
epoch 70 | loss: 0.12223 | val_0_rmse: 0.34113 | val_1_rmse: 0.33105 |  0:00:31s
epoch 71 | loss: 0.11599 | val_0_rmse: 0.32761 | val_1_rmse: 0.32161 |  0:00:32s
epoch 72 | loss: 0.11466 | val_0_rmse: 0.31628 | val_1_rmse: 0.30788 |  0:00:32s
epoch 73 | loss: 0.10457 | val_0_rmse: 0.31676 | val_1_rmse: 0.30971 |  0:00:33s
epoch 74 | loss: 0.10833 | val_0_rmse: 0.31652 | val_1_rmse: 0.31055 |  0:00:33s
epoch 75 | loss: 0.11449 | val_0_rmse: 0.32806 | val_1_rmse: 0.31896 |  0:00:34s
epoch 76 | loss: 0.10275 | val_0_rmse: 0.31321 | val_1_rmse: 0.30565 |  0:00:34s
epoch 77 | loss: 0.11176 | val_0_rmse: 0.36039 | val_1_rmse: 0.35818 |  0:00:35s
epoch 78 | loss: 0.16965 | val_0_rmse: 0.41149 | val_1_rmse: 0.41066 |  0:00:35s
epoch 79 | loss: 0.24839 | val_0_rmse: 0.55723 | val_1_rmse: 0.54993 |  0:00:35s
epoch 80 | loss: 0.19315 | val_0_rmse: 0.32558 | val_1_rmse: 0.31707 |  0:00:36s
epoch 81 | loss: 0.1108  | val_0_rmse: 0.3327  | val_1_rmse: 0.32815 |  0:00:36s
epoch 82 | loss: 0.11327 | val_0_rmse: 0.32123 | val_1_rmse: 0.31088 |  0:00:37s
epoch 83 | loss: 0.11258 | val_0_rmse: 0.32413 | val_1_rmse: 0.31836 |  0:00:37s
epoch 84 | loss: 0.11453 | val_0_rmse: 0.32059 | val_1_rmse: 0.31275 |  0:00:38s
epoch 85 | loss: 0.115   | val_0_rmse: 0.32591 | val_1_rmse: 0.3209  |  0:00:38s
epoch 86 | loss: 0.11254 | val_0_rmse: 0.33604 | val_1_rmse: 0.32464 |  0:00:39s
epoch 87 | loss: 0.11147 | val_0_rmse: 0.31566 | val_1_rmse: 0.3077  |  0:00:39s
epoch 88 | loss: 0.11194 | val_0_rmse: 0.33177 | val_1_rmse: 0.32292 |  0:00:39s
epoch 89 | loss: 0.11086 | val_0_rmse: 0.32558 | val_1_rmse: 0.31791 |  0:00:40s
epoch 90 | loss: 0.11096 | val_0_rmse: 0.32889 | val_1_rmse: 0.31959 |  0:00:40s
epoch 91 | loss: 0.11419 | val_0_rmse: 0.3151  | val_1_rmse: 0.3096  |  0:00:41s
epoch 92 | loss: 0.11244 | val_0_rmse: 0.33304 | val_1_rmse: 0.32391 |  0:00:41s
epoch 93 | loss: 0.11111 | val_0_rmse: 0.31535 | val_1_rmse: 0.30955 |  0:00:42s
epoch 94 | loss: 0.10987 | val_0_rmse: 0.32373 | val_1_rmse: 0.31457 |  0:00:42s
epoch 95 | loss: 0.10869 | val_0_rmse: 0.31943 | val_1_rmse: 0.3145  |  0:00:43s
epoch 96 | loss: 0.10846 | val_0_rmse: 0.31446 | val_1_rmse: 0.30647 |  0:00:43s
epoch 97 | loss: 0.10727 | val_0_rmse: 0.31868 | val_1_rmse: 0.31467 |  0:00:43s
epoch 98 | loss: 0.10875 | val_0_rmse: 0.31556 | val_1_rmse: 0.30711 |  0:00:44s
epoch 99 | loss: 0.1083  | val_0_rmse: 0.31646 | val_1_rmse: 0.30996 |  0:00:44s
epoch 100| loss: 0.10751 | val_0_rmse: 0.3246  | val_1_rmse: 0.31594 |  0:00:45s
epoch 101| loss: 0.10849 | val_0_rmse: 0.32063 | val_1_rmse: 0.31381 |  0:00:45s
epoch 102| loss: 0.10781 | val_0_rmse: 0.32108 | val_1_rmse: 0.31274 |  0:00:46s
epoch 103| loss: 0.10554 | val_0_rmse: 0.334   | val_1_rmse: 0.33125 |  0:00:46s
epoch 104| loss: 0.13006 | val_0_rmse: 0.30535 | val_1_rmse: 0.2993  |  0:00:47s
epoch 105| loss: 0.19002 | val_0_rmse: 0.53598 | val_1_rmse: 0.53516 |  0:00:47s
epoch 106| loss: 0.21807 | val_0_rmse: 0.31007 | val_1_rmse: 0.30397 |  0:00:48s
epoch 107| loss: 0.10297 | val_0_rmse: 0.30796 | val_1_rmse: 0.29818 |  0:00:48s
epoch 108| loss: 0.1156  | val_0_rmse: 0.34296 | val_1_rmse: 0.33634 |  0:00:48s
epoch 109| loss: 0.13381 | val_0_rmse: 0.35506 | val_1_rmse: 0.35178 |  0:00:49s
epoch 110| loss: 0.15265 | val_0_rmse: 0.37615 | val_1_rmse: 0.37249 |  0:00:49s
epoch 111| loss: 0.17244 | val_0_rmse: 0.31625 | val_1_rmse: 0.30573 |  0:00:50s
epoch 112| loss: 0.16028 | val_0_rmse: 0.3351  | val_1_rmse: 0.32593 |  0:00:50s
epoch 113| loss: 0.13241 | val_0_rmse: 0.33893 | val_1_rmse: 0.33458 |  0:00:51s
epoch 114| loss: 0.15551 | val_0_rmse: 0.33521 | val_1_rmse: 0.32519 |  0:00:51s
epoch 115| loss: 0.14735 | val_0_rmse: 0.33667 | val_1_rmse: 0.32687 |  0:00:52s
epoch 116| loss: 0.13445 | val_0_rmse: 0.34921 | val_1_rmse: 0.34411 |  0:00:52s
epoch 117| loss: 0.14883 | val_0_rmse: 0.35825 | val_1_rmse: 0.34572 |  0:00:52s
epoch 118| loss: 0.13563 | val_0_rmse: 0.33678 | val_1_rmse: 0.33027 |  0:00:53s
epoch 119| loss: 0.13912 | val_0_rmse: 0.33074 | val_1_rmse: 0.32306 |  0:00:53s
epoch 120| loss: 0.13617 | val_0_rmse: 0.32812 | val_1_rmse: 0.31968 |  0:00:54s
epoch 121| loss: 0.13575 | val_0_rmse: 0.32395 | val_1_rmse: 0.31681 |  0:00:54s
epoch 122| loss: 0.13289 | val_0_rmse: 0.33389 | val_1_rmse: 0.3249  |  0:00:55s
epoch 123| loss: 0.13062 | val_0_rmse: 0.33381 | val_1_rmse: 0.32642 |  0:00:55s
epoch 124| loss: 0.13549 | val_0_rmse: 0.33374 | val_1_rmse: 0.3253  |  0:00:56s
epoch 125| loss: 0.13264 | val_0_rmse: 0.33752 | val_1_rmse: 0.32972 |  0:00:56s
epoch 126| loss: 0.12625 | val_0_rmse: 0.32958 | val_1_rmse: 0.32195 |  0:00:57s
epoch 127| loss: 0.13225 | val_0_rmse: 0.33657 | val_1_rmse: 0.32832 |  0:00:57s
epoch 128| loss: 0.13822 | val_0_rmse: 0.32694 | val_1_rmse: 0.31866 |  0:00:57s
epoch 129| loss: 0.13069 | val_0_rmse: 0.32658 | val_1_rmse: 0.31849 |  0:00:58s
epoch 130| loss: 0.12705 | val_0_rmse: 0.32629 | val_1_rmse: 0.31854 |  0:00:58s
epoch 131| loss: 0.1327  | val_0_rmse: 0.33167 | val_1_rmse: 0.32196 |  0:00:59s
epoch 132| loss: 0.1274  | val_0_rmse: 0.32608 | val_1_rmse: 0.31817 |  0:00:59s
epoch 133| loss: 0.13221 | val_0_rmse: 0.33755 | val_1_rmse: 0.32812 |  0:01:00s
epoch 134| loss: 0.1322  | val_0_rmse: 0.33217 | val_1_rmse: 0.32334 |  0:01:00s
epoch 135| loss: 0.12972 | val_0_rmse: 0.3234  | val_1_rmse: 0.31585 |  0:01:01s
epoch 136| loss: 0.12892 | val_0_rmse: 0.32929 | val_1_rmse: 0.32227 |  0:01:01s
epoch 137| loss: 0.12822 | val_0_rmse: 0.32565 | val_1_rmse: 0.31899 |  0:01:01s

Early stopping occured at epoch 137 with best_epoch = 107 and best_val_1_rmse = 0.29818
Best weights from best epoch are automatically used!
ended training at: 05:42:25
Feature importance:
[('Area', 0.25130444263293406), ('Baths', 0.07903022402336121), ('Beds', 0.0), ('Latitude', 0.23730676826740332), ('Longitude', 0.0001596450959445103), ('Month', 0.06391866423691814), ('Year', 0.36828025574343876)]
Mean squared error is of 37820458771.816696
Mean absolute error:139714.24883786804
MAPE:0.23538327214052543
R2 score:0.525006899262635
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:42:26
epoch 0  | loss: 130.89635| val_0_rmse: 6.70179 | val_1_rmse: 6.58862 |  0:00:00s
epoch 1  | loss: 24.69513| val_0_rmse: 5.66666 | val_1_rmse: 5.68685 |  0:00:00s
epoch 2  | loss: 11.03806| val_0_rmse: 3.27338 | val_1_rmse: 3.29662 |  0:00:01s
epoch 3  | loss: 1.93804 | val_0_rmse: 1.8047  | val_1_rmse: 1.7665  |  0:00:01s
epoch 4  | loss: 0.72809 | val_0_rmse: 1.12833 | val_1_rmse: 1.14439 |  0:00:02s
epoch 5  | loss: 0.5656  | val_0_rmse: 2.05255 | val_1_rmse: 2.08087 |  0:00:02s
epoch 6  | loss: 0.65944 | val_0_rmse: 1.03029 | val_1_rmse: 1.04235 |  0:00:03s
epoch 7  | loss: 0.33813 | val_0_rmse: 0.54426 | val_1_rmse: 0.53915 |  0:00:03s
epoch 8  | loss: 0.26872 | val_0_rmse: 0.62624 | val_1_rmse: 0.63398 |  0:00:04s
epoch 9  | loss: 0.21443 | val_0_rmse: 0.63732 | val_1_rmse: 0.64658 |  0:00:04s
epoch 10 | loss: 0.28716 | val_0_rmse: 0.57167 | val_1_rmse: 0.56842 |  0:00:05s
epoch 11 | loss: 0.31643 | val_0_rmse: 0.65234 | val_1_rmse: 0.64939 |  0:00:05s
epoch 12 | loss: 0.23021 | val_0_rmse: 0.48345 | val_1_rmse: 0.47907 |  0:00:05s
epoch 13 | loss: 0.20592 | val_0_rmse: 0.42716 | val_1_rmse: 0.42058 |  0:00:06s
epoch 14 | loss: 0.27446 | val_0_rmse: 0.41092 | val_1_rmse: 0.41808 |  0:00:06s
epoch 15 | loss: 0.1764  | val_0_rmse: 0.37289 | val_1_rmse: 0.3737  |  0:00:07s
epoch 16 | loss: 0.16485 | val_0_rmse: 0.39865 | val_1_rmse: 0.40311 |  0:00:07s
epoch 17 | loss: 0.15783 | val_0_rmse: 0.3776  | val_1_rmse: 0.38022 |  0:00:08s
epoch 18 | loss: 0.13898 | val_0_rmse: 0.34392 | val_1_rmse: 0.33909 |  0:00:08s
epoch 19 | loss: 0.16364 | val_0_rmse: 0.44704 | val_1_rmse: 0.43909 |  0:00:09s
epoch 20 | loss: 0.17516 | val_0_rmse: 0.39336 | val_1_rmse: 0.38944 |  0:00:09s
epoch 21 | loss: 0.18723 | val_0_rmse: 0.34179 | val_1_rmse: 0.3356  |  0:00:10s
epoch 22 | loss: 0.16542 | val_0_rmse: 0.33318 | val_1_rmse: 0.32482 |  0:00:10s
epoch 23 | loss: 0.163   | val_0_rmse: 0.35266 | val_1_rmse: 0.34314 |  0:00:11s
epoch 24 | loss: 0.15659 | val_0_rmse: 0.35412 | val_1_rmse: 0.34534 |  0:00:11s
epoch 25 | loss: 0.15828 | val_0_rmse: 0.35582 | val_1_rmse: 0.34488 |  0:00:12s
epoch 26 | loss: 0.1572  | val_0_rmse: 0.33675 | val_1_rmse: 0.33362 |  0:00:12s
epoch 27 | loss: 0.16597 | val_0_rmse: 0.33255 | val_1_rmse: 0.3301  |  0:00:12s
epoch 28 | loss: 0.15732 | val_0_rmse: 0.33861 | val_1_rmse: 0.33293 |  0:00:13s
epoch 29 | loss: 0.14292 | val_0_rmse: 0.36315 | val_1_rmse: 0.35596 |  0:00:13s
epoch 30 | loss: 0.14353 | val_0_rmse: 0.35999 | val_1_rmse: 0.34904 |  0:00:14s
epoch 31 | loss: 0.15815 | val_0_rmse: 0.36432 | val_1_rmse: 0.35688 |  0:00:14s
epoch 32 | loss: 0.15589 | val_0_rmse: 0.35462 | val_1_rmse: 0.3441  |  0:00:15s
epoch 33 | loss: 0.15677 | val_0_rmse: 0.35447 | val_1_rmse: 0.34747 |  0:00:15s
epoch 34 | loss: 0.15628 | val_0_rmse: 0.34861 | val_1_rmse: 0.34642 |  0:00:16s
epoch 35 | loss: 0.15893 | val_0_rmse: 0.36001 | val_1_rmse: 0.35411 |  0:00:16s
epoch 36 | loss: 0.15699 | val_0_rmse: 0.33473 | val_1_rmse: 0.3291  |  0:00:16s
epoch 37 | loss: 0.14702 | val_0_rmse: 0.33415 | val_1_rmse: 0.32682 |  0:00:17s
epoch 38 | loss: 0.14338 | val_0_rmse: 0.33608 | val_1_rmse: 0.32663 |  0:00:17s
epoch 39 | loss: 0.14591 | val_0_rmse: 0.34655 | val_1_rmse: 0.33665 |  0:00:18s
epoch 40 | loss: 0.1394  | val_0_rmse: 0.36422 | val_1_rmse: 0.35612 |  0:00:18s
epoch 41 | loss: 0.14483 | val_0_rmse: 0.35558 | val_1_rmse: 0.34879 |  0:00:19s
epoch 42 | loss: 0.14148 | val_0_rmse: 0.35031 | val_1_rmse: 0.34304 |  0:00:19s
epoch 43 | loss: 0.14074 | val_0_rmse: 0.33204 | val_1_rmse: 0.33116 |  0:00:20s
epoch 44 | loss: 0.13347 | val_0_rmse: 0.34221 | val_1_rmse: 0.34297 |  0:00:20s
epoch 45 | loss: 0.1488  | val_0_rmse: 0.31583 | val_1_rmse: 0.31413 |  0:00:20s
epoch 46 | loss: 0.13861 | val_0_rmse: 0.32182 | val_1_rmse: 0.31533 |  0:00:21s
epoch 47 | loss: 0.13408 | val_0_rmse: 0.32842 | val_1_rmse: 0.32277 |  0:00:21s
epoch 48 | loss: 0.13847 | val_0_rmse: 0.32626 | val_1_rmse: 0.32214 |  0:00:22s
epoch 49 | loss: 0.12836 | val_0_rmse: 0.31497 | val_1_rmse: 0.31299 |  0:00:22s
epoch 50 | loss: 0.12788 | val_0_rmse: 0.33346 | val_1_rmse: 0.33061 |  0:00:23s
epoch 51 | loss: 0.12925 | val_0_rmse: 0.32652 | val_1_rmse: 0.32187 |  0:00:23s
epoch 52 | loss: 0.13488 | val_0_rmse: 0.32614 | val_1_rmse: 0.32181 |  0:00:24s
epoch 53 | loss: 0.13249 | val_0_rmse: 0.32329 | val_1_rmse: 0.31848 |  0:00:24s
epoch 54 | loss: 0.12704 | val_0_rmse: 0.3124  | val_1_rmse: 0.30951 |  0:00:25s
epoch 55 | loss: 0.13259 | val_0_rmse: 0.30698 | val_1_rmse: 0.30552 |  0:00:25s
epoch 56 | loss: 0.12888 | val_0_rmse: 0.29707 | val_1_rmse: 0.29333 |  0:00:25s
epoch 57 | loss: 0.12356 | val_0_rmse: 0.31035 | val_1_rmse: 0.3058  |  0:00:26s
epoch 58 | loss: 0.12551 | val_0_rmse: 0.32161 | val_1_rmse: 0.31358 |  0:00:26s
epoch 59 | loss: 0.12599 | val_0_rmse: 0.32922 | val_1_rmse: 0.32387 |  0:00:27s
epoch 60 | loss: 0.12556 | val_0_rmse: 0.31898 | val_1_rmse: 0.31258 |  0:00:27s
epoch 61 | loss: 0.13269 | val_0_rmse: 0.31665 | val_1_rmse: 0.30943 |  0:00:28s
epoch 62 | loss: 0.1256  | val_0_rmse: 0.32635 | val_1_rmse: 0.32295 |  0:00:28s
epoch 63 | loss: 0.13032 | val_0_rmse: 0.32664 | val_1_rmse: 0.32273 |  0:00:29s
epoch 64 | loss: 0.12198 | val_0_rmse: 0.30709 | val_1_rmse: 0.3022  |  0:00:29s
epoch 65 | loss: 0.12616 | val_0_rmse: 0.30355 | val_1_rmse: 0.29901 |  0:00:30s
epoch 66 | loss: 0.12169 | val_0_rmse: 0.30005 | val_1_rmse: 0.29895 |  0:00:30s
epoch 67 | loss: 0.1209  | val_0_rmse: 0.30221 | val_1_rmse: 0.3004  |  0:00:30s
epoch 68 | loss: 0.12963 | val_0_rmse: 0.29992 | val_1_rmse: 0.29887 |  0:00:31s
epoch 69 | loss: 0.12621 | val_0_rmse: 0.3052  | val_1_rmse: 0.30566 |  0:00:31s
epoch 70 | loss: 0.11526 | val_0_rmse: 0.3133  | val_1_rmse: 0.31544 |  0:00:32s
epoch 71 | loss: 0.12436 | val_0_rmse: 0.31936 | val_1_rmse: 0.31642 |  0:00:32s
epoch 72 | loss: 0.12555 | val_0_rmse: 0.30215 | val_1_rmse: 0.30264 |  0:00:33s
epoch 73 | loss: 0.12325 | val_0_rmse: 0.28434 | val_1_rmse: 0.28302 |  0:00:33s
epoch 74 | loss: 0.12256 | val_0_rmse: 0.30025 | val_1_rmse: 0.29827 |  0:00:34s
epoch 75 | loss: 0.11975 | val_0_rmse: 0.30432 | val_1_rmse: 0.29728 |  0:00:34s
epoch 76 | loss: 0.11728 | val_0_rmse: 0.30915 | val_1_rmse: 0.30875 |  0:00:34s
epoch 77 | loss: 0.11873 | val_0_rmse: 0.31479 | val_1_rmse: 0.31005 |  0:00:35s
epoch 78 | loss: 0.12201 | val_0_rmse: 0.31021 | val_1_rmse: 0.30455 |  0:00:35s
epoch 79 | loss: 0.11975 | val_0_rmse: 0.31415 | val_1_rmse: 0.31114 |  0:00:36s
epoch 80 | loss: 0.11914 | val_0_rmse: 0.29027 | val_1_rmse: 0.28555 |  0:00:36s
epoch 81 | loss: 0.11929 | val_0_rmse: 0.29897 | val_1_rmse: 0.29964 |  0:00:37s
epoch 82 | loss: 0.12479 | val_0_rmse: 0.31621 | val_1_rmse: 0.31805 |  0:00:37s
epoch 83 | loss: 0.11911 | val_0_rmse: 0.30171 | val_1_rmse: 0.29838 |  0:00:38s
epoch 84 | loss: 0.12156 | val_0_rmse: 0.29606 | val_1_rmse: 0.29323 |  0:00:38s
epoch 85 | loss: 0.1168  | val_0_rmse: 0.31127 | val_1_rmse: 0.31337 |  0:00:39s
epoch 86 | loss: 0.11114 | val_0_rmse: 0.3067  | val_1_rmse: 0.30486 |  0:00:39s
epoch 87 | loss: 0.11378 | val_0_rmse: 0.31335 | val_1_rmse: 0.31226 |  0:00:39s
epoch 88 | loss: 0.11416 | val_0_rmse: 0.30769 | val_1_rmse: 0.30624 |  0:00:40s
epoch 89 | loss: 0.11878 | val_0_rmse: 0.31408 | val_1_rmse: 0.31175 |  0:00:40s
epoch 90 | loss: 0.11678 | val_0_rmse: 0.29097 | val_1_rmse: 0.28499 |  0:00:41s
epoch 91 | loss: 0.12025 | val_0_rmse: 0.30808 | val_1_rmse: 0.30797 |  0:00:41s
epoch 92 | loss: 0.10987 | val_0_rmse: 0.28625 | val_1_rmse: 0.28413 |  0:00:42s
epoch 93 | loss: 0.10858 | val_0_rmse: 0.28664 | val_1_rmse: 0.28379 |  0:00:42s
epoch 94 | loss: 0.11152 | val_0_rmse: 0.29649 | val_1_rmse: 0.29321 |  0:00:43s
epoch 95 | loss: 0.11605 | val_0_rmse: 0.29396 | val_1_rmse: 0.29163 |  0:00:43s
epoch 96 | loss: 0.11413 | val_0_rmse: 0.3277  | val_1_rmse: 0.3267  |  0:00:43s
epoch 97 | loss: 0.11617 | val_0_rmse: 0.31797 | val_1_rmse: 0.30962 |  0:00:44s
epoch 98 | loss: 0.12323 | val_0_rmse: 0.3137  | val_1_rmse: 0.30536 |  0:00:44s
epoch 99 | loss: 0.12373 | val_0_rmse: 0.30987 | val_1_rmse: 0.30442 |  0:00:45s
epoch 100| loss: 0.12439 | val_0_rmse: 0.30002 | val_1_rmse: 0.29473 |  0:00:45s
epoch 101| loss: 0.11828 | val_0_rmse: 0.28819 | val_1_rmse: 0.28394 |  0:00:46s
epoch 102| loss: 0.11518 | val_0_rmse: 0.27668 | val_1_rmse: 0.27061 |  0:00:46s
epoch 103| loss: 0.11257 | val_0_rmse: 0.27427 | val_1_rmse: 0.26734 |  0:00:47s
epoch 104| loss: 0.10908 | val_0_rmse: 0.27703 | val_1_rmse: 0.27447 |  0:00:47s
epoch 105| loss: 0.10791 | val_0_rmse: 0.30249 | val_1_rmse: 0.30227 |  0:00:48s
epoch 106| loss: 0.11286 | val_0_rmse: 0.30227 | val_1_rmse: 0.30057 |  0:00:48s
epoch 107| loss: 0.11186 | val_0_rmse: 0.3025  | val_1_rmse: 0.30212 |  0:00:48s
epoch 108| loss: 0.10926 | val_0_rmse: 0.28937 | val_1_rmse: 0.29386 |  0:00:49s
epoch 109| loss: 0.1086  | val_0_rmse: 0.29413 | val_1_rmse: 0.29462 |  0:00:49s
epoch 110| loss: 0.11483 | val_0_rmse: 0.28135 | val_1_rmse: 0.27802 |  0:00:50s
epoch 111| loss: 0.11932 | val_0_rmse: 0.28    | val_1_rmse: 0.27808 |  0:00:50s
epoch 112| loss: 0.10345 | val_0_rmse: 0.32777 | val_1_rmse: 0.33197 |  0:00:51s
epoch 113| loss: 0.15544 | val_0_rmse: 0.3679  | val_1_rmse: 0.37005 |  0:00:51s
epoch 114| loss: 0.13024 | val_0_rmse: 0.30327 | val_1_rmse: 0.30448 |  0:00:52s
epoch 115| loss: 0.10347 | val_0_rmse: 0.26697 | val_1_rmse: 0.26137 |  0:00:52s
epoch 116| loss: 0.11604 | val_0_rmse: 0.37104 | val_1_rmse: 0.36345 |  0:00:52s
epoch 117| loss: 0.10231 | val_0_rmse: 0.27123 | val_1_rmse: 0.2642  |  0:00:53s
epoch 118| loss: 0.0812  | val_0_rmse: 0.33167 | val_1_rmse: 0.3251  |  0:00:53s
epoch 119| loss: 0.08123 | val_0_rmse: 0.25988 | val_1_rmse: 0.25474 |  0:00:54s
epoch 120| loss: 0.08932 | val_0_rmse: 0.25789 | val_1_rmse: 0.251   |  0:00:54s
epoch 121| loss: 0.08017 | val_0_rmse: 0.31532 | val_1_rmse: 0.31536 |  0:00:55s
epoch 122| loss: 0.08889 | val_0_rmse: 0.27856 | val_1_rmse: 0.2711  |  0:00:55s
epoch 123| loss: 0.07981 | val_0_rmse: 0.26224 | val_1_rmse: 0.26056 |  0:00:56s
epoch 124| loss: 0.07965 | val_0_rmse: 0.26074 | val_1_rmse: 0.25467 |  0:00:56s
epoch 125| loss: 0.07298 | val_0_rmse: 0.25291 | val_1_rmse: 0.25449 |  0:00:57s
epoch 126| loss: 0.07598 | val_0_rmse: 0.26122 | val_1_rmse: 0.25662 |  0:00:57s
epoch 127| loss: 0.07246 | val_0_rmse: 0.24919 | val_1_rmse: 0.25268 |  0:00:57s
epoch 128| loss: 0.07442 | val_0_rmse: 0.27276 | val_1_rmse: 0.26838 |  0:00:58s
epoch 129| loss: 0.07529 | val_0_rmse: 0.25375 | val_1_rmse: 0.25542 |  0:00:58s
epoch 130| loss: 0.07494 | val_0_rmse: 0.24378 | val_1_rmse: 0.24093 |  0:00:59s
epoch 131| loss: 0.07389 | val_0_rmse: 0.24729 | val_1_rmse: 0.25072 |  0:00:59s
epoch 132| loss: 0.07372 | val_0_rmse: 0.2517  | val_1_rmse: 0.25049 |  0:01:00s
epoch 133| loss: 0.0724  | val_0_rmse: 0.25019 | val_1_rmse: 0.2553  |  0:01:00s
epoch 134| loss: 0.07823 | val_0_rmse: 0.27559 | val_1_rmse: 0.27264 |  0:01:01s
epoch 135| loss: 0.07622 | val_0_rmse: 0.26857 | val_1_rmse: 0.27042 |  0:01:01s
epoch 136| loss: 0.07538 | val_0_rmse: 0.27754 | val_1_rmse: 0.27303 |  0:01:01s
epoch 137| loss: 0.07346 | val_0_rmse: 0.25565 | val_1_rmse: 0.26007 |  0:01:02s
epoch 138| loss: 0.07695 | val_0_rmse: 0.24918 | val_1_rmse: 0.24685 |  0:01:02s
epoch 139| loss: 0.07223 | val_0_rmse: 0.24475 | val_1_rmse: 0.24208 |  0:01:03s
epoch 140| loss: 0.06443 | val_0_rmse: 0.24423 | val_1_rmse: 0.24676 |  0:01:03s
epoch 141| loss: 0.07184 | val_0_rmse: 0.26156 | val_1_rmse: 0.26517 |  0:01:04s
epoch 142| loss: 0.08778 | val_0_rmse: 0.37762 | val_1_rmse: 0.38297 |  0:01:04s
epoch 143| loss: 0.10843 | val_0_rmse: 0.25424 | val_1_rmse: 0.25458 |  0:01:05s
epoch 144| loss: 0.07831 | val_0_rmse: 0.25319 | val_1_rmse: 0.25125 |  0:01:05s
epoch 145| loss: 0.07251 | val_0_rmse: 0.24051 | val_1_rmse: 0.24394 |  0:01:06s
epoch 146| loss: 0.06626 | val_0_rmse: 0.2781  | val_1_rmse: 0.2758  |  0:01:06s
epoch 147| loss: 0.08331 | val_0_rmse: 0.26557 | val_1_rmse: 0.26304 |  0:01:06s
epoch 148| loss: 0.06554 | val_0_rmse: 0.24892 | val_1_rmse: 0.25106 |  0:01:07s
epoch 149| loss: 0.07542 | val_0_rmse: 0.23709 | val_1_rmse: 0.23649 |  0:01:07s
Stop training because you reached max_epochs = 150 with best_epoch = 149 and best_val_1_rmse = 0.23649
Best weights from best epoch are automatically used!
ended training at: 05:43:34
Feature importance:
[('Area', 0.2000512514802739), ('Baths', 0.23373227898943266), ('Beds', 0.00019502116151603885), ('Latitude', 0.12301992786293592), ('Longitude', 0.27152989171884745), ('Month', 0.02939945122422082), ('Year', 0.1420721775627732)]
Mean squared error is of 25070143999.140263
Mean absolute error:115395.1618857124
MAPE:0.20007172376477522
R2 score:0.7002974883594255
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:43:34
epoch 0  | loss: 131.09242| val_0_rmse: 7.06134 | val_1_rmse: 6.9554  |  0:00:00s
epoch 1  | loss: 26.99022| val_0_rmse: 5.13905 | val_1_rmse: 5.3114  |  0:00:00s
epoch 2  | loss: 9.98909 | val_0_rmse: 2.34558 | val_1_rmse: 2.46464 |  0:00:01s
epoch 3  | loss: 1.5733  | val_0_rmse: 1.47664 | val_1_rmse: 1.5101  |  0:00:01s
epoch 4  | loss: 0.58573 | val_0_rmse: 1.35066 | val_1_rmse: 1.33546 |  0:00:02s
epoch 5  | loss: 0.44624 | val_0_rmse: 1.34612 | val_1_rmse: 1.28299 |  0:00:02s
epoch 6  | loss: 0.43152 | val_0_rmse: 0.72724 | val_1_rmse: 0.74192 |  0:00:03s
epoch 7  | loss: 0.54443 | val_0_rmse: 0.54048 | val_1_rmse: 0.54933 |  0:00:03s
epoch 8  | loss: 0.35451 | val_0_rmse: 0.42774 | val_1_rmse: 0.43863 |  0:00:04s
epoch 9  | loss: 0.39505 | val_0_rmse: 0.58311 | val_1_rmse: 0.58751 |  0:00:04s
epoch 10 | loss: 0.29759 | val_0_rmse: 0.53666 | val_1_rmse: 0.53272 |  0:00:05s
epoch 11 | loss: 0.20905 | val_0_rmse: 0.43624 | val_1_rmse: 0.43857 |  0:00:05s
epoch 12 | loss: 0.19977 | val_0_rmse: 0.46685 | val_1_rmse: 0.4681  |  0:00:05s
epoch 13 | loss: 0.22211 | val_0_rmse: 0.39467 | val_1_rmse: 0.39654 |  0:00:06s
epoch 14 | loss: 0.19677 | val_0_rmse: 0.42879 | val_1_rmse: 0.43101 |  0:00:06s
epoch 15 | loss: 0.18428 | val_0_rmse: 0.46112 | val_1_rmse: 0.46617 |  0:00:07s
epoch 16 | loss: 0.2188  | val_0_rmse: 0.4383  | val_1_rmse: 0.45029 |  0:00:07s
epoch 17 | loss: 0.18748 | val_0_rmse: 0.39459 | val_1_rmse: 0.4031  |  0:00:08s
epoch 18 | loss: 0.17536 | val_0_rmse: 0.38408 | val_1_rmse: 0.38948 |  0:00:08s
epoch 19 | loss: 0.185   | val_0_rmse: 0.4224  | val_1_rmse: 0.42213 |  0:00:09s
epoch 20 | loss: 0.25031 | val_0_rmse: 0.57318 | val_1_rmse: 0.5778  |  0:00:09s
epoch 21 | loss: 0.29122 | val_0_rmse: 0.46327 | val_1_rmse: 0.45993 |  0:00:09s
epoch 22 | loss: 0.20856 | val_0_rmse: 0.50746 | val_1_rmse: 0.50537 |  0:00:10s
epoch 23 | loss: 0.22901 | val_0_rmse: 0.52173 | val_1_rmse: 0.53329 |  0:00:10s
epoch 24 | loss: 0.20386 | val_0_rmse: 0.44826 | val_1_rmse: 0.44277 |  0:00:11s
epoch 25 | loss: 0.20185 | val_0_rmse: 0.44091 | val_1_rmse: 0.44303 |  0:00:11s
epoch 26 | loss: 0.17647 | val_0_rmse: 0.4099  | val_1_rmse: 0.40704 |  0:00:12s
epoch 27 | loss: 0.16289 | val_0_rmse: 0.36867 | val_1_rmse: 0.36591 |  0:00:12s
epoch 28 | loss: 0.15469 | val_0_rmse: 0.35006 | val_1_rmse: 0.35283 |  0:00:13s
epoch 29 | loss: 0.149   | val_0_rmse: 0.35036 | val_1_rmse: 0.35416 |  0:00:13s
epoch 30 | loss: 0.14858 | val_0_rmse: 0.37562 | val_1_rmse: 0.38347 |  0:00:14s
epoch 31 | loss: 0.13968 | val_0_rmse: 0.34408 | val_1_rmse: 0.35377 |  0:00:14s
epoch 32 | loss: 0.13137 | val_0_rmse: 0.35033 | val_1_rmse: 0.35902 |  0:00:14s
epoch 33 | loss: 0.13098 | val_0_rmse: 0.34046 | val_1_rmse: 0.3466  |  0:00:15s
epoch 34 | loss: 0.12171 | val_0_rmse: 0.33244 | val_1_rmse: 0.34276 |  0:00:15s
epoch 35 | loss: 0.12419 | val_0_rmse: 0.33006 | val_1_rmse: 0.33876 |  0:00:16s
epoch 36 | loss: 0.12216 | val_0_rmse: 0.35261 | val_1_rmse: 0.35999 |  0:00:16s
epoch 37 | loss: 0.14041 | val_0_rmse: 0.33779 | val_1_rmse: 0.34631 |  0:00:17s
epoch 38 | loss: 0.12453 | val_0_rmse: 0.35707 | val_1_rmse: 0.35711 |  0:00:17s
epoch 39 | loss: 0.12408 | val_0_rmse: 0.33981 | val_1_rmse: 0.34648 |  0:00:18s
epoch 40 | loss: 0.13559 | val_0_rmse: 0.34433 | val_1_rmse: 0.35088 |  0:00:18s
epoch 41 | loss: 0.13683 | val_0_rmse: 0.33616 | val_1_rmse: 0.33724 |  0:00:18s
epoch 42 | loss: 0.13694 | val_0_rmse: 0.35028 | val_1_rmse: 0.35617 |  0:00:19s
epoch 43 | loss: 0.126   | val_0_rmse: 0.31635 | val_1_rmse: 0.32368 |  0:00:19s
epoch 44 | loss: 0.11999 | val_0_rmse: 0.32086 | val_1_rmse: 0.32889 |  0:00:20s
epoch 45 | loss: 0.12981 | val_0_rmse: 0.35365 | val_1_rmse: 0.36053 |  0:00:20s
epoch 46 | loss: 0.15253 | val_0_rmse: 0.30221 | val_1_rmse: 0.31006 |  0:00:21s
epoch 47 | loss: 0.10204 | val_0_rmse: 0.32062 | val_1_rmse: 0.33148 |  0:00:21s
epoch 48 | loss: 0.13387 | val_0_rmse: 0.47391 | val_1_rmse: 0.48006 |  0:00:22s
epoch 49 | loss: 0.16794 | val_0_rmse: 0.30996 | val_1_rmse: 0.31832 |  0:00:22s
epoch 50 | loss: 0.13035 | val_0_rmse: 0.31343 | val_1_rmse: 0.32248 |  0:00:23s
epoch 51 | loss: 0.12096 | val_0_rmse: 0.40211 | val_1_rmse: 0.41398 |  0:00:23s
epoch 52 | loss: 0.14116 | val_0_rmse: 0.39918 | val_1_rmse: 0.41469 |  0:00:23s
epoch 53 | loss: 0.13687 | val_0_rmse: 0.38974 | val_1_rmse: 0.40641 |  0:00:24s
epoch 54 | loss: 0.12593 | val_0_rmse: 0.36967 | val_1_rmse: 0.38843 |  0:00:24s
epoch 55 | loss: 0.12271 | val_0_rmse: 0.35345 | val_1_rmse: 0.37064 |  0:00:25s
epoch 56 | loss: 0.12836 | val_0_rmse: 0.36011 | val_1_rmse: 0.37626 |  0:00:25s
epoch 57 | loss: 0.11865 | val_0_rmse: 0.36102 | val_1_rmse: 0.37906 |  0:00:26s
epoch 58 | loss: 0.11662 | val_0_rmse: 0.35849 | val_1_rmse: 0.37565 |  0:00:26s
epoch 59 | loss: 0.12295 | val_0_rmse: 0.3296  | val_1_rmse: 0.35052 |  0:00:27s
epoch 60 | loss: 0.11947 | val_0_rmse: 0.31465 | val_1_rmse: 0.32956 |  0:00:27s
epoch 61 | loss: 0.12599 | val_0_rmse: 0.30699 | val_1_rmse: 0.32131 |  0:00:27s
epoch 62 | loss: 0.12424 | val_0_rmse: 0.33002 | val_1_rmse: 0.34401 |  0:00:28s
epoch 63 | loss: 0.12045 | val_0_rmse: 0.31966 | val_1_rmse: 0.332   |  0:00:28s
epoch 64 | loss: 0.11848 | val_0_rmse: 0.35763 | val_1_rmse: 0.37188 |  0:00:29s
epoch 65 | loss: 0.11733 | val_0_rmse: 0.37147 | val_1_rmse: 0.38576 |  0:00:29s
epoch 66 | loss: 0.11389 | val_0_rmse: 0.37062 | val_1_rmse: 0.38439 |  0:00:30s
epoch 67 | loss: 0.12086 | val_0_rmse: 0.34137 | val_1_rmse: 0.35285 |  0:00:30s
epoch 68 | loss: 0.11236 | val_0_rmse: 0.32871 | val_1_rmse: 0.34167 |  0:00:31s
epoch 69 | loss: 0.11194 | val_0_rmse: 0.32765 | val_1_rmse: 0.34436 |  0:00:31s
epoch 70 | loss: 0.11851 | val_0_rmse: 0.32977 | val_1_rmse: 0.34568 |  0:00:31s
epoch 71 | loss: 0.10603 | val_0_rmse: 0.32629 | val_1_rmse: 0.33667 |  0:00:32s
epoch 72 | loss: 0.10847 | val_0_rmse: 0.32544 | val_1_rmse: 0.34085 |  0:00:32s
epoch 73 | loss: 0.10902 | val_0_rmse: 0.34192 | val_1_rmse: 0.35689 |  0:00:33s
epoch 74 | loss: 0.10605 | val_0_rmse: 0.33216 | val_1_rmse: 0.34692 |  0:00:33s
epoch 75 | loss: 0.11347 | val_0_rmse: 0.32056 | val_1_rmse: 0.333   |  0:00:34s
epoch 76 | loss: 0.10523 | val_0_rmse: 0.33592 | val_1_rmse: 0.3512  |  0:00:34s

Early stopping occured at epoch 76 with best_epoch = 46 and best_val_1_rmse = 0.31006
Best weights from best epoch are automatically used!
ended training at: 05:44:08
Feature importance:
[('Area', 0.10320462814626048), ('Baths', 0.18107195882650545), ('Beds', 0.0), ('Latitude', 0.06716659722849984), ('Longitude', 0.11767405475766282), ('Month', 0.29632449694102414), ('Year', 0.23455826410004724)]
Mean squared error is of 39734029482.079025
Mean absolute error:145141.63151071244
MAPE:0.23089648979200525
R2 score:0.5235833444823494
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:44:09
epoch 0  | loss: 131.21092| val_0_rmse: 7.3398  | val_1_rmse: 7.37747 |  0:00:00s
epoch 1  | loss: 26.84034| val_0_rmse: 5.71549 | val_1_rmse: 5.54265 |  0:00:00s
epoch 2  | loss: 7.21081 | val_0_rmse: 2.68634 | val_1_rmse: 2.64121 |  0:00:01s
epoch 3  | loss: 2.04539 | val_0_rmse: 1.46981 | val_1_rmse: 1.44865 |  0:00:01s
epoch 4  | loss: 1.77622 | val_0_rmse: 1.42121 | val_1_rmse: 1.42736 |  0:00:02s
epoch 5  | loss: 0.66367 | val_0_rmse: 1.54679 | val_1_rmse: 1.49918 |  0:00:02s
epoch 6  | loss: 0.57581 | val_0_rmse: 1.34242 | val_1_rmse: 1.31729 |  0:00:03s
epoch 7  | loss: 0.42845 | val_0_rmse: 0.87569 | val_1_rmse: 0.8836  |  0:00:03s
epoch 8  | loss: 0.29783 | val_0_rmse: 0.63763 | val_1_rmse: 0.63889 |  0:00:04s
epoch 9  | loss: 0.27443 | val_0_rmse: 0.52643 | val_1_rmse: 0.53504 |  0:00:04s
epoch 10 | loss: 0.25248 | val_0_rmse: 0.49646 | val_1_rmse: 0.49968 |  0:00:04s
epoch 11 | loss: 0.22957 | val_0_rmse: 0.45428 | val_1_rmse: 0.45457 |  0:00:05s
epoch 12 | loss: 0.17993 | val_0_rmse: 0.41383 | val_1_rmse: 0.40961 |  0:00:05s
epoch 13 | loss: 0.14185 | val_0_rmse: 0.41617 | val_1_rmse: 0.40863 |  0:00:06s
epoch 14 | loss: 0.15856 | val_0_rmse: 0.3538  | val_1_rmse: 0.34586 |  0:00:06s
epoch 15 | loss: 0.30497 | val_0_rmse: 0.60173 | val_1_rmse: 0.61202 |  0:00:07s
epoch 16 | loss: 0.32336 | val_0_rmse: 0.43809 | val_1_rmse: 0.42339 |  0:00:07s
epoch 17 | loss: 0.20935 | val_0_rmse: 0.39653 | val_1_rmse: 0.38891 |  0:00:08s
epoch 18 | loss: 0.18323 | val_0_rmse: 0.35849 | val_1_rmse: 0.34826 |  0:00:08s
epoch 19 | loss: 0.17721 | val_0_rmse: 0.32711 | val_1_rmse: 0.32385 |  0:00:09s
epoch 20 | loss: 0.1724  | val_0_rmse: 0.44543 | val_1_rmse: 0.43368 |  0:00:09s
epoch 21 | loss: 0.19445 | val_0_rmse: 0.45129 | val_1_rmse: 0.43327 |  0:00:09s
epoch 22 | loss: 0.17361 | val_0_rmse: 0.28289 | val_1_rmse: 0.28233 |  0:00:10s
epoch 23 | loss: 0.15667 | val_0_rmse: 0.32005 | val_1_rmse: 0.31803 |  0:00:10s
epoch 24 | loss: 0.14638 | val_0_rmse: 0.29112 | val_1_rmse: 0.28808 |  0:00:11s
epoch 25 | loss: 0.14541 | val_0_rmse: 0.31045 | val_1_rmse: 0.3052  |  0:00:11s
epoch 26 | loss: 0.131   | val_0_rmse: 0.30824 | val_1_rmse: 0.3024  |  0:00:12s
epoch 27 | loss: 0.13953 | val_0_rmse: 0.31916 | val_1_rmse: 0.31636 |  0:00:12s
epoch 28 | loss: 0.13089 | val_0_rmse: 0.32328 | val_1_rmse: 0.31737 |  0:00:13s
epoch 29 | loss: 0.14601 | val_0_rmse: 0.2895  | val_1_rmse: 0.2866  |  0:00:13s
epoch 30 | loss: 0.13935 | val_0_rmse: 0.29063 | val_1_rmse: 0.28686 |  0:00:13s
epoch 31 | loss: 0.12638 | val_0_rmse: 0.28111 | val_1_rmse: 0.27701 |  0:00:14s
epoch 32 | loss: 0.12603 | val_0_rmse: 0.30583 | val_1_rmse: 0.3025  |  0:00:14s
epoch 33 | loss: 0.12826 | val_0_rmse: 0.28827 | val_1_rmse: 0.28698 |  0:00:15s
epoch 34 | loss: 0.1216  | val_0_rmse: 0.29547 | val_1_rmse: 0.29799 |  0:00:15s
epoch 35 | loss: 0.12931 | val_0_rmse: 0.28507 | val_1_rmse: 0.28638 |  0:00:16s
epoch 36 | loss: 0.125   | val_0_rmse: 0.29492 | val_1_rmse: 0.29243 |  0:00:16s
epoch 37 | loss: 0.12352 | val_0_rmse: 0.29197 | val_1_rmse: 0.28962 |  0:00:17s
epoch 38 | loss: 0.12257 | val_0_rmse: 0.27733 | val_1_rmse: 0.27432 |  0:00:17s
epoch 39 | loss: 0.11891 | val_0_rmse: 0.28607 | val_1_rmse: 0.28511 |  0:00:18s
epoch 40 | loss: 0.11322 | val_0_rmse: 0.27914 | val_1_rmse: 0.27642 |  0:00:18s
epoch 41 | loss: 0.11185 | val_0_rmse: 0.28387 | val_1_rmse: 0.28047 |  0:00:18s
epoch 42 | loss: 0.12329 | val_0_rmse: 0.2806  | val_1_rmse: 0.27651 |  0:00:19s
epoch 43 | loss: 0.11445 | val_0_rmse: 0.28506 | val_1_rmse: 0.27927 |  0:00:19s
epoch 44 | loss: 0.11489 | val_0_rmse: 0.28186 | val_1_rmse: 0.27663 |  0:00:20s
epoch 45 | loss: 0.1147  | val_0_rmse: 0.28209 | val_1_rmse: 0.27885 |  0:00:20s
epoch 46 | loss: 0.11307 | val_0_rmse: 0.27819 | val_1_rmse: 0.27619 |  0:00:21s
epoch 47 | loss: 0.11042 | val_0_rmse: 0.27595 | val_1_rmse: 0.2712  |  0:00:21s
epoch 48 | loss: 0.10944 | val_0_rmse: 0.26843 | val_1_rmse: 0.26617 |  0:00:22s
epoch 49 | loss: 0.1201  | val_0_rmse: 0.26225 | val_1_rmse: 0.25923 |  0:00:22s
epoch 50 | loss: 0.11623 | val_0_rmse: 0.26379 | val_1_rmse: 0.25854 |  0:00:22s
epoch 51 | loss: 0.10797 | val_0_rmse: 0.2714  | val_1_rmse: 0.26716 |  0:00:23s
epoch 52 | loss: 0.11413 | val_0_rmse: 0.27035 | val_1_rmse: 0.26814 |  0:00:23s
epoch 53 | loss: 0.11002 | val_0_rmse: 0.26947 | val_1_rmse: 0.26688 |  0:00:24s
epoch 54 | loss: 0.10704 | val_0_rmse: 0.2678  | val_1_rmse: 0.26605 |  0:00:24s
epoch 55 | loss: 0.116   | val_0_rmse: 0.26633 | val_1_rmse: 0.2626  |  0:00:25s
epoch 56 | loss: 0.1143  | val_0_rmse: 0.26864 | val_1_rmse: 0.26577 |  0:00:25s
epoch 57 | loss: 0.11419 | val_0_rmse: 0.26861 | val_1_rmse: 0.26231 |  0:00:26s
epoch 58 | loss: 0.10461 | val_0_rmse: 0.27026 | val_1_rmse: 0.26522 |  0:00:26s
epoch 59 | loss: 0.10905 | val_0_rmse: 0.26302 | val_1_rmse: 0.258   |  0:00:27s
epoch 60 | loss: 0.11481 | val_0_rmse: 0.26067 | val_1_rmse: 0.25818 |  0:00:27s
epoch 61 | loss: 0.11098 | val_0_rmse: 0.26256 | val_1_rmse: 0.25832 |  0:00:27s
epoch 62 | loss: 0.10991 | val_0_rmse: 0.25993 | val_1_rmse: 0.25614 |  0:00:28s
epoch 63 | loss: 0.10739 | val_0_rmse: 0.26236 | val_1_rmse: 0.25814 |  0:00:28s
epoch 64 | loss: 0.11049 | val_0_rmse: 0.27114 | val_1_rmse: 0.26514 |  0:00:29s
epoch 65 | loss: 0.11225 | val_0_rmse: 0.26622 | val_1_rmse: 0.26112 |  0:00:29s
epoch 66 | loss: 0.11042 | val_0_rmse: 0.25726 | val_1_rmse: 0.25189 |  0:00:30s
epoch 67 | loss: 0.10974 | val_0_rmse: 0.26284 | val_1_rmse: 0.25707 |  0:00:30s
epoch 68 | loss: 0.1054  | val_0_rmse: 0.26368 | val_1_rmse: 0.25875 |  0:00:31s
epoch 69 | loss: 0.10841 | val_0_rmse: 0.26193 | val_1_rmse: 0.25585 |  0:00:31s
epoch 70 | loss: 0.10349 | val_0_rmse: 0.27345 | val_1_rmse: 0.26656 |  0:00:31s
epoch 71 | loss: 0.10676 | val_0_rmse: 0.26217 | val_1_rmse: 0.25776 |  0:00:32s
epoch 72 | loss: 0.10762 | val_0_rmse: 0.26647 | val_1_rmse: 0.26293 |  0:00:32s
epoch 73 | loss: 0.10782 | val_0_rmse: 0.26736 | val_1_rmse: 0.26248 |  0:00:33s
epoch 74 | loss: 0.09945 | val_0_rmse: 0.26473 | val_1_rmse: 0.26216 |  0:00:33s
epoch 75 | loss: 0.10497 | val_0_rmse: 0.26368 | val_1_rmse: 0.26084 |  0:00:34s
epoch 76 | loss: 0.10434 | val_0_rmse: 0.2587  | val_1_rmse: 0.25284 |  0:00:34s
epoch 77 | loss: 0.09972 | val_0_rmse: 0.25221 | val_1_rmse: 0.24683 |  0:00:35s
epoch 78 | loss: 0.10318 | val_0_rmse: 0.25926 | val_1_rmse: 0.25677 |  0:00:35s
epoch 79 | loss: 0.10246 | val_0_rmse: 0.26187 | val_1_rmse: 0.25854 |  0:00:36s
epoch 80 | loss: 0.1068  | val_0_rmse: 0.25406 | val_1_rmse: 0.25034 |  0:00:36s
epoch 81 | loss: 0.10259 | val_0_rmse: 0.2511  | val_1_rmse: 0.24666 |  0:00:36s
epoch 82 | loss: 0.10388 | val_0_rmse: 0.25937 | val_1_rmse: 0.25404 |  0:00:37s
epoch 83 | loss: 0.10795 | val_0_rmse: 0.28382 | val_1_rmse: 0.27777 |  0:00:37s
epoch 84 | loss: 0.10625 | val_0_rmse: 0.26088 | val_1_rmse: 0.25537 |  0:00:38s
epoch 85 | loss: 0.09724 | val_0_rmse: 0.26998 | val_1_rmse: 0.26317 |  0:00:38s
epoch 86 | loss: 0.10082 | val_0_rmse: 0.25595 | val_1_rmse: 0.24947 |  0:00:39s
epoch 87 | loss: 0.10202 | val_0_rmse: 0.26078 | val_1_rmse: 0.25593 |  0:00:39s
epoch 88 | loss: 0.10009 | val_0_rmse: 0.25144 | val_1_rmse: 0.24809 |  0:00:40s
epoch 89 | loss: 0.10003 | val_0_rmse: 0.25367 | val_1_rmse: 0.24899 |  0:00:40s
epoch 90 | loss: 0.10791 | val_0_rmse: 0.26187 | val_1_rmse: 0.25603 |  0:00:40s
epoch 91 | loss: 0.10229 | val_0_rmse: 0.27323 | val_1_rmse: 0.27107 |  0:00:41s
epoch 92 | loss: 0.10355 | val_0_rmse: 0.26981 | val_1_rmse: 0.26593 |  0:00:41s
epoch 93 | loss: 0.10157 | val_0_rmse: 0.2692  | val_1_rmse: 0.26294 |  0:00:42s
epoch 94 | loss: 0.09824 | val_0_rmse: 0.25816 | val_1_rmse: 0.2533  |  0:00:42s
epoch 95 | loss: 0.09757 | val_0_rmse: 0.27032 | val_1_rmse: 0.26827 |  0:00:43s
epoch 96 | loss: 0.0965  | val_0_rmse: 0.25665 | val_1_rmse: 0.25343 |  0:00:43s
epoch 97 | loss: 0.09784 | val_0_rmse: 0.26076 | val_1_rmse: 0.25654 |  0:00:44s
epoch 98 | loss: 0.09541 | val_0_rmse: 0.28175 | val_1_rmse: 0.27693 |  0:00:44s
epoch 99 | loss: 0.10106 | val_0_rmse: 0.28024 | val_1_rmse: 0.27833 |  0:00:44s
epoch 100| loss: 0.1038  | val_0_rmse: 0.25815 | val_1_rmse: 0.25537 |  0:00:45s
epoch 101| loss: 0.10058 | val_0_rmse: 0.25046 | val_1_rmse: 0.24719 |  0:00:45s
epoch 102| loss: 0.10514 | val_0_rmse: 0.25585 | val_1_rmse: 0.25442 |  0:00:46s
epoch 103| loss: 0.09862 | val_0_rmse: 0.26412 | val_1_rmse: 0.26254 |  0:00:46s
epoch 104| loss: 0.09893 | val_0_rmse: 0.26777 | val_1_rmse: 0.26528 |  0:00:47s
epoch 105| loss: 0.10006 | val_0_rmse: 0.26343 | val_1_rmse: 0.2612  |  0:00:47s
epoch 106| loss: 0.09724 | val_0_rmse: 0.25961 | val_1_rmse: 0.25532 |  0:00:48s
epoch 107| loss: 0.09626 | val_0_rmse: 0.25521 | val_1_rmse: 0.2507  |  0:00:48s
epoch 108| loss: 0.10191 | val_0_rmse: 0.26557 | val_1_rmse: 0.26186 |  0:00:48s
epoch 109| loss: 0.10514 | val_0_rmse: 0.26882 | val_1_rmse: 0.26653 |  0:00:49s
epoch 110| loss: 0.09629 | val_0_rmse: 0.25542 | val_1_rmse: 0.24995 |  0:00:49s
epoch 111| loss: 0.09641 | val_0_rmse: 0.2649  | val_1_rmse: 0.26054 |  0:00:50s

Early stopping occured at epoch 111 with best_epoch = 81 and best_val_1_rmse = 0.24666
Best weights from best epoch are automatically used!
ended training at: 05:44:59
Feature importance:
[('Area', 0.0), ('Baths', 0.0802558881014164), ('Beds', 0.16425763809500563), ('Latitude', 0.2670133900748999), ('Longitude', 0.24916386507922658), ('Month', 0.14142822971886754), ('Year', 0.09788098893058396)]
Mean squared error is of 25454868582.983227
Mean absolute error:118903.10789110148
MAPE:0.21160175484478239
R2 score:0.6697539974456079
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:44:59
epoch 0  | loss: 131.61453| val_0_rmse: 6.8697  | val_1_rmse: 6.84072 |  0:00:00s
epoch 1  | loss: 27.59288| val_0_rmse: 9.18196 | val_1_rmse: 9.36321 |  0:00:00s
epoch 2  | loss: 8.16765 | val_0_rmse: 2.76634 | val_1_rmse: 2.65252 |  0:00:01s
epoch 3  | loss: 1.54713 | val_0_rmse: 1.39614 | val_1_rmse: 1.38063 |  0:00:01s
epoch 4  | loss: 0.69965 | val_0_rmse: 1.70464 | val_1_rmse: 1.74446 |  0:00:02s
epoch 5  | loss: 0.38195 | val_0_rmse: 1.24256 | val_1_rmse: 1.25951 |  0:00:02s
epoch 6  | loss: 0.31176 | val_0_rmse: 0.73263 | val_1_rmse: 0.72169 |  0:00:03s
epoch 7  | loss: 0.30024 | val_0_rmse: 0.76748 | val_1_rmse: 0.78653 |  0:00:03s
epoch 8  | loss: 0.26414 | val_0_rmse: 0.69256 | val_1_rmse: 0.69991 |  0:00:04s
epoch 9  | loss: 0.21533 | val_0_rmse: 0.51168 | val_1_rmse: 0.52612 |  0:00:04s
epoch 10 | loss: 0.19823 | val_0_rmse: 0.52916 | val_1_rmse: 0.55284 |  0:00:04s
epoch 11 | loss: 0.18583 | val_0_rmse: 0.39919 | val_1_rmse: 0.41008 |  0:00:05s
epoch 12 | loss: 0.19092 | val_0_rmse: 0.37777 | val_1_rmse: 0.3866  |  0:00:05s
epoch 13 | loss: 0.16181 | val_0_rmse: 0.42424 | val_1_rmse: 0.43417 |  0:00:06s
epoch 14 | loss: 0.14928 | val_0_rmse: 0.37064 | val_1_rmse: 0.37634 |  0:00:06s
epoch 15 | loss: 0.17609 | val_0_rmse: 0.37771 | val_1_rmse: 0.38662 |  0:00:07s
epoch 16 | loss: 0.18371 | val_0_rmse: 0.42608 | val_1_rmse: 0.42667 |  0:00:07s
epoch 17 | loss: 0.17664 | val_0_rmse: 0.34445 | val_1_rmse: 0.35087 |  0:00:08s
epoch 18 | loss: 0.13716 | val_0_rmse: 0.34061 | val_1_rmse: 0.35117 |  0:00:08s
epoch 19 | loss: 0.12016 | val_0_rmse: 0.34327 | val_1_rmse: 0.35534 |  0:00:09s
epoch 20 | loss: 0.1328  | val_0_rmse: 0.35516 | val_1_rmse: 0.36615 |  0:00:09s
epoch 21 | loss: 0.12604 | val_0_rmse: 0.31874 | val_1_rmse: 0.32645 |  0:00:09s
epoch 22 | loss: 0.11378 | val_0_rmse: 0.31626 | val_1_rmse: 0.32348 |  0:00:10s
epoch 23 | loss: 0.1065  | val_0_rmse: 0.29545 | val_1_rmse: 0.30533 |  0:00:10s
epoch 24 | loss: 0.09989 | val_0_rmse: 0.29208 | val_1_rmse: 0.30137 |  0:00:11s
epoch 25 | loss: 0.10731 | val_0_rmse: 0.3115  | val_1_rmse: 0.32891 |  0:00:11s
epoch 26 | loss: 0.12396 | val_0_rmse: 0.31998 | val_1_rmse: 0.32639 |  0:00:12s
epoch 27 | loss: 0.13216 | val_0_rmse: 0.31599 | val_1_rmse: 0.31727 |  0:00:12s
epoch 28 | loss: 0.12455 | val_0_rmse: 0.29376 | val_1_rmse: 0.30501 |  0:00:13s
epoch 29 | loss: 0.09639 | val_0_rmse: 0.31753 | val_1_rmse: 0.31856 |  0:00:13s
epoch 30 | loss: 0.11445 | val_0_rmse: 0.32925 | val_1_rmse: 0.32857 |  0:00:14s
epoch 31 | loss: 0.09683 | val_0_rmse: 0.27416 | val_1_rmse: 0.28194 |  0:00:14s
epoch 32 | loss: 0.08811 | val_0_rmse: 0.28164 | val_1_rmse: 0.28654 |  0:00:14s
epoch 33 | loss: 0.09672 | val_0_rmse: 0.28567 | val_1_rmse: 0.29573 |  0:00:15s
epoch 34 | loss: 0.09199 | val_0_rmse: 0.30289 | val_1_rmse: 0.3109  |  0:00:15s
epoch 35 | loss: 0.09244 | val_0_rmse: 0.29604 | val_1_rmse: 0.30681 |  0:00:16s
epoch 36 | loss: 0.09856 | val_0_rmse: 0.31549 | val_1_rmse: 0.3191  |  0:00:16s
epoch 37 | loss: 0.10943 | val_0_rmse: 0.32724 | val_1_rmse: 0.34268 |  0:00:17s
epoch 38 | loss: 0.10241 | val_0_rmse: 0.28417 | val_1_rmse: 0.29034 |  0:00:17s
epoch 39 | loss: 0.09349 | val_0_rmse: 0.28893 | val_1_rmse: 0.30018 |  0:00:18s
epoch 40 | loss: 0.10795 | val_0_rmse: 0.29682 | val_1_rmse: 0.31083 |  0:00:18s
epoch 41 | loss: 0.10285 | val_0_rmse: 0.29997 | val_1_rmse: 0.3113  |  0:00:18s
epoch 42 | loss: 0.11168 | val_0_rmse: 0.38953 | val_1_rmse: 0.40121 |  0:00:19s
epoch 43 | loss: 0.13664 | val_0_rmse: 0.32187 | val_1_rmse: 0.33553 |  0:00:19s
epoch 44 | loss: 0.12529 | val_0_rmse: 0.32057 | val_1_rmse: 0.33369 |  0:00:20s
epoch 45 | loss: 0.12286 | val_0_rmse: 0.33783 | val_1_rmse: 0.35094 |  0:00:20s
epoch 46 | loss: 0.12577 | val_0_rmse: 0.34168 | val_1_rmse: 0.35647 |  0:00:21s
epoch 47 | loss: 0.11906 | val_0_rmse: 0.34136 | val_1_rmse: 0.35349 |  0:00:21s
epoch 48 | loss: 0.11995 | val_0_rmse: 0.35521 | val_1_rmse: 0.36802 |  0:00:22s
epoch 49 | loss: 0.12011 | val_0_rmse: 0.35599 | val_1_rmse: 0.37058 |  0:00:22s
epoch 50 | loss: 0.12261 | val_0_rmse: 0.34822 | val_1_rmse: 0.35869 |  0:00:22s
epoch 51 | loss: 0.11864 | val_0_rmse: 0.35388 | val_1_rmse: 0.37051 |  0:00:23s
epoch 52 | loss: 0.11449 | val_0_rmse: 0.34669 | val_1_rmse: 0.36021 |  0:00:23s
epoch 53 | loss: 0.11119 | val_0_rmse: 0.33416 | val_1_rmse: 0.34972 |  0:00:24s
epoch 54 | loss: 0.11866 | val_0_rmse: 0.33249 | val_1_rmse: 0.34644 |  0:00:24s
epoch 55 | loss: 0.11661 | val_0_rmse: 0.32493 | val_1_rmse: 0.33481 |  0:00:25s
epoch 56 | loss: 0.11325 | val_0_rmse: 0.33544 | val_1_rmse: 0.35173 |  0:00:25s
epoch 57 | loss: 0.11379 | val_0_rmse: 0.3399  | val_1_rmse: 0.35347 |  0:00:26s
epoch 58 | loss: 0.10902 | val_0_rmse: 0.34101 | val_1_rmse: 0.35632 |  0:00:26s
epoch 59 | loss: 0.11222 | val_0_rmse: 0.36063 | val_1_rmse: 0.37301 |  0:00:26s
epoch 60 | loss: 0.1154  | val_0_rmse: 0.37745 | val_1_rmse: 0.39063 |  0:00:27s
epoch 61 | loss: 0.11543 | val_0_rmse: 0.38408 | val_1_rmse: 0.39801 |  0:00:27s

Early stopping occured at epoch 61 with best_epoch = 31 and best_val_1_rmse = 0.28194
Best weights from best epoch are automatically used!
ended training at: 05:45:27
Feature importance:
[('Area', 0.18535502078018487), ('Baths', 0.03651472591538115), ('Beds', 0.008132134972701037), ('Latitude', 0.0), ('Longitude', 0.4001790850897411), ('Month', 0.2437659423484633), ('Year', 0.12605309089352856)]
Mean squared error is of 34536747651.34057
Mean absolute error:134904.9607646556
MAPE:0.22353288021128714
R2 score:0.5774284443464691
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:45:27
epoch 0  | loss: 141.73048| val_0_rmse: 10.91879| val_1_rmse: 10.85752|  0:00:00s
epoch 1  | loss: 106.19519| val_0_rmse: 9.5127  | val_1_rmse: 9.38433 |  0:00:00s
epoch 2  | loss: 74.76292| val_0_rmse: 7.25959 | val_1_rmse: 7.16524 |  0:00:00s
epoch 3  | loss: 41.44736| val_0_rmse: 4.80452 | val_1_rmse: 4.70124 |  0:00:00s
epoch 4  | loss: 13.69363| val_0_rmse: 2.98188 | val_1_rmse: 2.9433  |  0:00:00s
epoch 5  | loss: 3.33851 | val_0_rmse: 3.08301 | val_1_rmse: 2.75775 |  0:00:00s
epoch 6  | loss: 6.39842 | val_0_rmse: 2.94508 | val_1_rmse: 2.52987 |  0:00:00s
epoch 7  | loss: 6.32757 | val_0_rmse: 1.88939 | val_1_rmse: 1.92636 |  0:00:01s
epoch 8  | loss: 2.93804 | val_0_rmse: 2.27895 | val_1_rmse: 2.43188 |  0:00:01s
epoch 9  | loss: 2.47847 | val_0_rmse: 2.67232 | val_1_rmse: 2.95489 |  0:00:01s
epoch 10 | loss: 2.12556 | val_0_rmse: 2.44966 | val_1_rmse: 2.60318 |  0:00:01s
epoch 11 | loss: 1.37429 | val_0_rmse: 1.77836 | val_1_rmse: 1.77711 |  0:00:01s
epoch 12 | loss: 1.17165 | val_0_rmse: 1.88005 | val_1_rmse: 1.85051 |  0:00:01s
epoch 13 | loss: 0.67431 | val_0_rmse: 1.90049 | val_1_rmse: 1.89855 |  0:00:01s
epoch 14 | loss: 0.72272 | val_0_rmse: 1.55316 | val_1_rmse: 1.55671 |  0:00:02s
epoch 15 | loss: 0.70011 | val_0_rmse: 1.43089 | val_1_rmse: 1.41879 |  0:00:02s
epoch 16 | loss: 0.51819 | val_0_rmse: 1.57432 | val_1_rmse: 1.52815 |  0:00:02s
epoch 17 | loss: 0.52577 | val_0_rmse: 1.305   | val_1_rmse: 1.26705 |  0:00:02s
epoch 18 | loss: 0.4049  | val_0_rmse: 1.41081 | val_1_rmse: 1.38773 |  0:00:02s
epoch 19 | loss: 0.45226 | val_0_rmse: 1.03478 | val_1_rmse: 1.03495 |  0:00:02s
epoch 20 | loss: 0.34525 | val_0_rmse: 1.05425 | val_1_rmse: 1.05682 |  0:00:02s
epoch 21 | loss: 0.33091 | val_0_rmse: 0.96542 | val_1_rmse: 0.96958 |  0:00:02s
epoch 22 | loss: 0.30521 | val_0_rmse: 0.94852 | val_1_rmse: 0.97416 |  0:00:03s
epoch 23 | loss: 0.29531 | val_0_rmse: 0.76564 | val_1_rmse: 0.78896 |  0:00:03s
epoch 24 | loss: 0.29216 | val_0_rmse: 0.80247 | val_1_rmse: 0.84275 |  0:00:03s
epoch 25 | loss: 0.32788 | val_0_rmse: 0.88093 | val_1_rmse: 0.92024 |  0:00:03s
epoch 26 | loss: 0.33186 | val_0_rmse: 0.72274 | val_1_rmse: 0.77576 |  0:00:03s
epoch 27 | loss: 0.29223 | val_0_rmse: 0.82253 | val_1_rmse: 0.84341 |  0:00:03s
epoch 28 | loss: 0.3023  | val_0_rmse: 0.66548 | val_1_rmse: 0.68434 |  0:00:03s
epoch 29 | loss: 0.35916 | val_0_rmse: 0.88781 | val_1_rmse: 0.90921 |  0:00:04s
epoch 30 | loss: 0.32914 | val_0_rmse: 0.81117 | val_1_rmse: 0.82377 |  0:00:04s
epoch 31 | loss: 0.32542 | val_0_rmse: 0.54738 | val_1_rmse: 0.56203 |  0:00:04s
epoch 32 | loss: 0.33025 | val_0_rmse: 0.78091 | val_1_rmse: 0.79175 |  0:00:04s
epoch 33 | loss: 0.34023 | val_0_rmse: 0.60256 | val_1_rmse: 0.62496 |  0:00:04s
epoch 34 | loss: 0.26142 | val_0_rmse: 0.54239 | val_1_rmse: 0.54509 |  0:00:04s
epoch 35 | loss: 0.37915 | val_0_rmse: 0.5688  | val_1_rmse: 0.59131 |  0:00:04s
epoch 36 | loss: 0.25131 | val_0_rmse: 0.60288 | val_1_rmse: 0.61895 |  0:00:04s
epoch 37 | loss: 0.28502 | val_0_rmse: 0.48628 | val_1_rmse: 0.50899 |  0:00:05s
epoch 38 | loss: 0.28588 | val_0_rmse: 0.54279 | val_1_rmse: 0.55781 |  0:00:05s
epoch 39 | loss: 0.25401 | val_0_rmse: 0.49367 | val_1_rmse: 0.51355 |  0:00:05s
epoch 40 | loss: 0.24699 | val_0_rmse: 0.50257 | val_1_rmse: 0.52423 |  0:00:05s
epoch 41 | loss: 0.22512 | val_0_rmse: 0.51452 | val_1_rmse: 0.53439 |  0:00:05s
epoch 42 | loss: 0.25119 | val_0_rmse: 0.5157  | val_1_rmse: 0.5382  |  0:00:05s
epoch 43 | loss: 0.23217 | val_0_rmse: 0.51833 | val_1_rmse: 0.56071 |  0:00:05s
epoch 44 | loss: 0.25625 | val_0_rmse: 0.51189 | val_1_rmse: 0.54446 |  0:00:05s
epoch 45 | loss: 0.22567 | val_0_rmse: 0.47028 | val_1_rmse: 0.49416 |  0:00:06s
epoch 46 | loss: 0.2334  | val_0_rmse: 0.48438 | val_1_rmse: 0.51126 |  0:00:06s
epoch 47 | loss: 0.22159 | val_0_rmse: 0.4545  | val_1_rmse: 0.48851 |  0:00:06s
epoch 48 | loss: 0.22856 | val_0_rmse: 0.4582  | val_1_rmse: 0.48112 |  0:00:06s
epoch 49 | loss: 0.21119 | val_0_rmse: 0.46307 | val_1_rmse: 0.49447 |  0:00:06s
epoch 50 | loss: 0.22809 | val_0_rmse: 0.53559 | val_1_rmse: 0.56095 |  0:00:06s
epoch 51 | loss: 0.27273 | val_0_rmse: 0.47281 | val_1_rmse: 0.48937 |  0:00:06s
epoch 52 | loss: 0.23349 | val_0_rmse: 0.46852 | val_1_rmse: 0.47218 |  0:00:07s
epoch 53 | loss: 0.3496  | val_0_rmse: 0.46605 | val_1_rmse: 0.48182 |  0:00:07s
epoch 54 | loss: 0.21414 | val_0_rmse: 0.50588 | val_1_rmse: 0.52937 |  0:00:07s
epoch 55 | loss: 0.26847 | val_0_rmse: 0.43744 | val_1_rmse: 0.45449 |  0:00:07s
epoch 56 | loss: 0.24475 | val_0_rmse: 0.50596 | val_1_rmse: 0.54312 |  0:00:07s
epoch 57 | loss: 0.2449  | val_0_rmse: 0.54311 | val_1_rmse: 0.56465 |  0:00:07s
epoch 58 | loss: 0.30399 | val_0_rmse: 0.48326 | val_1_rmse: 0.50929 |  0:00:07s
epoch 59 | loss: 0.28147 | val_0_rmse: 0.47669 | val_1_rmse: 0.53476 |  0:00:07s
epoch 60 | loss: 0.28157 | val_0_rmse: 0.58865 | val_1_rmse: 0.64394 |  0:00:08s
epoch 61 | loss: 0.28815 | val_0_rmse: 0.43679 | val_1_rmse: 0.48083 |  0:00:08s
epoch 62 | loss: 0.24244 | val_0_rmse: 0.45062 | val_1_rmse: 0.47456 |  0:00:08s
epoch 63 | loss: 0.23281 | val_0_rmse: 0.509   | val_1_rmse: 0.54022 |  0:00:08s
epoch 64 | loss: 0.28106 | val_0_rmse: 0.45436 | val_1_rmse: 0.47688 |  0:00:08s
epoch 65 | loss: 0.23089 | val_0_rmse: 0.4328  | val_1_rmse: 0.4611  |  0:00:08s
epoch 66 | loss: 0.20245 | val_0_rmse: 0.51632 | val_1_rmse: 0.54733 |  0:00:08s
epoch 67 | loss: 0.25948 | val_0_rmse: 0.43172 | val_1_rmse: 0.45259 |  0:00:09s
epoch 68 | loss: 0.22038 | val_0_rmse: 0.43117 | val_1_rmse: 0.45124 |  0:00:09s
epoch 69 | loss: 0.21313 | val_0_rmse: 0.4847  | val_1_rmse: 0.51113 |  0:00:09s
epoch 70 | loss: 0.23985 | val_0_rmse: 0.43611 | val_1_rmse: 0.45433 |  0:00:09s
epoch 71 | loss: 0.23973 | val_0_rmse: 0.43183 | val_1_rmse: 0.44851 |  0:00:09s
epoch 72 | loss: 0.20977 | val_0_rmse: 0.4684  | val_1_rmse: 0.49228 |  0:00:09s
epoch 73 | loss: 0.22291 | val_0_rmse: 0.43803 | val_1_rmse: 0.45157 |  0:00:09s
epoch 74 | loss: 0.24604 | val_0_rmse: 0.43624 | val_1_rmse: 0.44956 |  0:00:09s
epoch 75 | loss: 0.21988 | val_0_rmse: 0.46655 | val_1_rmse: 0.48686 |  0:00:10s
epoch 76 | loss: 0.23544 | val_0_rmse: 0.44682 | val_1_rmse: 0.45678 |  0:00:10s
epoch 77 | loss: 0.22765 | val_0_rmse: 0.4357  | val_1_rmse: 0.44664 |  0:00:10s
epoch 78 | loss: 0.21442 | val_0_rmse: 0.46654 | val_1_rmse: 0.49128 |  0:00:10s
epoch 79 | loss: 0.2353  | val_0_rmse: 0.43486 | val_1_rmse: 0.44906 |  0:00:10s
epoch 80 | loss: 0.23174 | val_0_rmse: 0.42839 | val_1_rmse: 0.44135 |  0:00:10s
epoch 81 | loss: 0.20398 | val_0_rmse: 0.48031 | val_1_rmse: 0.49647 |  0:00:10s
epoch 82 | loss: 0.22965 | val_0_rmse: 0.4379  | val_1_rmse: 0.44446 |  0:00:10s
epoch 83 | loss: 0.23912 | val_0_rmse: 0.43794 | val_1_rmse: 0.44105 |  0:00:11s
epoch 84 | loss: 0.20258 | val_0_rmse: 0.45968 | val_1_rmse: 0.47275 |  0:00:11s
epoch 85 | loss: 0.23114 | val_0_rmse: 0.43113 | val_1_rmse: 0.44452 |  0:00:11s
epoch 86 | loss: 0.21947 | val_0_rmse: 0.4271  | val_1_rmse: 0.43696 |  0:00:11s
epoch 87 | loss: 0.21381 | val_0_rmse: 0.46902 | val_1_rmse: 0.48362 |  0:00:11s
epoch 88 | loss: 0.23937 | val_0_rmse: 0.4287  | val_1_rmse: 0.43592 |  0:00:11s
epoch 89 | loss: 0.21219 | val_0_rmse: 0.41793 | val_1_rmse: 0.42692 |  0:00:11s
epoch 90 | loss: 0.20226 | val_0_rmse: 0.46999 | val_1_rmse: 0.48765 |  0:00:12s
epoch 91 | loss: 0.24927 | val_0_rmse: 0.418   | val_1_rmse: 0.4378  |  0:00:12s
epoch 92 | loss: 0.20022 | val_0_rmse: 0.41654 | val_1_rmse: 0.43733 |  0:00:12s
epoch 93 | loss: 0.18846 | val_0_rmse: 0.46132 | val_1_rmse: 0.48274 |  0:00:12s
epoch 94 | loss: 0.25335 | val_0_rmse: 0.42448 | val_1_rmse: 0.43794 |  0:00:12s
epoch 95 | loss: 0.19381 | val_0_rmse: 0.41308 | val_1_rmse: 0.4278  |  0:00:12s
epoch 96 | loss: 0.19623 | val_0_rmse: 0.46932 | val_1_rmse: 0.49544 |  0:00:12s
epoch 97 | loss: 0.25009 | val_0_rmse: 0.41753 | val_1_rmse: 0.43168 |  0:00:12s
epoch 98 | loss: 0.19658 | val_0_rmse: 0.40983 | val_1_rmse: 0.42343 |  0:00:13s
epoch 99 | loss: 0.20476 | val_0_rmse: 0.4866  | val_1_rmse: 0.50153 |  0:00:13s
epoch 100| loss: 0.25388 | val_0_rmse: 0.4103  | val_1_rmse: 0.42136 |  0:00:13s
epoch 101| loss: 0.20511 | val_0_rmse: 0.40663 | val_1_rmse: 0.42516 |  0:00:13s
epoch 102| loss: 0.19901 | val_0_rmse: 0.48515 | val_1_rmse: 0.50395 |  0:00:13s
epoch 103| loss: 0.24318 | val_0_rmse: 0.42052 | val_1_rmse: 0.43419 |  0:00:13s
epoch 104| loss: 0.20509 | val_0_rmse: 0.41103 | val_1_rmse: 0.42577 |  0:00:13s
epoch 105| loss: 0.20226 | val_0_rmse: 0.49817 | val_1_rmse: 0.52135 |  0:00:13s
epoch 106| loss: 0.24311 | val_0_rmse: 0.41074 | val_1_rmse: 0.42338 |  0:00:14s
epoch 107| loss: 0.20319 | val_0_rmse: 0.40616 | val_1_rmse: 0.42052 |  0:00:14s
epoch 108| loss: 0.19631 | val_0_rmse: 0.47778 | val_1_rmse: 0.50086 |  0:00:14s
epoch 109| loss: 0.24168 | val_0_rmse: 0.41641 | val_1_rmse: 0.43235 |  0:00:14s
epoch 110| loss: 0.20728 | val_0_rmse: 0.41302 | val_1_rmse: 0.4254  |  0:00:14s
epoch 111| loss: 0.19629 | val_0_rmse: 0.49891 | val_1_rmse: 0.52171 |  0:00:14s
epoch 112| loss: 0.24923 | val_0_rmse: 0.40849 | val_1_rmse: 0.42893 |  0:00:14s
epoch 113| loss: 0.20284 | val_0_rmse: 0.40774 | val_1_rmse: 0.42168 |  0:00:15s
epoch 114| loss: 0.19051 | val_0_rmse: 0.45971 | val_1_rmse: 0.47551 |  0:00:15s
epoch 115| loss: 0.24214 | val_0_rmse: 0.41836 | val_1_rmse: 0.43866 |  0:00:15s
epoch 116| loss: 0.19453 | val_0_rmse: 0.40362 | val_1_rmse: 0.43146 |  0:00:15s
epoch 117| loss: 0.18424 | val_0_rmse: 0.4774  | val_1_rmse: 0.50892 |  0:00:15s
epoch 118| loss: 0.23679 | val_0_rmse: 0.40385 | val_1_rmse: 0.42575 |  0:00:15s
epoch 119| loss: 0.19991 | val_0_rmse: 0.40518 | val_1_rmse: 0.43012 |  0:00:15s
epoch 120| loss: 0.1997  | val_0_rmse: 0.45068 | val_1_rmse: 0.48786 |  0:00:15s
epoch 121| loss: 0.23839 | val_0_rmse: 0.41736 | val_1_rmse: 0.44885 |  0:00:16s
epoch 122| loss: 0.18888 | val_0_rmse: 0.40708 | val_1_rmse: 0.43224 |  0:00:16s
epoch 123| loss: 0.18955 | val_0_rmse: 0.45691 | val_1_rmse: 0.48671 |  0:00:16s
epoch 124| loss: 0.23609 | val_0_rmse: 0.40658 | val_1_rmse: 0.43817 |  0:00:16s
epoch 125| loss: 0.1881  | val_0_rmse: 0.40192 | val_1_rmse: 0.43278 |  0:00:16s
epoch 126| loss: 0.19362 | val_0_rmse: 0.44964 | val_1_rmse: 0.48076 |  0:00:16s
epoch 127| loss: 0.2275  | val_0_rmse: 0.41472 | val_1_rmse: 0.44162 |  0:00:16s
epoch 128| loss: 0.1981  | val_0_rmse: 0.40289 | val_1_rmse: 0.43698 |  0:00:16s
epoch 129| loss: 0.18209 | val_0_rmse: 0.47654 | val_1_rmse: 0.5164  |  0:00:17s
epoch 130| loss: 0.24588 | val_0_rmse: 0.40978 | val_1_rmse: 0.43516 |  0:00:17s
epoch 131| loss: 0.18073 | val_0_rmse: 0.41304 | val_1_rmse: 0.43784 |  0:00:17s
epoch 132| loss: 0.17528 | val_0_rmse: 0.43284 | val_1_rmse: 0.4551  |  0:00:17s
epoch 133| loss: 0.18617 | val_0_rmse: 0.42918 | val_1_rmse: 0.45338 |  0:00:17s
epoch 134| loss: 0.18599 | val_0_rmse: 0.43268 | val_1_rmse: 0.46383 |  0:00:17s
epoch 135| loss: 0.19151 | val_0_rmse: 0.41664 | val_1_rmse: 0.44214 |  0:00:17s
epoch 136| loss: 0.19536 | val_0_rmse: 0.42084 | val_1_rmse: 0.45819 |  0:00:18s
epoch 137| loss: 0.18083 | val_0_rmse: 0.44675 | val_1_rmse: 0.49217 |  0:00:18s

Early stopping occured at epoch 137 with best_epoch = 107 and best_val_1_rmse = 0.42052
Best weights from best epoch are automatically used!
ended training at: 05:45:45
Feature importance:
[('Area', 0.19171550052034125), ('Baths', 0.11961333489609155), ('Beds', 0.10235292549215226), ('Latitude', 0.04626609143582312), ('Longitude', 0.19856620299247957), ('Month', 0.29910418941435146), ('Year', 0.04238175524876081)]
Mean squared error is of 4478408251.808791
Mean absolute error:47558.20948049451
MAPE:0.3802639672699075
R2 score:0.44155439405889674
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:45:46
epoch 0  | loss: 142.05696| val_0_rmse: 10.25167| val_1_rmse: 10.23951|  0:00:00s
epoch 1  | loss: 107.6173| val_0_rmse: 8.89627 | val_1_rmse: 8.94901 |  0:00:00s
epoch 2  | loss: 79.38727| val_0_rmse: 7.23926 | val_1_rmse: 7.27674 |  0:00:00s
epoch 3  | loss: 47.8045 | val_0_rmse: 5.66181 | val_1_rmse: 5.67224 |  0:00:00s
epoch 4  | loss: 19.9281 | val_0_rmse: 4.89496 | val_1_rmse: 4.66149 |  0:00:00s
epoch 5  | loss: 4.559   | val_0_rmse: 4.05876 | val_1_rmse: 3.79295 |  0:00:00s
epoch 6  | loss: 5.25737 | val_0_rmse: 3.34132 | val_1_rmse: 3.0959  |  0:00:00s
epoch 7  | loss: 7.06044 | val_0_rmse: 2.91675 | val_1_rmse: 2.78676 |  0:00:01s
epoch 8  | loss: 2.8821  | val_0_rmse: 2.62164 | val_1_rmse: 2.6368  |  0:00:01s
epoch 9  | loss: 1.01015 | val_0_rmse: 2.20853 | val_1_rmse: 2.33055 |  0:00:01s
epoch 10 | loss: 0.78041 | val_0_rmse: 2.01783 | val_1_rmse: 2.19032 |  0:00:01s
epoch 11 | loss: 0.52103 | val_0_rmse: 1.82339 | val_1_rmse: 1.77429 |  0:00:01s
epoch 12 | loss: 0.3852  | val_0_rmse: 1.35421 | val_1_rmse: 1.29463 |  0:00:01s
epoch 13 | loss: 0.5267  | val_0_rmse: 1.4624  | val_1_rmse: 1.38258 |  0:00:01s
epoch 14 | loss: 0.44157 | val_0_rmse: 1.23142 | val_1_rmse: 1.1981  |  0:00:02s
epoch 15 | loss: 0.4213  | val_0_rmse: 1.01035 | val_1_rmse: 1.0307  |  0:00:02s
epoch 16 | loss: 0.38077 | val_0_rmse: 1.20789 | val_1_rmse: 1.22221 |  0:00:02s
epoch 17 | loss: 0.43383 | val_0_rmse: 1.09483 | val_1_rmse: 1.09369 |  0:00:02s
epoch 18 | loss: 0.36979 | val_0_rmse: 0.88284 | val_1_rmse: 0.91201 |  0:00:02s
epoch 19 | loss: 0.33419 | val_0_rmse: 0.77956 | val_1_rmse: 0.82922 |  0:00:02s
epoch 20 | loss: 0.28868 | val_0_rmse: 0.7031  | val_1_rmse: 0.72982 |  0:00:02s
epoch 21 | loss: 0.29643 | val_0_rmse: 0.67115 | val_1_rmse: 0.69856 |  0:00:03s
epoch 22 | loss: 0.22273 | val_0_rmse: 0.7026  | val_1_rmse: 0.7311  |  0:00:03s
epoch 23 | loss: 0.25736 | val_0_rmse: 0.60729 | val_1_rmse: 0.6263  |  0:00:03s
epoch 24 | loss: 0.22499 | val_0_rmse: 0.55776 | val_1_rmse: 0.58926 |  0:00:03s
epoch 25 | loss: 0.22766 | val_0_rmse: 0.65925 | val_1_rmse: 0.68931 |  0:00:03s
epoch 26 | loss: 0.22876 | val_0_rmse: 0.60156 | val_1_rmse: 0.60553 |  0:00:03s
epoch 27 | loss: 0.22153 | val_0_rmse: 0.54376 | val_1_rmse: 0.56812 |  0:00:03s
epoch 28 | loss: 0.22462 | val_0_rmse: 0.55166 | val_1_rmse: 0.58665 |  0:00:03s
epoch 29 | loss: 0.20658 | val_0_rmse: 0.61072 | val_1_rmse: 0.62861 |  0:00:04s
epoch 30 | loss: 0.20572 | val_0_rmse: 0.43846 | val_1_rmse: 0.48237 |  0:00:04s
epoch 31 | loss: 0.22027 | val_0_rmse: 0.52933 | val_1_rmse: 0.5834  |  0:00:04s
epoch 32 | loss: 0.22499 | val_0_rmse: 0.50673 | val_1_rmse: 0.53814 |  0:00:04s
epoch 33 | loss: 0.20355 | val_0_rmse: 0.4978  | val_1_rmse: 0.5327  |  0:00:04s
epoch 34 | loss: 0.20066 | val_0_rmse: 0.46529 | val_1_rmse: 0.52374 |  0:00:04s
epoch 35 | loss: 0.22549 | val_0_rmse: 0.48824 | val_1_rmse: 0.53832 |  0:00:04s
epoch 36 | loss: 0.19343 | val_0_rmse: 0.44643 | val_1_rmse: 0.49501 |  0:00:04s
epoch 37 | loss: 0.20021 | val_0_rmse: 0.50095 | val_1_rmse: 0.54397 |  0:00:05s
epoch 38 | loss: 0.20667 | val_0_rmse: 0.48375 | val_1_rmse: 0.52725 |  0:00:05s
epoch 39 | loss: 0.20798 | val_0_rmse: 0.44684 | val_1_rmse: 0.5166  |  0:00:05s
epoch 40 | loss: 0.1774  | val_0_rmse: 0.43178 | val_1_rmse: 0.50647 |  0:00:05s
epoch 41 | loss: 0.18169 | val_0_rmse: 0.42086 | val_1_rmse: 0.47672 |  0:00:05s
epoch 42 | loss: 0.18609 | val_0_rmse: 0.41653 | val_1_rmse: 0.47993 |  0:00:05s
epoch 43 | loss: 0.18743 | val_0_rmse: 0.48766 | val_1_rmse: 0.54452 |  0:00:05s
epoch 44 | loss: 0.23014 | val_0_rmse: 0.44557 | val_1_rmse: 0.50341 |  0:00:05s
epoch 45 | loss: 0.2175  | val_0_rmse: 0.43755 | val_1_rmse: 0.50139 |  0:00:06s
epoch 46 | loss: 0.25834 | val_0_rmse: 0.441   | val_1_rmse: 0.49244 |  0:00:06s
epoch 47 | loss: 0.19258 | val_0_rmse: 0.42402 | val_1_rmse: 0.48166 |  0:00:06s
epoch 48 | loss: 0.18083 | val_0_rmse: 0.42385 | val_1_rmse: 0.49878 |  0:00:06s
epoch 49 | loss: 0.23263 | val_0_rmse: 0.45417 | val_1_rmse: 0.51894 |  0:00:06s
epoch 50 | loss: 0.22671 | val_0_rmse: 0.43876 | val_1_rmse: 0.50106 |  0:00:06s
epoch 51 | loss: 0.18447 | val_0_rmse: 0.43042 | val_1_rmse: 0.49404 |  0:00:06s
epoch 52 | loss: 0.2273  | val_0_rmse: 0.42933 | val_1_rmse: 0.4822  |  0:00:07s
epoch 53 | loss: 0.18956 | val_0_rmse: 0.40652 | val_1_rmse: 0.46515 |  0:00:07s
epoch 54 | loss: 0.16545 | val_0_rmse: 0.41734 | val_1_rmse: 0.49342 |  0:00:07s
epoch 55 | loss: 0.1892  | val_0_rmse: 0.39988 | val_1_rmse: 0.47437 |  0:00:07s
epoch 56 | loss: 0.1639  | val_0_rmse: 0.40217 | val_1_rmse: 0.46306 |  0:00:07s
epoch 57 | loss: 0.1708  | val_0_rmse: 0.39154 | val_1_rmse: 0.45504 |  0:00:07s
epoch 58 | loss: 0.16662 | val_0_rmse: 0.39521 | val_1_rmse: 0.45966 |  0:00:07s
epoch 59 | loss: 0.16217 | val_0_rmse: 0.41188 | val_1_rmse: 0.47252 |  0:00:07s
epoch 60 | loss: 0.16912 | val_0_rmse: 0.41361 | val_1_rmse: 0.47914 |  0:00:08s
epoch 61 | loss: 0.22337 | val_0_rmse: 0.38934 | val_1_rmse: 0.45681 |  0:00:08s
epoch 62 | loss: 0.19185 | val_0_rmse: 0.52803 | val_1_rmse: 0.57164 |  0:00:08s
epoch 63 | loss: 0.34405 | val_0_rmse: 0.40637 | val_1_rmse: 0.47302 |  0:00:08s
epoch 64 | loss: 0.23355 | val_0_rmse: 0.60266 | val_1_rmse: 0.65533 |  0:00:08s
epoch 65 | loss: 0.52821 | val_0_rmse: 0.48913 | val_1_rmse: 0.54587 |  0:00:08s
epoch 66 | loss: 0.26822 | val_0_rmse: 0.45159 | val_1_rmse: 0.4967  |  0:00:08s
epoch 67 | loss: 0.25529 | val_0_rmse: 0.42903 | val_1_rmse: 0.46677 |  0:00:08s
epoch 68 | loss: 0.19108 | val_0_rmse: 0.44658 | val_1_rmse: 0.49838 |  0:00:09s
epoch 69 | loss: 0.23005 | val_0_rmse: 0.39751 | val_1_rmse: 0.45778 |  0:00:09s
epoch 70 | loss: 0.17197 | val_0_rmse: 0.38558 | val_1_rmse: 0.4506  |  0:00:09s
epoch 71 | loss: 0.16049 | val_0_rmse: 0.38866 | val_1_rmse: 0.44978 |  0:00:09s
epoch 72 | loss: 0.16612 | val_0_rmse: 0.39213 | val_1_rmse: 0.453   |  0:00:09s
epoch 73 | loss: 0.17931 | val_0_rmse: 0.43892 | val_1_rmse: 0.50524 |  0:00:09s
epoch 74 | loss: 0.20727 | val_0_rmse: 0.44894 | val_1_rmse: 0.50799 |  0:00:09s
epoch 75 | loss: 0.25002 | val_0_rmse: 0.46425 | val_1_rmse: 0.51737 |  0:00:09s
epoch 76 | loss: 0.22296 | val_0_rmse: 0.45893 | val_1_rmse: 0.51916 |  0:00:10s
epoch 77 | loss: 0.30669 | val_0_rmse: 0.50441 | val_1_rmse: 0.55129 |  0:00:10s
epoch 78 | loss: 0.28784 | val_0_rmse: 0.47527 | val_1_rmse: 0.51662 |  0:00:10s
epoch 79 | loss: 0.30077 | val_0_rmse: 0.51526 | val_1_rmse: 0.54077 |  0:00:10s
epoch 80 | loss: 0.29705 | val_0_rmse: 0.50259 | val_1_rmse: 0.53681 |  0:00:10s
epoch 81 | loss: 0.35054 | val_0_rmse: 0.59483 | val_1_rmse: 0.62375 |  0:00:10s
epoch 82 | loss: 0.40697 | val_0_rmse: 0.42    | val_1_rmse: 0.4664  |  0:00:10s
epoch 83 | loss: 0.19854 | val_0_rmse: 0.43379 | val_1_rmse: 0.46945 |  0:00:11s
epoch 84 | loss: 0.19729 | val_0_rmse: 0.4173  | val_1_rmse: 0.44768 |  0:00:11s
epoch 85 | loss: 0.18911 | val_0_rmse: 0.40807 | val_1_rmse: 0.44594 |  0:00:11s
epoch 86 | loss: 0.17114 | val_0_rmse: 0.40681 | val_1_rmse: 0.45238 |  0:00:11s
epoch 87 | loss: 0.18235 | val_0_rmse: 0.43481 | val_1_rmse: 0.4777  |  0:00:11s
epoch 88 | loss: 0.19047 | val_0_rmse: 0.42035 | val_1_rmse: 0.46374 |  0:00:11s
epoch 89 | loss: 0.1769  | val_0_rmse: 0.42291 | val_1_rmse: 0.46185 |  0:00:11s
epoch 90 | loss: 0.18029 | val_0_rmse: 0.41703 | val_1_rmse: 0.47011 |  0:00:11s
epoch 91 | loss: 0.1844  | val_0_rmse: 0.44945 | val_1_rmse: 0.49049 |  0:00:12s
epoch 92 | loss: 0.18428 | val_0_rmse: 0.39885 | val_1_rmse: 0.43993 |  0:00:12s
epoch 93 | loss: 0.16467 | val_0_rmse: 0.43822 | val_1_rmse: 0.48406 |  0:00:12s
epoch 94 | loss: 0.17436 | val_0_rmse: 0.40211 | val_1_rmse: 0.44887 |  0:00:12s
epoch 95 | loss: 0.19258 | val_0_rmse: 0.40558 | val_1_rmse: 0.44532 |  0:00:12s
epoch 96 | loss: 0.1747  | val_0_rmse: 0.40632 | val_1_rmse: 0.4616  |  0:00:12s
epoch 97 | loss: 0.17106 | val_0_rmse: 0.39156 | val_1_rmse: 0.44546 |  0:00:12s
epoch 98 | loss: 0.16083 | val_0_rmse: 0.4148  | val_1_rmse: 0.46114 |  0:00:13s
epoch 99 | loss: 0.16257 | val_0_rmse: 0.39307 | val_1_rmse: 0.45124 |  0:00:13s
epoch 100| loss: 0.17092 | val_0_rmse: 0.41669 | val_1_rmse: 0.47172 |  0:00:13s
epoch 101| loss: 0.17068 | val_0_rmse: 0.39059 | val_1_rmse: 0.44084 |  0:00:13s
epoch 102| loss: 0.18123 | val_0_rmse: 0.44088 | val_1_rmse: 0.48974 |  0:00:13s
epoch 103| loss: 0.1934  | val_0_rmse: 0.414   | val_1_rmse: 0.47246 |  0:00:13s
epoch 104| loss: 0.18335 | val_0_rmse: 0.43796 | val_1_rmse: 0.48838 |  0:00:13s
epoch 105| loss: 0.18623 | val_0_rmse: 0.39576 | val_1_rmse: 0.44609 |  0:00:13s
epoch 106| loss: 0.17072 | val_0_rmse: 0.4011  | val_1_rmse: 0.45075 |  0:00:14s
epoch 107| loss: 0.17074 | val_0_rmse: 0.39939 | val_1_rmse: 0.45171 |  0:00:14s
epoch 108| loss: 0.16762 | val_0_rmse: 0.39506 | val_1_rmse: 0.45003 |  0:00:14s
epoch 109| loss: 0.16986 | val_0_rmse: 0.39151 | val_1_rmse: 0.44805 |  0:00:14s
epoch 110| loss: 0.16168 | val_0_rmse: 0.39294 | val_1_rmse: 0.44594 |  0:00:14s
epoch 111| loss: 0.15849 | val_0_rmse: 0.38707 | val_1_rmse: 0.44336 |  0:00:14s
epoch 112| loss: 0.16073 | val_0_rmse: 0.38579 | val_1_rmse: 0.44844 |  0:00:14s
epoch 113| loss: 0.16974 | val_0_rmse: 0.42661 | val_1_rmse: 0.47903 |  0:00:14s
epoch 114| loss: 0.19293 | val_0_rmse: 0.4568  | val_1_rmse: 0.50911 |  0:00:15s
epoch 115| loss: 0.28747 | val_0_rmse: 0.47936 | val_1_rmse: 0.52832 |  0:00:15s
epoch 116| loss: 0.25305 | val_0_rmse: 0.45973 | val_1_rmse: 0.50524 |  0:00:15s
epoch 117| loss: 0.31282 | val_0_rmse: 0.55139 | val_1_rmse: 0.59212 |  0:00:15s
epoch 118| loss: 0.35519 | val_0_rmse: 0.48661 | val_1_rmse: 0.54506 |  0:00:15s
epoch 119| loss: 0.30783 | val_0_rmse: 0.561   | val_1_rmse: 0.60709 |  0:00:15s
epoch 120| loss: 0.37266 | val_0_rmse: 0.48231 | val_1_rmse: 0.53557 |  0:00:15s
epoch 121| loss: 0.26897 | val_0_rmse: 0.49643 | val_1_rmse: 0.54128 |  0:00:15s
epoch 122| loss: 0.25833 | val_0_rmse: 0.4353  | val_1_rmse: 0.49384 |  0:00:16s

Early stopping occured at epoch 122 with best_epoch = 92 and best_val_1_rmse = 0.43993
Best weights from best epoch are automatically used!
ended training at: 05:46:02
Feature importance:
[('Area', 0.22031697102391895), ('Baths', 0.21191778219356727), ('Beds', 0.23035267308639745), ('Latitude', 0.04981478323001463), ('Longitude', 0.10803724838532548), ('Month', 0.10284739864017263), ('Year', 0.07671314344060362)]
Mean squared error is of 3753659934.7492347
Mean absolute error:43757.29638193681
MAPE:0.3276614316548024
R2 score:0.5188828245893395
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:46:02
epoch 0  | loss: 142.87709| val_0_rmse: 10.5526 | val_1_rmse: 10.43953|  0:00:00s
epoch 1  | loss: 105.82753| val_0_rmse: 8.75809 | val_1_rmse: 8.7697  |  0:00:00s
epoch 2  | loss: 73.75299| val_0_rmse: 6.49317 | val_1_rmse: 6.38121 |  0:00:00s
epoch 3  | loss: 40.96363| val_0_rmse: 4.55678 | val_1_rmse: 4.34241 |  0:00:00s
epoch 4  | loss: 14.81676| val_0_rmse: 4.61905 | val_1_rmse: 4.19343 |  0:00:00s
epoch 5  | loss: 2.78656 | val_0_rmse: 3.99158 | val_1_rmse: 3.50032 |  0:00:00s
epoch 6  | loss: 8.68297 | val_0_rmse: 2.66637 | val_1_rmse: 2.77553 |  0:00:00s
epoch 7  | loss: 9.44962 | val_0_rmse: 1.95917 | val_1_rmse: 1.97917 |  0:00:01s
epoch 8  | loss: 4.07485 | val_0_rmse: 2.22193 | val_1_rmse: 2.3162  |  0:00:01s
epoch 9  | loss: 1.30856 | val_0_rmse: 2.26418 | val_1_rmse: 2.19252 |  0:00:01s
epoch 10 | loss: 1.46709 | val_0_rmse: 2.84213 | val_1_rmse: 2.78943 |  0:00:01s
epoch 11 | loss: 0.92645 | val_0_rmse: 2.20902 | val_1_rmse: 2.3469  |  0:00:01s
epoch 12 | loss: 0.85833 | val_0_rmse: 1.97966 | val_1_rmse: 1.97946 |  0:00:01s
epoch 13 | loss: 0.54087 | val_0_rmse: 1.94882 | val_1_rmse: 1.87301 |  0:00:01s
epoch 14 | loss: 0.42028 | val_0_rmse: 1.83335 | val_1_rmse: 1.73749 |  0:00:02s
epoch 15 | loss: 0.38407 | val_0_rmse: 1.84899 | val_1_rmse: 1.76725 |  0:00:02s
epoch 16 | loss: 0.41838 | val_0_rmse: 1.62526 | val_1_rmse: 1.55819 |  0:00:02s
epoch 17 | loss: 0.33792 | val_0_rmse: 1.37121 | val_1_rmse: 1.31349 |  0:00:02s
epoch 18 | loss: 0.50621 | val_0_rmse: 1.2695  | val_1_rmse: 1.18555 |  0:00:02s
epoch 19 | loss: 0.41384 | val_0_rmse: 1.38911 | val_1_rmse: 1.3134  |  0:00:02s
epoch 20 | loss: 0.44938 | val_0_rmse: 1.32808 | val_1_rmse: 1.28293 |  0:00:02s
epoch 21 | loss: 0.49425 | val_0_rmse: 0.85354 | val_1_rmse: 0.81704 |  0:00:03s
epoch 22 | loss: 0.42641 | val_0_rmse: 0.917   | val_1_rmse: 0.90119 |  0:00:03s
epoch 23 | loss: 0.49439 | val_0_rmse: 1.14391 | val_1_rmse: 1.1029  |  0:00:03s
epoch 24 | loss: 0.40663 | val_0_rmse: 0.90631 | val_1_rmse: 0.89963 |  0:00:03s
epoch 25 | loss: 0.35435 | val_0_rmse: 0.61306 | val_1_rmse: 0.62383 |  0:00:03s
epoch 26 | loss: 0.29822 | val_0_rmse: 0.64245 | val_1_rmse: 0.63887 |  0:00:03s
epoch 27 | loss: 0.24668 | val_0_rmse: 0.72707 | val_1_rmse: 0.71471 |  0:00:03s
epoch 28 | loss: 0.2478  | val_0_rmse: 0.72817 | val_1_rmse: 0.69841 |  0:00:03s
epoch 29 | loss: 0.24286 | val_0_rmse: 0.56632 | val_1_rmse: 0.57267 |  0:00:04s
epoch 30 | loss: 0.22922 | val_0_rmse: 0.53668 | val_1_rmse: 0.55881 |  0:00:04s
epoch 31 | loss: 0.23419 | val_0_rmse: 0.56485 | val_1_rmse: 0.56245 |  0:00:04s
epoch 32 | loss: 0.23924 | val_0_rmse: 0.54168 | val_1_rmse: 0.55458 |  0:00:04s
epoch 33 | loss: 0.23564 | val_0_rmse: 0.48065 | val_1_rmse: 0.48752 |  0:00:04s
epoch 34 | loss: 0.27363 | val_0_rmse: 0.56277 | val_1_rmse: 0.54128 |  0:00:04s
epoch 35 | loss: 0.24295 | val_0_rmse: 0.51319 | val_1_rmse: 0.51398 |  0:00:04s
epoch 36 | loss: 0.24733 | val_0_rmse: 0.47322 | val_1_rmse: 0.48605 |  0:00:04s
epoch 37 | loss: 0.22682 | val_0_rmse: 0.51196 | val_1_rmse: 0.51333 |  0:00:05s
epoch 38 | loss: 0.23965 | val_0_rmse: 0.46458 | val_1_rmse: 0.48694 |  0:00:05s
epoch 39 | loss: 0.26784 | val_0_rmse: 0.4761  | val_1_rmse: 0.48578 |  0:00:05s
epoch 40 | loss: 0.22131 | val_0_rmse: 0.57494 | val_1_rmse: 0.5653  |  0:00:05s
epoch 41 | loss: 0.23777 | val_0_rmse: 0.49852 | val_1_rmse: 0.49063 |  0:00:05s
epoch 42 | loss: 0.22461 | val_0_rmse: 0.51141 | val_1_rmse: 0.5218  |  0:00:05s
epoch 43 | loss: 0.23181 | val_0_rmse: 0.46405 | val_1_rmse: 0.47531 |  0:00:05s
epoch 44 | loss: 0.24783 | val_0_rmse: 0.45947 | val_1_rmse: 0.46761 |  0:00:06s
epoch 45 | loss: 0.21579 | val_0_rmse: 0.456   | val_1_rmse: 0.4846  |  0:00:06s
epoch 46 | loss: 0.25851 | val_0_rmse: 0.43009 | val_1_rmse: 0.45079 |  0:00:06s
epoch 47 | loss: 0.2123  | val_0_rmse: 0.53197 | val_1_rmse: 0.5413  |  0:00:06s
epoch 48 | loss: 0.28423 | val_0_rmse: 0.45949 | val_1_rmse: 0.47837 |  0:00:06s
epoch 49 | loss: 0.22525 | val_0_rmse: 0.43042 | val_1_rmse: 0.45667 |  0:00:06s
epoch 50 | loss: 0.20113 | val_0_rmse: 0.4455  | val_1_rmse: 0.46369 |  0:00:06s
epoch 51 | loss: 0.20198 | val_0_rmse: 0.42881 | val_1_rmse: 0.44773 |  0:00:06s
epoch 52 | loss: 0.19536 | val_0_rmse: 0.44027 | val_1_rmse: 0.44985 |  0:00:07s
epoch 53 | loss: 0.20626 | val_0_rmse: 0.42678 | val_1_rmse: 0.44656 |  0:00:07s
epoch 54 | loss: 0.18563 | val_0_rmse: 0.43305 | val_1_rmse: 0.45069 |  0:00:07s
epoch 55 | loss: 0.18958 | val_0_rmse: 0.42793 | val_1_rmse: 0.44328 |  0:00:07s
epoch 56 | loss: 0.19729 | val_0_rmse: 0.48907 | val_1_rmse: 0.49594 |  0:00:07s
epoch 57 | loss: 0.25952 | val_0_rmse: 0.4674  | val_1_rmse: 0.49809 |  0:00:07s
epoch 58 | loss: 0.22947 | val_0_rmse: 0.52234 | val_1_rmse: 0.54554 |  0:00:07s
epoch 59 | loss: 0.3155  | val_0_rmse: 0.432   | val_1_rmse: 0.44685 |  0:00:07s
epoch 60 | loss: 0.2228  | val_0_rmse: 0.50527 | val_1_rmse: 0.51146 |  0:00:08s
epoch 61 | loss: 0.27484 | val_0_rmse: 0.43216 | val_1_rmse: 0.4647  |  0:00:08s
epoch 62 | loss: 0.22969 | val_0_rmse: 0.42875 | val_1_rmse: 0.44719 |  0:00:08s
epoch 63 | loss: 0.20626 | val_0_rmse: 0.4775  | val_1_rmse: 0.48545 |  0:00:08s
epoch 64 | loss: 0.21439 | val_0_rmse: 0.47213 | val_1_rmse: 0.48462 |  0:00:08s
epoch 65 | loss: 0.28751 | val_0_rmse: 0.46143 | val_1_rmse: 0.48475 |  0:00:08s
epoch 66 | loss: 0.24932 | val_0_rmse: 0.45399 | val_1_rmse: 0.47929 |  0:00:08s
epoch 67 | loss: 0.2064  | val_0_rmse: 0.43639 | val_1_rmse: 0.48309 |  0:00:09s
epoch 68 | loss: 0.20984 | val_0_rmse: 0.43571 | val_1_rmse: 0.46738 |  0:00:09s
epoch 69 | loss: 0.19897 | val_0_rmse: 0.42421 | val_1_rmse: 0.44439 |  0:00:09s
epoch 70 | loss: 0.18529 | val_0_rmse: 0.41614 | val_1_rmse: 0.43795 |  0:00:09s
epoch 71 | loss: 0.18035 | val_0_rmse: 0.42378 | val_1_rmse: 0.44744 |  0:00:09s
epoch 72 | loss: 0.18655 | val_0_rmse: 0.41779 | val_1_rmse: 0.43629 |  0:00:09s
epoch 73 | loss: 0.17764 | val_0_rmse: 0.4279  | val_1_rmse: 0.45122 |  0:00:09s
epoch 74 | loss: 0.18407 | val_0_rmse: 0.42106 | val_1_rmse: 0.43709 |  0:00:09s
epoch 75 | loss: 0.1932  | val_0_rmse: 0.44295 | val_1_rmse: 0.45693 |  0:00:10s
epoch 76 | loss: 0.21594 | val_0_rmse: 0.41403 | val_1_rmse: 0.43508 |  0:00:10s
epoch 77 | loss: 0.21546 | val_0_rmse: 0.53571 | val_1_rmse: 0.5471  |  0:00:10s
epoch 78 | loss: 0.29487 | val_0_rmse: 0.42289 | val_1_rmse: 0.45098 |  0:00:10s
epoch 79 | loss: 0.19597 | val_0_rmse: 0.44863 | val_1_rmse: 0.45868 |  0:00:10s
epoch 80 | loss: 0.216   | val_0_rmse: 0.43904 | val_1_rmse: 0.45432 |  0:00:10s
epoch 81 | loss: 0.22248 | val_0_rmse: 0.55136 | val_1_rmse: 0.57548 |  0:00:10s
epoch 82 | loss: 0.20349 | val_0_rmse: 0.48866 | val_1_rmse: 0.49399 |  0:00:11s
epoch 83 | loss: 0.19477 | val_0_rmse: 0.58105 | val_1_rmse: 0.58365 |  0:00:11s
epoch 84 | loss: 0.19551 | val_0_rmse: 0.54807 | val_1_rmse: 0.55465 |  0:00:11s
epoch 85 | loss: 0.19273 | val_0_rmse: 0.47342 | val_1_rmse: 0.48014 |  0:00:11s
epoch 86 | loss: 0.20038 | val_0_rmse: 0.54736 | val_1_rmse: 0.56824 |  0:00:11s
epoch 87 | loss: 0.20126 | val_0_rmse: 0.44288 | val_1_rmse: 0.45976 |  0:00:11s
epoch 88 | loss: 0.18879 | val_0_rmse: 0.47044 | val_1_rmse: 0.47552 |  0:00:11s
epoch 89 | loss: 0.20402 | val_0_rmse: 0.42875 | val_1_rmse: 0.44215 |  0:00:11s
epoch 90 | loss: 0.21414 | val_0_rmse: 0.47485 | val_1_rmse: 0.49966 |  0:00:12s
epoch 91 | loss: 0.20212 | val_0_rmse: 0.41387 | val_1_rmse: 0.43982 |  0:00:12s
epoch 92 | loss: 0.1876  | val_0_rmse: 0.42017 | val_1_rmse: 0.44559 |  0:00:12s
epoch 93 | loss: 0.17678 | val_0_rmse: 0.43996 | val_1_rmse: 0.4623  |  0:00:12s
epoch 94 | loss: 0.19243 | val_0_rmse: 0.4516  | val_1_rmse: 0.47559 |  0:00:12s
epoch 95 | loss: 0.25286 | val_0_rmse: 0.43223 | val_1_rmse: 0.4582  |  0:00:12s
epoch 96 | loss: 0.21929 | val_0_rmse: 0.48827 | val_1_rmse: 0.51094 |  0:00:12s
epoch 97 | loss: 0.25075 | val_0_rmse: 0.44879 | val_1_rmse: 0.47092 |  0:00:12s
epoch 98 | loss: 0.20373 | val_0_rmse: 0.44388 | val_1_rmse: 0.47568 |  0:00:13s
epoch 99 | loss: 0.20232 | val_0_rmse: 0.50857 | val_1_rmse: 0.54662 |  0:00:13s
epoch 100| loss: 0.29158 | val_0_rmse: 0.43598 | val_1_rmse: 0.47597 |  0:00:13s
epoch 101| loss: 0.22968 | val_0_rmse: 0.45176 | val_1_rmse: 0.48372 |  0:00:13s
epoch 102| loss: 0.21125 | val_0_rmse: 0.48638 | val_1_rmse: 0.51757 |  0:00:13s
epoch 103| loss: 0.24571 | val_0_rmse: 0.45594 | val_1_rmse: 0.479   |  0:00:13s
epoch 104| loss: 0.23477 | val_0_rmse: 0.44239 | val_1_rmse: 0.47308 |  0:00:13s
epoch 105| loss: 0.218   | val_0_rmse: 0.45764 | val_1_rmse: 0.49206 |  0:00:13s
epoch 106| loss: 0.21863 | val_0_rmse: 0.44035 | val_1_rmse: 0.47302 |  0:00:14s

Early stopping occured at epoch 106 with best_epoch = 76 and best_val_1_rmse = 0.43508
Best weights from best epoch are automatically used!
ended training at: 05:46:16
Feature importance:
[('Area', 0.14114380748762378), ('Baths', 0.2691254899443029), ('Beds', 0.05676818796217224), ('Latitude', 0.12526291259155375), ('Longitude', 0.27460414981446135), ('Month', 0.0738894527868742), ('Year', 0.05920599941301173)]
Mean squared error is of 3371386259.0543633
Mean absolute error:41554.78816998626
MAPE:0.36486850792998765
R2 score:0.5279050571513305
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:46:16
epoch 0  | loss: 142.39597| val_0_rmse: 10.43249| val_1_rmse: 10.24407|  0:00:00s
epoch 1  | loss: 104.16637| val_0_rmse: 8.76822 | val_1_rmse: 8.93645 |  0:00:00s
epoch 2  | loss: 72.71477| val_0_rmse: 6.83763 | val_1_rmse: 6.97442 |  0:00:00s
epoch 3  | loss: 41.35042| val_0_rmse: 6.04566 | val_1_rmse: 5.84979 |  0:00:00s
epoch 4  | loss: 15.73284| val_0_rmse: 5.2027  | val_1_rmse: 5.17707 |  0:00:00s
epoch 5  | loss: 3.88297 | val_0_rmse: 3.44122 | val_1_rmse: 18.20181|  0:00:00s
epoch 6  | loss: 8.01198 | val_0_rmse: 2.3008  | val_1_rmse: 12.87042|  0:00:00s
epoch 7  | loss: 9.52693 | val_0_rmse: 1.51768 | val_1_rmse: 1.6343  |  0:00:01s
epoch 8  | loss: 2.69996 | val_0_rmse: 1.86117 | val_1_rmse: 1.81172 |  0:00:01s
epoch 9  | loss: 1.18651 | val_0_rmse: 2.46016 | val_1_rmse: 2.34039 |  0:00:01s
epoch 10 | loss: 1.1883  | val_0_rmse: 1.60791 | val_1_rmse: 1.6197  |  0:00:01s
epoch 11 | loss: 0.65137 | val_0_rmse: 1.10205 | val_1_rmse: 1.17441 |  0:00:01s
epoch 12 | loss: 0.60379 | val_0_rmse: 1.2291  | val_1_rmse: 1.23886 |  0:00:01s
epoch 13 | loss: 0.39981 | val_0_rmse: 1.13496 | val_1_rmse: 1.16105 |  0:00:01s
epoch 14 | loss: 0.55392 | val_0_rmse: 1.22692 | val_1_rmse: 1.26259 |  0:00:01s
epoch 15 | loss: 0.46009 | val_0_rmse: 1.06722 | val_1_rmse: 1.33287 |  0:00:02s
epoch 16 | loss: 0.47933 | val_0_rmse: 0.83025 | val_1_rmse: 0.85266 |  0:00:02s
epoch 17 | loss: 0.50807 | val_0_rmse: 0.91788 | val_1_rmse: 1.10658 |  0:00:02s
epoch 18 | loss: 0.42659 | val_0_rmse: 0.96409 | val_1_rmse: 1.14936 |  0:00:02s
epoch 19 | loss: 0.41571 | val_0_rmse: 0.84823 | val_1_rmse: 0.95415 |  0:00:02s
epoch 20 | loss: 0.32556 | val_0_rmse: 0.9196  | val_1_rmse: 1.0425  |  0:00:02s
epoch 21 | loss: 0.3505  | val_0_rmse: 0.72009 | val_1_rmse: 0.86333 |  0:00:02s
epoch 22 | loss: 0.35089 | val_0_rmse: 0.89237 | val_1_rmse: 0.93991 |  0:00:03s
epoch 23 | loss: 0.35305 | val_0_rmse: 1.05864 | val_1_rmse: 1.08928 |  0:00:03s
epoch 24 | loss: 0.47969 | val_0_rmse: 0.78042 | val_1_rmse: 0.90044 |  0:00:03s
epoch 25 | loss: 0.32974 | val_0_rmse: 0.70984 | val_1_rmse: 0.71415 |  0:00:03s
epoch 26 | loss: 0.38357 | val_0_rmse: 0.74922 | val_1_rmse: 0.79143 |  0:00:03s
epoch 27 | loss: 0.30884 | val_0_rmse: 0.65741 | val_1_rmse: 0.84628 |  0:00:03s
epoch 28 | loss: 0.3425  | val_0_rmse: 0.72863 | val_1_rmse: 0.87427 |  0:00:03s
epoch 29 | loss: 0.30419 | val_0_rmse: 0.69832 | val_1_rmse: 0.75713 |  0:00:03s
epoch 30 | loss: 0.28634 | val_0_rmse: 0.55394 | val_1_rmse: 0.63139 |  0:00:04s
epoch 31 | loss: 0.28325 | val_0_rmse: 0.72909 | val_1_rmse: 0.79451 |  0:00:04s
epoch 32 | loss: 0.30807 | val_0_rmse: 0.6294  | val_1_rmse: 0.74697 |  0:00:04s
epoch 33 | loss: 0.28895 | val_0_rmse: 0.65489 | val_1_rmse: 0.65205 |  0:00:04s
epoch 34 | loss: 0.26254 | val_0_rmse: 0.59638 | val_1_rmse: 0.6038  |  0:00:04s
epoch 35 | loss: 0.25487 | val_0_rmse: 0.66774 | val_1_rmse: 0.72009 |  0:00:04s
epoch 36 | loss: 0.2661  | val_0_rmse: 0.56572 | val_1_rmse: 0.65338 |  0:00:04s
epoch 37 | loss: 0.2369  | val_0_rmse: 0.56708 | val_1_rmse: 0.63537 |  0:00:05s
epoch 38 | loss: 0.25605 | val_0_rmse: 0.57018 | val_1_rmse: 0.68225 |  0:00:05s
epoch 39 | loss: 0.23851 | val_0_rmse: 0.5337  | val_1_rmse: 0.7225  |  0:00:05s
epoch 40 | loss: 0.25306 | val_0_rmse: 0.55634 | val_1_rmse: 0.68057 |  0:00:05s
epoch 41 | loss: 0.22495 | val_0_rmse: 0.48088 | val_1_rmse: 0.61839 |  0:00:05s
epoch 42 | loss: 0.2234  | val_0_rmse: 0.63126 | val_1_rmse: 0.74469 |  0:00:05s
epoch 43 | loss: 0.28233 | val_0_rmse: 0.57851 | val_1_rmse: 0.77911 |  0:00:05s
epoch 44 | loss: 0.27687 | val_0_rmse: 0.49774 | val_1_rmse: 0.69763 |  0:00:05s
epoch 45 | loss: 0.23293 | val_0_rmse: 0.53464 | val_1_rmse: 0.66313 |  0:00:06s
epoch 46 | loss: 0.24775 | val_0_rmse: 0.47488 | val_1_rmse: 0.63134 |  0:00:06s
epoch 47 | loss: 0.22543 | val_0_rmse: 0.50875 | val_1_rmse: 0.68305 |  0:00:06s
epoch 48 | loss: 0.23863 | val_0_rmse: 0.47948 | val_1_rmse: 0.64441 |  0:00:06s
epoch 49 | loss: 0.24199 | val_0_rmse: 0.55492 | val_1_rmse: 0.65501 |  0:00:06s
epoch 50 | loss: 0.26275 | val_0_rmse: 0.4624  | val_1_rmse: 0.61884 |  0:00:06s
epoch 51 | loss: 0.29239 | val_0_rmse: 0.47142 | val_1_rmse: 0.62861 |  0:00:06s
epoch 52 | loss: 0.24832 | val_0_rmse: 0.49445 | val_1_rmse: 0.61966 |  0:00:06s
epoch 53 | loss: 0.23331 | val_0_rmse: 0.44434 | val_1_rmse: 0.60664 |  0:00:07s
epoch 54 | loss: 0.20253 | val_0_rmse: 0.48462 | val_1_rmse: 0.61272 |  0:00:07s
epoch 55 | loss: 0.21859 | val_0_rmse: 0.43848 | val_1_rmse: 0.58067 |  0:00:07s
epoch 56 | loss: 0.21186 | val_0_rmse: 0.50802 | val_1_rmse: 0.62719 |  0:00:07s
epoch 57 | loss: 0.26404 | val_0_rmse: 0.44978 | val_1_rmse: 0.61578 |  0:00:07s
epoch 58 | loss: 0.23272 | val_0_rmse: 0.55383 | val_1_rmse: 0.73604 |  0:00:07s
epoch 59 | loss: 0.36349 | val_0_rmse: 0.45839 | val_1_rmse: 0.64755 |  0:00:07s
epoch 60 | loss: 0.2571  | val_0_rmse: 0.59676 | val_1_rmse: 0.69483 |  0:00:07s
epoch 61 | loss: 0.45967 | val_0_rmse: 0.51362 | val_1_rmse: 0.65757 |  0:00:08s
epoch 62 | loss: 0.27429 | val_0_rmse: 0.50134 | val_1_rmse: 0.71342 |  0:00:08s
epoch 63 | loss: 0.27532 | val_0_rmse: 0.49787 | val_1_rmse: 0.70885 |  0:00:08s
epoch 64 | loss: 0.24943 | val_0_rmse: 0.47439 | val_1_rmse: 0.63583 |  0:00:08s
epoch 65 | loss: 0.27953 | val_0_rmse: 0.46444 | val_1_rmse: 0.67202 |  0:00:08s
epoch 66 | loss: 0.22784 | val_0_rmse: 0.47106 | val_1_rmse: 0.66578 |  0:00:08s
epoch 67 | loss: 0.22931 | val_0_rmse: 0.42964 | val_1_rmse: 0.60392 |  0:00:08s
epoch 68 | loss: 0.20192 | val_0_rmse: 0.43364 | val_1_rmse: 0.60264 |  0:00:09s
epoch 69 | loss: 0.20515 | val_0_rmse: 0.46113 | val_1_rmse: 0.59591 |  0:00:09s
epoch 70 | loss: 0.21049 | val_0_rmse: 0.46268 | val_1_rmse: 0.63263 |  0:00:09s
epoch 71 | loss: 0.23387 | val_0_rmse: 0.44457 | val_1_rmse: 0.59879 |  0:00:09s
epoch 72 | loss: 0.20187 | val_0_rmse: 0.43936 | val_1_rmse: 0.60316 |  0:00:09s
epoch 73 | loss: 0.19769 | val_0_rmse: 0.45225 | val_1_rmse: 0.62399 |  0:00:09s
epoch 74 | loss: 0.21169 | val_0_rmse: 0.44901 | val_1_rmse: 0.58899 |  0:00:09s
epoch 75 | loss: 0.21674 | val_0_rmse: 0.48447 | val_1_rmse: 0.65387 |  0:00:09s
epoch 76 | loss: 0.23082 | val_0_rmse: 0.43586 | val_1_rmse: 0.60408 |  0:00:10s
epoch 77 | loss: 0.21331 | val_0_rmse: 0.45181 | val_1_rmse: 0.6082  |  0:00:10s
epoch 78 | loss: 0.2042  | val_0_rmse: 0.44713 | val_1_rmse: 0.5598  |  0:00:10s
epoch 79 | loss: 0.22391 | val_0_rmse: 0.43791 | val_1_rmse: 0.56594 |  0:00:10s
epoch 80 | loss: 0.20295 | val_0_rmse: 0.43751 | val_1_rmse: 0.55814 |  0:00:10s
epoch 81 | loss: 0.20553 | val_0_rmse: 0.44235 | val_1_rmse: 0.52872 |  0:00:10s
epoch 82 | loss: 0.20571 | val_0_rmse: 0.45984 | val_1_rmse: 0.57284 |  0:00:10s
epoch 83 | loss: 0.21233 | val_0_rmse: 0.44215 | val_1_rmse: 0.53724 |  0:00:11s
epoch 84 | loss: 0.19846 | val_0_rmse: 0.42653 | val_1_rmse: 0.54604 |  0:00:11s
epoch 85 | loss: 0.19128 | val_0_rmse: 0.42106 | val_1_rmse: 0.53624 |  0:00:11s
epoch 86 | loss: 0.18561 | val_0_rmse: 0.43967 | val_1_rmse: 0.55226 |  0:00:11s
epoch 87 | loss: 0.18824 | val_0_rmse: 0.41756 | val_1_rmse: 0.52684 |  0:00:11s
epoch 88 | loss: 0.19241 | val_0_rmse: 0.46389 | val_1_rmse: 0.59654 |  0:00:11s
epoch 89 | loss: 0.19743 | val_0_rmse: 0.42713 | val_1_rmse: 0.54627 |  0:00:11s
epoch 90 | loss: 0.1931  | val_0_rmse: 0.45253 | val_1_rmse: 0.56797 |  0:00:11s
epoch 91 | loss: 0.20129 | val_0_rmse: 0.42288 | val_1_rmse: 0.5215  |  0:00:12s
epoch 92 | loss: 0.18861 | val_0_rmse: 0.42787 | val_1_rmse: 0.54767 |  0:00:12s
epoch 93 | loss: 0.19116 | val_0_rmse: 0.4236  | val_1_rmse: 0.51789 |  0:00:12s
epoch 94 | loss: 0.1857  | val_0_rmse: 0.43906 | val_1_rmse: 0.54193 |  0:00:12s
epoch 95 | loss: 0.20988 | val_0_rmse: 0.43664 | val_1_rmse: 0.50811 |  0:00:12s
epoch 96 | loss: 0.20419 | val_0_rmse: 0.45159 | val_1_rmse: 0.53957 |  0:00:12s
epoch 97 | loss: 0.20267 | val_0_rmse: 0.45312 | val_1_rmse: 0.53881 |  0:00:12s
epoch 98 | loss: 0.21088 | val_0_rmse: 0.43624 | val_1_rmse: 0.52007 |  0:00:12s
epoch 99 | loss: 0.19381 | val_0_rmse: 0.47372 | val_1_rmse: 0.56196 |  0:00:13s
epoch 100| loss: 0.20108 | val_0_rmse: 0.44484 | val_1_rmse: 0.53452 |  0:00:13s
epoch 101| loss: 0.18866 | val_0_rmse: 0.42929 | val_1_rmse: 0.49889 |  0:00:13s
epoch 102| loss: 0.18925 | val_0_rmse: 0.44496 | val_1_rmse: 0.51387 |  0:00:13s
epoch 103| loss: 0.19217 | val_0_rmse: 0.42478 | val_1_rmse: 0.49431 |  0:00:13s
epoch 104| loss: 0.18371 | val_0_rmse: 0.44945 | val_1_rmse: 0.52704 |  0:00:13s
epoch 105| loss: 0.20425 | val_0_rmse: 0.448   | val_1_rmse: 0.5011  |  0:00:13s
epoch 106| loss: 0.22948 | val_0_rmse: 0.45129 | val_1_rmse: 0.51374 |  0:00:14s
epoch 107| loss: 0.21903 | val_0_rmse: 0.49184 | val_1_rmse: 0.57744 |  0:00:14s
epoch 108| loss: 0.27802 | val_0_rmse: 0.43637 | val_1_rmse: 0.5008  |  0:00:14s
epoch 109| loss: 0.19816 | val_0_rmse: 0.44477 | val_1_rmse: 0.5108  |  0:00:14s
epoch 110| loss: 0.20173 | val_0_rmse: 0.44091 | val_1_rmse: 0.51251 |  0:00:14s
epoch 111| loss: 0.20837 | val_0_rmse: 0.46425 | val_1_rmse: 0.50668 |  0:00:14s
epoch 112| loss: 0.21141 | val_0_rmse: 0.43307 | val_1_rmse: 0.50108 |  0:00:14s
epoch 113| loss: 0.20052 | val_0_rmse: 0.42942 | val_1_rmse: 0.48826 |  0:00:14s
epoch 114| loss: 0.18874 | val_0_rmse: 0.41749 | val_1_rmse: 0.47548 |  0:00:15s
epoch 115| loss: 0.18291 | val_0_rmse: 0.42166 | val_1_rmse: 0.45889 |  0:00:15s
epoch 116| loss: 0.19251 | val_0_rmse: 0.41387 | val_1_rmse: 0.45159 |  0:00:15s
epoch 117| loss: 0.1822  | val_0_rmse: 0.41211 | val_1_rmse: 0.45494 |  0:00:15s
epoch 118| loss: 0.17642 | val_0_rmse: 0.41556 | val_1_rmse: 0.50131 |  0:00:15s
epoch 119| loss: 0.17582 | val_0_rmse: 0.4422  | val_1_rmse: 0.53114 |  0:00:15s
epoch 120| loss: 0.21178 | val_0_rmse: 0.4415  | val_1_rmse: 0.51508 |  0:00:15s
epoch 121| loss: 0.19511 | val_0_rmse: 0.44066 | val_1_rmse: 0.53335 |  0:00:15s
epoch 122| loss: 0.21307 | val_0_rmse: 0.43414 | val_1_rmse: 0.49752 |  0:00:16s
epoch 123| loss: 0.21236 | val_0_rmse: 0.47741 | val_1_rmse: 0.57307 |  0:00:16s
epoch 124| loss: 0.21438 | val_0_rmse: 0.4723  | val_1_rmse: 0.53831 |  0:00:16s
epoch 125| loss: 0.22093 | val_0_rmse: 0.44199 | val_1_rmse: 0.50073 |  0:00:16s
epoch 126| loss: 0.20959 | val_0_rmse: 0.45349 | val_1_rmse: 0.501   |  0:00:16s
epoch 127| loss: 0.2075  | val_0_rmse: 0.42332 | val_1_rmse: 0.47936 |  0:00:16s
epoch 128| loss: 0.20339 | val_0_rmse: 0.44583 | val_1_rmse: 0.50277 |  0:00:16s
epoch 129| loss: 0.22009 | val_0_rmse: 0.46999 | val_1_rmse: 0.51234 |  0:00:16s
epoch 130| loss: 0.24108 | val_0_rmse: 0.46672 | val_1_rmse: 0.51112 |  0:00:17s
epoch 131| loss: 0.2116  | val_0_rmse: 0.46518 | val_1_rmse: 0.53138 |  0:00:17s
epoch 132| loss: 0.20502 | val_0_rmse: 0.46972 | val_1_rmse: 0.52367 |  0:00:17s
epoch 133| loss: 0.23499 | val_0_rmse: 0.44818 | val_1_rmse: 0.53565 |  0:00:17s
epoch 134| loss: 0.23554 | val_0_rmse: 0.49446 | val_1_rmse: 0.57187 |  0:00:17s
epoch 135| loss: 0.22736 | val_0_rmse: 0.44402 | val_1_rmse: 0.53312 |  0:00:17s
epoch 136| loss: 0.20552 | val_0_rmse: 0.44347 | val_1_rmse: 0.53207 |  0:00:17s
epoch 137| loss: 0.20309 | val_0_rmse: 0.45721 | val_1_rmse: 0.55575 |  0:00:18s
epoch 138| loss: 0.21119 | val_0_rmse: 0.42973 | val_1_rmse: 0.52689 |  0:00:18s
epoch 139| loss: 0.19374 | val_0_rmse: 0.43295 | val_1_rmse: 0.53687 |  0:00:18s
epoch 140| loss: 0.18933 | val_0_rmse: 0.44184 | val_1_rmse: 0.53063 |  0:00:18s
epoch 141| loss: 0.20195 | val_0_rmse: 0.44131 | val_1_rmse: 0.58673 |  0:00:18s
epoch 142| loss: 0.20025 | val_0_rmse: 0.43497 | val_1_rmse: 0.58237 |  0:00:18s
epoch 143| loss: 0.19228 | val_0_rmse: 0.48795 | val_1_rmse: 0.64145 |  0:00:18s
epoch 144| loss: 0.21092 | val_0_rmse: 0.42592 | val_1_rmse: 0.58138 |  0:00:18s
epoch 145| loss: 0.21417 | val_0_rmse: 0.45868 | val_1_rmse: 0.63329 |  0:00:19s
epoch 146| loss: 0.21684 | val_0_rmse: 0.44218 | val_1_rmse: 0.60401 |  0:00:19s

Early stopping occured at epoch 146 with best_epoch = 116 and best_val_1_rmse = 0.45159
Best weights from best epoch are automatically used!
ended training at: 05:46:35
Feature importance:
[('Area', 0.030774557279981293), ('Baths', 0.15148142200389453), ('Beds', 0.13562790206306768), ('Latitude', 0.1271041984062683), ('Longitude', 0.21768477979116507), ('Month', 0.31718242924073203), ('Year', 0.020144711214891087)]
Mean squared error is of 3454773815.47155
Mean absolute error:42038.180379464284
MAPE:0.36588647787692247
R2 score:0.4858633054614345
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:46:35
epoch 0  | loss: 142.19682| val_0_rmse: 10.47493| val_1_rmse: 10.48443|  0:00:00s
epoch 1  | loss: 104.14254| val_0_rmse: 9.07332 | val_1_rmse: 9.17082 |  0:00:00s
epoch 2  | loss: 75.99311| val_0_rmse: 10.8759 | val_1_rmse: 7.23726 |  0:00:00s
epoch 3  | loss: 46.47292| val_0_rmse: 13.48781| val_1_rmse: 5.49854 |  0:00:00s
epoch 4  | loss: 22.77327| val_0_rmse: 4.24082 | val_1_rmse: 3.97361 |  0:00:00s
epoch 5  | loss: 6.24402 | val_0_rmse: 3.86541 | val_1_rmse: 3.63737 |  0:00:00s
epoch 6  | loss: 6.48698 | val_0_rmse: 3.12866 | val_1_rmse: 3.14331 |  0:00:00s
epoch 7  | loss: 10.26725| val_0_rmse: 2.47369 | val_1_rmse: 2.56904 |  0:00:01s
epoch 8  | loss: 7.57965 | val_0_rmse: 2.43175 | val_1_rmse: 2.61804 |  0:00:01s
epoch 9  | loss: 2.29101 | val_0_rmse: 2.61116 | val_1_rmse: 2.80088 |  0:00:01s
epoch 10 | loss: 1.68869 | val_0_rmse: 2.13591 | val_1_rmse: 2.12598 |  0:00:01s
epoch 11 | loss: 2.2875  | val_0_rmse: 1.70795 | val_1_rmse: 1.70291 |  0:00:01s
epoch 12 | loss: 1.1747  | val_0_rmse: 1.55445 | val_1_rmse: 1.59378 |  0:00:01s
epoch 13 | loss: 1.138   | val_0_rmse: 1.3121  | val_1_rmse: 1.29958 |  0:00:01s
epoch 14 | loss: 0.78225 | val_0_rmse: 1.25769 | val_1_rmse: 1.27352 |  0:00:02s
epoch 15 | loss: 0.62153 | val_0_rmse: 1.17455 | val_1_rmse: 1.21573 |  0:00:02s
epoch 16 | loss: 0.63155 | val_0_rmse: 2.13674 | val_1_rmse: 0.92408 |  0:00:02s
epoch 17 | loss: 0.46765 | val_0_rmse: 1.17671 | val_1_rmse: 1.11679 |  0:00:02s
epoch 18 | loss: 0.48368 | val_0_rmse: 0.93785 | val_1_rmse: 0.95053 |  0:00:02s
epoch 19 | loss: 0.36277 | val_0_rmse: 0.84312 | val_1_rmse: 0.86768 |  0:00:02s
epoch 20 | loss: 0.38106 | val_0_rmse: 1.29206 | val_1_rmse: 0.86334 |  0:00:02s
epoch 21 | loss: 0.33512 | val_0_rmse: 1.57187 | val_1_rmse: 1.13879 |  0:00:02s
epoch 22 | loss: 0.3644  | val_0_rmse: 1.41571 | val_1_rmse: 0.74938 |  0:00:03s
epoch 23 | loss: 0.38475 | val_0_rmse: 2.18321 | val_1_rmse: 0.67992 |  0:00:03s
epoch 24 | loss: 0.35817 | val_0_rmse: 0.90331 | val_1_rmse: 0.87061 |  0:00:03s
epoch 25 | loss: 0.33759 | val_0_rmse: 0.67847 | val_1_rmse: 0.66712 |  0:00:03s
epoch 26 | loss: 0.30098 | val_0_rmse: 0.7477  | val_1_rmse: 0.75792 |  0:00:03s
epoch 27 | loss: 0.31045 | val_0_rmse: 0.87724 | val_1_rmse: 0.8919  |  0:00:03s
epoch 28 | loss: 0.35614 | val_0_rmse: 0.60092 | val_1_rmse: 0.60329 |  0:00:03s
epoch 29 | loss: 0.263   | val_0_rmse: 0.58372 | val_1_rmse: 0.59872 |  0:00:04s
epoch 30 | loss: 0.26155 | val_0_rmse: 0.69554 | val_1_rmse: 0.73536 |  0:00:04s
epoch 31 | loss: 0.29539 | val_0_rmse: 0.53623 | val_1_rmse: 0.56098 |  0:00:04s
epoch 32 | loss: 0.23871 | val_0_rmse: 0.56155 | val_1_rmse: 0.54485 |  0:00:04s
epoch 33 | loss: 0.24874 | val_0_rmse: 0.63373 | val_1_rmse: 0.59073 |  0:00:04s
epoch 34 | loss: 0.29114 | val_0_rmse: 0.67218 | val_1_rmse: 0.56966 |  0:00:04s
epoch 35 | loss: 0.29267 | val_0_rmse: 0.64317 | val_1_rmse: 0.55503 |  0:00:04s
epoch 36 | loss: 0.27994 | val_0_rmse: 0.64933 | val_1_rmse: 0.65129 |  0:00:04s
epoch 37 | loss: 0.30848 | val_0_rmse: 0.65153 | val_1_rmse: 0.68194 |  0:00:05s
epoch 38 | loss: 0.33647 | val_0_rmse: 0.46868 | val_1_rmse: 0.49335 |  0:00:05s
epoch 39 | loss: 0.23437 | val_0_rmse: 0.5706  | val_1_rmse: 0.60893 |  0:00:05s
epoch 40 | loss: 0.27653 | val_0_rmse: 0.48988 | val_1_rmse: 0.51889 |  0:00:05s
epoch 41 | loss: 0.29867 | val_0_rmse: 0.6288  | val_1_rmse: 0.66428 |  0:00:05s
epoch 42 | loss: 0.35923 | val_0_rmse: 0.54592 | val_1_rmse: 0.59538 |  0:00:05s
epoch 43 | loss: 0.23329 | val_0_rmse: 0.5034  | val_1_rmse: 0.51954 |  0:00:05s
epoch 44 | loss: 0.39273 | val_0_rmse: 0.61197 | val_1_rmse: 0.61638 |  0:00:05s
epoch 45 | loss: 0.45025 | val_0_rmse: 0.63775 | val_1_rmse: 0.65559 |  0:00:06s
epoch 46 | loss: 0.36635 | val_0_rmse: 0.65738 | val_1_rmse: 0.64719 |  0:00:06s
epoch 47 | loss: 0.3034  | val_0_rmse: 0.5613  | val_1_rmse: 0.47279 |  0:00:06s
epoch 48 | loss: 0.33851 | val_0_rmse: 0.64588 | val_1_rmse: 0.54462 |  0:00:06s
epoch 49 | loss: 0.38816 | val_0_rmse: 0.60655 | val_1_rmse: 0.57675 |  0:00:06s
epoch 50 | loss: 0.30235 | val_0_rmse: 0.60353 | val_1_rmse: 0.62153 |  0:00:06s
epoch 51 | loss: 0.31497 | val_0_rmse: 0.47604 | val_1_rmse: 0.47386 |  0:00:06s
epoch 52 | loss: 0.26463 | val_0_rmse: 0.50079 | val_1_rmse: 0.49721 |  0:00:07s
epoch 53 | loss: 0.27404 | val_0_rmse: 0.51874 | val_1_rmse: 0.55472 |  0:00:07s
epoch 54 | loss: 0.31624 | val_0_rmse: 0.57802 | val_1_rmse: 0.60586 |  0:00:07s
epoch 55 | loss: 0.28672 | val_0_rmse: 0.53574 | val_1_rmse: 0.51337 |  0:00:07s
epoch 56 | loss: 0.41306 | val_0_rmse: 0.64196 | val_1_rmse: 0.62326 |  0:00:07s
epoch 57 | loss: 0.48252 | val_0_rmse: 0.50267 | val_1_rmse: 0.53281 |  0:00:07s
epoch 58 | loss: 0.24783 | val_0_rmse: 0.51147 | val_1_rmse: 0.5351  |  0:00:07s
epoch 59 | loss: 0.24513 | val_0_rmse: 0.50111 | val_1_rmse: 0.50188 |  0:00:07s
epoch 60 | loss: 0.32473 | val_0_rmse: 0.52181 | val_1_rmse: 0.53802 |  0:00:08s
epoch 61 | loss: 0.31685 | val_0_rmse: 0.50563 | val_1_rmse: 0.5658  |  0:00:08s
epoch 62 | loss: 0.337   | val_0_rmse: 0.91684 | val_1_rmse: 0.60549 |  0:00:08s
epoch 63 | loss: 0.35147 | val_0_rmse: 0.47201 | val_1_rmse: 0.48662 |  0:00:08s
epoch 64 | loss: 0.2391  | val_0_rmse: 0.52545 | val_1_rmse: 0.53009 |  0:00:08s
epoch 65 | loss: 0.23954 | val_0_rmse: 0.47948 | val_1_rmse: 0.53192 |  0:00:08s
epoch 66 | loss: 0.29836 | val_0_rmse: 0.50582 | val_1_rmse: 0.56586 |  0:00:08s
epoch 67 | loss: 0.26379 | val_0_rmse: 0.49974 | val_1_rmse: 0.49723 |  0:00:08s
epoch 68 | loss: 0.30532 | val_0_rmse: 0.55651 | val_1_rmse: 0.53169 |  0:00:09s
epoch 69 | loss: 0.27513 | val_0_rmse: 0.49989 | val_1_rmse: 0.53293 |  0:00:09s
epoch 70 | loss: 0.33683 | val_0_rmse: 0.57926 | val_1_rmse: 0.62145 |  0:00:09s
epoch 71 | loss: 0.36017 | val_0_rmse: 0.47972 | val_1_rmse: 0.47129 |  0:00:09s
epoch 72 | loss: 0.21804 | val_0_rmse: 0.51528 | val_1_rmse: 0.48337 |  0:00:09s
epoch 73 | loss: 0.22224 | val_0_rmse: 0.48269 | val_1_rmse: 0.50226 |  0:00:09s
epoch 74 | loss: 0.22856 | val_0_rmse: 0.46502 | val_1_rmse: 0.4684  |  0:00:09s
epoch 75 | loss: 0.21056 | val_0_rmse: 0.50073 | val_1_rmse: 0.48379 |  0:00:10s
epoch 76 | loss: 0.22288 | val_0_rmse: 0.51844 | val_1_rmse: 0.55591 |  0:00:10s
epoch 77 | loss: 0.30806 | val_0_rmse: 0.54817 | val_1_rmse: 0.57919 |  0:00:10s
epoch 78 | loss: 0.28214 | val_0_rmse: 0.53984 | val_1_rmse: 0.51982 |  0:00:10s
epoch 79 | loss: 0.34512 | val_0_rmse: 0.62966 | val_1_rmse: 0.5852  |  0:00:10s
epoch 80 | loss: 0.38628 | val_0_rmse: 0.46547 | val_1_rmse: 0.48208 |  0:00:10s
epoch 81 | loss: 0.22026 | val_0_rmse: 0.46281 | val_1_rmse: 0.49115 |  0:00:10s
epoch 82 | loss: 0.20429 | val_0_rmse: 0.494   | val_1_rmse: 0.48349 |  0:00:10s
epoch 83 | loss: 0.23976 | val_0_rmse: 0.4383  | val_1_rmse: 0.45567 |  0:00:11s
epoch 84 | loss: 0.19142 | val_0_rmse: 0.44854 | val_1_rmse: 0.4878  |  0:00:11s
epoch 85 | loss: 0.2024  | val_0_rmse: 0.44178 | val_1_rmse: 0.45852 |  0:00:11s
epoch 86 | loss: 0.18815 | val_0_rmse: 0.45043 | val_1_rmse: 0.4964  |  0:00:11s
epoch 87 | loss: 0.21068 | val_0_rmse: 0.44078 | val_1_rmse: 0.45433 |  0:00:11s
epoch 88 | loss: 0.18626 | val_0_rmse: 0.44477 | val_1_rmse: 0.45402 |  0:00:11s
epoch 89 | loss: 0.17655 | val_0_rmse: 0.44209 | val_1_rmse: 0.45469 |  0:00:11s
epoch 90 | loss: 0.1762  | val_0_rmse: 0.43386 | val_1_rmse: 0.45698 |  0:00:11s
epoch 91 | loss: 0.18626 | val_0_rmse: 0.44889 | val_1_rmse: 0.45224 |  0:00:12s
epoch 92 | loss: 0.1893  | val_0_rmse: 0.43568 | val_1_rmse: 0.45559 |  0:00:12s
epoch 93 | loss: 0.17758 | val_0_rmse: 0.45057 | val_1_rmse: 0.48736 |  0:00:12s
epoch 94 | loss: 0.18622 | val_0_rmse: 0.47227 | val_1_rmse: 0.4643  |  0:00:12s
epoch 95 | loss: 0.23341 | val_0_rmse: 0.48435 | val_1_rmse: 0.47312 |  0:00:12s
epoch 96 | loss: 0.21881 | val_0_rmse: 0.51589 | val_1_rmse: 0.55147 |  0:00:12s
epoch 97 | loss: 0.26336 | val_0_rmse: 0.53884 | val_1_rmse: 0.54961 |  0:00:12s
epoch 98 | loss: 0.25872 | val_0_rmse: 0.47733 | val_1_rmse: 0.47109 |  0:00:12s
epoch 99 | loss: 0.20249 | val_0_rmse: 0.58806 | val_1_rmse: 0.6015  |  0:00:13s
epoch 100| loss: 0.33905 | val_0_rmse: 0.50742 | val_1_rmse: 0.48897 |  0:00:13s
epoch 101| loss: 0.25253 | val_0_rmse: 0.49281 | val_1_rmse: 0.4919  |  0:00:13s
epoch 102| loss: 0.24549 | val_0_rmse: 0.53115 | val_1_rmse: 0.56219 |  0:00:13s
epoch 103| loss: 0.23065 | val_0_rmse: 0.47863 | val_1_rmse: 0.4609  |  0:00:13s
epoch 104| loss: 0.21927 | val_0_rmse: 0.47977 | val_1_rmse: 0.47422 |  0:00:13s
epoch 105| loss: 0.20982 | val_0_rmse: 0.46762 | val_1_rmse: 0.50295 |  0:00:13s
epoch 106| loss: 0.21154 | val_0_rmse: 0.48703 | val_1_rmse: 0.50049 |  0:00:14s
epoch 107| loss: 0.25103 | val_0_rmse: 0.45786 | val_1_rmse: 0.47015 |  0:00:14s
epoch 108| loss: 0.18771 | val_0_rmse: 0.49439 | val_1_rmse: 0.46002 |  0:00:14s
epoch 109| loss: 0.21376 | val_0_rmse: 0.46343 | val_1_rmse: 0.46789 |  0:00:14s
epoch 110| loss: 0.18751 | val_0_rmse: 0.46302 | val_1_rmse: 0.46651 |  0:00:14s
epoch 111| loss: 0.21284 | val_0_rmse: 0.49127 | val_1_rmse: 0.51517 |  0:00:14s
epoch 112| loss: 0.20822 | val_0_rmse: 0.45841 | val_1_rmse: 0.43792 |  0:00:14s
epoch 113| loss: 0.18212 | val_0_rmse: 0.47402 | val_1_rmse: 0.48432 |  0:00:14s
epoch 114| loss: 0.19859 | val_0_rmse: 0.44305 | val_1_rmse: 0.4472  |  0:00:15s
epoch 115| loss: 0.18426 | val_0_rmse: 0.43851 | val_1_rmse: 0.47017 |  0:00:15s
epoch 116| loss: 0.18512 | val_0_rmse: 0.42658 | val_1_rmse: 0.46329 |  0:00:15s
epoch 117| loss: 0.18001 | val_0_rmse: 0.44862 | val_1_rmse: 0.46627 |  0:00:15s
epoch 118| loss: 0.18452 | val_0_rmse: 0.48288 | val_1_rmse: 0.46775 |  0:00:15s
epoch 119| loss: 0.1872  | val_0_rmse: 0.4563  | val_1_rmse: 0.46253 |  0:00:15s
epoch 120| loss: 0.19566 | val_0_rmse: 0.45981 | val_1_rmse: 0.45029 |  0:00:15s
epoch 121| loss: 0.17895 | val_0_rmse: 0.46493 | val_1_rmse: 0.45408 |  0:00:16s
epoch 122| loss: 0.18031 | val_0_rmse: 0.41774 | val_1_rmse: 0.45384 |  0:00:16s
epoch 123| loss: 0.17103 | val_0_rmse: 0.4098  | val_1_rmse: 0.4536  |  0:00:16s
epoch 124| loss: 0.17266 | val_0_rmse: 0.41691 | val_1_rmse: 0.45532 |  0:00:16s
epoch 125| loss: 0.17873 | val_0_rmse: 0.42006 | val_1_rmse: 0.47538 |  0:00:16s
epoch 126| loss: 0.19902 | val_0_rmse: 0.41339 | val_1_rmse: 0.45291 |  0:00:16s
epoch 127| loss: 0.17654 | val_0_rmse: 0.41273 | val_1_rmse: 0.46595 |  0:00:16s
epoch 128| loss: 0.18543 | val_0_rmse: 0.43404 | val_1_rmse: 0.46797 |  0:00:16s
epoch 129| loss: 0.19555 | val_0_rmse: 0.43391 | val_1_rmse: 0.49176 |  0:00:17s
epoch 130| loss: 0.20038 | val_0_rmse: 0.4056  | val_1_rmse: 0.44804 |  0:00:17s
epoch 131| loss: 0.17047 | val_0_rmse: 0.40651 | val_1_rmse: 0.46181 |  0:00:17s
epoch 132| loss: 0.17478 | val_0_rmse: 0.41409 | val_1_rmse: 0.44936 |  0:00:17s
epoch 133| loss: 0.17835 | val_0_rmse: 0.40907 | val_1_rmse: 0.46541 |  0:00:17s
epoch 134| loss: 0.1788  | val_0_rmse: 0.40304 | val_1_rmse: 0.44053 |  0:00:17s
epoch 135| loss: 0.16943 | val_0_rmse: 0.39956 | val_1_rmse: 0.44664 |  0:00:17s
epoch 136| loss: 0.1646  | val_0_rmse: 0.41178 | val_1_rmse: 0.44815 |  0:00:17s
epoch 137| loss: 0.17702 | val_0_rmse: 0.42611 | val_1_rmse: 0.49106 |  0:00:18s
epoch 138| loss: 0.19883 | val_0_rmse: 0.39981 | val_1_rmse: 0.45013 |  0:00:18s
epoch 139| loss: 0.1834  | val_0_rmse: 0.45374 | val_1_rmse: 0.46664 |  0:00:18s
epoch 140| loss: 0.22136 | val_0_rmse: 0.4369  | val_1_rmse: 0.49989 |  0:00:18s
epoch 141| loss: 0.24348 | val_0_rmse: 0.44936 | val_1_rmse: 0.51406 |  0:00:18s
epoch 142| loss: 0.20699 | val_0_rmse: 0.46871 | val_1_rmse: 0.48925 |  0:00:18s

Early stopping occured at epoch 142 with best_epoch = 112 and best_val_1_rmse = 0.43792
Best weights from best epoch are automatically used!
ended training at: 05:46:54
Feature importance:
[('Area', 0.1944192625293406), ('Baths', 0.0877812677860861), ('Beds', 0.13413476764895257), ('Latitude', 0.08082249426409281), ('Longitude', 0.28469331087039434), ('Month', 0.20154325888884603), ('Year', 0.016605638012287528)]
Mean squared error is of 3667878570.138958
Mean absolute error:44614.681736057704
MAPE:0.4032996274278852
R2 score:0.5170575852767789
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:46:54
epoch 0  | loss: 127.28779| val_0_rmse: 8.54521 | val_1_rmse: 8.57353 |  0:00:00s
epoch 1  | loss: 58.69371| val_0_rmse: 7.85551 | val_1_rmse: 8.03285 |  0:00:00s
epoch 2  | loss: 7.98015 | val_0_rmse: 15.39088| val_1_rmse: 15.54535|  0:00:00s
epoch 3  | loss: 8.70335 | val_0_rmse: 6.63404 | val_1_rmse: 6.59089 |  0:00:01s
epoch 4  | loss: 2.21584 | val_0_rmse: 2.73705 | val_1_rmse: 2.85345 |  0:00:01s
epoch 5  | loss: 1.26363 | val_0_rmse: 2.51229 | val_1_rmse: 2.53282 |  0:00:01s
epoch 6  | loss: 0.84716 | val_0_rmse: 0.89712 | val_1_rmse: 0.88366 |  0:00:02s
epoch 7  | loss: 0.69348 | val_0_rmse: 1.13424 | val_1_rmse: 1.11098 |  0:00:02s
epoch 8  | loss: 0.66075 | val_0_rmse: 0.94412 | val_1_rmse: 0.96824 |  0:00:02s
epoch 9  | loss: 0.55751 | val_0_rmse: 0.76792 | val_1_rmse: 0.7645  |  0:00:02s
epoch 10 | loss: 0.41057 | val_0_rmse: 1.17283 | val_1_rmse: 1.09609 |  0:00:03s
epoch 11 | loss: 0.46851 | val_0_rmse: 0.8707  | val_1_rmse: 0.87165 |  0:00:03s
epoch 12 | loss: 0.50785 | val_0_rmse: 0.62383 | val_1_rmse: 0.64554 |  0:00:03s
epoch 13 | loss: 0.30944 | val_0_rmse: 0.62171 | val_1_rmse: 0.63522 |  0:00:04s
epoch 14 | loss: 0.28432 | val_0_rmse: 0.61563 | val_1_rmse: 0.63703 |  0:00:04s
epoch 15 | loss: 0.26523 | val_0_rmse: 0.5209  | val_1_rmse: 0.52702 |  0:00:04s
epoch 16 | loss: 0.29086 | val_0_rmse: 0.54755 | val_1_rmse: 0.57083 |  0:00:04s
epoch 17 | loss: 0.2455  | val_0_rmse: 0.46541 | val_1_rmse: 0.48093 |  0:00:05s
epoch 18 | loss: 0.22011 | val_0_rmse: 0.45343 | val_1_rmse: 0.46777 |  0:00:05s
epoch 19 | loss: 0.21262 | val_0_rmse: 0.48491 | val_1_rmse: 0.503   |  0:00:05s
epoch 20 | loss: 0.22199 | val_0_rmse: 0.44871 | val_1_rmse: 0.47517 |  0:00:06s
epoch 21 | loss: 0.20304 | val_0_rmse: 0.43056 | val_1_rmse: 0.44918 |  0:00:06s
epoch 22 | loss: 0.20177 | val_0_rmse: 0.43428 | val_1_rmse: 0.45106 |  0:00:06s
epoch 23 | loss: 0.2094  | val_0_rmse: 0.43694 | val_1_rmse: 0.44605 |  0:00:06s
epoch 24 | loss: 0.20938 | val_0_rmse: 0.43758 | val_1_rmse: 0.45338 |  0:00:07s
epoch 25 | loss: 0.21456 | val_0_rmse: 0.49138 | val_1_rmse: 0.51036 |  0:00:07s
epoch 26 | loss: 0.26012 | val_0_rmse: 0.4677  | val_1_rmse: 0.4739  |  0:00:07s
epoch 27 | loss: 0.2084  | val_0_rmse: 0.43928 | val_1_rmse: 0.44704 |  0:00:08s
epoch 28 | loss: 0.21794 | val_0_rmse: 0.4439  | val_1_rmse: 0.46134 |  0:00:08s
epoch 29 | loss: 0.20043 | val_0_rmse: 0.4175  | val_1_rmse: 0.43783 |  0:00:08s
epoch 30 | loss: 0.18899 | val_0_rmse: 0.42447 | val_1_rmse: 0.44117 |  0:00:08s
epoch 31 | loss: 0.19246 | val_0_rmse: 0.42282 | val_1_rmse: 0.43996 |  0:00:09s
epoch 32 | loss: 0.18726 | val_0_rmse: 0.4142  | val_1_rmse: 0.43    |  0:00:09s
epoch 33 | loss: 0.18629 | val_0_rmse: 0.41161 | val_1_rmse: 0.43143 |  0:00:09s
epoch 34 | loss: 0.17939 | val_0_rmse: 0.43608 | val_1_rmse: 0.45298 |  0:00:10s
epoch 35 | loss: 0.18113 | val_0_rmse: 0.42096 | val_1_rmse: 0.44314 |  0:00:10s
epoch 36 | loss: 0.17842 | val_0_rmse: 0.42459 | val_1_rmse: 0.44491 |  0:00:10s
epoch 37 | loss: 0.19904 | val_0_rmse: 0.44737 | val_1_rmse: 0.46438 |  0:00:10s
epoch 38 | loss: 0.22601 | val_0_rmse: 0.54771 | val_1_rmse: 0.54945 |  0:00:11s
epoch 39 | loss: 0.24701 | val_0_rmse: 0.43361 | val_1_rmse: 0.44899 |  0:00:11s
epoch 40 | loss: 0.19835 | val_0_rmse: 0.42597 | val_1_rmse: 0.44095 |  0:00:11s
epoch 41 | loss: 0.20536 | val_0_rmse: 0.40434 | val_1_rmse: 0.4207  |  0:00:12s
epoch 42 | loss: 0.17974 | val_0_rmse: 0.42321 | val_1_rmse: 0.4333  |  0:00:12s
epoch 43 | loss: 0.17592 | val_0_rmse: 0.40584 | val_1_rmse: 0.42102 |  0:00:12s
epoch 44 | loss: 0.17052 | val_0_rmse: 0.40244 | val_1_rmse: 0.41662 |  0:00:12s
epoch 45 | loss: 0.17244 | val_0_rmse: 0.39865 | val_1_rmse: 0.41181 |  0:00:13s
epoch 46 | loss: 0.18141 | val_0_rmse: 0.4029  | val_1_rmse: 0.41383 |  0:00:13s
epoch 47 | loss: 0.17533 | val_0_rmse: 0.41252 | val_1_rmse: 0.42144 |  0:00:13s
epoch 48 | loss: 0.18222 | val_0_rmse: 0.42805 | val_1_rmse: 0.43913 |  0:00:14s
epoch 49 | loss: 0.18172 | val_0_rmse: 0.42507 | val_1_rmse: 0.43442 |  0:00:14s
epoch 50 | loss: 0.17872 | val_0_rmse: 0.43289 | val_1_rmse: 0.44788 |  0:00:14s
epoch 51 | loss: 0.17182 | val_0_rmse: 0.40943 | val_1_rmse: 0.42466 |  0:00:14s
epoch 52 | loss: 0.16889 | val_0_rmse: 0.41703 | val_1_rmse: 0.4267  |  0:00:15s
epoch 53 | loss: 0.17516 | val_0_rmse: 0.40941 | val_1_rmse: 0.42567 |  0:00:15s
epoch 54 | loss: 0.17632 | val_0_rmse: 0.40524 | val_1_rmse: 0.41955 |  0:00:15s
epoch 55 | loss: 0.17548 | val_0_rmse: 0.41861 | val_1_rmse: 0.43171 |  0:00:16s
epoch 56 | loss: 0.18268 | val_0_rmse: 0.44195 | val_1_rmse: 0.4575  |  0:00:16s
epoch 57 | loss: 0.19861 | val_0_rmse: 0.42651 | val_1_rmse: 0.44457 |  0:00:16s
epoch 58 | loss: 0.18325 | val_0_rmse: 0.41894 | val_1_rmse: 0.43449 |  0:00:17s
epoch 59 | loss: 0.18731 | val_0_rmse: 0.40644 | val_1_rmse: 0.42149 |  0:00:17s
epoch 60 | loss: 0.19129 | val_0_rmse: 0.42058 | val_1_rmse: 0.43214 |  0:00:17s
epoch 61 | loss: 0.18842 | val_0_rmse: 0.41207 | val_1_rmse: 0.42647 |  0:00:17s
epoch 62 | loss: 0.18089 | val_0_rmse: 0.42299 | val_1_rmse: 0.43318 |  0:00:18s
epoch 63 | loss: 0.19296 | val_0_rmse: 0.42213 | val_1_rmse: 0.4353  |  0:00:18s
epoch 64 | loss: 0.21159 | val_0_rmse: 0.42846 | val_1_rmse: 0.44288 |  0:00:18s
epoch 65 | loss: 0.2306  | val_0_rmse: 0.4494  | val_1_rmse: 0.46524 |  0:00:18s
epoch 66 | loss: 0.21625 | val_0_rmse: 0.4415  | val_1_rmse: 0.45759 |  0:00:19s
epoch 67 | loss: 0.19113 | val_0_rmse: 0.43528 | val_1_rmse: 0.44779 |  0:00:19s
epoch 68 | loss: 0.18561 | val_0_rmse: 0.40164 | val_1_rmse: 0.42353 |  0:00:19s
epoch 69 | loss: 0.19129 | val_0_rmse: 0.44238 | val_1_rmse: 0.4591  |  0:00:20s
epoch 70 | loss: 0.19147 | val_0_rmse: 0.41834 | val_1_rmse: 0.43307 |  0:00:20s
epoch 71 | loss: 0.19019 | val_0_rmse: 0.42759 | val_1_rmse: 0.44292 |  0:00:20s
epoch 72 | loss: 0.1953  | val_0_rmse: 0.51208 | val_1_rmse: 0.53357 |  0:00:21s
epoch 73 | loss: 0.19346 | val_0_rmse: 0.40554 | val_1_rmse: 0.4205  |  0:00:21s
epoch 74 | loss: 0.18248 | val_0_rmse: 0.43504 | val_1_rmse: 0.4547  |  0:00:21s
epoch 75 | loss: 0.18884 | val_0_rmse: 0.43082 | val_1_rmse: 0.45194 |  0:00:21s

Early stopping occured at epoch 75 with best_epoch = 45 and best_val_1_rmse = 0.41181
Best weights from best epoch are automatically used!
ended training at: 05:47:16
Feature importance:
[('Area', 0.11931785882375825), ('Baths', 0.34514451115718026), ('Beds', 0.07302558333393509), ('Latitude', 0.08359829998824728), ('Longitude', 0.12324973792383039), ('Month', 0.14258229532877287), ('Year', 0.11308171344427591)]
Mean squared error is of 4155863976.737817
Mean absolute error:44237.19278241723
MAPE:0.3876366426831307
R2 score:0.5109664472208242
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:47:17
epoch 0  | loss: 127.89055| val_0_rmse: 8.70819 | val_1_rmse: 8.82315 |  0:00:00s
epoch 1  | loss: 56.956  | val_0_rmse: 5.7763  | val_1_rmse: 6.4683  |  0:00:00s
epoch 2  | loss: 9.05539 | val_0_rmse: 12.12597| val_1_rmse: 11.99877|  0:00:00s
epoch 3  | loss: 8.59918 | val_0_rmse: 8.44707 | val_1_rmse: 8.58098 |  0:00:01s
epoch 4  | loss: 2.11221 | val_0_rmse: 2.39013 | val_1_rmse: 2.4733  |  0:00:01s
epoch 5  | loss: 1.62684 | val_0_rmse: 2.84927 | val_1_rmse: 3.36715 |  0:00:01s
epoch 6  | loss: 0.88633 | val_0_rmse: 2.25552 | val_1_rmse: 2.35781 |  0:00:01s
epoch 7  | loss: 0.57537 | val_0_rmse: 1.09284 | val_1_rmse: 1.19681 |  0:00:02s
epoch 8  | loss: 0.41949 | val_0_rmse: 1.14499 | val_1_rmse: 1.3169  |  0:00:02s
epoch 9  | loss: 0.38867 | val_0_rmse: 0.99585 | val_1_rmse: 1.08629 |  0:00:02s
epoch 10 | loss: 0.25194 | val_0_rmse: 0.98984 | val_1_rmse: 1.03468 |  0:00:03s
epoch 11 | loss: 0.24209 | val_0_rmse: 1.0743  | val_1_rmse: 1.13516 |  0:00:03s
epoch 12 | loss: 0.28214 | val_0_rmse: 0.6957  | val_1_rmse: 0.73864 |  0:00:03s
epoch 13 | loss: 0.25809 | val_0_rmse: 0.80451 | val_1_rmse: 0.85729 |  0:00:04s
epoch 14 | loss: 0.29892 | val_0_rmse: 0.62276 | val_1_rmse: 0.67284 |  0:00:04s
epoch 15 | loss: 0.42162 | val_0_rmse: 0.49918 | val_1_rmse: 0.56676 |  0:00:04s
epoch 16 | loss: 0.34045 | val_0_rmse: 0.53268 | val_1_rmse: 0.60142 |  0:00:04s
epoch 17 | loss: 0.30019 | val_0_rmse: 0.51294 | val_1_rmse: 0.56406 |  0:00:05s
epoch 18 | loss: 0.32098 | val_0_rmse: 0.53323 | val_1_rmse: 0.57911 |  0:00:05s
epoch 19 | loss: 0.29528 | val_0_rmse: 0.49724 | val_1_rmse: 0.53884 |  0:00:05s
epoch 20 | loss: 0.23921 | val_0_rmse: 0.49442 | val_1_rmse: 0.53298 |  0:00:06s
epoch 21 | loss: 0.2304  | val_0_rmse: 0.43838 | val_1_rmse: 0.52403 |  0:00:06s
epoch 22 | loss: 0.25023 | val_0_rmse: 0.43559 | val_1_rmse: 0.5378  |  0:00:06s
epoch 23 | loss: 0.28065 | val_0_rmse: 0.59123 | val_1_rmse: 0.66919 |  0:00:06s
epoch 24 | loss: 0.23745 | val_0_rmse: 0.49932 | val_1_rmse: 0.54595 |  0:00:07s
epoch 25 | loss: 0.2441  | val_0_rmse: 0.48483 | val_1_rmse: 0.51634 |  0:00:07s
epoch 26 | loss: 0.24903 | val_0_rmse: 0.46675 | val_1_rmse: 0.50966 |  0:00:07s
epoch 27 | loss: 0.23848 | val_0_rmse: 0.47705 | val_1_rmse: 0.55004 |  0:00:08s
epoch 28 | loss: 0.26183 | val_0_rmse: 0.443   | val_1_rmse: 0.53318 |  0:00:08s
epoch 29 | loss: 0.23781 | val_0_rmse: 0.44925 | val_1_rmse: 0.50621 |  0:00:08s
epoch 30 | loss: 0.25033 | val_0_rmse: 0.42416 | val_1_rmse: 0.47131 |  0:00:09s
epoch 31 | loss: 0.19214 | val_0_rmse: 0.43116 | val_1_rmse: 0.53681 |  0:00:09s
epoch 32 | loss: 0.21347 | val_0_rmse: 0.49194 | val_1_rmse: 0.51326 |  0:00:09s
epoch 33 | loss: 0.25401 | val_0_rmse: 0.44519 | val_1_rmse: 0.46534 |  0:00:09s
epoch 34 | loss: 0.26176 | val_0_rmse: 0.46948 | val_1_rmse: 0.4801  |  0:00:10s
epoch 35 | loss: 0.23724 | val_0_rmse: 0.4847  | val_1_rmse: 0.49473 |  0:00:10s
epoch 36 | loss: 0.27624 | val_0_rmse: 0.44814 | val_1_rmse: 0.45851 |  0:00:10s
epoch 37 | loss: 0.22855 | val_0_rmse: 0.45071 | val_1_rmse: 0.46375 |  0:00:11s
epoch 38 | loss: 0.25178 | val_0_rmse: 0.44566 | val_1_rmse: 0.4589  |  0:00:11s
epoch 39 | loss: 0.22838 | val_0_rmse: 0.44801 | val_1_rmse: 0.45523 |  0:00:11s
epoch 40 | loss: 0.21356 | val_0_rmse: 0.46503 | val_1_rmse: 0.48046 |  0:00:11s
epoch 41 | loss: 0.22265 | val_0_rmse: 0.45918 | val_1_rmse: 0.47672 |  0:00:12s
epoch 42 | loss: 0.22015 | val_0_rmse: 0.44458 | val_1_rmse: 0.45968 |  0:00:12s
epoch 43 | loss: 0.21842 | val_0_rmse: 0.4279  | val_1_rmse: 0.43593 |  0:00:12s
epoch 44 | loss: 0.20144 | val_0_rmse: 0.48713 | val_1_rmse: 0.49221 |  0:00:13s
epoch 45 | loss: 0.22827 | val_0_rmse: 0.42606 | val_1_rmse: 0.43738 |  0:00:13s
epoch 46 | loss: 0.2122  | val_0_rmse: 0.42286 | val_1_rmse: 0.44    |  0:00:13s
epoch 47 | loss: 0.20632 | val_0_rmse: 0.43292 | val_1_rmse: 0.44955 |  0:00:13s
epoch 48 | loss: 0.21138 | val_0_rmse: 0.41799 | val_1_rmse: 0.42797 |  0:00:14s
epoch 49 | loss: 0.19105 | val_0_rmse: 0.42818 | val_1_rmse: 0.43983 |  0:00:14s
epoch 50 | loss: 0.19937 | val_0_rmse: 0.47135 | val_1_rmse: 0.48674 |  0:00:14s
epoch 51 | loss: 0.19031 | val_0_rmse: 0.41793 | val_1_rmse: 0.43356 |  0:00:15s
epoch 52 | loss: 0.18071 | val_0_rmse: 0.40788 | val_1_rmse: 0.4243  |  0:00:15s
epoch 53 | loss: 0.17758 | val_0_rmse: 0.41792 | val_1_rmse: 0.43509 |  0:00:15s
epoch 54 | loss: 0.16931 | val_0_rmse: 0.45471 | val_1_rmse: 0.47552 |  0:00:15s
epoch 55 | loss: 0.18949 | val_0_rmse: 0.41585 | val_1_rmse: 0.43307 |  0:00:16s
epoch 56 | loss: 0.18773 | val_0_rmse: 0.40138 | val_1_rmse: 0.41106 |  0:00:16s
epoch 57 | loss: 0.20547 | val_0_rmse: 0.41644 | val_1_rmse: 0.42838 |  0:00:16s
epoch 58 | loss: 0.19846 | val_0_rmse: 0.4835  | val_1_rmse: 0.49224 |  0:00:17s
epoch 59 | loss: 0.2281  | val_0_rmse: 0.46747 | val_1_rmse: 0.48693 |  0:00:17s
epoch 60 | loss: 0.22647 | val_0_rmse: 0.41569 | val_1_rmse: 0.43268 |  0:00:17s
epoch 61 | loss: 0.21706 | val_0_rmse: 0.4386  | val_1_rmse: 0.45322 |  0:00:18s
epoch 62 | loss: 0.19644 | val_0_rmse: 0.41998 | val_1_rmse: 0.44005 |  0:00:18s
epoch 63 | loss: 0.18424 | val_0_rmse: 0.40147 | val_1_rmse: 0.42006 |  0:00:18s
epoch 64 | loss: 0.20025 | val_0_rmse: 0.45811 | val_1_rmse: 0.47371 |  0:00:18s
epoch 65 | loss: 0.20836 | val_0_rmse: 0.39576 | val_1_rmse: 0.41368 |  0:00:19s
epoch 66 | loss: 0.19046 | val_0_rmse: 0.39793 | val_1_rmse: 0.41446 |  0:00:19s
epoch 67 | loss: 0.18586 | val_0_rmse: 0.46378 | val_1_rmse: 0.48635 |  0:00:19s
epoch 68 | loss: 0.19753 | val_0_rmse: 0.39611 | val_1_rmse: 0.40743 |  0:00:20s
epoch 69 | loss: 0.19237 | val_0_rmse: 0.39762 | val_1_rmse: 0.4098  |  0:00:20s
epoch 70 | loss: 0.17673 | val_0_rmse: 0.48965 | val_1_rmse: 0.49822 |  0:00:20s
epoch 71 | loss: 0.19414 | val_0_rmse: 0.39222 | val_1_rmse: 0.40364 |  0:00:20s
epoch 72 | loss: 0.21088 | val_0_rmse: 0.4018  | val_1_rmse: 0.41285 |  0:00:21s
epoch 73 | loss: 0.17209 | val_0_rmse: 0.45688 | val_1_rmse: 0.46274 |  0:00:21s
epoch 74 | loss: 0.1977  | val_0_rmse: 0.41736 | val_1_rmse: 0.42228 |  0:00:21s
epoch 75 | loss: 0.17219 | val_0_rmse: 0.43669 | val_1_rmse: 0.50949 |  0:00:22s
epoch 76 | loss: 0.18439 | val_0_rmse: 0.47192 | val_1_rmse: 0.51624 |  0:00:22s
epoch 77 | loss: 0.22858 | val_0_rmse: 0.45525 | val_1_rmse: 0.46271 |  0:00:22s
epoch 78 | loss: 0.1835  | val_0_rmse: 0.41826 | val_1_rmse: 0.42151 |  0:00:22s
epoch 79 | loss: 0.19533 | val_0_rmse: 0.52521 | val_1_rmse: 0.63886 |  0:00:23s
epoch 80 | loss: 0.22989 | val_0_rmse: 0.45376 | val_1_rmse: 0.51555 |  0:00:23s
epoch 81 | loss: 0.22024 | val_0_rmse: 0.40533 | val_1_rmse: 0.4111  |  0:00:23s
epoch 82 | loss: 0.17764 | val_0_rmse: 0.41622 | val_1_rmse: 0.43089 |  0:00:24s
epoch 83 | loss: 0.20341 | val_0_rmse: 0.40434 | val_1_rmse: 0.42465 |  0:00:24s
epoch 84 | loss: 0.18241 | val_0_rmse: 0.40711 | val_1_rmse: 0.42449 |  0:00:24s
epoch 85 | loss: 0.16532 | val_0_rmse: 0.42097 | val_1_rmse: 0.50758 |  0:00:24s
epoch 86 | loss: 0.16824 | val_0_rmse: 0.42762 | val_1_rmse: 0.4786  |  0:00:25s
epoch 87 | loss: 0.16851 | val_0_rmse: 0.40807 | val_1_rmse: 0.43835 |  0:00:25s
epoch 88 | loss: 0.16368 | val_0_rmse: 0.42754 | val_1_rmse: 0.44481 |  0:00:25s
epoch 89 | loss: 0.17268 | val_0_rmse: 0.39051 | val_1_rmse: 0.42759 |  0:00:26s
epoch 90 | loss: 0.16951 | val_0_rmse: 0.42206 | val_1_rmse: 0.43769 |  0:00:26s
epoch 91 | loss: 0.16151 | val_0_rmse: 0.40221 | val_1_rmse: 0.42261 |  0:00:26s
epoch 92 | loss: 0.1631  | val_0_rmse: 0.41254 | val_1_rmse: 0.44395 |  0:00:26s
epoch 93 | loss: 0.15961 | val_0_rmse: 0.39601 | val_1_rmse: 0.41019 |  0:00:27s
epoch 94 | loss: 0.15719 | val_0_rmse: 0.38611 | val_1_rmse: 0.39954 |  0:00:27s
epoch 95 | loss: 0.15375 | val_0_rmse: 0.39189 | val_1_rmse: 0.41189 |  0:00:27s
epoch 96 | loss: 0.16492 | val_0_rmse: 0.38281 | val_1_rmse: 0.39947 |  0:00:28s
epoch 97 | loss: 0.16617 | val_0_rmse: 0.38293 | val_1_rmse: 0.39654 |  0:00:28s
epoch 98 | loss: 0.16108 | val_0_rmse: 0.39015 | val_1_rmse: 0.41106 |  0:00:28s
epoch 99 | loss: 0.17559 | val_0_rmse: 0.43046 | val_1_rmse: 0.44027 |  0:00:28s
epoch 100| loss: 0.1675  | val_0_rmse: 0.4201  | val_1_rmse: 0.43487 |  0:00:29s
epoch 101| loss: 0.16286 | val_0_rmse: 0.38897 | val_1_rmse: 0.40549 |  0:00:29s
epoch 102| loss: 0.15388 | val_0_rmse: 0.37583 | val_1_rmse: 0.39266 |  0:00:29s
epoch 103| loss: 0.15638 | val_0_rmse: 0.38664 | val_1_rmse: 0.40447 |  0:00:30s
epoch 104| loss: 0.15657 | val_0_rmse: 0.43443 | val_1_rmse: 0.45056 |  0:00:30s
epoch 105| loss: 0.16271 | val_0_rmse: 0.37882 | val_1_rmse: 0.39202 |  0:00:30s
epoch 106| loss: 0.15327 | val_0_rmse: 0.42878 | val_1_rmse: 0.44702 |  0:00:30s
epoch 107| loss: 0.15297 | val_0_rmse: 0.39167 | val_1_rmse: 0.41383 |  0:00:31s
epoch 108| loss: 0.1554  | val_0_rmse: 0.40048 | val_1_rmse: 0.43215 |  0:00:31s
epoch 109| loss: 0.1523  | val_0_rmse: 0.3811  | val_1_rmse: 0.411   |  0:00:31s
epoch 110| loss: 0.15611 | val_0_rmse: 0.39584 | val_1_rmse: 0.42365 |  0:00:32s
epoch 111| loss: 0.15154 | val_0_rmse: 0.38844 | val_1_rmse: 0.41475 |  0:00:32s
epoch 112| loss: 0.14914 | val_0_rmse: 0.38991 | val_1_rmse: 0.40955 |  0:00:32s
epoch 113| loss: 0.1542  | val_0_rmse: 0.39066 | val_1_rmse: 0.41859 |  0:00:32s
epoch 114| loss: 0.15975 | val_0_rmse: 0.37665 | val_1_rmse: 0.39906 |  0:00:33s
epoch 115| loss: 0.1543  | val_0_rmse: 0.37594 | val_1_rmse: 0.39713 |  0:00:33s
epoch 116| loss: 0.15019 | val_0_rmse: 0.37348 | val_1_rmse: 0.3956  |  0:00:33s
epoch 117| loss: 0.14817 | val_0_rmse: 0.36844 | val_1_rmse: 0.38983 |  0:00:34s
epoch 118| loss: 0.14602 | val_0_rmse: 0.38363 | val_1_rmse: 0.40418 |  0:00:34s
epoch 119| loss: 0.14968 | val_0_rmse: 0.37389 | val_1_rmse: 0.38895 |  0:00:34s
epoch 120| loss: 0.15047 | val_0_rmse: 0.37061 | val_1_rmse: 0.385   |  0:00:34s
epoch 121| loss: 0.15019 | val_0_rmse: 0.37069 | val_1_rmse: 0.38934 |  0:00:35s
epoch 122| loss: 0.14842 | val_0_rmse: 0.36828 | val_1_rmse: 0.38829 |  0:00:35s
epoch 123| loss: 0.14761 | val_0_rmse: 0.36938 | val_1_rmse: 0.39161 |  0:00:35s
epoch 124| loss: 0.14732 | val_0_rmse: 0.37169 | val_1_rmse: 0.40257 |  0:00:36s
epoch 125| loss: 0.14729 | val_0_rmse: 0.37743 | val_1_rmse: 0.4111  |  0:00:36s
epoch 126| loss: 0.15254 | val_0_rmse: 0.42133 | val_1_rmse: 0.44353 |  0:00:36s
epoch 127| loss: 0.16756 | val_0_rmse: 0.38395 | val_1_rmse: 0.40158 |  0:00:36s
epoch 128| loss: 0.15225 | val_0_rmse: 0.37358 | val_1_rmse: 0.38983 |  0:00:37s
epoch 129| loss: 0.14268 | val_0_rmse: 0.37581 | val_1_rmse: 0.39521 |  0:00:37s
epoch 130| loss: 0.14262 | val_0_rmse: 0.38979 | val_1_rmse: 0.4082  |  0:00:37s
epoch 131| loss: 0.13981 | val_0_rmse: 0.40542 | val_1_rmse: 0.41952 |  0:00:38s
epoch 132| loss: 0.15161 | val_0_rmse: 0.37294 | val_1_rmse: 0.39041 |  0:00:38s
epoch 133| loss: 0.158   | val_0_rmse: 0.3843  | val_1_rmse: 0.39482 |  0:00:38s
epoch 134| loss: 0.14497 | val_0_rmse: 0.38224 | val_1_rmse: 0.39792 |  0:00:38s
epoch 135| loss: 0.14833 | val_0_rmse: 0.40359 | val_1_rmse: 0.41965 |  0:00:39s
epoch 136| loss: 0.1537  | val_0_rmse: 0.38977 | val_1_rmse: 0.40323 |  0:00:39s
epoch 137| loss: 0.14699 | val_0_rmse: 0.39979 | val_1_rmse: 0.41577 |  0:00:39s
epoch 138| loss: 0.14338 | val_0_rmse: 0.36682 | val_1_rmse: 0.38549 |  0:00:40s
epoch 139| loss: 0.14659 | val_0_rmse: 0.36584 | val_1_rmse: 0.38004 |  0:00:40s
epoch 140| loss: 0.14243 | val_0_rmse: 0.36817 | val_1_rmse: 0.38743 |  0:00:40s
epoch 141| loss: 0.1455  | val_0_rmse: 0.47832 | val_1_rmse: 0.48987 |  0:00:41s
epoch 142| loss: 0.16546 | val_0_rmse: 0.38808 | val_1_rmse: 0.40055 |  0:00:41s
epoch 143| loss: 0.18311 | val_0_rmse: 0.46135 | val_1_rmse: 0.47582 |  0:00:41s
epoch 144| loss: 0.15999 | val_0_rmse: 0.48007 | val_1_rmse: 0.49362 |  0:00:41s
epoch 145| loss: 0.18768 | val_0_rmse: 0.37771 | val_1_rmse: 0.38936 |  0:00:42s
epoch 146| loss: 0.17826 | val_0_rmse: 0.40657 | val_1_rmse: 0.42457 |  0:00:42s
epoch 147| loss: 0.16617 | val_0_rmse: 0.41655 | val_1_rmse: 0.43465 |  0:00:42s
epoch 148| loss: 0.17517 | val_0_rmse: 0.39125 | val_1_rmse: 0.40777 |  0:00:43s
epoch 149| loss: 0.16984 | val_0_rmse: 0.38962 | val_1_rmse: 0.41044 |  0:00:43s
Stop training because you reached max_epochs = 150 with best_epoch = 139 and best_val_1_rmse = 0.38004
Best weights from best epoch are automatically used!
ended training at: 05:48:00
Feature importance:
[('Area', 0.09730896946383265), ('Baths', 0.2835577985560017), ('Beds', 0.06547449087819109), ('Latitude', 0.1085855345759107), ('Longitude', 0.3180733487273773), ('Month', 0.0661494674605108), ('Year', 0.06085039033817572)]
Mean squared error is of 3491503495.7561545
Mean absolute error:39664.03203801465
MAPE:0.32598748501106134
R2 score:0.5897435314583608
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:48:00
epoch 0  | loss: 129.05176| val_0_rmse: 9.17132 | val_1_rmse: 9.0582  |  0:00:00s
epoch 1  | loss: 65.17175| val_0_rmse: 6.75507 | val_1_rmse: 6.43141 |  0:00:00s
epoch 2  | loss: 11.14238| val_0_rmse: 5.58528 | val_1_rmse: 5.7185  |  0:00:00s
epoch 3  | loss: 15.59766| val_0_rmse: 7.65626 | val_1_rmse: 7.46225 |  0:00:01s
epoch 4  | loss: 4.94553 | val_0_rmse: 1.46397 | val_1_rmse: 1.49996 |  0:00:01s
epoch 5  | loss: 2.15081 | val_0_rmse: 3.59271 | val_1_rmse: 3.48183 |  0:00:01s
epoch 6  | loss: 0.9758  | val_0_rmse: 3.11239 | val_1_rmse: 3.0096  |  0:00:02s
epoch 7  | loss: 0.69349 | val_0_rmse: 1.55065 | val_1_rmse: 1.51917 |  0:00:02s
epoch 8  | loss: 0.6295  | val_0_rmse: 1.49064 | val_1_rmse: 1.46041 |  0:00:02s
epoch 9  | loss: 0.41606 | val_0_rmse: 0.94169 | val_1_rmse: 0.93901 |  0:00:02s
epoch 10 | loss: 0.38469 | val_0_rmse: 0.78587 | val_1_rmse: 0.78784 |  0:00:03s
epoch 11 | loss: 0.35954 | val_0_rmse: 0.66263 | val_1_rmse: 0.64697 |  0:00:03s
epoch 12 | loss: 0.32508 | val_0_rmse: 0.64631 | val_1_rmse: 0.64128 |  0:00:03s
epoch 13 | loss: 0.31737 | val_0_rmse: 0.49283 | val_1_rmse: 0.47814 |  0:00:04s
epoch 14 | loss: 0.35745 | val_0_rmse: 0.57017 | val_1_rmse: 0.56235 |  0:00:04s
epoch 15 | loss: 0.33057 | val_0_rmse: 0.5265  | val_1_rmse: 0.52553 |  0:00:04s
epoch 16 | loss: 0.25622 | val_0_rmse: 0.54147 | val_1_rmse: 0.5316  |  0:00:04s
epoch 17 | loss: 0.23309 | val_0_rmse: 0.44022 | val_1_rmse: 0.43592 |  0:00:05s
epoch 18 | loss: 0.22142 | val_0_rmse: 0.45352 | val_1_rmse: 0.44779 |  0:00:05s
epoch 19 | loss: 0.23144 | val_0_rmse: 0.45222 | val_1_rmse: 0.44468 |  0:00:05s
epoch 20 | loss: 0.21644 | val_0_rmse: 0.46004 | val_1_rmse: 0.45163 |  0:00:06s
epoch 21 | loss: 0.21858 | val_0_rmse: 0.45414 | val_1_rmse: 0.45569 |  0:00:06s
epoch 22 | loss: 0.21082 | val_0_rmse: 0.5693  | val_1_rmse: 0.56684 |  0:00:06s
epoch 23 | loss: 0.21939 | val_0_rmse: 0.48476 | val_1_rmse: 0.477   |  0:00:06s
epoch 24 | loss: 0.23342 | val_0_rmse: 0.47691 | val_1_rmse: 0.47056 |  0:00:07s
epoch 25 | loss: 0.22901 | val_0_rmse: 0.43886 | val_1_rmse: 0.43753 |  0:00:07s
epoch 26 | loss: 0.20173 | val_0_rmse: 0.42267 | val_1_rmse: 0.42518 |  0:00:07s
epoch 27 | loss: 0.1944  | val_0_rmse: 0.44154 | val_1_rmse: 0.43629 |  0:00:08s
epoch 28 | loss: 0.2024  | val_0_rmse: 0.42142 | val_1_rmse: 0.42388 |  0:00:08s
epoch 29 | loss: 0.19667 | val_0_rmse: 0.44213 | val_1_rmse: 0.44127 |  0:00:08s
epoch 30 | loss: 0.19363 | val_0_rmse: 0.42213 | val_1_rmse: 0.4207  |  0:00:09s
epoch 31 | loss: 0.18744 | val_0_rmse: 0.44754 | val_1_rmse: 0.44318 |  0:00:09s
epoch 32 | loss: 0.18799 | val_0_rmse: 0.42207 | val_1_rmse: 0.41862 |  0:00:09s
epoch 33 | loss: 0.19254 | val_0_rmse: 0.4258  | val_1_rmse: 0.42759 |  0:00:09s
epoch 34 | loss: 0.19636 | val_0_rmse: 0.49744 | val_1_rmse: 0.49122 |  0:00:10s
epoch 35 | loss: 0.18245 | val_0_rmse: 0.43074 | val_1_rmse: 0.42778 |  0:00:10s
epoch 36 | loss: 0.18377 | val_0_rmse: 0.40718 | val_1_rmse: 0.4061  |  0:00:10s
epoch 37 | loss: 0.20347 | val_0_rmse: 0.45819 | val_1_rmse: 0.45411 |  0:00:11s
epoch 38 | loss: 0.20289 | val_0_rmse: 0.42416 | val_1_rmse: 0.42099 |  0:00:11s
epoch 39 | loss: 0.19257 | val_0_rmse: 0.42336 | val_1_rmse: 0.41872 |  0:00:11s
epoch 40 | loss: 0.20605 | val_0_rmse: 0.46353 | val_1_rmse: 0.46068 |  0:00:11s
epoch 41 | loss: 0.21006 | val_0_rmse: 0.48392 | val_1_rmse: 0.48823 |  0:00:12s
epoch 42 | loss: 0.2032  | val_0_rmse: 0.65259 | val_1_rmse: 0.65453 |  0:00:12s
epoch 43 | loss: 0.28925 | val_0_rmse: 1.14916 | val_1_rmse: 1.12542 |  0:00:12s
epoch 44 | loss: 0.28054 | val_0_rmse: 0.4835  | val_1_rmse: 0.47991 |  0:00:13s
epoch 45 | loss: 0.30353 | val_0_rmse: 0.76566 | val_1_rmse: 0.74322 |  0:00:13s
epoch 46 | loss: 0.32016 | val_0_rmse: 0.44775 | val_1_rmse: 0.44247 |  0:00:13s
epoch 47 | loss: 0.27171 | val_0_rmse: 0.46143 | val_1_rmse: 0.45965 |  0:00:13s
epoch 48 | loss: 0.2434  | val_0_rmse: 0.47673 | val_1_rmse: 0.47462 |  0:00:14s
epoch 49 | loss: 0.23809 | val_0_rmse: 0.47123 | val_1_rmse: 0.45573 |  0:00:14s
epoch 50 | loss: 0.19601 | val_0_rmse: 0.44678 | val_1_rmse: 0.44489 |  0:00:14s
epoch 51 | loss: 0.18695 | val_0_rmse: 0.45998 | val_1_rmse: 0.45246 |  0:00:15s
epoch 52 | loss: 0.19197 | val_0_rmse: 0.49219 | val_1_rmse: 0.4857  |  0:00:15s
epoch 53 | loss: 0.20195 | val_0_rmse: 0.44927 | val_1_rmse: 0.44543 |  0:00:15s
epoch 54 | loss: 0.21587 | val_0_rmse: 0.43502 | val_1_rmse: 0.43441 |  0:00:15s
epoch 55 | loss: 0.28915 | val_0_rmse: 0.48563 | val_1_rmse: 0.47772 |  0:00:16s
epoch 56 | loss: 0.22645 | val_0_rmse: 0.50532 | val_1_rmse: 0.50848 |  0:00:16s
epoch 57 | loss: 0.24024 | val_0_rmse: 0.44697 | val_1_rmse: 0.44027 |  0:00:16s
epoch 58 | loss: 0.20257 | val_0_rmse: 0.41845 | val_1_rmse: 0.41817 |  0:00:17s
epoch 59 | loss: 0.19675 | val_0_rmse: 0.41713 | val_1_rmse: 0.41633 |  0:00:17s
epoch 60 | loss: 0.184   | val_0_rmse: 0.41372 | val_1_rmse: 0.41434 |  0:00:17s
epoch 61 | loss: 0.18081 | val_0_rmse: 0.4475  | val_1_rmse: 0.45058 |  0:00:17s
epoch 62 | loss: 0.20274 | val_0_rmse: 0.41401 | val_1_rmse: 0.41147 |  0:00:18s
epoch 63 | loss: 0.18787 | val_0_rmse: 0.44137 | val_1_rmse: 0.43676 |  0:00:18s
epoch 64 | loss: 0.20551 | val_0_rmse: 0.41362 | val_1_rmse: 0.41048 |  0:00:18s
epoch 65 | loss: 0.19446 | val_0_rmse: 0.44645 | val_1_rmse: 0.45046 |  0:00:19s
epoch 66 | loss: 0.24678 | val_0_rmse: 0.47899 | val_1_rmse: 0.48043 |  0:00:19s

Early stopping occured at epoch 66 with best_epoch = 36 and best_val_1_rmse = 0.4061
Best weights from best epoch are automatically used!
ended training at: 05:48:20
Feature importance:
[('Area', 0.11231806990380291), ('Baths', 0.17320238026189738), ('Beds', 0.13519315214391686), ('Latitude', 0.18506521097056172), ('Longitude', 0.005972138827078228), ('Month', 0.3448303659549051), ('Year', 0.04341868193783779)]
Mean squared error is of 4598724237.554459
Mean absolute error:46048.515715062604
MAPE:0.33849015435864566
R2 score:0.47279947917117915
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:48:20
epoch 0  | loss: 126.88227| val_0_rmse: 8.49899 | val_1_rmse: 8.61677 |  0:00:00s
epoch 1  | loss: 57.14614| val_0_rmse: 6.40739 | val_1_rmse: 6.27262 |  0:00:00s
epoch 2  | loss: 7.89383 | val_0_rmse: 9.13832 | val_1_rmse: 8.74607 |  0:00:00s
epoch 3  | loss: 7.14508 | val_0_rmse: 3.23089 | val_1_rmse: 3.04698 |  0:00:01s
epoch 4  | loss: 1.55627 | val_0_rmse: 1.21289 | val_1_rmse: 1.24552 |  0:00:01s
epoch 5  | loss: 0.83523 | val_0_rmse: 1.41937 | val_1_rmse: 1.39845 |  0:00:01s
epoch 6  | loss: 0.63983 | val_0_rmse: 0.96395 | val_1_rmse: 0.92853 |  0:00:02s
epoch 7  | loss: 0.47201 | val_0_rmse: 1.15617 | val_1_rmse: 1.14397 |  0:00:02s
epoch 8  | loss: 0.45067 | val_0_rmse: 0.62425 | val_1_rmse: 0.6191  |  0:00:02s
epoch 9  | loss: 0.57248 | val_0_rmse: 0.93303 | val_1_rmse: 0.93363 |  0:00:02s
epoch 10 | loss: 0.49459 | val_0_rmse: 0.60064 | val_1_rmse: 0.58704 |  0:00:03s
epoch 11 | loss: 0.6249  | val_0_rmse: 0.57642 | val_1_rmse: 0.56105 |  0:00:03s
epoch 12 | loss: 0.51091 | val_0_rmse: 0.8826  | val_1_rmse: 0.85978 |  0:00:03s
epoch 13 | loss: 0.42829 | val_0_rmse: 0.5601  | val_1_rmse: 0.56034 |  0:00:04s
epoch 14 | loss: 0.45383 | val_0_rmse: 0.5995  | val_1_rmse: 0.5791  |  0:00:04s
epoch 15 | loss: 0.53041 | val_0_rmse: 0.71743 | val_1_rmse: 0.73085 |  0:00:04s
epoch 16 | loss: 0.42977 | val_0_rmse: 0.55284 | val_1_rmse: 0.55343 |  0:00:04s
epoch 17 | loss: 0.3621  | val_0_rmse: 0.68784 | val_1_rmse: 0.7025  |  0:00:05s
epoch 18 | loss: 0.47636 | val_0_rmse: 0.53575 | val_1_rmse: 0.55157 |  0:00:05s
epoch 19 | loss: 0.45728 | val_0_rmse: 0.53075 | val_1_rmse: 0.52992 |  0:00:05s
epoch 20 | loss: 0.36756 | val_0_rmse: 0.56854 | val_1_rmse: 0.57689 |  0:00:06s
epoch 21 | loss: 0.38195 | val_0_rmse: 0.55571 | val_1_rmse: 0.56786 |  0:00:06s
epoch 22 | loss: 0.35312 | val_0_rmse: 0.51838 | val_1_rmse: 0.51862 |  0:00:06s
epoch 23 | loss: 0.35555 | val_0_rmse: 0.54246 | val_1_rmse: 0.54214 |  0:00:06s
epoch 24 | loss: 0.35044 | val_0_rmse: 0.46269 | val_1_rmse: 0.4496  |  0:00:07s
epoch 25 | loss: 0.30685 | val_0_rmse: 0.54776 | val_1_rmse: 0.54011 |  0:00:07s
epoch 26 | loss: 0.38785 | val_0_rmse: 0.47686 | val_1_rmse: 0.45923 |  0:00:07s
epoch 27 | loss: 0.35364 | val_0_rmse: 0.53686 | val_1_rmse: 0.51915 |  0:00:08s
epoch 28 | loss: 0.32566 | val_0_rmse: 0.49794 | val_1_rmse: 0.47161 |  0:00:08s
epoch 29 | loss: 0.36749 | val_0_rmse: 0.51976 | val_1_rmse: 0.50422 |  0:00:08s
epoch 30 | loss: 0.32348 | val_0_rmse: 0.46833 | val_1_rmse: 0.45482 |  0:00:08s
epoch 31 | loss: 0.30943 | val_0_rmse: 0.52623 | val_1_rmse: 0.51921 |  0:00:09s
epoch 32 | loss: 0.32258 | val_0_rmse: 0.45269 | val_1_rmse: 0.44059 |  0:00:09s
epoch 33 | loss: 0.28742 | val_0_rmse: 0.53868 | val_1_rmse: 0.53133 |  0:00:09s
epoch 34 | loss: 0.34533 | val_0_rmse: 0.52724 | val_1_rmse: 0.50105 |  0:00:10s
epoch 35 | loss: 0.31428 | val_0_rmse: 0.50627 | val_1_rmse: 0.49977 |  0:00:10s
epoch 36 | loss: 0.28651 | val_0_rmse: 0.49961 | val_1_rmse: 0.4722  |  0:00:10s
epoch 37 | loss: 0.26221 | val_0_rmse: 0.49663 | val_1_rmse: 0.47306 |  0:00:11s
epoch 38 | loss: 0.29756 | val_0_rmse: 0.57543 | val_1_rmse: 0.56196 |  0:00:11s
epoch 39 | loss: 0.33199 | val_0_rmse: 0.53214 | val_1_rmse: 0.52217 |  0:00:11s
epoch 40 | loss: 0.36015 | val_0_rmse: 0.54056 | val_1_rmse: 0.54263 |  0:00:11s
epoch 41 | loss: 0.28065 | val_0_rmse: 0.52617 | val_1_rmse: 0.51006 |  0:00:12s
epoch 42 | loss: 0.31045 | val_0_rmse: 0.62759 | val_1_rmse: 0.62839 |  0:00:12s
epoch 43 | loss: 0.31499 | val_0_rmse: 0.49117 | val_1_rmse: 0.48223 |  0:00:12s
epoch 44 | loss: 0.24855 | val_0_rmse: 0.51098 | val_1_rmse: 0.50956 |  0:00:13s
epoch 45 | loss: 0.2521  | val_0_rmse: 0.46535 | val_1_rmse: 0.45    |  0:00:13s
epoch 46 | loss: 0.23337 | val_0_rmse: 0.44913 | val_1_rmse: 0.43815 |  0:00:13s
epoch 47 | loss: 0.22496 | val_0_rmse: 0.49005 | val_1_rmse: 0.48892 |  0:00:13s
epoch 48 | loss: 0.22674 | val_0_rmse: 0.49333 | val_1_rmse: 0.48989 |  0:00:14s
epoch 49 | loss: 0.2217  | val_0_rmse: 0.53557 | val_1_rmse: 0.53382 |  0:00:14s
epoch 50 | loss: 0.29229 | val_0_rmse: 0.49888 | val_1_rmse: 0.48458 |  0:00:14s
epoch 51 | loss: 0.22531 | val_0_rmse: 0.50712 | val_1_rmse: 0.49559 |  0:00:15s
epoch 52 | loss: 0.22162 | val_0_rmse: 0.49209 | val_1_rmse: 0.48455 |  0:00:15s
epoch 53 | loss: 0.21621 | val_0_rmse: 0.48748 | val_1_rmse: 0.49838 |  0:00:15s
epoch 54 | loss: 0.20325 | val_0_rmse: 0.4996  | val_1_rmse: 0.50524 |  0:00:15s
epoch 55 | loss: 0.1969  | val_0_rmse: 0.43695 | val_1_rmse: 0.45089 |  0:00:16s
epoch 56 | loss: 0.22531 | val_0_rmse: 0.56003 | val_1_rmse: 0.56491 |  0:00:16s
epoch 57 | loss: 0.20112 | val_0_rmse: 0.44216 | val_1_rmse: 0.44079 |  0:00:16s
epoch 58 | loss: 0.1937  | val_0_rmse: 0.42967 | val_1_rmse: 0.42899 |  0:00:17s
epoch 59 | loss: 0.18388 | val_0_rmse: 0.42555 | val_1_rmse: 0.41453 |  0:00:17s
epoch 60 | loss: 0.18158 | val_0_rmse: 0.45875 | val_1_rmse: 0.46095 |  0:00:17s
epoch 61 | loss: 0.17705 | val_0_rmse: 0.42495 | val_1_rmse: 0.41836 |  0:00:17s
epoch 62 | loss: 0.17088 | val_0_rmse: 0.44135 | val_1_rmse: 0.4389  |  0:00:18s
epoch 63 | loss: 0.18345 | val_0_rmse: 0.42856 | val_1_rmse: 0.4307  |  0:00:18s
epoch 64 | loss: 0.20942 | val_0_rmse: 0.45729 | val_1_rmse: 0.45774 |  0:00:18s
epoch 65 | loss: 0.18715 | val_0_rmse: 0.48916 | val_1_rmse: 0.48996 |  0:00:19s
epoch 66 | loss: 0.21021 | val_0_rmse: 0.4302  | val_1_rmse: 0.43098 |  0:00:19s
epoch 67 | loss: 0.20467 | val_0_rmse: 0.43293 | val_1_rmse: 0.43776 |  0:00:19s
epoch 68 | loss: 0.19875 | val_0_rmse: 0.45576 | val_1_rmse: 0.46217 |  0:00:19s
epoch 69 | loss: 0.18935 | val_0_rmse: 0.43107 | val_1_rmse: 0.42482 |  0:00:20s
epoch 70 | loss: 0.19855 | val_0_rmse: 0.42122 | val_1_rmse: 0.41525 |  0:00:20s
epoch 71 | loss: 0.23007 | val_0_rmse: 0.41288 | val_1_rmse: 0.4098  |  0:00:20s
epoch 72 | loss: 0.18068 | val_0_rmse: 0.41729 | val_1_rmse: 0.41346 |  0:00:21s
epoch 73 | loss: 0.17562 | val_0_rmse: 0.42212 | val_1_rmse: 0.41295 |  0:00:21s
epoch 74 | loss: 0.17892 | val_0_rmse: 0.42851 | val_1_rmse: 0.41963 |  0:00:21s
epoch 75 | loss: 0.1738  | val_0_rmse: 0.54779 | val_1_rmse: 0.46312 |  0:00:21s
epoch 76 | loss: 0.19817 | val_0_rmse: 0.51889 | val_1_rmse: 0.51392 |  0:00:22s
epoch 77 | loss: 0.24152 | val_0_rmse: 0.57588 | val_1_rmse: 0.56785 |  0:00:22s
epoch 78 | loss: 0.31059 | val_0_rmse: 0.54678 | val_1_rmse: 0.55263 |  0:00:22s
epoch 79 | loss: 0.2614  | val_0_rmse: 0.49199 | val_1_rmse: 0.48256 |  0:00:23s
epoch 80 | loss: 0.24687 | val_0_rmse: 0.56996 | val_1_rmse: 0.57939 |  0:00:23s
epoch 81 | loss: 0.29246 | val_0_rmse: 0.46133 | val_1_rmse: 0.45597 |  0:00:23s
epoch 82 | loss: 0.2243  | val_0_rmse: 0.57675 | val_1_rmse: 0.58199 |  0:00:23s
epoch 83 | loss: 0.2824  | val_0_rmse: 0.52815 | val_1_rmse: 0.51728 |  0:00:24s
epoch 84 | loss: 0.23306 | val_0_rmse: 0.53717 | val_1_rmse: 0.53731 |  0:00:24s
epoch 85 | loss: 0.26528 | val_0_rmse: 0.47651 | val_1_rmse: 0.46892 |  0:00:24s
epoch 86 | loss: 0.23681 | val_0_rmse: 0.5727  | val_1_rmse: 0.57446 |  0:00:25s
epoch 87 | loss: 0.26335 | val_0_rmse: 0.45912 | val_1_rmse: 0.44757 |  0:00:25s
epoch 88 | loss: 0.233   | val_0_rmse: 0.60761 | val_1_rmse: 0.60785 |  0:00:25s
epoch 89 | loss: 0.26571 | val_0_rmse: 0.43313 | val_1_rmse: 0.4259  |  0:00:25s
epoch 90 | loss: 0.22035 | val_0_rmse: 0.61505 | val_1_rmse: 0.62093 |  0:00:26s
epoch 91 | loss: 0.26442 | val_0_rmse: 0.46934 | val_1_rmse: 0.44839 |  0:00:26s
epoch 92 | loss: 0.22828 | val_0_rmse: 0.55293 | val_1_rmse: 0.55595 |  0:00:26s
epoch 93 | loss: 0.24199 | val_0_rmse: 0.48381 | val_1_rmse: 0.47929 |  0:00:27s
epoch 94 | loss: 0.27036 | val_0_rmse: 0.48541 | val_1_rmse: 0.48633 |  0:00:27s
epoch 95 | loss: 0.19605 | val_0_rmse: 0.4861  | val_1_rmse: 0.48369 |  0:00:27s
epoch 96 | loss: 0.23082 | val_0_rmse: 0.57128 | val_1_rmse: 0.57388 |  0:00:27s
epoch 97 | loss: 0.27127 | val_0_rmse: 0.47562 | val_1_rmse: 0.46929 |  0:00:28s
epoch 98 | loss: 0.24462 | val_0_rmse: 0.51867 | val_1_rmse: 0.52481 |  0:00:28s
epoch 99 | loss: 0.22469 | val_0_rmse: 0.47176 | val_1_rmse: 0.47026 |  0:00:28s
epoch 100| loss: 0.26916 | val_0_rmse: 0.51722 | val_1_rmse: 0.52037 |  0:00:29s
epoch 101| loss: 0.21905 | val_0_rmse: 0.53313 | val_1_rmse: 0.53535 |  0:00:29s

Early stopping occured at epoch 101 with best_epoch = 71 and best_val_1_rmse = 0.4098
Best weights from best epoch are automatically used!
ended training at: 05:48:49
Feature importance:
[('Area', 0.053957514111087256), ('Baths', 0.11375543992219614), ('Beds', 0.01654024295159927), ('Latitude', 0.29695710655913), ('Longitude', 0.26926389928736166), ('Month', 0.07346904632873266), ('Year', 0.17605675083989303)]
Mean squared error is of 4170867225.511364
Mean absolute error:45129.31105637203
MAPE:0.32098664516763853
R2 score:0.5083124891443505
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:48:49
epoch 0  | loss: 128.01077| val_0_rmse: 9.42664 | val_1_rmse: 9.30312 |  0:00:00s
epoch 1  | loss: 64.86697| val_0_rmse: 6.27725 | val_1_rmse: 6.36798 |  0:00:00s
epoch 2  | loss: 10.88915| val_0_rmse: 8.85904 | val_1_rmse: 9.00686 |  0:00:00s
epoch 3  | loss: 10.70709| val_0_rmse: 6.27632 | val_1_rmse: 6.30091 |  0:00:01s
epoch 4  | loss: 4.09654 | val_0_rmse: 1.8435  | val_1_rmse: 1.83783 |  0:00:01s
epoch 5  | loss: 1.50027 | val_0_rmse: 2.02264 | val_1_rmse: 2.08835 |  0:00:01s
epoch 6  | loss: 0.83823 | val_0_rmse: 1.68829 | val_1_rmse: 1.70043 |  0:00:02s
epoch 7  | loss: 0.80072 | val_0_rmse: 1.20684 | val_1_rmse: 1.2394  |  0:00:02s
epoch 8  | loss: 0.53102 | val_0_rmse: 1.0982  | val_1_rmse: 1.09531 |  0:00:02s
epoch 9  | loss: 0.43961 | val_0_rmse: 0.93223 | val_1_rmse: 0.95459 |  0:00:02s
epoch 10 | loss: 1.17208 | val_0_rmse: 0.61743 | val_1_rmse: 0.63366 |  0:00:03s
epoch 11 | loss: 0.69098 | val_0_rmse: 0.8663  | val_1_rmse: 0.87317 |  0:00:03s
epoch 12 | loss: 0.57734 | val_0_rmse: 0.66496 | val_1_rmse: 0.69881 |  0:00:03s
epoch 13 | loss: 0.37276 | val_0_rmse: 0.59533 | val_1_rmse: 0.62495 |  0:00:04s
epoch 14 | loss: 0.30369 | val_0_rmse: 0.58685 | val_1_rmse: 0.60928 |  0:00:04s
epoch 15 | loss: 0.34869 | val_0_rmse: 0.54729 | val_1_rmse: 0.5778  |  0:00:04s
epoch 16 | loss: 0.41663 | val_0_rmse: 0.6462  | val_1_rmse: 0.65697 |  0:00:04s
epoch 17 | loss: 0.29257 | val_0_rmse: 0.45897 | val_1_rmse: 0.48918 |  0:00:05s
epoch 18 | loss: 0.32692 | val_0_rmse: 0.56815 | val_1_rmse: 0.59611 |  0:00:05s
epoch 19 | loss: 0.44858 | val_0_rmse: 0.62015 | val_1_rmse: 0.6209  |  0:00:05s
epoch 20 | loss: 0.4713  | val_0_rmse: 0.60061 | val_1_rmse: 0.65106 |  0:00:06s
epoch 21 | loss: 0.32996 | val_0_rmse: 0.53528 | val_1_rmse: 0.54984 |  0:00:06s
epoch 22 | loss: 0.34451 | val_0_rmse: 0.45608 | val_1_rmse: 0.49108 |  0:00:06s
epoch 23 | loss: 0.31    | val_0_rmse: 0.56651 | val_1_rmse: 0.57975 |  0:00:06s
epoch 24 | loss: 0.32899 | val_0_rmse: 0.46759 | val_1_rmse: 0.49423 |  0:00:07s
epoch 25 | loss: 0.37166 | val_0_rmse: 0.51182 | val_1_rmse: 0.5272  |  0:00:07s
epoch 26 | loss: 0.28069 | val_0_rmse: 0.43133 | val_1_rmse: 0.46456 |  0:00:07s
epoch 27 | loss: 0.25629 | val_0_rmse: 0.45047 | val_1_rmse: 0.47364 |  0:00:08s
epoch 28 | loss: 0.22634 | val_0_rmse: 0.46692 | val_1_rmse: 0.48872 |  0:00:08s
epoch 29 | loss: 0.21377 | val_0_rmse: 0.4296  | val_1_rmse: 0.45761 |  0:00:08s
epoch 30 | loss: 0.19641 | val_0_rmse: 0.44396 | val_1_rmse: 0.46797 |  0:00:09s
epoch 31 | loss: 0.20366 | val_0_rmse: 0.42795 | val_1_rmse: 0.46486 |  0:00:09s
epoch 32 | loss: 0.19485 | val_0_rmse: 0.43676 | val_1_rmse: 0.45966 |  0:00:09s
epoch 33 | loss: 0.24521 | val_0_rmse: 0.43662 | val_1_rmse: 0.47563 |  0:00:09s
epoch 34 | loss: 0.23816 | val_0_rmse: 0.44756 | val_1_rmse: 0.46618 |  0:00:10s
epoch 35 | loss: 0.195   | val_0_rmse: 0.41708 | val_1_rmse: 0.45063 |  0:00:10s
epoch 36 | loss: 0.18026 | val_0_rmse: 0.41191 | val_1_rmse: 0.44218 |  0:00:10s
epoch 37 | loss: 0.17698 | val_0_rmse: 0.4067  | val_1_rmse: 0.44083 |  0:00:11s
epoch 38 | loss: 0.16969 | val_0_rmse: 0.4063  | val_1_rmse: 0.44955 |  0:00:11s
epoch 39 | loss: 0.17474 | val_0_rmse: 0.40477 | val_1_rmse: 0.44394 |  0:00:11s
epoch 40 | loss: 0.19647 | val_0_rmse: 0.41699 | val_1_rmse: 0.45262 |  0:00:11s
epoch 41 | loss: 0.18446 | val_0_rmse: 0.42284 | val_1_rmse: 0.45332 |  0:00:12s
epoch 42 | loss: 0.17626 | val_0_rmse: 0.40308 | val_1_rmse: 0.4392  |  0:00:12s
epoch 43 | loss: 0.18102 | val_0_rmse: 0.4063  | val_1_rmse: 0.44036 |  0:00:12s
epoch 44 | loss: 0.18316 | val_0_rmse: 0.44698 | val_1_rmse: 0.47781 |  0:00:13s
epoch 45 | loss: 0.19868 | val_0_rmse: 0.39878 | val_1_rmse: 0.43807 |  0:00:13s
epoch 46 | loss: 0.18151 | val_0_rmse: 0.42159 | val_1_rmse: 0.46363 |  0:00:13s
epoch 47 | loss: 0.18379 | val_0_rmse: 0.39918 | val_1_rmse: 0.43468 |  0:00:13s
epoch 48 | loss: 0.16798 | val_0_rmse: 0.40961 | val_1_rmse: 0.44453 |  0:00:14s
epoch 49 | loss: 0.18735 | val_0_rmse: 0.40721 | val_1_rmse: 0.43845 |  0:00:14s
epoch 50 | loss: 0.19957 | val_0_rmse: 0.47258 | val_1_rmse: 0.49252 |  0:00:14s
epoch 51 | loss: 0.23284 | val_0_rmse: 0.41265 | val_1_rmse: 0.45119 |  0:00:15s
epoch 52 | loss: 0.20238 | val_0_rmse: 0.41206 | val_1_rmse: 0.4404  |  0:00:15s
epoch 53 | loss: 0.22465 | val_0_rmse: 0.39742 | val_1_rmse: 0.43169 |  0:00:15s
epoch 54 | loss: 0.17845 | val_0_rmse: 0.47283 | val_1_rmse: 0.50727 |  0:00:15s
epoch 55 | loss: 0.21769 | val_0_rmse: 0.43835 | val_1_rmse: 0.4598  |  0:00:16s
epoch 56 | loss: 0.18577 | val_0_rmse: 0.42915 | val_1_rmse: 0.46712 |  0:00:16s
epoch 57 | loss: 0.17321 | val_0_rmse: 0.41917 | val_1_rmse: 0.4489  |  0:00:16s
epoch 58 | loss: 0.17893 | val_0_rmse: 0.4685  | val_1_rmse: 0.49593 |  0:00:17s
epoch 59 | loss: 0.20628 | val_0_rmse: 0.45349 | val_1_rmse: 0.49157 |  0:00:17s
epoch 60 | loss: 0.24614 | val_0_rmse: 0.48751 | val_1_rmse: 0.50893 |  0:00:17s
epoch 61 | loss: 0.25556 | val_0_rmse: 0.4906  | val_1_rmse: 0.52084 |  0:00:18s
epoch 62 | loss: 0.26841 | val_0_rmse: 0.54959 | val_1_rmse: 0.57629 |  0:00:18s
epoch 63 | loss: 0.278   | val_0_rmse: 0.44063 | val_1_rmse: 0.46117 |  0:00:18s
epoch 64 | loss: 0.19983 | val_0_rmse: 0.43582 | val_1_rmse: 0.47621 |  0:00:18s
epoch 65 | loss: 0.18648 | val_0_rmse: 0.40809 | val_1_rmse: 0.43995 |  0:00:19s
epoch 66 | loss: 0.17369 | val_0_rmse: 0.41096 | val_1_rmse: 0.44587 |  0:00:19s
epoch 67 | loss: 0.17455 | val_0_rmse: 0.40801 | val_1_rmse: 0.44531 |  0:00:19s
epoch 68 | loss: 0.17579 | val_0_rmse: 0.40744 | val_1_rmse: 0.43754 |  0:00:19s
epoch 69 | loss: 0.18554 | val_0_rmse: 0.40831 | val_1_rmse: 0.44453 |  0:00:20s
epoch 70 | loss: 0.17422 | val_0_rmse: 0.41034 | val_1_rmse: 0.43443 |  0:00:20s
epoch 71 | loss: 0.17042 | val_0_rmse: 0.42717 | val_1_rmse: 0.4563  |  0:00:20s
epoch 72 | loss: 0.18244 | val_0_rmse: 0.40956 | val_1_rmse: 0.44293 |  0:00:21s
epoch 73 | loss: 0.17219 | val_0_rmse: 0.40318 | val_1_rmse: 0.43739 |  0:00:21s
epoch 74 | loss: 0.17297 | val_0_rmse: 0.40801 | val_1_rmse: 0.4389  |  0:00:21s
epoch 75 | loss: 0.16985 | val_0_rmse: 0.40958 | val_1_rmse: 0.44262 |  0:00:21s
epoch 76 | loss: 0.1738  | val_0_rmse: 0.43349 | val_1_rmse: 0.46217 |  0:00:22s
epoch 77 | loss: 0.17966 | val_0_rmse: 0.43119 | val_1_rmse: 0.46229 |  0:00:22s
epoch 78 | loss: 0.18061 | val_0_rmse: 0.42172 | val_1_rmse: 0.44721 |  0:00:22s
epoch 79 | loss: 0.1787  | val_0_rmse: 0.40229 | val_1_rmse: 0.43541 |  0:00:23s
epoch 80 | loss: 0.18296 | val_0_rmse: 0.40733 | val_1_rmse: 0.43494 |  0:00:23s
epoch 81 | loss: 0.17227 | val_0_rmse: 0.44829 | val_1_rmse: 0.47221 |  0:00:23s
epoch 82 | loss: 0.21316 | val_0_rmse: 0.45618 | val_1_rmse: 0.4921  |  0:00:24s
epoch 83 | loss: 0.19786 | val_0_rmse: 0.40521 | val_1_rmse: 0.43886 |  0:00:24s

Early stopping occured at epoch 83 with best_epoch = 53 and best_val_1_rmse = 0.43169
Best weights from best epoch are automatically used!
ended training at: 05:49:14
Feature importance:
[('Area', 0.10696251220258211), ('Baths', 0.20582573092068857), ('Beds', 0.1377993897449841), ('Latitude', 0.12718057288544732), ('Longitude', 0.19382377384626), ('Month', 0.1987541945965695), ('Year', 0.02965382580346839)]
Mean squared error is of 4240985378.5316954
Mean absolute error:43773.99940762946
MAPE:0.33573362876040425
R2 score:0.4910122894615744
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:49:14
epoch 0  | loss: 12.7207 | val_0_rmse: 0.62216 | val_1_rmse: 0.6057  |  0:00:03s
epoch 1  | loss: 0.29814 | val_0_rmse: 0.50957 | val_1_rmse: 0.49942 |  0:00:07s
epoch 2  | loss: 0.24195 | val_0_rmse: 0.464   | val_1_rmse: 0.45867 |  0:00:11s
epoch 3  | loss: 0.22065 | val_0_rmse: 0.43293 | val_1_rmse: 0.42731 |  0:00:15s
epoch 4  | loss: 0.21174 | val_0_rmse: 0.44975 | val_1_rmse: 0.44117 |  0:00:18s
epoch 5  | loss: 0.22307 | val_0_rmse: 0.45117 | val_1_rmse: 0.44493 |  0:00:22s
epoch 6  | loss: 0.20378 | val_0_rmse: 0.43314 | val_1_rmse: 0.42532 |  0:00:26s
epoch 7  | loss: 0.20388 | val_0_rmse: 0.42636 | val_1_rmse: 0.42135 |  0:00:30s
epoch 8  | loss: 0.21297 | val_0_rmse: 0.42783 | val_1_rmse: 0.42093 |  0:00:34s
epoch 9  | loss: 0.19898 | val_0_rmse: 0.45587 | val_1_rmse: 0.45231 |  0:00:37s
epoch 10 | loss: 0.21969 | val_0_rmse: 0.45276 | val_1_rmse: 0.44379 |  0:00:41s
epoch 11 | loss: 0.20212 | val_0_rmse: 0.46356 | val_1_rmse: 0.45923 |  0:00:45s
epoch 12 | loss: 0.22613 | val_0_rmse: 0.46731 | val_1_rmse: 0.45631 |  0:00:49s
epoch 13 | loss: 0.21974 | val_0_rmse: 0.43199 | val_1_rmse: 0.42361 |  0:00:53s
epoch 14 | loss: 0.20475 | val_0_rmse: 0.43542 | val_1_rmse: 0.42807 |  0:00:56s
epoch 15 | loss: 0.20188 | val_0_rmse: 0.4375  | val_1_rmse: 0.43428 |  0:01:00s
epoch 16 | loss: 0.20256 | val_0_rmse: 0.444   | val_1_rmse: 0.43509 |  0:01:04s
epoch 17 | loss: 0.19406 | val_0_rmse: 0.49759 | val_1_rmse: 0.48841 |  0:01:08s
epoch 18 | loss: 0.23813 | val_0_rmse: 0.54848 | val_1_rmse: 0.53817 |  0:01:12s
epoch 19 | loss: 0.22222 | val_0_rmse: 0.54526 | val_1_rmse: 0.53617 |  0:01:15s
epoch 20 | loss: 0.29438 | val_0_rmse: 0.47606 | val_1_rmse: 0.47395 |  0:01:19s
epoch 21 | loss: 0.26189 | val_0_rmse: 0.42856 | val_1_rmse: 0.42459 |  0:01:23s
epoch 22 | loss: 0.22753 | val_0_rmse: 0.46053 | val_1_rmse: 0.45071 |  0:01:27s
epoch 23 | loss: 0.19817 | val_0_rmse: 0.444   | val_1_rmse: 0.43371 |  0:01:30s
epoch 24 | loss: 0.21568 | val_0_rmse: 0.50837 | val_1_rmse: 0.49836 |  0:01:34s
epoch 25 | loss: 0.24655 | val_0_rmse: 0.45978 | val_1_rmse: 0.45239 |  0:01:38s
epoch 26 | loss: 0.20618 | val_0_rmse: 0.43313 | val_1_rmse: 0.42861 |  0:01:42s
epoch 27 | loss: 0.19928 | val_0_rmse: 0.45144 | val_1_rmse: 0.4392  |  0:01:46s
epoch 28 | loss: 0.20919 | val_0_rmse: 0.43911 | val_1_rmse: 0.43633 |  0:01:49s
epoch 29 | loss: 0.19465 | val_0_rmse: 0.44038 | val_1_rmse: 0.43718 |  0:01:53s
epoch 30 | loss: 0.20715 | val_0_rmse: 0.44219 | val_1_rmse: 0.43805 |  0:01:57s
epoch 31 | loss: 0.19547 | val_0_rmse: 0.42226 | val_1_rmse: 0.41548 |  0:02:01s
epoch 32 | loss: 0.21841 | val_0_rmse: 0.49702 | val_1_rmse: 0.48588 |  0:02:05s
epoch 33 | loss: 0.19627 | val_0_rmse: 0.45214 | val_1_rmse: 0.44185 |  0:02:08s
epoch 34 | loss: 0.21283 | val_0_rmse: 0.45265 | val_1_rmse: 0.44733 |  0:02:12s
epoch 35 | loss: 0.20075 | val_0_rmse: 0.42837 | val_1_rmse: 0.42035 |  0:02:16s
epoch 36 | loss: 0.17834 | val_0_rmse: 0.4172  | val_1_rmse: 0.40955 |  0:02:20s
epoch 37 | loss: 0.1857  | val_0_rmse: 0.40346 | val_1_rmse: 0.39817 |  0:02:24s
epoch 38 | loss: 0.23278 | val_0_rmse: 0.39942 | val_1_rmse: 0.39598 |  0:02:27s
epoch 39 | loss: 0.19067 | val_0_rmse: 0.44754 | val_1_rmse: 0.44915 |  0:02:31s
epoch 40 | loss: 0.17274 | val_0_rmse: 0.44913 | val_1_rmse: 0.45102 |  0:02:35s
epoch 41 | loss: 0.2024  | val_0_rmse: 0.45164 | val_1_rmse: 0.4262  |  0:02:39s
epoch 42 | loss: 0.17763 | val_0_rmse: 0.4278  | val_1_rmse: 0.42686 |  0:02:43s
epoch 43 | loss: 0.17151 | val_0_rmse: 0.40329 | val_1_rmse: 0.39588 |  0:02:46s
epoch 44 | loss: 0.1739  | val_0_rmse: 0.41359 | val_1_rmse: 0.40862 |  0:02:50s
epoch 45 | loss: 0.17211 | val_0_rmse: 0.42465 | val_1_rmse: 0.42008 |  0:02:54s
epoch 46 | loss: 0.19141 | val_0_rmse: 0.3895  | val_1_rmse: 0.3821  |  0:02:58s
epoch 47 | loss: 0.16768 | val_0_rmse: 0.3959  | val_1_rmse: 0.39292 |  0:03:02s
epoch 48 | loss: 0.17578 | val_0_rmse: 0.38433 | val_1_rmse: 0.3785  |  0:03:05s
epoch 49 | loss: 0.18046 | val_0_rmse: 0.40068 | val_1_rmse: 0.39238 |  0:03:09s
epoch 50 | loss: 0.16285 | val_0_rmse: 0.41487 | val_1_rmse: 0.40712 |  0:03:13s
epoch 51 | loss: 0.15919 | val_0_rmse: 0.38746 | val_1_rmse: 0.38253 |  0:03:17s
epoch 52 | loss: 0.1612  | val_0_rmse: 0.38456 | val_1_rmse: 0.38033 |  0:03:21s
epoch 53 | loss: 0.18154 | val_0_rmse: 0.39493 | val_1_rmse: 0.39048 |  0:03:24s
epoch 54 | loss: 0.17363 | val_0_rmse: 0.40824 | val_1_rmse: 0.40549 |  0:03:28s
epoch 55 | loss: 0.17844 | val_0_rmse: 0.4765  | val_1_rmse: 0.46608 |  0:03:32s
epoch 56 | loss: 0.16126 | val_0_rmse: 0.3835  | val_1_rmse: 0.37773 |  0:03:36s
epoch 57 | loss: 0.16862 | val_0_rmse: 0.39611 | val_1_rmse: 0.38779 |  0:03:40s
epoch 58 | loss: 0.16274 | val_0_rmse: 0.39694 | val_1_rmse: 0.38977 |  0:03:43s
epoch 59 | loss: 0.17699 | val_0_rmse: 0.4003  | val_1_rmse: 0.39754 |  0:03:47s
epoch 60 | loss: 0.17546 | val_0_rmse: 0.3922  | val_1_rmse: 0.38481 |  0:03:51s
epoch 61 | loss: 0.17113 | val_0_rmse: 0.40561 | val_1_rmse: 0.39791 |  0:03:55s
epoch 62 | loss: 0.15761 | val_0_rmse: 0.46302 | val_1_rmse: 0.45301 |  0:03:59s
epoch 63 | loss: 0.17965 | val_0_rmse: 0.38058 | val_1_rmse: 0.37513 |  0:04:02s
epoch 64 | loss: 0.15682 | val_0_rmse: 0.41521 | val_1_rmse: 0.40689 |  0:04:06s
epoch 65 | loss: 0.15899 | val_0_rmse: 0.39798 | val_1_rmse: 0.3938  |  0:04:10s
epoch 66 | loss: 0.19514 | val_0_rmse: 0.38717 | val_1_rmse: 0.37957 |  0:04:14s
epoch 67 | loss: 0.17978 | val_0_rmse: 0.42334 | val_1_rmse: 0.41794 |  0:04:18s
epoch 68 | loss: 0.15627 | val_0_rmse: 0.38214 | val_1_rmse: 0.37594 |  0:04:21s
epoch 69 | loss: 0.15889 | val_0_rmse: 0.38509 | val_1_rmse: 0.3811  |  0:04:25s
epoch 70 | loss: 0.16546 | val_0_rmse: 0.41886 | val_1_rmse: 0.41023 |  0:04:29s
epoch 71 | loss: 0.15994 | val_0_rmse: 0.38216 | val_1_rmse: 0.37552 |  0:04:33s
epoch 72 | loss: 0.1613  | val_0_rmse: 0.38705 | val_1_rmse: 0.3837  |  0:04:37s
epoch 73 | loss: 0.15747 | val_0_rmse: 0.40675 | val_1_rmse: 0.40359 |  0:04:40s
epoch 74 | loss: 0.1649  | val_0_rmse: 0.38114 | val_1_rmse: 0.37782 |  0:04:44s
epoch 75 | loss: 0.16884 | val_0_rmse: 0.46945 | val_1_rmse: 0.4588  |  0:04:48s
epoch 76 | loss: 0.15818 | val_0_rmse: 0.41107 | val_1_rmse: 0.4039  |  0:04:52s
epoch 77 | loss: 0.15546 | val_0_rmse: 0.38948 | val_1_rmse: 0.38271 |  0:04:55s
epoch 78 | loss: 0.16743 | val_0_rmse: 0.39326 | val_1_rmse: 0.38958 |  0:04:59s
epoch 79 | loss: 0.16309 | val_0_rmse: 0.38447 | val_1_rmse: 0.38066 |  0:05:03s
epoch 80 | loss: 0.16353 | val_0_rmse: 0.39545 | val_1_rmse: 0.39416 |  0:05:07s
epoch 81 | loss: 0.15192 | val_0_rmse: 0.4002  | val_1_rmse: 0.39506 |  0:05:11s
epoch 82 | loss: 0.15907 | val_0_rmse: 0.38834 | val_1_rmse: 0.38265 |  0:05:14s
epoch 83 | loss: 0.16793 | val_0_rmse: 0.39118 | val_1_rmse: 0.38778 |  0:05:18s
epoch 84 | loss: 0.16052 | val_0_rmse: 0.38929 | val_1_rmse: 0.38342 |  0:05:22s
epoch 85 | loss: 0.15522 | val_0_rmse: 0.38642 | val_1_rmse: 0.38165 |  0:05:26s
epoch 86 | loss: 0.17437 | val_0_rmse: 0.39558 | val_1_rmse: 0.38876 |  0:05:30s
epoch 87 | loss: 0.18827 | val_0_rmse: 0.43309 | val_1_rmse: 0.42687 |  0:05:33s
epoch 88 | loss: 0.159   | val_0_rmse: 0.42443 | val_1_rmse: 0.41657 |  0:05:37s
epoch 89 | loss: 0.15365 | val_0_rmse: 0.41429 | val_1_rmse: 0.40635 |  0:05:41s
epoch 90 | loss: 0.15291 | val_0_rmse: 0.40204 | val_1_rmse: 0.39458 |  0:05:45s
epoch 91 | loss: 0.16452 | val_0_rmse: 0.4216  | val_1_rmse: 0.41472 |  0:05:48s
epoch 92 | loss: 0.16115 | val_0_rmse: 0.39125 | val_1_rmse: 0.38754 |  0:05:52s
epoch 93 | loss: 0.15702 | val_0_rmse: 0.48136 | val_1_rmse: 0.47253 |  0:05:56s

Early stopping occured at epoch 93 with best_epoch = 63 and best_val_1_rmse = 0.37513
Best weights from best epoch are automatically used!
ended training at: 05:55:11
Feature importance:
[('Area', 0.3506725239326785), ('Baths', 0.0), ('Beds', 0.21227288981518536), ('Latitude', 0.0), ('Longitude', 0.2639082259245013), ('Month', 0.17314636032763486), ('Year', 0.0)]
Mean squared error is of 1035049420.5887653
Mean absolute error:21269.979868628667
MAPE:0.3105255338302873
R2 score:0.6955473892728967
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:55:12
epoch 0  | loss: 12.8373 | val_0_rmse: 0.64327 | val_1_rmse: 0.64809 |  0:00:03s
epoch 1  | loss: 0.27648 | val_0_rmse: 0.48981 | val_1_rmse: 0.48723 |  0:00:07s
epoch 2  | loss: 0.22095 | val_0_rmse: 0.45665 | val_1_rmse: 0.456   |  0:00:11s
epoch 3  | loss: 0.22395 | val_0_rmse: 0.40423 | val_1_rmse: 0.40767 |  0:00:15s
epoch 4  | loss: 0.20993 | val_0_rmse: 0.51629 | val_1_rmse: 0.51753 |  0:00:19s
epoch 5  | loss: 0.20393 | val_0_rmse: 0.40914 | val_1_rmse: 0.41422 |  0:00:22s
epoch 6  | loss: 0.18004 | val_0_rmse: 0.41129 | val_1_rmse: 0.41358 |  0:00:26s
epoch 7  | loss: 0.17158 | val_0_rmse: 0.43572 | val_1_rmse: 0.44182 |  0:00:30s
epoch 8  | loss: 0.18844 | val_0_rmse: 0.40881 | val_1_rmse: 0.41338 |  0:00:34s
epoch 9  | loss: 0.19032 | val_0_rmse: 0.39366 | val_1_rmse: 0.39832 |  0:00:38s
epoch 10 | loss: 0.18048 | val_0_rmse: 0.41835 | val_1_rmse: 0.4227  |  0:00:41s
epoch 11 | loss: 0.17564 | val_0_rmse: 0.48045 | val_1_rmse: 0.4768  |  0:00:45s
epoch 12 | loss: 0.16679 | val_0_rmse: 0.48188 | val_1_rmse: 0.47505 |  0:00:49s
epoch 13 | loss: 0.16618 | val_0_rmse: 0.45377 | val_1_rmse: 0.46061 |  0:00:53s
epoch 14 | loss: 0.15808 | val_0_rmse: 0.45283 | val_1_rmse: 0.46078 |  0:00:57s
epoch 15 | loss: 0.17312 | val_0_rmse: 0.39503 | val_1_rmse: 0.40155 |  0:01:00s
epoch 16 | loss: 0.17546 | val_0_rmse: 0.39559 | val_1_rmse: 0.39632 |  0:01:04s
epoch 17 | loss: 0.16634 | val_0_rmse: 0.49216 | val_1_rmse: 0.48595 |  0:01:08s
epoch 18 | loss: 0.15575 | val_0_rmse: 0.47138 | val_1_rmse: 0.48033 |  0:01:12s
epoch 19 | loss: 0.1861  | val_0_rmse: 0.38817 | val_1_rmse: 0.38918 |  0:01:16s
epoch 20 | loss: 0.16124 | val_0_rmse: 0.51905 | val_1_rmse: 0.52905 |  0:01:19s
epoch 21 | loss: 0.17518 | val_0_rmse: 0.3963  | val_1_rmse: 0.4031  |  0:01:23s
epoch 22 | loss: 0.15726 | val_0_rmse: 0.41314 | val_1_rmse: 0.4092  |  0:01:27s
epoch 23 | loss: 0.15585 | val_0_rmse: 0.39934 | val_1_rmse: 0.39923 |  0:01:31s
epoch 24 | loss: 0.1663  | val_0_rmse: 0.62484 | val_1_rmse: 0.61396 |  0:01:35s
epoch 25 | loss: 0.20147 | val_0_rmse: 0.38456 | val_1_rmse: 0.38843 |  0:01:38s
epoch 26 | loss: 0.22607 | val_0_rmse: 0.55174 | val_1_rmse: 0.56064 |  0:01:42s
epoch 27 | loss: 0.22389 | val_0_rmse: 0.43102 | val_1_rmse: 0.4321  |  0:01:46s
epoch 28 | loss: 0.2016  | val_0_rmse: 0.41174 | val_1_rmse: 0.41619 |  0:01:50s
epoch 29 | loss: 0.1647  | val_0_rmse: 0.43673 | val_1_rmse: 0.44263 |  0:01:54s
epoch 30 | loss: 0.16925 | val_0_rmse: 0.43557 | val_1_rmse: 0.44337 |  0:01:57s
epoch 31 | loss: 0.16477 | val_0_rmse: 0.49064 | val_1_rmse: 0.49842 |  0:02:01s
epoch 32 | loss: 0.17535 | val_0_rmse: 0.40271 | val_1_rmse: 0.40484 |  0:02:05s
epoch 33 | loss: 0.16096 | val_0_rmse: 0.54916 | val_1_rmse: 0.54122 |  0:02:09s
epoch 34 | loss: 0.18366 | val_0_rmse: 0.42803 | val_1_rmse: 0.42633 |  0:02:13s
epoch 35 | loss: 0.22424 | val_0_rmse: 0.38382 | val_1_rmse: 0.3883  |  0:02:16s
epoch 36 | loss: 0.17501 | val_0_rmse: 0.40195 | val_1_rmse: 0.41076 |  0:02:20s
epoch 37 | loss: 0.15901 | val_0_rmse: 0.39135 | val_1_rmse: 0.3915  |  0:02:24s
epoch 38 | loss: 0.19169 | val_0_rmse: 0.39196 | val_1_rmse: 0.39706 |  0:02:28s
epoch 39 | loss: 0.15879 | val_0_rmse: 0.41073 | val_1_rmse: 0.4183  |  0:02:32s
epoch 40 | loss: 0.16105 | val_0_rmse: 0.42263 | val_1_rmse: 0.4306  |  0:02:36s
epoch 41 | loss: 0.15656 | val_0_rmse: 0.37651 | val_1_rmse: 0.38037 |  0:02:39s
epoch 42 | loss: 0.16752 | val_0_rmse: 0.4084  | val_1_rmse: 0.41691 |  0:02:43s
epoch 43 | loss: 0.16115 | val_0_rmse: 0.38751 | val_1_rmse: 0.39416 |  0:02:47s
epoch 44 | loss: 0.17128 | val_0_rmse: 0.43501 | val_1_rmse: 0.44452 |  0:02:51s
epoch 45 | loss: 0.1722  | val_0_rmse: 0.38458 | val_1_rmse: 0.38765 |  0:02:55s
epoch 46 | loss: 0.17043 | val_0_rmse: 0.42175 | val_1_rmse: 0.41909 |  0:02:58s
epoch 47 | loss: 0.17189 | val_0_rmse: 0.4595  | val_1_rmse: 0.46086 |  0:03:02s
epoch 48 | loss: 0.16778 | val_0_rmse: 0.41976 | val_1_rmse: 0.41837 |  0:03:06s
epoch 49 | loss: 0.15525 | val_0_rmse: 0.39803 | val_1_rmse: 0.40398 |  0:03:10s
epoch 50 | loss: 0.15261 | val_0_rmse: 0.38309 | val_1_rmse: 0.38807 |  0:03:14s
epoch 51 | loss: 0.15888 | val_0_rmse: 0.5019  | val_1_rmse: 0.5122  |  0:03:17s
epoch 52 | loss: 0.17569 | val_0_rmse: 0.59083 | val_1_rmse: 0.581   |  0:03:21s
epoch 53 | loss: 0.15244 | val_0_rmse: 0.41659 | val_1_rmse: 0.42372 |  0:03:25s
epoch 54 | loss: 0.1636  | val_0_rmse: 0.41629 | val_1_rmse: 0.42064 |  0:03:29s
epoch 55 | loss: 0.16181 | val_0_rmse: 0.40236 | val_1_rmse: 0.40662 |  0:03:33s
epoch 56 | loss: 0.18331 | val_0_rmse: 0.45732 | val_1_rmse: 0.4635  |  0:03:36s
epoch 57 | loss: 0.18514 | val_0_rmse: 0.38818 | val_1_rmse: 0.38842 |  0:03:40s
epoch 58 | loss: 0.14907 | val_0_rmse: 0.37791 | val_1_rmse: 0.38438 |  0:03:44s
epoch 59 | loss: 0.15589 | val_0_rmse: 0.39269 | val_1_rmse: 0.39325 |  0:03:48s
epoch 60 | loss: 0.14918 | val_0_rmse: 0.40731 | val_1_rmse: 0.41543 |  0:03:52s
epoch 61 | loss: 0.14815 | val_0_rmse: 0.37539 | val_1_rmse: 0.38074 |  0:03:55s
epoch 62 | loss: 0.15441 | val_0_rmse: 0.44318 | val_1_rmse: 0.4518  |  0:03:59s
epoch 63 | loss: 0.15199 | val_0_rmse: 0.43837 | val_1_rmse: 0.44816 |  0:04:03s
epoch 64 | loss: 0.15465 | val_0_rmse: 0.38404 | val_1_rmse: 0.38923 |  0:04:07s
epoch 65 | loss: 0.1694  | val_0_rmse: 0.42203 | val_1_rmse: 0.42912 |  0:04:11s
epoch 66 | loss: 0.15902 | val_0_rmse: 0.39386 | val_1_rmse: 0.3944  |  0:04:14s
epoch 67 | loss: 0.15905 | val_0_rmse: 0.3746  | val_1_rmse: 0.3787  |  0:04:18s
epoch 68 | loss: 0.16562 | val_0_rmse: 0.40698 | val_1_rmse: 0.40878 |  0:04:22s
epoch 69 | loss: 0.16981 | val_0_rmse: 0.40224 | val_1_rmse: 0.40295 |  0:04:26s
epoch 70 | loss: 0.16963 | val_0_rmse: 0.63819 | val_1_rmse: 0.6294  |  0:04:30s
epoch 71 | loss: 0.1768  | val_0_rmse: 0.39957 | val_1_rmse: 0.40639 |  0:04:33s
epoch 72 | loss: 0.15613 | val_0_rmse: 0.38598 | val_1_rmse: 0.39311 |  0:04:37s
epoch 73 | loss: 0.14744 | val_0_rmse: 0.39188 | val_1_rmse: 0.39965 |  0:04:41s
epoch 74 | loss: 0.1691  | val_0_rmse: 0.41179 | val_1_rmse: 0.41728 |  0:04:45s
epoch 75 | loss: 0.1562  | val_0_rmse: 0.38185 | val_1_rmse: 0.38927 |  0:04:49s
epoch 76 | loss: 0.15527 | val_0_rmse: 0.3849  | val_1_rmse: 0.38461 |  0:04:52s
epoch 77 | loss: 0.15106 | val_0_rmse: 0.43053 | val_1_rmse: 0.43996 |  0:04:56s
epoch 78 | loss: 0.14835 | val_0_rmse: 0.37812 | val_1_rmse: 0.38303 |  0:05:00s
epoch 79 | loss: 0.16432 | val_0_rmse: 0.42083 | val_1_rmse: 0.42006 |  0:05:04s
epoch 80 | loss: 0.17097 | val_0_rmse: 0.40967 | val_1_rmse: 0.41568 |  0:05:08s
epoch 81 | loss: 0.15726 | val_0_rmse: 0.44574 | val_1_rmse: 0.4422  |  0:05:11s
epoch 82 | loss: 0.15993 | val_0_rmse: 0.38952 | val_1_rmse: 0.38867 |  0:05:15s
epoch 83 | loss: 0.14811 | val_0_rmse: 0.38343 | val_1_rmse: 0.38441 |  0:05:19s
epoch 84 | loss: 0.14963 | val_0_rmse: 0.39683 | val_1_rmse: 0.40447 |  0:05:23s
epoch 85 | loss: 0.19831 | val_0_rmse: 0.41929 | val_1_rmse: 0.41742 |  0:05:26s
epoch 86 | loss: 0.16577 | val_0_rmse: 0.58336 | val_1_rmse: 0.57351 |  0:05:30s
epoch 87 | loss: 0.16826 | val_0_rmse: 0.37907 | val_1_rmse: 0.38537 |  0:05:34s
epoch 88 | loss: 0.17713 | val_0_rmse: 0.64946 | val_1_rmse: 0.64007 |  0:05:38s
epoch 89 | loss: 0.18081 | val_0_rmse: 0.38355 | val_1_rmse: 0.38881 |  0:05:42s
epoch 90 | loss: 0.201   | val_0_rmse: 0.43299 | val_1_rmse: 0.42848 |  0:05:45s
epoch 91 | loss: 0.16171 | val_0_rmse: 0.42469 | val_1_rmse: 0.43228 |  0:05:49s
epoch 92 | loss: 0.16527 | val_0_rmse: 0.38497 | val_1_rmse: 0.38878 |  0:05:53s
epoch 93 | loss: 0.15062 | val_0_rmse: 0.41019 | val_1_rmse: 0.41674 |  0:05:57s
epoch 94 | loss: 0.1495  | val_0_rmse: 0.3735  | val_1_rmse: 0.37784 |  0:06:01s
epoch 95 | loss: 0.15375 | val_0_rmse: 0.42881 | val_1_rmse: 0.42528 |  0:06:04s
epoch 96 | loss: 0.15189 | val_0_rmse: 0.3894  | val_1_rmse: 0.39413 |  0:06:08s
epoch 97 | loss: 0.15988 | val_0_rmse: 0.3797  | val_1_rmse: 0.38381 |  0:06:12s
epoch 98 | loss: 0.14629 | val_0_rmse: 0.39022 | val_1_rmse: 0.39661 |  0:06:16s
epoch 99 | loss: 0.1652  | val_0_rmse: 0.40715 | val_1_rmse: 0.40591 |  0:06:20s
epoch 100| loss: 0.1553  | val_0_rmse: 0.38756 | val_1_rmse: 0.39272 |  0:06:23s
epoch 101| loss: 0.15878 | val_0_rmse: 0.41907 | val_1_rmse: 0.4213  |  0:06:27s
epoch 102| loss: 0.1565  | val_0_rmse: 0.3818  | val_1_rmse: 0.38954 |  0:06:31s
epoch 103| loss: 0.15321 | val_0_rmse: 0.37706 | val_1_rmse: 0.38112 |  0:06:35s
epoch 104| loss: 0.15118 | val_0_rmse: 0.60764 | val_1_rmse: 0.59725 |  0:06:39s
epoch 105| loss: 0.16001 | val_0_rmse: 0.39449 | val_1_rmse: 0.39897 |  0:06:42s
epoch 106| loss: 0.16386 | val_0_rmse: 0.38816 | val_1_rmse: 0.3911  |  0:06:46s
epoch 107| loss: 0.15999 | val_0_rmse: 0.43292 | val_1_rmse: 0.42957 |  0:06:50s
epoch 108| loss: 0.16419 | val_0_rmse: 0.40367 | val_1_rmse: 0.40631 |  0:06:54s
epoch 109| loss: 0.16399 | val_0_rmse: 0.41105 | val_1_rmse: 0.4087  |  0:06:58s
epoch 110| loss: 0.16099 | val_0_rmse: 0.4204  | val_1_rmse: 0.42036 |  0:07:01s
epoch 111| loss: 0.1667  | val_0_rmse: 0.37381 | val_1_rmse: 0.37772 |  0:07:05s
epoch 112| loss: 0.15586 | val_0_rmse: 0.38914 | val_1_rmse: 0.38834 |  0:07:09s
epoch 113| loss: 0.15756 | val_0_rmse: 0.40938 | val_1_rmse: 0.41587 |  0:07:13s
epoch 114| loss: 0.16844 | val_0_rmse: 0.37142 | val_1_rmse: 0.3757  |  0:07:17s
epoch 115| loss: 0.15644 | val_0_rmse: 0.39642 | val_1_rmse: 0.39751 |  0:07:20s
epoch 116| loss: 0.16743 | val_0_rmse: 0.38705 | val_1_rmse: 0.39227 |  0:07:24s
epoch 117| loss: 0.17019 | val_0_rmse: 0.39773 | val_1_rmse: 0.40399 |  0:07:28s
epoch 118| loss: 0.1482  | val_0_rmse: 0.38249 | val_1_rmse: 0.38848 |  0:07:32s
epoch 119| loss: 0.19042 | val_0_rmse: 0.39629 | val_1_rmse: 0.4008  |  0:07:36s
epoch 120| loss: 0.16411 | val_0_rmse: 0.41333 | val_1_rmse: 0.4214  |  0:07:39s
epoch 121| loss: 0.1512  | val_0_rmse: 0.39244 | val_1_rmse: 0.39963 |  0:07:43s
epoch 122| loss: 0.14833 | val_0_rmse: 0.42964 | val_1_rmse: 0.43624 |  0:07:47s
epoch 123| loss: 0.15993 | val_0_rmse: 0.37784 | val_1_rmse: 0.38086 |  0:07:51s
epoch 124| loss: 0.15599 | val_0_rmse: 0.41438 | val_1_rmse: 0.42243 |  0:07:55s
epoch 125| loss: 0.1488  | val_0_rmse: 0.37489 | val_1_rmse: 0.38005 |  0:07:58s
epoch 126| loss: 0.16606 | val_0_rmse: 0.49829 | val_1_rmse: 0.50786 |  0:08:02s
epoch 127| loss: 0.16563 | val_0_rmse: 0.37734 | val_1_rmse: 0.38202 |  0:08:06s
epoch 128| loss: 0.15196 | val_0_rmse: 0.39272 | val_1_rmse: 0.39698 |  0:08:10s
epoch 129| loss: 0.1706  | val_0_rmse: 1.09211 | val_1_rmse: 1.07791 |  0:08:14s
epoch 130| loss: 0.16721 | val_0_rmse: 0.38894 | val_1_rmse: 0.38921 |  0:08:17s
epoch 131| loss: 0.15362 | val_0_rmse: 0.39884 | val_1_rmse: 0.40019 |  0:08:21s
epoch 132| loss: 0.17874 | val_0_rmse: 0.4169  | val_1_rmse: 0.42072 |  0:08:25s
epoch 133| loss: 0.17186 | val_0_rmse: 0.39187 | val_1_rmse: 0.39566 |  0:08:29s
epoch 134| loss: 0.1601  | val_0_rmse: 0.48142 | val_1_rmse: 0.47373 |  0:08:33s
epoch 135| loss: 0.21467 | val_0_rmse: 0.41097 | val_1_rmse: 0.41377 |  0:08:36s
epoch 136| loss: 0.17755 | val_0_rmse: 0.39443 | val_1_rmse: 0.39654 |  0:08:40s
epoch 137| loss: 0.17446 | val_0_rmse: 0.40272 | val_1_rmse: 0.40809 |  0:08:44s
epoch 138| loss: 0.16018 | val_0_rmse: 0.43627 | val_1_rmse: 0.44325 |  0:08:48s
epoch 139| loss: 0.16096 | val_0_rmse: 0.40376 | val_1_rmse: 0.40691 |  0:08:52s
epoch 140| loss: 0.16819 | val_0_rmse: 0.40402 | val_1_rmse: 0.40766 |  0:08:56s
epoch 141| loss: 0.16594 | val_0_rmse: 0.39153 | val_1_rmse: 0.39511 |  0:08:59s
epoch 142| loss: 0.16249 | val_0_rmse: 0.39956 | val_1_rmse: 0.40078 |  0:09:03s
epoch 143| loss: 0.16131 | val_0_rmse: 0.39006 | val_1_rmse: 0.39142 |  0:09:07s
epoch 144| loss: 0.16661 | val_0_rmse: 0.42595 | val_1_rmse: 0.43217 |  0:09:11s

Early stopping occured at epoch 144 with best_epoch = 114 and best_val_1_rmse = 0.3757
Best weights from best epoch are automatically used!
ended training at: 06:04:25
Feature importance:
[('Area', 0.3496719396654398), ('Baths', 0.17589814833200684), ('Beds', 0.10847451783406835), ('Latitude', 0.1269240478690503), ('Longitude', 0.20496579566521345), ('Month', 0.0), ('Year', 0.03406555063422132)]
Mean squared error is of 2712381959.930049
Mean absolute error:20991.18670040573
MAPE:0.3095840972212596
R2 score:0.18978876139599288
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:04:25
epoch 0  | loss: 12.52805| val_0_rmse: 0.58506 | val_1_rmse: 0.57924 |  0:00:03s
epoch 1  | loss: 0.31309 | val_0_rmse: 0.4731  | val_1_rmse: 0.47058 |  0:00:07s
epoch 2  | loss: 0.23995 | val_0_rmse: 0.47688 | val_1_rmse: 0.47528 |  0:00:11s
epoch 3  | loss: 0.24122 | val_0_rmse: 0.44449 | val_1_rmse: 0.44105 |  0:00:15s
epoch 4  | loss: 0.20916 | val_0_rmse: 0.44312 | val_1_rmse: 0.44554 |  0:00:19s
epoch 5  | loss: 0.20153 | val_0_rmse: 0.43999 | val_1_rmse: 0.43738 |  0:00:22s
epoch 6  | loss: 0.2315  | val_0_rmse: 0.49311 | val_1_rmse: 0.48847 |  0:00:26s
epoch 7  | loss: 0.21584 | val_0_rmse: 0.45048 | val_1_rmse: 0.45119 |  0:00:30s
epoch 8  | loss: 0.18995 | val_0_rmse: 0.41228 | val_1_rmse: 0.41376 |  0:00:34s
epoch 9  | loss: 0.18923 | val_0_rmse: 0.51115 | val_1_rmse: 0.5077  |  0:00:38s
epoch 10 | loss: 0.19928 | val_0_rmse: 0.43512 | val_1_rmse: 0.43465 |  0:00:41s
epoch 11 | loss: 0.18057 | val_0_rmse: 0.45247 | val_1_rmse: 0.45382 |  0:00:45s
epoch 12 | loss: 0.19444 | val_0_rmse: 0.40382 | val_1_rmse: 0.40214 |  0:00:49s
epoch 13 | loss: 0.19074 | val_0_rmse: 0.41726 | val_1_rmse: 0.41709 |  0:00:53s
epoch 14 | loss: 0.19416 | val_0_rmse: 0.41034 | val_1_rmse: 0.41039 |  0:00:57s
epoch 15 | loss: 0.18942 | val_0_rmse: 0.40446 | val_1_rmse: 0.40142 |  0:01:00s
epoch 16 | loss: 0.16823 | val_0_rmse: 0.4011  | val_1_rmse: 0.40122 |  0:01:04s
epoch 17 | loss: 0.16821 | val_0_rmse: 0.41058 | val_1_rmse: 0.41208 |  0:01:08s
epoch 18 | loss: 0.18541 | val_0_rmse: 0.41898 | val_1_rmse: 0.42162 |  0:01:12s
epoch 19 | loss: 0.16677 | val_0_rmse: 0.39291 | val_1_rmse: 0.39264 |  0:01:15s
epoch 20 | loss: 0.16448 | val_0_rmse: 0.50629 | val_1_rmse: 0.51091 |  0:01:19s
epoch 21 | loss: 0.16839 | val_0_rmse: 0.45442 | val_1_rmse: 0.45522 |  0:01:23s
epoch 22 | loss: 0.16781 | val_0_rmse: 0.436   | val_1_rmse: 0.43555 |  0:01:27s
epoch 23 | loss: 0.1933  | val_0_rmse: 0.39469 | val_1_rmse: 0.39379 |  0:01:31s
epoch 24 | loss: 0.17599 | val_0_rmse: 0.45157 | val_1_rmse: 0.45119 |  0:01:34s
epoch 25 | loss: 0.21241 | val_0_rmse: 0.45537 | val_1_rmse: 0.45495 |  0:01:38s
epoch 26 | loss: 0.18883 | val_0_rmse: 0.39533 | val_1_rmse: 0.39524 |  0:01:42s
epoch 27 | loss: 0.17746 | val_0_rmse: 0.41254 | val_1_rmse: 0.41458 |  0:01:46s
epoch 28 | loss: 0.1677  | val_0_rmse: 0.52566 | val_1_rmse: 0.52656 |  0:01:50s
epoch 29 | loss: 0.18625 | val_0_rmse: 0.47345 | val_1_rmse: 0.47329 |  0:01:53s
epoch 30 | loss: 0.20188 | val_0_rmse: 0.41271 | val_1_rmse: 0.40877 |  0:01:57s
epoch 31 | loss: 0.23987 | val_0_rmse: 0.44981 | val_1_rmse: 0.44618 |  0:02:01s
epoch 32 | loss: 0.23427 | val_0_rmse: 0.45805 | val_1_rmse: 0.46011 |  0:02:05s
epoch 33 | loss: 0.22323 | val_0_rmse: 0.49033 | val_1_rmse: 0.48656 |  0:02:08s
epoch 34 | loss: 0.22099 | val_0_rmse: 0.4781  | val_1_rmse: 0.47749 |  0:02:12s
epoch 35 | loss: 0.1695  | val_0_rmse: 0.45891 | val_1_rmse: 0.45942 |  0:02:16s
epoch 36 | loss: 0.17605 | val_0_rmse: 0.4124  | val_1_rmse: 0.40861 |  0:02:20s
epoch 37 | loss: 0.17888 | val_0_rmse: 0.46111 | val_1_rmse: 0.46295 |  0:02:24s
epoch 38 | loss: 0.22051 | val_0_rmse: 0.51075 | val_1_rmse: 0.50601 |  0:02:27s
epoch 39 | loss: 0.22675 | val_0_rmse: 0.42111 | val_1_rmse: 0.42231 |  0:02:31s
epoch 40 | loss: 0.22028 | val_0_rmse: 0.40362 | val_1_rmse: 0.40216 |  0:02:35s
epoch 41 | loss: 0.21761 | val_0_rmse: 0.38811 | val_1_rmse: 0.38706 |  0:02:39s
epoch 42 | loss: 0.1596  | val_0_rmse: 0.44509 | val_1_rmse: 0.44286 |  0:02:43s
epoch 43 | loss: 0.17045 | val_0_rmse: 0.38768 | val_1_rmse: 0.38788 |  0:02:46s
epoch 44 | loss: 0.16296 | val_0_rmse: 0.39513 | val_1_rmse: 0.39533 |  0:02:50s
epoch 45 | loss: 0.1572  | val_0_rmse: 0.38809 | val_1_rmse: 0.38664 |  0:02:54s
epoch 46 | loss: 0.1607  | val_0_rmse: 0.40653 | val_1_rmse: 0.40599 |  0:02:58s
epoch 47 | loss: 0.17899 | val_0_rmse: 0.44189 | val_1_rmse: 0.44381 |  0:03:02s
epoch 48 | loss: 0.17863 | val_0_rmse: 0.42373 | val_1_rmse: 0.42279 |  0:03:05s
epoch 49 | loss: 0.17457 | val_0_rmse: 0.40297 | val_1_rmse: 0.40408 |  0:03:09s
epoch 50 | loss: 0.16676 | val_0_rmse: 0.46872 | val_1_rmse: 0.47063 |  0:03:13s
epoch 51 | loss: 0.16655 | val_0_rmse: 0.39779 | val_1_rmse: 0.39666 |  0:03:17s
epoch 52 | loss: 0.16082 | val_0_rmse: 0.39896 | val_1_rmse: 0.39645 |  0:03:21s
epoch 53 | loss: 0.16278 | val_0_rmse: 0.39414 | val_1_rmse: 0.39357 |  0:03:24s
epoch 54 | loss: 0.1558  | val_0_rmse: 0.41222 | val_1_rmse: 0.41327 |  0:03:28s
epoch 55 | loss: 0.16195 | val_0_rmse: 0.43562 | val_1_rmse: 0.43706 |  0:03:32s
epoch 56 | loss: 0.1737  | val_0_rmse: 0.41337 | val_1_rmse: 0.41062 |  0:03:36s
epoch 57 | loss: 0.15984 | val_0_rmse: 0.41389 | val_1_rmse: 0.41201 |  0:03:40s
epoch 58 | loss: 0.17522 | val_0_rmse: 0.39633 | val_1_rmse: 0.3951  |  0:03:43s
epoch 59 | loss: 0.17647 | val_0_rmse: 0.4518  | val_1_rmse: 0.45337 |  0:03:47s
epoch 60 | loss: 0.17351 | val_0_rmse: 0.38673 | val_1_rmse: 0.38776 |  0:03:51s
epoch 61 | loss: 0.15503 | val_0_rmse: 0.38356 | val_1_rmse: 0.3839  |  0:03:55s
epoch 62 | loss: 0.1583  | val_0_rmse: 0.50173 | val_1_rmse: 0.50589 |  0:03:59s
epoch 63 | loss: 0.17723 | val_0_rmse: 0.38897 | val_1_rmse: 0.38984 |  0:04:02s
epoch 64 | loss: 0.16274 | val_0_rmse: 0.40946 | val_1_rmse: 0.41124 |  0:04:06s
epoch 65 | loss: 0.16203 | val_0_rmse: 0.46366 | val_1_rmse: 0.463   |  0:04:10s
epoch 66 | loss: 0.16814 | val_0_rmse: 0.39715 | val_1_rmse: 0.39865 |  0:04:14s
epoch 67 | loss: 0.16165 | val_0_rmse: 0.41112 | val_1_rmse: 0.41179 |  0:04:18s
epoch 68 | loss: 0.16057 | val_0_rmse: 0.43707 | val_1_rmse: 0.43622 |  0:04:21s
epoch 69 | loss: 0.16547 | val_0_rmse: 0.40776 | val_1_rmse: 0.4055  |  0:04:25s
epoch 70 | loss: 0.15684 | val_0_rmse: 0.39449 | val_1_rmse: 0.39343 |  0:04:29s
epoch 71 | loss: 0.17587 | val_0_rmse: 0.49247 | val_1_rmse: 0.49278 |  0:04:33s
epoch 72 | loss: 0.15941 | val_0_rmse: 0.40477 | val_1_rmse: 0.40618 |  0:04:37s
epoch 73 | loss: 0.16405 | val_0_rmse: 0.4461  | val_1_rmse: 0.44857 |  0:04:40s
epoch 74 | loss: 0.15876 | val_0_rmse: 0.3915  | val_1_rmse: 0.39058 |  0:04:44s
epoch 75 | loss: 0.15619 | val_0_rmse: 0.40164 | val_1_rmse: 0.40223 |  0:04:48s
epoch 76 | loss: 0.16914 | val_0_rmse: 0.39772 | val_1_rmse: 0.39734 |  0:04:52s
epoch 77 | loss: 0.15513 | val_0_rmse: 0.41925 | val_1_rmse: 0.41574 |  0:04:55s
epoch 78 | loss: 0.1632  | val_0_rmse: 0.42467 | val_1_rmse: 0.42628 |  0:04:59s
epoch 79 | loss: 0.16037 | val_0_rmse: 0.38187 | val_1_rmse: 0.38171 |  0:05:03s
epoch 80 | loss: 0.19196 | val_0_rmse: 0.44294 | val_1_rmse: 0.44036 |  0:05:07s
epoch 81 | loss: 0.21544 | val_0_rmse: 0.5346  | val_1_rmse: 0.53799 |  0:05:11s
epoch 82 | loss: 0.20271 | val_0_rmse: 0.40442 | val_1_rmse: 0.40381 |  0:05:14s
epoch 83 | loss: 0.1572  | val_0_rmse: 0.38148 | val_1_rmse: 0.38038 |  0:05:18s
epoch 84 | loss: 0.15332 | val_0_rmse: 0.38713 | val_1_rmse: 0.38597 |  0:05:22s
epoch 85 | loss: 0.15173 | val_0_rmse: 0.6404  | val_1_rmse: 0.64481 |  0:05:26s
epoch 86 | loss: 0.15543 | val_0_rmse: 0.38947 | val_1_rmse: 0.39142 |  0:05:30s
epoch 87 | loss: 0.15482 | val_0_rmse: 0.39822 | val_1_rmse: 0.39683 |  0:05:33s
epoch 88 | loss: 0.1537  | val_0_rmse: 0.38789 | val_1_rmse: 0.38741 |  0:05:37s
epoch 89 | loss: 0.15562 | val_0_rmse: 0.38722 | val_1_rmse: 0.38602 |  0:05:41s
epoch 90 | loss: 0.16307 | val_0_rmse: 0.41641 | val_1_rmse: 0.41285 |  0:05:45s
epoch 91 | loss: 0.1646  | val_0_rmse: 0.40713 | val_1_rmse: 0.40492 |  0:05:49s
epoch 92 | loss: 0.15494 | val_0_rmse: 0.39013 | val_1_rmse: 0.3881  |  0:05:52s
epoch 93 | loss: 0.15588 | val_0_rmse: 0.50657 | val_1_rmse: 0.50363 |  0:05:56s
epoch 94 | loss: 0.18357 | val_0_rmse: 0.41524 | val_1_rmse: 0.41301 |  0:06:00s
epoch 95 | loss: 0.1634  | val_0_rmse: 0.39448 | val_1_rmse: 0.39402 |  0:06:04s
epoch 96 | loss: 0.15063 | val_0_rmse: 0.54184 | val_1_rmse: 0.54629 |  0:06:07s
epoch 97 | loss: 0.15832 | val_0_rmse: 0.41613 | val_1_rmse: 0.4158  |  0:06:11s
epoch 98 | loss: 0.16143 | val_0_rmse: 0.39853 | val_1_rmse: 0.39713 |  0:06:15s
epoch 99 | loss: 0.15336 | val_0_rmse: 0.4413  | val_1_rmse: 0.44398 |  0:06:19s
epoch 100| loss: 0.15427 | val_0_rmse: 0.51333 | val_1_rmse: 0.51866 |  0:06:23s
epoch 101| loss: 0.17534 | val_0_rmse: 0.42503 | val_1_rmse: 0.42469 |  0:06:26s
epoch 102| loss: 0.16205 | val_0_rmse: 0.39226 | val_1_rmse: 0.39228 |  0:06:30s
epoch 103| loss: 0.16486 | val_0_rmse: 0.43701 | val_1_rmse: 0.43423 |  0:06:34s
epoch 104| loss: 0.15866 | val_0_rmse: 0.56733 | val_1_rmse: 0.56983 |  0:06:38s
epoch 105| loss: 0.15425 | val_0_rmse: 0.38543 | val_1_rmse: 0.38681 |  0:06:42s
epoch 106| loss: 0.15711 | val_0_rmse: 0.46542 | val_1_rmse: 0.46614 |  0:06:45s
epoch 107| loss: 0.15532 | val_0_rmse: 0.46114 | val_1_rmse: 0.46238 |  0:06:49s
epoch 108| loss: 0.15679 | val_0_rmse: 0.39637 | val_1_rmse: 0.39465 |  0:06:53s
epoch 109| loss: 0.16723 | val_0_rmse: 0.41322 | val_1_rmse: 0.39665 |  0:06:57s
epoch 110| loss: 0.17013 | val_0_rmse: 0.40601 | val_1_rmse: 0.40455 |  0:07:00s
epoch 111| loss: 0.16236 | val_0_rmse: 0.46969 | val_1_rmse: 0.41675 |  0:07:04s
epoch 112| loss: 0.16021 | val_0_rmse: 0.37815 | val_1_rmse: 0.37669 |  0:07:08s
epoch 113| loss: 0.16107 | val_0_rmse: 0.40315 | val_1_rmse: 0.40468 |  0:07:12s
epoch 114| loss: 0.16011 | val_0_rmse: 0.43291 | val_1_rmse: 0.39385 |  0:07:16s
epoch 115| loss: 0.16599 | val_0_rmse: 0.49044 | val_1_rmse: 0.49295 |  0:07:20s
epoch 116| loss: 0.16688 | val_0_rmse: 0.37986 | val_1_rmse: 0.37986 |  0:07:23s
epoch 117| loss: 0.21528 | val_0_rmse: 0.54471 | val_1_rmse: 0.53879 |  0:07:27s
epoch 118| loss: 0.16803 | val_0_rmse: 0.40073 | val_1_rmse: 0.39899 |  0:07:31s
epoch 119| loss: 0.16596 | val_0_rmse: 0.47948 | val_1_rmse: 0.48394 |  0:07:35s
epoch 120| loss: 0.15784 | val_0_rmse: 0.38528 | val_1_rmse: 0.38199 |  0:07:39s
epoch 121| loss: 0.15469 | val_0_rmse: 0.39751 | val_1_rmse: 0.39826 |  0:07:42s
epoch 122| loss: 0.17058 | val_0_rmse: 0.50084 | val_1_rmse: 0.49932 |  0:07:46s
epoch 123| loss: 0.15758 | val_0_rmse: 0.39233 | val_1_rmse: 0.39085 |  0:07:50s
epoch 124| loss: 0.16019 | val_0_rmse: 0.37545 | val_1_rmse: 0.37356 |  0:07:54s
epoch 125| loss: 0.14849 | val_0_rmse: 0.3972  | val_1_rmse: 0.3968  |  0:07:58s
epoch 126| loss: 0.15777 | val_0_rmse: 0.3893  | val_1_rmse: 0.38944 |  0:08:01s
epoch 127| loss: 0.14823 | val_0_rmse: 0.40823 | val_1_rmse: 0.4111  |  0:08:05s
epoch 128| loss: 0.14969 | val_0_rmse: 0.39927 | val_1_rmse: 0.39693 |  0:08:09s
epoch 129| loss: 0.15447 | val_0_rmse: 0.38052 | val_1_rmse: 0.38071 |  0:08:13s
epoch 130| loss: 0.15812 | val_0_rmse: 0.42932 | val_1_rmse: 0.42599 |  0:08:16s
epoch 131| loss: 0.15591 | val_0_rmse: 0.42564 | val_1_rmse: 0.42796 |  0:08:20s
epoch 132| loss: 0.15769 | val_0_rmse: 0.43416 | val_1_rmse: 0.43461 |  0:08:24s
epoch 133| loss: 0.14983 | val_0_rmse: 0.40361 | val_1_rmse: 0.4026  |  0:08:28s
epoch 134| loss: 0.15426 | val_0_rmse: 0.41083 | val_1_rmse: 0.41414 |  0:08:32s
epoch 135| loss: 0.15167 | val_0_rmse: 0.38247 | val_1_rmse: 0.38259 |  0:08:35s
epoch 136| loss: 0.14883 | val_0_rmse: 0.38194 | val_1_rmse: 0.37865 |  0:08:39s
epoch 137| loss: 0.15863 | val_0_rmse: 0.42639 | val_1_rmse: 0.42948 |  0:08:43s
epoch 138| loss: 0.15389 | val_0_rmse: 0.39688 | val_1_rmse: 0.39494 |  0:08:47s
epoch 139| loss: 0.16585 | val_0_rmse: 0.39621 | val_1_rmse: 0.39627 |  0:08:51s
epoch 140| loss: 0.15708 | val_0_rmse: 0.39541 | val_1_rmse: 0.39462 |  0:08:54s
epoch 141| loss: 0.16913 | val_0_rmse: 0.44244 | val_1_rmse: 0.44343 |  0:08:58s
epoch 142| loss: 0.16912 | val_0_rmse: 0.48917 | val_1_rmse: 0.49298 |  0:09:02s
epoch 143| loss: 0.14738 | val_0_rmse: 0.37763 | val_1_rmse: 0.37525 |  0:09:06s
epoch 144| loss: 0.14552 | val_0_rmse: 0.43067 | val_1_rmse: 0.43102 |  0:09:10s
epoch 145| loss: 0.15121 | val_0_rmse: 0.39146 | val_1_rmse: 0.39214 |  0:09:13s
epoch 146| loss: 0.1548  | val_0_rmse: 0.38305 | val_1_rmse: 0.38386 |  0:09:17s
epoch 147| loss: 0.16284 | val_0_rmse: 0.51633 | val_1_rmse: 0.51158 |  0:09:21s
epoch 148| loss: 0.19872 | val_0_rmse: 0.45717 | val_1_rmse: 0.46052 |  0:09:25s
epoch 149| loss: 0.16392 | val_0_rmse: 0.37533 | val_1_rmse: 0.37497 |  0:09:29s
Stop training because you reached max_epochs = 150 with best_epoch = 124 and best_val_1_rmse = 0.37356
Best weights from best epoch are automatically used!
ended training at: 06:13:55
Feature importance:
[('Area', 0.2397154096706618), ('Baths', 0.2597431808353542), ('Beds', 0.13086611239363882), ('Latitude', 0.053699621142533904), ('Longitude', 0.3159706845843872), ('Month', 4.991373424063369e-06), ('Year', 0.0)]
Mean squared error is of 1012070303.1348356
Mean absolute error:20894.48494684958
MAPE:0.3052076360044167
R2 score:0.6982043250217063
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:13:56
epoch 0  | loss: 12.75123| val_0_rmse: 0.78949 | val_1_rmse: 0.78908 |  0:00:03s
epoch 1  | loss: 0.34045 | val_0_rmse: 0.49777 | val_1_rmse: 0.49439 |  0:00:07s
epoch 2  | loss: 0.24791 | val_0_rmse: 0.53505 | val_1_rmse: 0.53445 |  0:00:11s
epoch 3  | loss: 0.22381 | val_0_rmse: 0.47157 | val_1_rmse: 0.47236 |  0:00:15s
epoch 4  | loss: 0.21527 | val_0_rmse: 0.44326 | val_1_rmse: 0.44504 |  0:00:19s
epoch 5  | loss: 0.20692 | val_0_rmse: 0.56205 | val_1_rmse: 0.56195 |  0:00:22s
epoch 6  | loss: 0.19057 | val_0_rmse: 0.45688 | val_1_rmse: 0.4554  |  0:00:26s
epoch 7  | loss: 0.19616 | val_0_rmse: 0.40762 | val_1_rmse: 0.40729 |  0:00:30s
epoch 8  | loss: 0.18342 | val_0_rmse: 0.43308 | val_1_rmse: 0.43367 |  0:00:34s
epoch 9  | loss: 0.19687 | val_0_rmse: 0.41596 | val_1_rmse: 0.41329 |  0:00:38s
epoch 10 | loss: 0.17843 | val_0_rmse: 0.41591 | val_1_rmse: 0.41679 |  0:00:41s
epoch 11 | loss: 0.18342 | val_0_rmse: 0.41609 | val_1_rmse: 0.41894 |  0:00:45s
epoch 12 | loss: 0.18114 | val_0_rmse: 0.39187 | val_1_rmse: 0.39175 |  0:00:49s
epoch 13 | loss: 0.17109 | val_0_rmse: 0.41436 | val_1_rmse: 0.41322 |  0:00:53s
epoch 14 | loss: 0.16675 | val_0_rmse: 0.41213 | val_1_rmse: 0.41057 |  0:00:57s
epoch 15 | loss: 0.18012 | val_0_rmse: 0.43945 | val_1_rmse: 0.43924 |  0:01:00s
epoch 16 | loss: 0.18345 | val_0_rmse: 0.39043 | val_1_rmse: 0.38898 |  0:01:04s
epoch 17 | loss: 0.18538 | val_0_rmse: 0.4008  | val_1_rmse: 0.39951 |  0:01:08s
epoch 18 | loss: 0.16344 | val_0_rmse: 0.40817 | val_1_rmse: 0.40947 |  0:01:12s
epoch 19 | loss: 0.17526 | val_0_rmse: 0.42584 | val_1_rmse: 0.42532 |  0:01:16s
epoch 20 | loss: 0.16293 | val_0_rmse: 0.38683 | val_1_rmse: 0.38636 |  0:01:19s
epoch 21 | loss: 0.17318 | val_0_rmse: 0.39255 | val_1_rmse: 0.39264 |  0:01:23s
epoch 22 | loss: 0.16848 | val_0_rmse: 0.41976 | val_1_rmse: 0.42093 |  0:01:27s
epoch 23 | loss: 0.16175 | val_0_rmse: 0.39752 | val_1_rmse: 0.39869 |  0:01:31s
epoch 24 | loss: 0.16392 | val_0_rmse: 0.4337  | val_1_rmse: 0.43465 |  0:01:35s
epoch 25 | loss: 0.17971 | val_0_rmse: 0.39456 | val_1_rmse: 0.39338 |  0:01:38s
epoch 26 | loss: 0.16388 | val_0_rmse: 0.39535 | val_1_rmse: 0.39553 |  0:01:42s
epoch 27 | loss: 0.15828 | val_0_rmse: 0.48828 | val_1_rmse: 0.49068 |  0:01:46s
epoch 28 | loss: 0.22156 | val_0_rmse: 0.39533 | val_1_rmse: 0.39504 |  0:01:50s
epoch 29 | loss: 0.15457 | val_0_rmse: 0.38332 | val_1_rmse: 0.38251 |  0:01:54s
epoch 30 | loss: 0.16403 | val_0_rmse: 0.38293 | val_1_rmse: 0.38249 |  0:01:57s
epoch 31 | loss: 0.15521 | val_0_rmse: 0.48034 | val_1_rmse: 0.47848 |  0:02:01s
epoch 32 | loss: 0.17531 | val_0_rmse: 0.41187 | val_1_rmse: 0.40959 |  0:02:05s
epoch 33 | loss: 0.16067 | val_0_rmse: 0.40826 | val_1_rmse: 0.40828 |  0:02:09s
epoch 34 | loss: 0.16098 | val_0_rmse: 0.39455 | val_1_rmse: 0.39415 |  0:02:13s
epoch 35 | loss: 0.16704 | val_0_rmse: 0.38355 | val_1_rmse: 0.3839  |  0:02:16s
epoch 36 | loss: 0.16834 | val_0_rmse: 0.40669 | val_1_rmse: 0.40864 |  0:02:20s
epoch 37 | loss: 0.1567  | val_0_rmse: 0.39671 | val_1_rmse: 0.39703 |  0:02:24s
epoch 38 | loss: 0.17967 | val_0_rmse: 0.40639 | val_1_rmse: 0.40696 |  0:02:28s
epoch 39 | loss: 0.17578 | val_0_rmse: 0.47528 | val_1_rmse: 0.47427 |  0:02:32s
epoch 40 | loss: 0.17316 | val_0_rmse: 0.39921 | val_1_rmse: 0.39889 |  0:02:35s
epoch 41 | loss: 0.15969 | val_0_rmse: 0.41458 | val_1_rmse: 0.41418 |  0:02:39s
epoch 42 | loss: 0.17617 | val_0_rmse: 0.56354 | val_1_rmse: 0.55916 |  0:02:43s
epoch 43 | loss: 0.17148 | val_0_rmse: 0.41821 | val_1_rmse: 0.41474 |  0:02:47s
epoch 44 | loss: 0.17129 | val_0_rmse: 0.39219 | val_1_rmse: 0.39134 |  0:02:51s
epoch 45 | loss: 0.15909 | val_0_rmse: 0.39149 | val_1_rmse: 0.38947 |  0:02:54s
epoch 46 | loss: 0.15523 | val_0_rmse: 0.39706 | val_1_rmse: 0.39638 |  0:02:58s
epoch 47 | loss: 0.1654  | val_0_rmse: 0.42085 | val_1_rmse: 0.41966 |  0:03:02s
epoch 48 | loss: 0.15757 | val_0_rmse: 0.39926 | val_1_rmse: 0.39979 |  0:03:06s
epoch 49 | loss: 0.16134 | val_0_rmse: 0.44224 | val_1_rmse: 0.44154 |  0:03:10s
epoch 50 | loss: 0.15638 | val_0_rmse: 0.43174 | val_1_rmse: 0.42924 |  0:03:13s
epoch 51 | loss: 0.17647 | val_0_rmse: 0.39765 | val_1_rmse: 0.39522 |  0:03:17s
epoch 52 | loss: 0.16568 | val_0_rmse: 0.42841 | val_1_rmse: 0.42711 |  0:03:21s
epoch 53 | loss: 0.17446 | val_0_rmse: 0.45049 | val_1_rmse: 0.45089 |  0:03:25s
epoch 54 | loss: 0.15934 | val_0_rmse: 0.38758 | val_1_rmse: 0.3884  |  0:03:29s
epoch 55 | loss: 0.16071 | val_0_rmse: 0.51036 | val_1_rmse: 0.50822 |  0:03:32s
epoch 56 | loss: 0.16741 | val_0_rmse: 0.5056  | val_1_rmse: 0.50669 |  0:03:36s
epoch 57 | loss: 0.17044 | val_0_rmse: 0.39297 | val_1_rmse: 0.3924  |  0:03:40s
epoch 58 | loss: 0.15899 | val_0_rmse: 0.41309 | val_1_rmse: 0.41299 |  0:03:44s
epoch 59 | loss: 0.14945 | val_0_rmse: 0.38623 | val_1_rmse: 0.38628 |  0:03:47s
epoch 60 | loss: 0.15993 | val_0_rmse: 0.41569 | val_1_rmse: 0.41432 |  0:03:51s

Early stopping occured at epoch 60 with best_epoch = 30 and best_val_1_rmse = 0.38249
Best weights from best epoch are automatically used!
ended training at: 06:17:49
Feature importance:
[('Area', 0.21969549769104846), ('Baths', 0.13480936318195635), ('Beds', 0.05670319423925261), ('Latitude', 0.21659654448385027), ('Longitude', 0.3518758183316149), ('Month', 0.0), ('Year', 0.020319582072277416)]
Mean squared error is of 958924876.6195703
Mean absolute error:20871.779124534798
MAPE:0.30573573583639113
R2 score:0.7208375100699969
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:17:49
epoch 0  | loss: 12.35069| val_0_rmse: 1.00735 | val_1_rmse: 1.01163 |  0:00:03s
epoch 1  | loss: 0.40985 | val_0_rmse: 0.49747 | val_1_rmse: 0.49111 |  0:00:07s
epoch 2  | loss: 0.27739 | val_0_rmse: 0.53017 | val_1_rmse: 0.52354 |  0:00:11s
epoch 3  | loss: 0.25836 | val_0_rmse: 0.48669 | val_1_rmse: 0.49017 |  0:00:15s
epoch 4  | loss: 0.20979 | val_0_rmse: 0.43545 | val_1_rmse: 0.44152 |  0:00:19s
epoch 5  | loss: 0.19157 | val_0_rmse: 0.45668 | val_1_rmse: 0.4567  |  0:00:22s
epoch 6  | loss: 0.21084 | val_0_rmse: 0.4848  | val_1_rmse: 0.49402 |  0:00:26s
epoch 7  | loss: 0.19806 | val_0_rmse: 0.43009 | val_1_rmse: 0.43762 |  0:00:30s
epoch 8  | loss: 0.2298  | val_0_rmse: 0.50781 | val_1_rmse: 0.5041  |  0:00:34s
epoch 9  | loss: 0.25344 | val_0_rmse: 0.44156 | val_1_rmse: 0.44201 |  0:00:38s
epoch 10 | loss: 0.22964 | val_0_rmse: 0.47379 | val_1_rmse: 0.48164 |  0:00:41s
epoch 11 | loss: 0.19209 | val_0_rmse: 0.46619 | val_1_rmse: 0.46938 |  0:00:45s
epoch 12 | loss: 0.1853  | val_0_rmse: 0.40688 | val_1_rmse: 0.41268 |  0:00:49s
epoch 13 | loss: 0.1749  | val_0_rmse: 0.42925 | val_1_rmse: 0.43168 |  0:00:53s
epoch 14 | loss: 0.17623 | val_0_rmse: 0.45841 | val_1_rmse: 0.46768 |  0:00:57s
epoch 15 | loss: 0.17945 | val_0_rmse: 0.46312 | val_1_rmse: 0.47293 |  0:01:00s
epoch 16 | loss: 0.19498 | val_0_rmse: 0.46035 | val_1_rmse: 0.46751 |  0:01:04s
epoch 17 | loss: 0.18051 | val_0_rmse: 0.434   | val_1_rmse: 0.44139 |  0:01:08s
epoch 18 | loss: 0.21039 | val_0_rmse: 0.40716 | val_1_rmse: 0.41439 |  0:01:12s
epoch 19 | loss: 0.18026 | val_0_rmse: 0.39837 | val_1_rmse: 0.40538 |  0:01:16s
epoch 20 | loss: 0.17303 | val_0_rmse: 0.40223 | val_1_rmse: 0.40513 |  0:01:19s
epoch 21 | loss: 0.17782 | val_0_rmse: 0.47035 | val_1_rmse: 0.4698  |  0:01:23s
epoch 22 | loss: 0.16745 | val_0_rmse: 0.41743 | val_1_rmse: 0.42252 |  0:01:27s
epoch 23 | loss: 0.17057 | val_0_rmse: 0.4488  | val_1_rmse: 0.45636 |  0:01:31s
epoch 24 | loss: 0.16333 | val_0_rmse: 0.43854 | val_1_rmse: 0.43863 |  0:01:34s
epoch 25 | loss: 0.1734  | val_0_rmse: 0.40145 | val_1_rmse: 0.40414 |  0:01:38s
epoch 26 | loss: 0.17332 | val_0_rmse: 0.46781 | val_1_rmse: 0.47632 |  0:01:42s
epoch 27 | loss: 0.17668 | val_0_rmse: 0.42287 | val_1_rmse: 0.42435 |  0:01:46s
epoch 28 | loss: 0.16978 | val_0_rmse: 0.47723 | val_1_rmse: 0.48514 |  0:01:50s
epoch 29 | loss: 0.16182 | val_0_rmse: 0.41711 | val_1_rmse: 0.42433 |  0:01:54s
epoch 30 | loss: 0.18649 | val_0_rmse: 0.53615 | val_1_rmse: 0.5349  |  0:01:57s
epoch 31 | loss: 0.18128 | val_0_rmse: 0.48716 | val_1_rmse: 0.49301 |  0:02:01s
epoch 32 | loss: 0.21033 | val_0_rmse: 0.41173 | val_1_rmse: 0.41519 |  0:02:05s
epoch 33 | loss: 0.17592 | val_0_rmse: 0.44257 | val_1_rmse: 0.45015 |  0:02:09s
epoch 34 | loss: 0.17064 | val_0_rmse: 0.43173 | val_1_rmse: 0.441   |  0:02:12s
epoch 35 | loss: 0.21777 | val_0_rmse: 0.4027  | val_1_rmse: 0.40513 |  0:02:16s
epoch 36 | loss: 0.16526 | val_0_rmse: 0.43292 | val_1_rmse: 0.44077 |  0:02:20s
epoch 37 | loss: 0.1639  | val_0_rmse: 0.41135 | val_1_rmse: 0.41696 |  0:02:24s
epoch 38 | loss: 0.17087 | val_0_rmse: 0.46261 | val_1_rmse: 0.46879 |  0:02:28s
epoch 39 | loss: 0.16821 | val_0_rmse: 0.41026 | val_1_rmse: 0.41504 |  0:02:31s
epoch 40 | loss: 0.17023 | val_0_rmse: 0.42786 | val_1_rmse: 0.43412 |  0:02:35s
epoch 41 | loss: 0.17768 | val_0_rmse: 0.44254 | val_1_rmse: 0.44231 |  0:02:39s
epoch 42 | loss: 0.16194 | val_0_rmse: 0.407   | val_1_rmse: 0.40936 |  0:02:43s
epoch 43 | loss: 0.16591 | val_0_rmse: 0.43323 | val_1_rmse: 0.43234 |  0:02:47s
epoch 44 | loss: 0.23871 | val_0_rmse: 0.50793 | val_1_rmse: 0.51741 |  0:02:50s
epoch 45 | loss: 0.21051 | val_0_rmse: 0.40835 | val_1_rmse: 0.41246 |  0:02:54s
epoch 46 | loss: 0.18047 | val_0_rmse: 0.39412 | val_1_rmse: 0.39942 |  0:02:58s
epoch 47 | loss: 0.17034 | val_0_rmse: 0.45064 | val_1_rmse: 0.45958 |  0:03:02s
epoch 48 | loss: 0.16427 | val_0_rmse: 0.39479 | val_1_rmse: 0.39851 |  0:03:06s
epoch 49 | loss: 0.22337 | val_0_rmse: 0.98034 | val_1_rmse: 0.98679 |  0:03:09s
epoch 50 | loss: 0.19724 | val_0_rmse: 0.41544 | val_1_rmse: 0.42118 |  0:03:13s
epoch 51 | loss: 0.18829 | val_0_rmse: 0.42711 | val_1_rmse: 0.43248 |  0:03:17s
epoch 52 | loss: 0.19907 | val_0_rmse: 0.45489 | val_1_rmse: 0.46276 |  0:03:21s
epoch 53 | loss: 0.22912 | val_0_rmse: 0.43078 | val_1_rmse: 0.4393  |  0:03:24s
epoch 54 | loss: 0.19344 | val_0_rmse: 0.41353 | val_1_rmse: 0.41442 |  0:03:28s
epoch 55 | loss: 0.16382 | val_0_rmse: 0.41703 | val_1_rmse: 0.42381 |  0:03:32s
epoch 56 | loss: 0.1608  | val_0_rmse: 0.41029 | val_1_rmse: 0.41376 |  0:03:36s
epoch 57 | loss: 0.15928 | val_0_rmse: 0.38317 | val_1_rmse: 0.38993 |  0:03:40s
epoch 58 | loss: 0.16541 | val_0_rmse: 0.3867  | val_1_rmse: 0.39042 |  0:03:43s
epoch 59 | loss: 0.1583  | val_0_rmse: 0.39162 | val_1_rmse: 0.39701 |  0:03:47s
epoch 60 | loss: 0.17469 | val_0_rmse: 0.38729 | val_1_rmse: 0.39126 |  0:03:51s
epoch 61 | loss: 0.16234 | val_0_rmse: 0.39657 | val_1_rmse: 0.39973 |  0:03:55s
epoch 62 | loss: 0.17216 | val_0_rmse: 0.40133 | val_1_rmse: 0.40589 |  0:03:59s
epoch 63 | loss: 0.1745  | val_0_rmse: 0.39185 | val_1_rmse: 0.39807 |  0:04:02s
epoch 64 | loss: 0.18329 | val_0_rmse: 0.47515 | val_1_rmse: 0.48413 |  0:04:06s
epoch 65 | loss: 0.1825  | val_0_rmse: 0.41794 | val_1_rmse: 0.42418 |  0:04:10s
epoch 66 | loss: 0.1727  | val_0_rmse: 0.42981 | val_1_rmse: 0.42972 |  0:04:14s
epoch 67 | loss: 0.16885 | val_0_rmse: 0.43315 | val_1_rmse: 0.43973 |  0:04:18s
epoch 68 | loss: 0.15967 | val_0_rmse: 0.43742 | val_1_rmse: 0.44325 |  0:04:21s
epoch 69 | loss: 0.196   | val_0_rmse: 0.49552 | val_1_rmse: 0.50518 |  0:04:25s
epoch 70 | loss: 0.18494 | val_0_rmse: 0.50072 | val_1_rmse: 0.49799 |  0:04:29s
epoch 71 | loss: 0.1772  | val_0_rmse: 0.4078  | val_1_rmse: 0.41558 |  0:04:33s
epoch 72 | loss: 0.17069 | val_0_rmse: 0.43301 | val_1_rmse: 0.43248 |  0:04:37s
epoch 73 | loss: 0.15809 | val_0_rmse: 0.40612 | val_1_rmse: 0.40798 |  0:04:40s
epoch 74 | loss: 0.16627 | val_0_rmse: 0.39026 | val_1_rmse: 0.39487 |  0:04:44s
epoch 75 | loss: 0.15944 | val_0_rmse: 0.41203 | val_1_rmse: 0.41958 |  0:04:48s
epoch 76 | loss: 0.15642 | val_0_rmse: 0.55574 | val_1_rmse: 0.56122 |  0:04:52s
epoch 77 | loss: 0.167   | val_0_rmse: 0.40593 | val_1_rmse: 0.41147 |  0:04:55s
epoch 78 | loss: 0.18965 | val_0_rmse: 0.41231 | val_1_rmse: 0.41812 |  0:04:59s
epoch 79 | loss: 0.16388 | val_0_rmse: 0.42577 | val_1_rmse: 0.42644 |  0:05:03s
epoch 80 | loss: 0.16349 | val_0_rmse: 0.38287 | val_1_rmse: 0.38816 |  0:05:07s
epoch 81 | loss: 0.17318 | val_0_rmse: 0.49411 | val_1_rmse: 0.49198 |  0:05:11s
epoch 82 | loss: 0.16975 | val_0_rmse: 0.39152 | val_1_rmse: 0.39559 |  0:05:15s
epoch 83 | loss: 0.16146 | val_0_rmse: 0.38786 | val_1_rmse: 0.38982 |  0:05:18s
epoch 84 | loss: 0.16967 | val_0_rmse: 0.40002 | val_1_rmse: 0.40396 |  0:05:22s
epoch 85 | loss: 0.16996 | val_0_rmse: 0.50371 | val_1_rmse: 0.50935 |  0:05:26s
epoch 86 | loss: 0.16711 | val_0_rmse: 0.39299 | val_1_rmse: 0.3957  |  0:05:30s
epoch 87 | loss: 0.1768  | val_0_rmse: 0.41708 | val_1_rmse: 0.42057 |  0:05:34s
epoch 88 | loss: 0.17047 | val_0_rmse: 0.40298 | val_1_rmse: 0.40221 |  0:05:38s
epoch 89 | loss: 0.16477 | val_0_rmse: 0.39021 | val_1_rmse: 0.39211 |  0:05:41s
epoch 90 | loss: 0.15694 | val_0_rmse: 0.40068 | val_1_rmse: 0.40634 |  0:05:45s
epoch 91 | loss: 0.17029 | val_0_rmse: 0.40079 | val_1_rmse: 0.40121 |  0:05:49s
epoch 92 | loss: 0.16913 | val_0_rmse: 0.49219 | val_1_rmse: 0.49856 |  0:05:53s
epoch 93 | loss: 0.15376 | val_0_rmse: 0.50062 | val_1_rmse: 0.49788 |  0:05:57s
epoch 94 | loss: 0.15523 | val_0_rmse: 0.40227 | val_1_rmse: 0.40515 |  0:06:01s
epoch 95 | loss: 0.16376 | val_0_rmse: 0.39945 | val_1_rmse: 0.39841 |  0:06:05s
epoch 96 | loss: 0.16166 | val_0_rmse: 0.39956 | val_1_rmse: 0.40379 |  0:06:09s
epoch 97 | loss: 0.16767 | val_0_rmse: 0.41573 | val_1_rmse: 0.41517 |  0:06:13s
epoch 98 | loss: 0.15898 | val_0_rmse: 0.44875 | val_1_rmse: 0.44358 |  0:06:17s
epoch 99 | loss: 0.16676 | val_0_rmse: 0.39317 | val_1_rmse: 0.39395 |  0:06:21s
epoch 100| loss: 0.16641 | val_0_rmse: 0.43512 | val_1_rmse: 0.43396 |  0:06:25s
epoch 101| loss: 0.16679 | val_0_rmse: 0.38979 | val_1_rmse: 0.39174 |  0:06:29s
epoch 102| loss: 0.15492 | val_0_rmse: 0.48346 | val_1_rmse: 0.48349 |  0:06:32s
epoch 103| loss: 0.15585 | val_0_rmse: 0.43945 | val_1_rmse: 0.43755 |  0:06:36s
epoch 104| loss: 0.15203 | val_0_rmse: 0.38978 | val_1_rmse: 0.39178 |  0:06:40s
epoch 105| loss: 0.15308 | val_0_rmse: 0.49854 | val_1_rmse: 0.50411 |  0:06:44s
epoch 106| loss: 0.15711 | val_0_rmse: 0.40268 | val_1_rmse: 0.40657 |  0:06:48s
epoch 107| loss: 0.1581  | val_0_rmse: 0.43614 | val_1_rmse: 0.4332  |  0:06:51s
epoch 108| loss: 0.16948 | val_0_rmse: 0.42437 | val_1_rmse: 0.43104 |  0:06:55s
epoch 109| loss: 0.17106 | val_0_rmse: 0.37684 | val_1_rmse: 0.37891 |  0:06:59s
epoch 110| loss: 0.16811 | val_0_rmse: 0.39696 | val_1_rmse: 0.40331 |  0:07:03s
epoch 111| loss: 0.17943 | val_0_rmse: 0.38639 | val_1_rmse: 0.39103 |  0:07:06s
epoch 112| loss: 0.18352 | val_0_rmse: 0.38693 | val_1_rmse: 0.38868 |  0:07:10s
epoch 113| loss: 0.16152 | val_0_rmse: 0.4416  | val_1_rmse: 0.43616 |  0:07:14s
epoch 114| loss: 0.17094 | val_0_rmse: 0.38675 | val_1_rmse: 0.38707 |  0:07:18s
epoch 115| loss: 0.15402 | val_0_rmse: 0.46371 | val_1_rmse: 0.4571  |  0:07:22s
epoch 116| loss: 0.15662 | val_0_rmse: 0.47187 | val_1_rmse: 0.4651  |  0:07:25s
epoch 117| loss: 0.16019 | val_0_rmse: 0.42004 | val_1_rmse: 0.41852 |  0:07:29s
epoch 118| loss: 0.15737 | val_0_rmse: 0.42373 | val_1_rmse: 0.42917 |  0:07:33s
epoch 119| loss: 0.20514 | val_0_rmse: 0.47658 | val_1_rmse: 0.47304 |  0:07:37s
epoch 120| loss: 0.17999 | val_0_rmse: 0.4002  | val_1_rmse: 0.40171 |  0:07:41s
epoch 121| loss: 0.14949 | val_0_rmse: 0.38738 | val_1_rmse: 0.38698 |  0:07:44s
epoch 122| loss: 0.15981 | val_0_rmse: 0.39258 | val_1_rmse: 0.39412 |  0:07:48s
epoch 123| loss: 0.15704 | val_0_rmse: 0.41282 | val_1_rmse: 0.41363 |  0:07:52s
epoch 124| loss: 0.15467 | val_0_rmse: 0.46278 | val_1_rmse: 0.46234 |  0:07:56s
epoch 125| loss: 0.16746 | val_0_rmse: 0.43857 | val_1_rmse: 0.4347  |  0:08:00s
epoch 126| loss: 0.14927 | val_0_rmse: 0.39605 | val_1_rmse: 0.3965  |  0:08:03s
epoch 127| loss: 0.15086 | val_0_rmse: 0.42302 | val_1_rmse: 0.42324 |  0:08:07s
epoch 128| loss: 0.15649 | val_0_rmse: 0.41825 | val_1_rmse: 0.41726 |  0:08:11s
epoch 129| loss: 0.1574  | val_0_rmse: 0.38144 | val_1_rmse: 0.38339 |  0:08:15s
epoch 130| loss: 0.16229 | val_0_rmse: 0.39288 | val_1_rmse: 0.39845 |  0:08:18s
epoch 131| loss: 0.15461 | val_0_rmse: 0.40137 | val_1_rmse: 0.40434 |  0:08:22s
epoch 132| loss: 0.14928 | val_0_rmse: 0.39831 | val_1_rmse: 0.40121 |  0:08:26s
epoch 133| loss: 0.16239 | val_0_rmse: 0.39833 | val_1_rmse: 0.40287 |  0:08:30s
epoch 134| loss: 0.15302 | val_0_rmse: 0.39989 | val_1_rmse: 0.40295 |  0:08:34s
epoch 135| loss: 0.15966 | val_0_rmse: 0.39114 | val_1_rmse: 0.39427 |  0:08:38s
epoch 136| loss: 0.16713 | val_0_rmse: 0.41056 | val_1_rmse: 0.41646 |  0:08:41s
epoch 137| loss: 0.15694 | val_0_rmse: 0.38724 | val_1_rmse: 0.3925  |  0:08:45s
epoch 138| loss: 0.14779 | val_0_rmse: 0.38725 | val_1_rmse: 0.38833 |  0:08:49s
epoch 139| loss: 0.14844 | val_0_rmse: 0.39354 | val_1_rmse: 0.39349 |  0:08:53s

Early stopping occured at epoch 139 with best_epoch = 109 and best_val_1_rmse = 0.37891
Best weights from best epoch are automatically used!
ended training at: 06:26:43
Feature importance:
[('Area', 0.12390468906678771), ('Baths', 0.0), ('Beds', 0.21875912709958126), ('Latitude', 0.10198039051228681), ('Longitude', 0.40238696756920833), ('Month', 0.026611059279157752), ('Year', 0.1263577664729781)]
Mean squared error is of 979637524.695848
Mean absolute error:20691.16690316385
MAPE:0.30704263109123714
R2 score:0.7041698416824884
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:26:44
epoch 0  | loss: 3.59714 | val_0_rmse: 0.52847 | val_1_rmse: 0.5295  |  0:00:17s
epoch 1  | loss: 0.29117 | val_0_rmse: 0.52225 | val_1_rmse: 0.52286 |  0:00:35s
epoch 2  | loss: 0.27459 | val_0_rmse: 0.49442 | val_1_rmse: 0.49514 |  0:00:52s
epoch 3  | loss: 0.28441 | val_0_rmse: 0.5165  | val_1_rmse: 0.51767 |  0:01:10s
epoch 4  | loss: 0.2596  | val_0_rmse: 0.52871 | val_1_rmse: 0.53033 |  0:01:28s
epoch 5  | loss: 0.25028 | val_0_rmse: 0.4744  | val_1_rmse: 0.47483 |  0:01:46s
epoch 6  | loss: 0.25467 | val_0_rmse: 0.46473 | val_1_rmse: 0.46582 |  0:02:03s
epoch 7  | loss: 0.23538 | val_0_rmse: 0.47299 | val_1_rmse: 0.47279 |  0:02:21s
epoch 8  | loss: 0.2377  | val_0_rmse: 0.48509 | val_1_rmse: 0.4868  |  0:02:39s
epoch 9  | loss: 0.23376 | val_0_rmse: 0.47944 | val_1_rmse: 0.48037 |  0:02:57s
epoch 10 | loss: 0.2312  | val_0_rmse: 0.52792 | val_1_rmse: 0.52957 |  0:03:14s
epoch 11 | loss: 0.23051 | val_0_rmse: 0.45427 | val_1_rmse: 0.45429 |  0:03:32s
epoch 12 | loss: 0.27507 | val_0_rmse: 0.48572 | val_1_rmse: 0.48687 |  0:03:50s
epoch 13 | loss: 0.27502 | val_0_rmse: 0.49063 | val_1_rmse: 0.49324 |  0:04:07s
epoch 14 | loss: 0.21853 | val_0_rmse: 0.46641 | val_1_rmse: 0.46649 |  0:04:25s
epoch 15 | loss: 0.21498 | val_0_rmse: 0.45428 | val_1_rmse: 0.4546  |  0:04:43s
epoch 16 | loss: 0.21551 | val_0_rmse: 0.49658 | val_1_rmse: 0.49611 |  0:05:00s
epoch 17 | loss: 0.2103  | val_0_rmse: 0.46098 | val_1_rmse: 0.46306 |  0:05:18s
epoch 18 | loss: 0.23422 | val_0_rmse: 0.45287 | val_1_rmse: 0.45354 |  0:05:36s
epoch 19 | loss: 0.21747 | val_0_rmse: 0.4363  | val_1_rmse: 0.43767 |  0:05:54s
epoch 20 | loss: 0.20761 | val_0_rmse: 0.5357  | val_1_rmse: 0.53569 |  0:06:11s
epoch 21 | loss: 0.20892 | val_0_rmse: 0.55889 | val_1_rmse: 0.56091 |  0:06:29s
epoch 22 | loss: 0.20001 | val_0_rmse: 0.44967 | val_1_rmse: 0.45039 |  0:06:47s
epoch 23 | loss: 0.21487 | val_0_rmse: 0.45541 | val_1_rmse: 0.45788 |  0:07:04s
epoch 24 | loss: 0.20893 | val_0_rmse: 0.50468 | val_1_rmse: 0.50597 |  0:07:22s
epoch 25 | loss: 0.20338 | val_0_rmse: 0.44642 | val_1_rmse: 0.44797 |  0:07:40s
epoch 26 | loss: 0.19927 | val_0_rmse: 0.43579 | val_1_rmse: 0.43711 |  0:07:57s
epoch 27 | loss: 0.20612 | val_0_rmse: 0.44475 | val_1_rmse: 0.44543 |  0:08:15s
epoch 28 | loss: 0.19694 | val_0_rmse: 0.45769 | val_1_rmse: 0.45839 |  0:08:33s
epoch 29 | loss: 0.20187 | val_0_rmse: 0.48645 | val_1_rmse: 0.48854 |  0:08:50s
epoch 30 | loss: 0.19834 | val_0_rmse: 0.48119 | val_1_rmse: 0.48234 |  0:09:08s
epoch 31 | loss: 0.20094 | val_0_rmse: 0.45726 | val_1_rmse: 0.45873 |  0:09:26s
epoch 32 | loss: 0.20568 | val_0_rmse: 0.45466 | val_1_rmse: 0.45651 |  0:09:43s
epoch 33 | loss: 0.20846 | val_0_rmse: 0.49241 | val_1_rmse: 0.49272 |  0:10:01s
epoch 34 | loss: 0.19846 | val_0_rmse: 0.44065 | val_1_rmse: 0.44168 |  0:10:19s
epoch 35 | loss: 0.20042 | val_0_rmse: 0.46494 | val_1_rmse: 0.46635 |  0:10:37s
epoch 36 | loss: 0.19304 | val_0_rmse: 0.44051 | val_1_rmse: 0.44254 |  0:10:54s
epoch 37 | loss: 0.1949  | val_0_rmse: 0.46368 | val_1_rmse: 0.46409 |  0:11:12s
epoch 38 | loss: 0.19062 | val_0_rmse: 0.43182 | val_1_rmse: 0.43382 |  0:11:30s
epoch 39 | loss: 0.19685 | val_0_rmse: 0.47058 | val_1_rmse: 0.47296 |  0:11:47s
epoch 40 | loss: 0.20884 | val_0_rmse: 0.44522 | val_1_rmse: 0.44671 |  0:12:05s
epoch 41 | loss: 0.19499 | val_0_rmse: 0.52436 | val_1_rmse: 0.52541 |  0:12:23s
epoch 42 | loss: 0.19456 | val_0_rmse: 0.48007 | val_1_rmse: 0.48196 |  0:12:40s
epoch 43 | loss: 0.20089 | val_0_rmse: 0.45041 | val_1_rmse: 0.45201 |  0:12:58s
epoch 44 | loss: 0.2207  | val_0_rmse: 0.47157 | val_1_rmse: 0.47396 |  0:13:16s
epoch 45 | loss: 0.21165 | val_0_rmse: 0.45268 | val_1_rmse: 0.45354 |  0:13:33s
epoch 46 | loss: 0.20116 | val_0_rmse: 0.44019 | val_1_rmse: 0.44076 |  0:13:51s
epoch 47 | loss: 0.19417 | val_0_rmse: 0.43747 | val_1_rmse: 0.4385  |  0:14:09s
epoch 48 | loss: 0.19549 | val_0_rmse: 0.43703 | val_1_rmse: 0.43857 |  0:14:26s
epoch 49 | loss: 0.19591 | val_0_rmse: 0.47745 | val_1_rmse: 0.47877 |  0:14:44s
epoch 50 | loss: 0.18981 | val_0_rmse: 0.49048 | val_1_rmse: 0.49174 |  0:15:02s
epoch 51 | loss: 0.18876 | val_0_rmse: 0.43929 | val_1_rmse: 0.44019 |  0:15:19s
epoch 52 | loss: 0.18923 | val_0_rmse: 0.45036 | val_1_rmse: 0.4511  |  0:15:37s
epoch 53 | loss: 0.18779 | val_0_rmse: 0.46033 | val_1_rmse: 0.46163 |  0:15:55s
epoch 54 | loss: 0.18845 | val_0_rmse: 0.43777 | val_1_rmse: 0.43819 |  0:16:12s
epoch 55 | loss: 0.19258 | val_0_rmse: 0.45569 | val_1_rmse: 0.45666 |  0:16:30s
epoch 56 | loss: 0.18851 | val_0_rmse: 0.46643 | val_1_rmse: 0.46758 |  0:16:48s
epoch 57 | loss: 0.18805 | val_0_rmse: 0.45332 | val_1_rmse: 0.45425 |  0:17:06s
epoch 58 | loss: 0.1857  | val_0_rmse: 0.46961 | val_1_rmse: 0.47184 |  0:17:23s
epoch 59 | loss: 0.1883  | val_0_rmse: 0.43243 | val_1_rmse: 0.43392 |  0:17:41s
epoch 60 | loss: 0.19026 | val_0_rmse: 0.49916 | val_1_rmse: 0.49979 |  0:17:59s
epoch 61 | loss: 0.19449 | val_0_rmse: 0.49733 | val_1_rmse: 0.49878 |  0:18:16s
epoch 62 | loss: 0.1862  | val_0_rmse: 0.46089 | val_1_rmse: 0.46083 |  0:18:34s
epoch 63 | loss: 0.1904  | val_0_rmse: 0.51381 | val_1_rmse: 0.51525 |  0:18:52s
epoch 64 | loss: 0.19115 | val_0_rmse: 0.43655 | val_1_rmse: 0.43874 |  0:19:10s
epoch 65 | loss: 0.18368 | val_0_rmse: 0.44626 | val_1_rmse: 0.44741 |  0:19:27s
epoch 66 | loss: 0.20013 | val_0_rmse: 0.47037 | val_1_rmse: 0.47132 |  0:19:45s
epoch 67 | loss: 0.19344 | val_0_rmse: 0.43613 | val_1_rmse: 0.43696 |  0:20:03s
epoch 68 | loss: 0.18507 | val_0_rmse: 0.4421  | val_1_rmse: 0.44313 |  0:20:20s

Early stopping occured at epoch 68 with best_epoch = 38 and best_val_1_rmse = 0.43382
Best weights from best epoch are automatically used!
ended training at: 06:47:10
Feature importance:
[('Area', 0.0005502988793717005), ('Baths', 0.3740822494891387), ('Beds', 0.0), ('Latitude', 0.3562430919458596), ('Longitude', 0.09321515101703896), ('Month', 0.17590920866859097), ('Year', 0.0)]
Mean squared error is of 11257664313.058622
Mean absolute error:64948.00118607683
MAPE:0.39693746393501894
R2 score:0.7185825097931186
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:47:13
epoch 0  | loss: 3.6088  | val_0_rmse: 0.56936 | val_1_rmse: 0.57063 |  0:00:17s
epoch 1  | loss: 0.32493 | val_0_rmse: 0.50242 | val_1_rmse: 0.50237 |  0:00:35s
epoch 2  | loss: 0.26983 | val_0_rmse: 0.48191 | val_1_rmse: 0.48181 |  0:00:52s
epoch 3  | loss: 0.25411 | val_0_rmse: 0.47092 | val_1_rmse: 0.47114 |  0:01:10s
epoch 4  | loss: 0.2482  | val_0_rmse: 0.47324 | val_1_rmse: 0.47295 |  0:01:28s
epoch 5  | loss: 0.25574 | val_0_rmse: 0.54403 | val_1_rmse: 0.54349 |  0:01:45s
epoch 6  | loss: 0.24151 | val_0_rmse: 0.5004  | val_1_rmse: 0.50056 |  0:02:03s
epoch 7  | loss: 0.24219 | val_0_rmse: 0.58688 | val_1_rmse: 0.58729 |  0:02:21s
epoch 8  | loss: 0.25781 | val_0_rmse: 0.57947 | val_1_rmse: 0.57933 |  0:02:38s
epoch 9  | loss: 0.23442 | val_0_rmse: 0.46086 | val_1_rmse: 0.46191 |  0:02:56s
epoch 10 | loss: 0.23303 | val_0_rmse: 0.44814 | val_1_rmse: 0.44918 |  0:03:14s
epoch 11 | loss: 0.22219 | val_0_rmse: 0.46309 | val_1_rmse: 0.46366 |  0:03:31s
epoch 12 | loss: 0.21883 | val_0_rmse: 0.46455 | val_1_rmse: 0.46488 |  0:03:49s
epoch 13 | loss: 0.21866 | val_0_rmse: 0.50579 | val_1_rmse: 0.50562 |  0:04:07s
epoch 14 | loss: 0.2229  | val_0_rmse: 0.45365 | val_1_rmse: 0.4543  |  0:04:24s
epoch 15 | loss: 0.21741 | val_0_rmse: 0.48438 | val_1_rmse: 0.48522 |  0:04:42s
epoch 16 | loss: 0.22782 | val_0_rmse: 0.46166 | val_1_rmse: 0.46217 |  0:04:59s
epoch 17 | loss: 0.21783 | val_0_rmse: 0.44726 | val_1_rmse: 0.44722 |  0:05:17s
epoch 18 | loss: 0.2141  | val_0_rmse: 0.44577 | val_1_rmse: 0.44729 |  0:05:35s
epoch 19 | loss: 0.22264 | val_0_rmse: 0.45367 | val_1_rmse: 0.45483 |  0:05:52s
epoch 20 | loss: 0.22675 | val_0_rmse: 0.43918 | val_1_rmse: 0.44062 |  0:06:10s
epoch 21 | loss: 0.21919 | val_0_rmse: 0.49237 | val_1_rmse: 0.49276 |  0:06:28s
epoch 22 | loss: 0.20963 | val_0_rmse: 0.45113 | val_1_rmse: 0.45232 |  0:06:45s
epoch 23 | loss: 0.21378 | val_0_rmse: 0.43784 | val_1_rmse: 0.43924 |  0:07:03s
epoch 24 | loss: 0.20723 | val_0_rmse: 0.44058 | val_1_rmse: 0.44153 |  0:07:20s
epoch 25 | loss: 0.20976 | val_0_rmse: 0.44755 | val_1_rmse: 0.44947 |  0:07:38s
epoch 26 | loss: 0.21213 | val_0_rmse: 0.45642 | val_1_rmse: 0.45885 |  0:07:56s
epoch 27 | loss: 0.21812 | val_0_rmse: 0.44042 | val_1_rmse: 0.44173 |  0:08:13s
epoch 28 | loss: 0.21378 | val_0_rmse: 0.44409 | val_1_rmse: 0.44538 |  0:08:31s
epoch 29 | loss: 0.20318 | val_0_rmse: 0.44558 | val_1_rmse: 0.44625 |  0:08:48s
epoch 30 | loss: 0.20893 | val_0_rmse: 0.45903 | val_1_rmse: 0.46025 |  0:09:06s
epoch 31 | loss: 0.21352 | val_0_rmse: 0.4487  | val_1_rmse: 0.45031 |  0:09:24s
epoch 32 | loss: 0.19983 | val_0_rmse: 0.45419 | val_1_rmse: 0.45596 |  0:09:41s
epoch 33 | loss: 0.21184 | val_0_rmse: 0.43852 | val_1_rmse: 0.44077 |  0:09:59s
epoch 34 | loss: 0.20833 | val_0_rmse: 0.44168 | val_1_rmse: 0.44326 |  0:10:17s
epoch 35 | loss: 0.20221 | val_0_rmse: 0.45756 | val_1_rmse: 0.45887 |  0:10:35s
epoch 36 | loss: 0.20653 | val_0_rmse: 0.47071 | val_1_rmse: 0.47345 |  0:10:52s
epoch 37 | loss: 0.20464 | val_0_rmse: 0.45499 | val_1_rmse: 0.45551 |  0:11:10s
epoch 38 | loss: 0.20479 | val_0_rmse: 0.45659 | val_1_rmse: 0.45789 |  0:11:27s
epoch 39 | loss: 0.22334 | val_0_rmse: 0.50604 | val_1_rmse: 0.50697 |  0:11:45s
epoch 40 | loss: 0.20631 | val_0_rmse: 0.46407 | val_1_rmse: 0.46596 |  0:12:03s
epoch 41 | loss: 0.20341 | val_0_rmse: 0.44547 | val_1_rmse: 0.44562 |  0:12:20s
epoch 42 | loss: 0.19872 | val_0_rmse: 0.44977 | val_1_rmse: 0.45171 |  0:12:38s
epoch 43 | loss: 0.19581 | val_0_rmse: 0.45018 | val_1_rmse: 0.45186 |  0:12:56s
epoch 44 | loss: 0.20702 | val_0_rmse: 0.46866 | val_1_rmse: 0.47118 |  0:13:13s
epoch 45 | loss: 0.19686 | val_0_rmse: 0.43681 | val_1_rmse: 0.43818 |  0:13:31s
epoch 46 | loss: 0.20424 | val_0_rmse: 0.43747 | val_1_rmse: 0.43923 |  0:13:49s
epoch 47 | loss: 0.19786 | val_0_rmse: 0.42735 | val_1_rmse: 0.42879 |  0:14:06s
epoch 48 | loss: 0.19819 | val_0_rmse: 0.52912 | val_1_rmse: 0.53143 |  0:14:24s
epoch 49 | loss: 0.19445 | val_0_rmse: 0.4811  | val_1_rmse: 0.4839  |  0:14:42s
epoch 50 | loss: 0.19154 | val_0_rmse: 0.45319 | val_1_rmse: 0.45542 |  0:14:59s
epoch 51 | loss: 0.19732 | val_0_rmse: 0.46068 | val_1_rmse: 0.46276 |  0:15:17s
epoch 52 | loss: 0.19938 | val_0_rmse: 0.46854 | val_1_rmse: 0.47065 |  0:15:35s
epoch 53 | loss: 0.20168 | val_0_rmse: 0.47293 | val_1_rmse: 0.4757  |  0:15:52s
epoch 54 | loss: 0.19495 | val_0_rmse: 0.43952 | val_1_rmse: 0.44149 |  0:16:10s
epoch 55 | loss: 0.19276 | val_0_rmse: 0.44491 | val_1_rmse: 0.44714 |  0:16:27s
epoch 56 | loss: 0.19191 | val_0_rmse: 0.47812 | val_1_rmse: 0.48113 |  0:16:45s
epoch 57 | loss: 0.19143 | val_0_rmse: 0.5603  | val_1_rmse: 0.56132 |  0:17:03s
epoch 58 | loss: 0.19653 | val_0_rmse: 0.52295 | val_1_rmse: 0.52394 |  0:17:21s
epoch 59 | loss: 0.19503 | val_0_rmse: 0.4896  | val_1_rmse: 0.49203 |  0:17:39s
epoch 60 | loss: 0.18783 | val_0_rmse: 0.51275 | val_1_rmse: 0.51563 |  0:17:56s
epoch 61 | loss: 0.19276 | val_0_rmse: 0.46987 | val_1_rmse: 0.47171 |  0:18:14s
epoch 62 | loss: 0.19332 | val_0_rmse: 0.51419 | val_1_rmse: 0.51677 |  0:18:32s
epoch 63 | loss: 0.18569 | val_0_rmse: 0.47416 | val_1_rmse: 0.47666 |  0:18:49s
epoch 64 | loss: 0.18718 | val_0_rmse: 0.47132 | val_1_rmse: 0.47409 |  0:19:07s
epoch 65 | loss: 0.18509 | val_0_rmse: 0.52372 | val_1_rmse: 0.52613 |  0:19:25s
epoch 66 | loss: 0.19028 | val_0_rmse: 0.56447 | val_1_rmse: 0.56844 |  0:19:42s
epoch 67 | loss: 0.19502 | val_0_rmse: 0.46617 | val_1_rmse: 0.46763 |  0:20:00s
epoch 68 | loss: 0.19858 | val_0_rmse: 0.47103 | val_1_rmse: 0.47326 |  0:20:18s
epoch 69 | loss: 0.18898 | val_0_rmse: 0.46712 | val_1_rmse: 0.46966 |  0:20:35s
epoch 70 | loss: 0.18673 | val_0_rmse: 0.48751 | val_1_rmse: 0.4894  |  0:20:53s
epoch 71 | loss: 0.18667 | val_0_rmse: 0.45369 | val_1_rmse: 0.45573 |  0:21:10s
epoch 72 | loss: 0.18477 | val_0_rmse: 0.48871 | val_1_rmse: 0.49075 |  0:21:28s
epoch 73 | loss: 0.18324 | val_0_rmse: 0.47496 | val_1_rmse: 0.47672 |  0:21:46s
epoch 74 | loss: 0.18543 | val_0_rmse: 0.48309 | val_1_rmse: 0.48505 |  0:22:03s
epoch 75 | loss: 0.18205 | val_0_rmse: 0.51478 | val_1_rmse: 0.51825 |  0:22:21s
epoch 76 | loss: 0.18336 | val_0_rmse: 0.44768 | val_1_rmse: 0.44905 |  0:22:39s
epoch 77 | loss: 0.18093 | val_0_rmse: 0.45599 | val_1_rmse: 0.45834 |  0:22:56s

Early stopping occured at epoch 77 with best_epoch = 47 and best_val_1_rmse = 0.42879
Best weights from best epoch are automatically used!
ended training at: 07:10:15
Feature importance:
[('Area', 0.0), ('Baths', 0.19590551835771033), ('Beds', 1.2668438211502216e-06), ('Latitude', 0.17907471827538468), ('Longitude', 0.5383097976194016), ('Month', 0.08670869890368228), ('Year', 0.0)]
Mean squared error is of 12454737298.185024
Mean absolute error:65747.51322683628
MAPE:0.3516928708470746
R2 score:0.6870104132761705
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:10:16
epoch 0  | loss: 3.58402 | val_0_rmse: 0.4882  | val_1_rmse: 0.48883 |  0:00:17s
epoch 1  | loss: 0.28478 | val_0_rmse: 0.51274 | val_1_rmse: 0.51465 |  0:00:35s
epoch 2  | loss: 0.27615 | val_0_rmse: 0.5207  | val_1_rmse: 0.51969 |  0:00:52s
epoch 3  | loss: 0.25686 | val_0_rmse: 0.51036 | val_1_rmse: 0.51254 |  0:01:10s
epoch 4  | loss: 0.24423 | val_0_rmse: 0.54639 | val_1_rmse: 0.54895 |  0:01:27s
epoch 5  | loss: 0.23762 | val_0_rmse: 0.46433 | val_1_rmse: 0.46549 |  0:01:45s
epoch 6  | loss: 0.2547  | val_0_rmse: 0.49163 | val_1_rmse: 0.49407 |  0:02:03s
epoch 7  | loss: 0.23072 | val_0_rmse: 0.47553 | val_1_rmse: 0.47759 |  0:02:21s
epoch 8  | loss: 0.24905 | val_0_rmse: 0.44983 | val_1_rmse: 0.45058 |  0:02:38s
epoch 9  | loss: 0.21383 | val_0_rmse: 0.47038 | val_1_rmse: 0.47284 |  0:02:56s
epoch 10 | loss: 0.2221  | val_0_rmse: 0.45653 | val_1_rmse: 0.45692 |  0:03:14s
epoch 11 | loss: 0.229   | val_0_rmse: 0.49186 | val_1_rmse: 0.49424 |  0:03:31s
epoch 12 | loss: 0.22645 | val_0_rmse: 0.44115 | val_1_rmse: 0.44171 |  0:03:49s
epoch 13 | loss: 0.22967 | val_0_rmse: 0.4684  | val_1_rmse: 0.46779 |  0:04:06s
epoch 14 | loss: 0.21416 | val_0_rmse: 0.47168 | val_1_rmse: 0.47143 |  0:04:24s
epoch 15 | loss: 0.22246 | val_0_rmse: 0.4565  | val_1_rmse: 0.45793 |  0:04:41s
epoch 16 | loss: 0.21241 | val_0_rmse: 0.47682 | val_1_rmse: 0.47578 |  0:04:59s
epoch 17 | loss: 0.21952 | val_0_rmse: 0.4461  | val_1_rmse: 0.44723 |  0:05:17s
epoch 18 | loss: 0.21311 | val_0_rmse: 0.44591 | val_1_rmse: 0.4464  |  0:05:34s
epoch 19 | loss: 0.21791 | val_0_rmse: 0.4358  | val_1_rmse: 0.43664 |  0:05:52s
epoch 20 | loss: 0.20543 | val_0_rmse: 0.4452  | val_1_rmse: 0.44598 |  0:06:09s
epoch 21 | loss: 0.22185 | val_0_rmse: 0.46081 | val_1_rmse: 0.46046 |  0:06:27s
epoch 22 | loss: 0.20479 | val_0_rmse: 0.44227 | val_1_rmse: 0.44289 |  0:06:45s
epoch 23 | loss: 0.20729 | val_0_rmse: 0.45592 | val_1_rmse: 0.45798 |  0:07:02s
epoch 24 | loss: 0.20318 | val_0_rmse: 0.43627 | val_1_rmse: 0.43703 |  0:07:20s
epoch 25 | loss: 0.20243 | val_0_rmse: 0.48473 | val_1_rmse: 0.48727 |  0:07:37s
epoch 26 | loss: 0.20538 | val_0_rmse: 0.43815 | val_1_rmse: 0.4402  |  0:07:55s
epoch 27 | loss: 0.20894 | val_0_rmse: 0.4552  | val_1_rmse: 0.45529 |  0:08:12s
epoch 28 | loss: 0.19834 | val_0_rmse: 0.42972 | val_1_rmse: 0.43087 |  0:08:30s
epoch 29 | loss: 0.19991 | val_0_rmse: 0.51397 | val_1_rmse: 0.51674 |  0:08:48s
epoch 30 | loss: 0.21047 | val_0_rmse: 0.45253 | val_1_rmse: 0.45379 |  0:09:05s
epoch 31 | loss: 0.21201 | val_0_rmse: 0.46396 | val_1_rmse: 0.46585 |  0:09:23s
epoch 32 | loss: 0.2005  | val_0_rmse: 0.4754  | val_1_rmse: 0.47813 |  0:09:41s
epoch 33 | loss: 0.19697 | val_0_rmse: 0.43922 | val_1_rmse: 0.43968 |  0:09:58s
epoch 34 | loss: 0.19647 | val_0_rmse: 0.44312 | val_1_rmse: 0.44464 |  0:10:16s
epoch 35 | loss: 0.20071 | val_0_rmse: 0.63544 | val_1_rmse: 0.6373  |  0:10:33s
epoch 36 | loss: 0.21447 | val_0_rmse: 0.49063 | val_1_rmse: 0.49466 |  0:10:51s
epoch 37 | loss: 0.19977 | val_0_rmse: 0.48877 | val_1_rmse: 0.49117 |  0:11:09s
epoch 38 | loss: 0.19866 | val_0_rmse: 0.45639 | val_1_rmse: 0.45925 |  0:11:26s
epoch 39 | loss: 0.20927 | val_0_rmse: 0.45389 | val_1_rmse: 0.45419 |  0:11:44s
epoch 40 | loss: 0.19865 | val_0_rmse: 0.43544 | val_1_rmse: 0.43662 |  0:12:02s
epoch 41 | loss: 0.19416 | val_0_rmse: 0.43859 | val_1_rmse: 0.43938 |  0:12:19s
epoch 42 | loss: 0.19386 | val_0_rmse: 0.43543 | val_1_rmse: 0.43739 |  0:12:37s
epoch 43 | loss: 0.20195 | val_0_rmse: 0.45269 | val_1_rmse: 0.45571 |  0:12:54s
epoch 44 | loss: 0.19899 | val_0_rmse: 0.45878 | val_1_rmse: 0.46102 |  0:13:12s
epoch 45 | loss: 0.20142 | val_0_rmse: 0.45528 | val_1_rmse: 0.4562  |  0:13:30s
epoch 46 | loss: 0.20057 | val_0_rmse: 0.44379 | val_1_rmse: 0.44639 |  0:13:47s
epoch 47 | loss: 0.19413 | val_0_rmse: 0.42729 | val_1_rmse: 0.42871 |  0:14:05s
epoch 48 | loss: 0.19398 | val_0_rmse: 0.51947 | val_1_rmse: 0.52322 |  0:14:23s
epoch 49 | loss: 0.21118 | val_0_rmse: 0.43401 | val_1_rmse: 0.4363  |  0:14:40s
epoch 50 | loss: 0.19219 | val_0_rmse: 0.44853 | val_1_rmse: 0.45114 |  0:14:58s
epoch 51 | loss: 0.19087 | val_0_rmse: 0.45742 | val_1_rmse: 0.46009 |  0:15:16s
epoch 52 | loss: 0.18929 | val_0_rmse: 0.43873 | val_1_rmse: 0.44045 |  0:15:33s
epoch 53 | loss: 0.18956 | val_0_rmse: 0.43921 | val_1_rmse: 0.4413  |  0:15:51s
epoch 54 | loss: 0.19204 | val_0_rmse: 0.65815 | val_1_rmse: 0.66002 |  0:16:09s
epoch 55 | loss: 0.19354 | val_0_rmse: 0.51434 | val_1_rmse: 0.51768 |  0:16:26s
epoch 56 | loss: 0.1935  | val_0_rmse: 0.48619 | val_1_rmse: 0.48806 |  0:16:44s
epoch 57 | loss: 0.19047 | val_0_rmse: 0.51885 | val_1_rmse: 0.52197 |  0:17:01s
epoch 58 | loss: 0.19391 | val_0_rmse: 0.43017 | val_1_rmse: 0.43082 |  0:17:19s
epoch 59 | loss: 0.19229 | val_0_rmse: 0.42367 | val_1_rmse: 0.42513 |  0:17:37s
epoch 60 | loss: 0.19706 | val_0_rmse: 0.43675 | val_1_rmse: 0.43902 |  0:17:54s
epoch 61 | loss: 0.18814 | val_0_rmse: 0.44474 | val_1_rmse: 0.44686 |  0:18:12s
epoch 62 | loss: 0.18694 | val_0_rmse: 0.4501  | val_1_rmse: 0.45212 |  0:18:30s
epoch 63 | loss: 0.1868  | val_0_rmse: 0.47439 | val_1_rmse: 0.47451 |  0:18:47s
epoch 64 | loss: 0.18557 | val_0_rmse: 0.43097 | val_1_rmse: 0.43267 |  0:19:05s
epoch 65 | loss: 0.18578 | val_0_rmse: 0.46765 | val_1_rmse: 0.46924 |  0:19:23s
epoch 66 | loss: 0.18707 | val_0_rmse: 0.51043 | val_1_rmse: 0.51301 |  0:19:40s
epoch 67 | loss: 0.18306 | val_0_rmse: 0.51936 | val_1_rmse: 0.52113 |  0:19:58s
epoch 68 | loss: 0.1856  | val_0_rmse: 0.44867 | val_1_rmse: 0.4508  |  0:20:16s
epoch 69 | loss: 0.18485 | val_0_rmse: 0.47108 | val_1_rmse: 0.47343 |  0:20:33s
epoch 70 | loss: 0.18192 | val_0_rmse: 0.47075 | val_1_rmse: 0.47255 |  0:20:51s
epoch 71 | loss: 0.18225 | val_0_rmse: 0.57295 | val_1_rmse: 0.57512 |  0:21:09s
epoch 72 | loss: 0.18412 | val_0_rmse: 0.4369  | val_1_rmse: 0.43748 |  0:21:26s
epoch 73 | loss: 0.18168 | val_0_rmse: 0.45026 | val_1_rmse: 0.4527  |  0:21:44s
epoch 74 | loss: 0.18194 | val_0_rmse: 0.47778 | val_1_rmse: 0.4815  |  0:22:01s
epoch 75 | loss: 0.18312 | val_0_rmse: 0.51518 | val_1_rmse: 0.51623 |  0:22:19s
epoch 76 | loss: 0.18503 | val_0_rmse: 0.45615 | val_1_rmse: 0.45756 |  0:22:37s
epoch 77 | loss: 0.18491 | val_0_rmse: 0.44751 | val_1_rmse: 0.44966 |  0:22:54s
epoch 78 | loss: 0.18079 | val_0_rmse: 0.47436 | val_1_rmse: 0.47671 |  0:23:12s
epoch 79 | loss: 0.17972 | val_0_rmse: 0.46026 | val_1_rmse: 0.46213 |  0:23:29s
epoch 80 | loss: 0.17969 | val_0_rmse: 0.44522 | val_1_rmse: 0.4463  |  0:23:47s
epoch 81 | loss: 0.18012 | val_0_rmse: 0.49002 | val_1_rmse: 0.49196 |  0:24:05s
epoch 82 | loss: 0.18364 | val_0_rmse: 0.53398 | val_1_rmse: 0.53675 |  0:24:22s
epoch 83 | loss: 0.18163 | val_0_rmse: 0.47309 | val_1_rmse: 0.4761  |  0:24:40s
epoch 84 | loss: 0.17963 | val_0_rmse: 0.49454 | val_1_rmse: 0.49511 |  0:24:57s
epoch 85 | loss: 0.18109 | val_0_rmse: 0.43856 | val_1_rmse: 0.44017 |  0:25:15s
epoch 86 | loss: 0.18418 | val_0_rmse: 0.46254 | val_1_rmse: 0.46463 |  0:25:33s
epoch 87 | loss: 0.18011 | val_0_rmse: 0.46709 | val_1_rmse: 0.46962 |  0:25:50s
epoch 88 | loss: 0.18543 | val_0_rmse: 0.4287  | val_1_rmse: 0.42972 |  0:26:08s
epoch 89 | loss: 0.18095 | val_0_rmse: 0.44764 | val_1_rmse: 0.44962 |  0:26:25s

Early stopping occured at epoch 89 with best_epoch = 59 and best_val_1_rmse = 0.42513
Best weights from best epoch are automatically used!
ended training at: 07:36:47
Feature importance:
[('Area', 0.07466340149408099), ('Baths', 0.07754889825935032), ('Beds', 0.3496118507099155), ('Latitude', 0.24352626130911567), ('Longitude', 0.21039032897362359), ('Month', 0.012833075555437973), ('Year', 0.03142618369847591)]
Mean squared error is of 11064155703.250824
Mean absolute error:63294.26283904609
MAPE:0.38616436809033905
R2 score:0.7184939579885965
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:36:49
epoch 0  | loss: 3.67485 | val_0_rmse: 0.61907 | val_1_rmse: 0.61639 |  0:00:17s
epoch 1  | loss: 0.29889 | val_0_rmse: 0.50936 | val_1_rmse: 0.50796 |  0:00:35s
epoch 2  | loss: 0.29298 | val_0_rmse: 0.57999 | val_1_rmse: 0.57918 |  0:00:52s
epoch 3  | loss: 0.28814 | val_0_rmse: 0.513   | val_1_rmse: 0.51262 |  0:01:10s
epoch 4  | loss: 0.26628 | val_0_rmse: 0.47676 | val_1_rmse: 0.47499 |  0:01:28s
epoch 5  | loss: 0.23927 | val_0_rmse: 0.51966 | val_1_rmse: 0.51909 |  0:01:45s
epoch 6  | loss: 0.24043 | val_0_rmse: 0.48039 | val_1_rmse: 0.47972 |  0:02:03s
epoch 7  | loss: 0.23212 | val_0_rmse: 0.45659 | val_1_rmse: 0.45514 |  0:02:21s
epoch 8  | loss: 0.22249 | val_0_rmse: 0.51791 | val_1_rmse: 0.51793 |  0:02:38s
epoch 9  | loss: 0.21488 | val_0_rmse: 0.44486 | val_1_rmse: 0.44269 |  0:02:56s
epoch 10 | loss: 0.22484 | val_0_rmse: 0.52174 | val_1_rmse: 0.52167 |  0:03:14s
epoch 11 | loss: 0.22492 | val_0_rmse: 0.48687 | val_1_rmse: 0.48518 |  0:03:31s
epoch 12 | loss: 0.21721 | val_0_rmse: 0.44727 | val_1_rmse: 0.44438 |  0:03:49s
epoch 13 | loss: 0.21762 | val_0_rmse: 0.47803 | val_1_rmse: 0.47671 |  0:04:07s
epoch 14 | loss: 0.21778 | val_0_rmse: 0.54634 | val_1_rmse: 0.54555 |  0:04:24s
epoch 15 | loss: 0.22502 | val_0_rmse: 0.50631 | val_1_rmse: 0.50434 |  0:04:42s
epoch 16 | loss: 0.22948 | val_0_rmse: 0.48596 | val_1_rmse: 0.486   |  0:05:00s
epoch 17 | loss: 0.21582 | val_0_rmse: 0.47703 | val_1_rmse: 0.47561 |  0:05:17s
epoch 18 | loss: 0.20889 | val_0_rmse: 0.51486 | val_1_rmse: 0.51392 |  0:05:35s
epoch 19 | loss: 0.21486 | val_0_rmse: 0.49554 | val_1_rmse: 0.49475 |  0:05:53s
epoch 20 | loss: 0.21618 | val_0_rmse: 0.48075 | val_1_rmse: 0.47898 |  0:06:11s
epoch 21 | loss: 0.20763 | val_0_rmse: 0.5021  | val_1_rmse: 0.50192 |  0:06:28s
epoch 22 | loss: 0.20577 | val_0_rmse: 0.53268 | val_1_rmse: 0.53217 |  0:06:46s
epoch 23 | loss: 0.20373 | val_0_rmse: 0.54403 | val_1_rmse: 0.54417 |  0:07:03s
epoch 24 | loss: 0.20912 | val_0_rmse: 0.45462 | val_1_rmse: 0.45204 |  0:07:21s
epoch 25 | loss: 0.20803 | val_0_rmse: 0.53128 | val_1_rmse: 0.5308  |  0:07:39s
epoch 26 | loss: 0.20686 | val_0_rmse: 0.45513 | val_1_rmse: 0.45349 |  0:07:56s
epoch 27 | loss: 0.20233 | val_0_rmse: 0.53194 | val_1_rmse: 0.53159 |  0:08:14s
epoch 28 | loss: 0.202   | val_0_rmse: 0.52098 | val_1_rmse: 0.5205  |  0:08:32s
epoch 29 | loss: 0.20583 | val_0_rmse: 0.43668 | val_1_rmse: 0.43352 |  0:08:49s
epoch 30 | loss: 0.20089 | val_0_rmse: 0.44692 | val_1_rmse: 0.44474 |  0:09:07s
epoch 31 | loss: 0.20794 | val_0_rmse: 0.44198 | val_1_rmse: 0.44057 |  0:09:25s
epoch 32 | loss: 0.2045  | val_0_rmse: 0.47585 | val_1_rmse: 0.47533 |  0:09:42s
epoch 33 | loss: 0.20224 | val_0_rmse: 0.45446 | val_1_rmse: 0.45336 |  0:10:00s
epoch 34 | loss: 0.19757 | val_0_rmse: 0.48476 | val_1_rmse: 0.48331 |  0:10:17s
epoch 35 | loss: 0.19519 | val_0_rmse: 0.61542 | val_1_rmse: 0.61584 |  0:10:35s
epoch 36 | loss: 0.2043  | val_0_rmse: 0.45905 | val_1_rmse: 0.45623 |  0:10:53s
epoch 37 | loss: 0.19559 | val_0_rmse: 0.50061 | val_1_rmse: 0.49962 |  0:11:11s
epoch 38 | loss: 0.19822 | val_0_rmse: 0.51272 | val_1_rmse: 0.51141 |  0:11:28s
epoch 39 | loss: 0.19786 | val_0_rmse: 0.47734 | val_1_rmse: 0.47585 |  0:11:46s
epoch 40 | loss: 0.1964  | val_0_rmse: 0.44457 | val_1_rmse: 0.44225 |  0:12:04s
epoch 41 | loss: 0.19356 | val_0_rmse: 0.44887 | val_1_rmse: 0.4473  |  0:12:21s
epoch 42 | loss: 0.19326 | val_0_rmse: 0.43562 | val_1_rmse: 0.43282 |  0:12:39s
epoch 43 | loss: 0.18995 | val_0_rmse: 0.44036 | val_1_rmse: 0.43932 |  0:12:57s
epoch 44 | loss: 0.1891  | val_0_rmse: 0.45339 | val_1_rmse: 0.45184 |  0:13:14s
epoch 45 | loss: 0.19005 | val_0_rmse: 0.48387 | val_1_rmse: 0.48346 |  0:13:32s
epoch 46 | loss: 0.19781 | val_0_rmse: 0.44188 | val_1_rmse: 0.44167 |  0:13:50s
epoch 47 | loss: 0.19163 | val_0_rmse: 0.49488 | val_1_rmse: 0.49464 |  0:14:07s
epoch 48 | loss: 0.19361 | val_0_rmse: 0.48482 | val_1_rmse: 0.48459 |  0:14:25s
epoch 49 | loss: 0.19176 | val_0_rmse: 0.47236 | val_1_rmse: 0.47195 |  0:14:42s
epoch 50 | loss: 0.19373 | val_0_rmse: 0.43653 | val_1_rmse: 0.4347  |  0:15:00s
epoch 51 | loss: 0.19071 | val_0_rmse: 0.54292 | val_1_rmse: 0.54324 |  0:15:18s
epoch 52 | loss: 0.18858 | val_0_rmse: 0.59744 | val_1_rmse: 0.59761 |  0:15:35s
epoch 53 | loss: 0.19111 | val_0_rmse: 0.48535 | val_1_rmse: 0.4851  |  0:15:53s
epoch 54 | loss: 0.18767 | val_0_rmse: 0.45405 | val_1_rmse: 0.4517  |  0:16:11s
epoch 55 | loss: 0.18852 | val_0_rmse: 0.42811 | val_1_rmse: 0.42677 |  0:16:28s
epoch 56 | loss: 0.19591 | val_0_rmse: 0.52282 | val_1_rmse: 0.52148 |  0:16:46s
epoch 57 | loss: 0.19502 | val_0_rmse: 0.44301 | val_1_rmse: 0.44029 |  0:17:04s
epoch 58 | loss: 0.18762 | val_0_rmse: 0.47523 | val_1_rmse: 0.47428 |  0:17:21s
epoch 59 | loss: 0.18432 | val_0_rmse: 0.46351 | val_1_rmse: 0.46328 |  0:17:39s
epoch 60 | loss: 0.18931 | val_0_rmse: 0.52335 | val_1_rmse: 0.52082 |  0:17:57s
epoch 61 | loss: 0.19002 | val_0_rmse: 0.47188 | val_1_rmse: 0.47007 |  0:18:14s
epoch 62 | loss: 0.1844  | val_0_rmse: 0.46562 | val_1_rmse: 0.46541 |  0:18:32s
epoch 63 | loss: 0.18328 | val_0_rmse: 0.46511 | val_1_rmse: 0.46341 |  0:18:50s
epoch 64 | loss: 0.18484 | val_0_rmse: 0.49597 | val_1_rmse: 0.49596 |  0:19:07s
epoch 65 | loss: 0.18263 | val_0_rmse: 0.47455 | val_1_rmse: 0.47362 |  0:19:25s
epoch 66 | loss: 0.18196 | val_0_rmse: 0.47146 | val_1_rmse: 0.47027 |  0:19:42s
epoch 67 | loss: 0.18335 | val_0_rmse: 0.53591 | val_1_rmse: 0.53512 |  0:20:00s
epoch 68 | loss: 0.18384 | val_0_rmse: 0.54176 | val_1_rmse: 0.54155 |  0:20:18s
epoch 69 | loss: 0.18048 | val_0_rmse: 0.4963  | val_1_rmse: 0.49596 |  0:20:35s
epoch 70 | loss: 0.18681 | val_0_rmse: 0.45437 | val_1_rmse: 0.4535  |  0:20:53s
epoch 71 | loss: 0.1835  | val_0_rmse: 0.54665 | val_1_rmse: 0.54579 |  0:21:11s
epoch 72 | loss: 0.18339 | val_0_rmse: 0.616   | val_1_rmse: 0.61511 |  0:21:28s
epoch 73 | loss: 0.18662 | val_0_rmse: 0.50187 | val_1_rmse: 0.50191 |  0:21:46s
epoch 74 | loss: 0.18347 | val_0_rmse: 0.48173 | val_1_rmse: 0.48101 |  0:22:04s
epoch 75 | loss: 0.18129 | val_0_rmse: 0.47842 | val_1_rmse: 0.47629 |  0:22:21s
epoch 76 | loss: 0.18495 | val_0_rmse: 0.43897 | val_1_rmse: 0.43868 |  0:22:39s
epoch 77 | loss: 0.179   | val_0_rmse: 0.53255 | val_1_rmse: 0.53147 |  0:22:57s
epoch 78 | loss: 0.18442 | val_0_rmse: 0.62663 | val_1_rmse: 0.62712 |  0:23:14s
epoch 79 | loss: 0.18592 | val_0_rmse: 0.62491 | val_1_rmse: 0.62423 |  0:23:32s
epoch 80 | loss: 0.18483 | val_0_rmse: 0.5624  | val_1_rmse: 0.56183 |  0:23:50s
epoch 81 | loss: 0.18148 | val_0_rmse: 0.50987 | val_1_rmse: 0.50982 |  0:24:07s
epoch 82 | loss: 0.18029 | val_0_rmse: 0.4976  | val_1_rmse: 0.49705 |  0:24:25s
epoch 83 | loss: 0.18081 | val_0_rmse: 0.57269 | val_1_rmse: 0.57225 |  0:24:43s
epoch 84 | loss: 0.18164 | val_0_rmse: 0.54249 | val_1_rmse: 0.54304 |  0:25:00s
epoch 85 | loss: 0.17964 | val_0_rmse: 0.44667 | val_1_rmse: 0.44589 |  0:25:18s

Early stopping occured at epoch 85 with best_epoch = 55 and best_val_1_rmse = 0.42677
Best weights from best epoch are automatically used!
ended training at: 08:02:13
Feature importance:
[('Area', 0.27626906322396905), ('Baths', 0.12871810051457086), ('Beds', 0.005632070280777112), ('Latitude', 0.1909689278973639), ('Longitude', 0.25077571569910145), ('Month', 0.0), ('Year', 0.14763612238421767)]
Mean squared error is of 12856410297.844273
Mean absolute error:66094.01473161773
MAPE:0.35478705861442184
R2 score:0.6733053761897572
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:02:14
epoch 0  | loss: 3.51353 | val_0_rmse: 0.5152  | val_1_rmse: 0.51633 |  0:00:17s
epoch 1  | loss: 0.25978 | val_0_rmse: 0.51334 | val_1_rmse: 0.51312 |  0:00:35s
epoch 2  | loss: 0.24322 | val_0_rmse: 0.46915 | val_1_rmse: 0.47128 |  0:00:52s
epoch 3  | loss: 0.23834 | val_0_rmse: 0.47835 | val_1_rmse: 0.48083 |  0:01:10s
epoch 4  | loss: 0.24243 | val_0_rmse: 0.52225 | val_1_rmse: 0.52294 |  0:01:28s
epoch 5  | loss: 0.2257  | val_0_rmse: 0.45126 | val_1_rmse: 0.45352 |  0:01:46s
epoch 6  | loss: 0.23545 | val_0_rmse: 0.48278 | val_1_rmse: 0.48392 |  0:02:03s
epoch 7  | loss: 0.23111 | val_0_rmse: 0.46778 | val_1_rmse: 0.47105 |  0:02:21s
epoch 8  | loss: 0.23266 | val_0_rmse: 0.49821 | val_1_rmse: 0.50005 |  0:02:39s
epoch 9  | loss: 0.22924 | val_0_rmse: 0.45048 | val_1_rmse: 0.45311 |  0:02:56s
epoch 10 | loss: 0.22493 | val_0_rmse: 0.56151 | val_1_rmse: 0.56209 |  0:03:14s
epoch 11 | loss: 0.22956 | val_0_rmse: 0.46796 | val_1_rmse: 0.47171 |  0:03:31s
epoch 12 | loss: 0.21884 | val_0_rmse: 0.45174 | val_1_rmse: 0.45335 |  0:03:49s
epoch 13 | loss: 0.2183  | val_0_rmse: 0.47451 | val_1_rmse: 0.47622 |  0:04:07s
epoch 14 | loss: 0.21433 | val_0_rmse: 0.44132 | val_1_rmse: 0.44369 |  0:04:24s
epoch 15 | loss: 0.2227  | val_0_rmse: 0.50999 | val_1_rmse: 0.51194 |  0:04:42s
epoch 16 | loss: 0.21173 | val_0_rmse: 0.50215 | val_1_rmse: 0.50416 |  0:05:00s
epoch 17 | loss: 0.23589 | val_0_rmse: 0.5355  | val_1_rmse: 0.53781 |  0:05:17s
epoch 18 | loss: 0.21914 | val_0_rmse: 0.44362 | val_1_rmse: 0.44583 |  0:05:35s
epoch 19 | loss: 0.22449 | val_0_rmse: 0.48902 | val_1_rmse: 0.49217 |  0:05:53s
epoch 20 | loss: 0.21593 | val_0_rmse: 0.48203 | val_1_rmse: 0.48315 |  0:06:11s
epoch 21 | loss: 0.21964 | val_0_rmse: 0.43477 | val_1_rmse: 0.43747 |  0:06:28s
epoch 22 | loss: 0.21526 | val_0_rmse: 0.45244 | val_1_rmse: 0.45387 |  0:06:46s
epoch 23 | loss: 0.21251 | val_0_rmse: 0.44426 | val_1_rmse: 0.44673 |  0:07:03s
epoch 24 | loss: 0.21726 | val_0_rmse: 0.4662  | val_1_rmse: 0.46952 |  0:07:21s
epoch 25 | loss: 0.20262 | val_0_rmse: 0.46374 | val_1_rmse: 0.46641 |  0:07:39s
epoch 26 | loss: 0.20764 | val_0_rmse: 0.4589  | val_1_rmse: 0.46237 |  0:07:56s
epoch 27 | loss: 0.2165  | val_0_rmse: 0.51033 | val_1_rmse: 0.51139 |  0:08:14s
epoch 28 | loss: 0.20806 | val_0_rmse: 0.45303 | val_1_rmse: 0.45487 |  0:08:32s
epoch 29 | loss: 0.21133 | val_0_rmse: 0.4411  | val_1_rmse: 0.44388 |  0:08:49s
epoch 30 | loss: 0.20751 | val_0_rmse: 0.49823 | val_1_rmse: 0.49984 |  0:09:07s
epoch 31 | loss: 0.21624 | val_0_rmse: 0.49955 | val_1_rmse: 0.502   |  0:09:24s
epoch 32 | loss: 0.22157 | val_0_rmse: 0.60038 | val_1_rmse: 0.6013  |  0:09:42s
epoch 33 | loss: 0.22299 | val_0_rmse: 0.4736  | val_1_rmse: 0.47636 |  0:10:00s
epoch 34 | loss: 0.20387 | val_0_rmse: 0.46313 | val_1_rmse: 0.46431 |  0:10:17s
epoch 35 | loss: 0.20756 | val_0_rmse: 0.53016 | val_1_rmse: 0.53023 |  0:10:35s
epoch 36 | loss: 0.20825 | val_0_rmse: 0.45861 | val_1_rmse: 0.46208 |  0:10:53s
epoch 37 | loss: 0.2065  | val_0_rmse: 0.44234 | val_1_rmse: 0.44601 |  0:11:10s
epoch 38 | loss: 0.20011 | val_0_rmse: 0.50046 | val_1_rmse: 0.5023  |  0:11:28s
epoch 39 | loss: 0.20341 | val_0_rmse: 0.44974 | val_1_rmse: 0.4535  |  0:11:46s
epoch 40 | loss: 0.19731 | val_0_rmse: 0.47754 | val_1_rmse: 0.48023 |  0:12:03s
epoch 41 | loss: 0.20138 | val_0_rmse: 0.48072 | val_1_rmse: 0.48247 |  0:12:21s
epoch 42 | loss: 0.19635 | val_0_rmse: 0.47074 | val_1_rmse: 0.47368 |  0:12:38s
epoch 43 | loss: 0.20916 | val_0_rmse: 0.46748 | val_1_rmse: 0.47065 |  0:12:56s
epoch 44 | loss: 0.19916 | val_0_rmse: 0.4315  | val_1_rmse: 0.43473 |  0:13:14s
epoch 45 | loss: 0.19674 | val_0_rmse: 0.47435 | val_1_rmse: 0.47618 |  0:13:31s
epoch 46 | loss: 0.19228 | val_0_rmse: 0.48619 | val_1_rmse: 0.48929 |  0:13:49s
epoch 47 | loss: 0.19453 | val_0_rmse: 0.44846 | val_1_rmse: 0.45152 |  0:14:07s
epoch 48 | loss: 0.19115 | val_0_rmse: 0.48721 | val_1_rmse: 0.48906 |  0:14:24s
epoch 49 | loss: 0.20127 | val_0_rmse: 0.47406 | val_1_rmse: 0.47745 |  0:14:42s
epoch 50 | loss: 0.25819 | val_0_rmse: 0.57892 | val_1_rmse: 0.58015 |  0:15:00s
epoch 51 | loss: 0.22967 | val_0_rmse: 0.44657 | val_1_rmse: 0.44888 |  0:15:18s
epoch 52 | loss: 0.21664 | val_0_rmse: 0.46577 | val_1_rmse: 0.46909 |  0:15:35s
epoch 53 | loss: 0.21379 | val_0_rmse: 0.49724 | val_1_rmse: 0.50087 |  0:15:53s
epoch 54 | loss: 0.20692 | val_0_rmse: 0.46332 | val_1_rmse: 0.46677 |  0:16:11s
epoch 55 | loss: 0.20542 | val_0_rmse: 0.45656 | val_1_rmse: 0.45888 |  0:16:28s
epoch 56 | loss: 0.20382 | val_0_rmse: 0.45857 | val_1_rmse: 0.46235 |  0:16:46s
epoch 57 | loss: 0.20126 | val_0_rmse: 0.46678 | val_1_rmse: 0.47095 |  0:17:04s
epoch 58 | loss: 0.2024  | val_0_rmse: 0.44208 | val_1_rmse: 0.44518 |  0:17:21s
epoch 59 | loss: 0.20111 | val_0_rmse: 0.49735 | val_1_rmse: 0.49891 |  0:17:39s
epoch 60 | loss: 0.20041 | val_0_rmse: 0.45423 | val_1_rmse: 0.45566 |  0:17:57s
epoch 61 | loss: 0.19833 | val_0_rmse: 0.46721 | val_1_rmse: 0.46907 |  0:18:14s
epoch 62 | loss: 0.19736 | val_0_rmse: 0.45184 | val_1_rmse: 0.4545  |  0:18:32s
epoch 63 | loss: 0.19744 | val_0_rmse: 0.46107 | val_1_rmse: 0.46437 |  0:18:50s
epoch 64 | loss: 0.20045 | val_0_rmse: 0.46566 | val_1_rmse: 0.46912 |  0:19:07s
epoch 65 | loss: 0.19619 | val_0_rmse: 0.45376 | val_1_rmse: 0.45675 |  0:19:25s
epoch 66 | loss: 0.19224 | val_0_rmse: 0.4758  | val_1_rmse: 0.47791 |  0:19:43s
epoch 67 | loss: 0.19535 | val_0_rmse: 0.48342 | val_1_rmse: 0.48651 |  0:20:00s
epoch 68 | loss: 0.19676 | val_0_rmse: 0.43972 | val_1_rmse: 0.44242 |  0:20:18s
epoch 69 | loss: 0.19595 | val_0_rmse: 0.47616 | val_1_rmse: 0.47649 |  0:20:36s
epoch 70 | loss: 0.1971  | val_0_rmse: 0.47614 | val_1_rmse: 0.47977 |  0:20:53s
epoch 71 | loss: 0.19295 | val_0_rmse: 0.55392 | val_1_rmse: 0.55563 |  0:21:11s
epoch 72 | loss: 0.19527 | val_0_rmse: 0.47287 | val_1_rmse: 0.47579 |  0:21:29s
epoch 73 | loss: 0.1918  | val_0_rmse: 0.46612 | val_1_rmse: 0.46878 |  0:21:46s
epoch 74 | loss: 0.19175 | val_0_rmse: 0.46177 | val_1_rmse: 0.46374 |  0:22:04s

Early stopping occured at epoch 74 with best_epoch = 44 and best_val_1_rmse = 0.43473
Best weights from best epoch are automatically used!
ended training at: 08:24:24
Feature importance:
[('Area', 0.04479479549429722), ('Baths', 0.017798692291556506), ('Beds', 0.0), ('Latitude', 0.38069335400469084), ('Longitude', 0.36434774003750614), ('Month', 0.1923654181719493), ('Year', 0.0)]
Mean squared error is of 12929195772.979933
Mean absolute error:66433.21132810976
MAPE:0.35945175443309774
R2 score:0.6720014319473342
------------------------------------------------------------------
