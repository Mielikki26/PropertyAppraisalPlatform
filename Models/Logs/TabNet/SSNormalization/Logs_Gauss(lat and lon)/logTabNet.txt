TabNet Logs:

Saving copy of script...
In this script all datasets are increased in size up to the size of the biggest dataset by sampling random rows and modifying them with a noise depending on the standard deviation of the value in questionOnly latitude and longitude are used in trainingThis is done to test the possibility that the variance in datasets sizes is decreasing performanceBy evening out the sizes its excepted that the model achieves better performance
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:32:21
epoch 0  | loss: 1.0186  | val_0_rmse: 0.99994 | val_1_rmse: 1.00464 |  0:00:09s
epoch 1  | loss: 0.9719  | val_0_rmse: 1.01241 | val_1_rmse: 1.01826 |  0:00:17s
epoch 2  | loss: 0.97224 | val_0_rmse: 1.00079 | val_1_rmse: 1.00521 |  0:00:24s
epoch 3  | loss: 0.97125 | val_0_rmse: 1.00927 | val_1_rmse: 1.01518 |  0:00:31s
epoch 4  | loss: 0.97049 | val_0_rmse: 1.00971 | val_1_rmse: 1.0151  |  0:00:38s
epoch 5  | loss: 0.96931 | val_0_rmse: 0.9976  | val_1_rmse: 1.00285 |  0:00:45s
epoch 6  | loss: 0.9698  | val_0_rmse: 0.99992 | val_1_rmse: 1.0059  |  0:00:52s
epoch 7  | loss: 0.9691  | val_0_rmse: 1.01022 | val_1_rmse: 1.01464 |  0:01:00s
epoch 8  | loss: 0.97007 | val_0_rmse: 1.00505 | val_1_rmse: 1.0097  |  0:01:07s
epoch 9  | loss: 0.96955 | val_0_rmse: 1.01422 | val_1_rmse: 1.01996 |  0:01:14s
epoch 10 | loss: 0.96881 | val_0_rmse: 0.99917 | val_1_rmse: 1.00465 |  0:01:21s
epoch 11 | loss: 0.96771 | val_0_rmse: 1.00871 | val_1_rmse: 1.01246 |  0:01:28s
epoch 12 | loss: 0.96804 | val_0_rmse: 1.00428 | val_1_rmse: 1.00926 |  0:01:36s
epoch 13 | loss: 0.96864 | val_0_rmse: 1.01079 | val_1_rmse: 1.01678 |  0:01:43s
epoch 14 | loss: 0.96853 | val_0_rmse: 1.00976 | val_1_rmse: 1.01569 |  0:01:50s
epoch 15 | loss: 0.96832 | val_0_rmse: 1.00165 | val_1_rmse: 1.00724 |  0:01:57s
epoch 16 | loss: 0.96794 | val_0_rmse: 1.00614 | val_1_rmse: 1.01233 |  0:02:04s
epoch 17 | loss: 0.96713 | val_0_rmse: 1.00156 | val_1_rmse: 1.00713 |  0:02:12s
epoch 18 | loss: 0.96827 | val_0_rmse: 1.01417 | val_1_rmse: 1.02066 |  0:02:19s
epoch 19 | loss: 0.96884 | val_0_rmse: 1.00864 | val_1_rmse: 1.01494 |  0:02:26s
epoch 20 | loss: 0.96858 | val_0_rmse: 1.01268 | val_1_rmse: 1.01623 |  0:02:33s
epoch 21 | loss: 0.96777 | val_0_rmse: 1.00815 | val_1_rmse: 1.01454 |  0:02:40s
epoch 22 | loss: 0.96777 | val_0_rmse: 1.0024  | val_1_rmse: 1.00832 |  0:02:47s
epoch 23 | loss: 0.96843 | val_0_rmse: 1.01193 | val_1_rmse: 1.01722 |  0:02:54s
epoch 24 | loss: 0.96687 | val_0_rmse: 1.00095 | val_1_rmse: 1.00722 |  0:03:01s
epoch 25 | loss: 0.9687  | val_0_rmse: 1.00065 | val_1_rmse: 1.00606 |  0:03:09s
epoch 26 | loss: 0.96708 | val_0_rmse: 1.00853 | val_1_rmse: 1.01472 |  0:03:16s
epoch 27 | loss: 0.96739 | val_0_rmse: 1.00271 | val_1_rmse: 1.00852 |  0:03:23s
epoch 28 | loss: 0.96768 | val_0_rmse: 1.00942 | val_1_rmse: 1.01509 |  0:03:30s
epoch 29 | loss: 0.96674 | val_0_rmse: 1.00238 | val_1_rmse: 1.00864 |  0:03:37s
epoch 30 | loss: 0.96789 | val_0_rmse: 1.00341 | val_1_rmse: 1.00882 |  0:03:44s
epoch 31 | loss: 0.9676  | val_0_rmse: 1.00074 | val_1_rmse: 1.00606 |  0:03:51s
epoch 32 | loss: 0.96725 | val_0_rmse: 1.01351 | val_1_rmse: 1.01967 |  0:03:59s
epoch 33 | loss: 0.96775 | val_0_rmse: 1.00279 | val_1_rmse: 1.0086  |  0:04:06s
epoch 34 | loss: 0.96757 | val_0_rmse: 1.01012 | val_1_rmse: 1.01534 |  0:04:14s
epoch 35 | loss: 0.96762 | val_0_rmse: 0.9964  | val_1_rmse: 1.00162 |  0:04:21s
epoch 36 | loss: 0.96801 | val_0_rmse: 1.01158 | val_1_rmse: 1.01323 |  0:04:28s
epoch 37 | loss: 0.96745 | val_0_rmse: 1.01334 | val_1_rmse: 1.01952 |  0:04:35s
epoch 38 | loss: 0.96826 | val_0_rmse: 1.00971 | val_1_rmse: 1.01119 |  0:04:42s
epoch 39 | loss: 0.96713 | val_0_rmse: 1.01025 | val_1_rmse: 1.01617 |  0:04:50s
epoch 40 | loss: 0.96772 | val_0_rmse: 1.01594 | val_1_rmse: 1.02249 |  0:04:57s
epoch 41 | loss: 0.96772 | val_0_rmse: 1.0014  | val_1_rmse: 1.00726 |  0:05:04s
epoch 42 | loss: 0.96737 | val_0_rmse: 1.00619 | val_1_rmse: 1.01218 |  0:05:11s
epoch 43 | loss: 0.96774 | val_0_rmse: 1.0037  | val_1_rmse: 1.0093  |  0:05:18s
epoch 44 | loss: 0.96631 | val_0_rmse: 1.00088 | val_1_rmse: 1.00667 |  0:05:26s
epoch 45 | loss: 0.96695 | val_0_rmse: 1.00769 | val_1_rmse: 1.01376 |  0:05:33s
epoch 46 | loss: 0.96669 | val_0_rmse: 1.0068  | val_1_rmse: 1.00867 |  0:05:40s
epoch 47 | loss: 0.9669  | val_0_rmse: 1.00358 | val_1_rmse: 1.00937 |  0:05:47s
epoch 48 | loss: 0.96698 | val_0_rmse: 1.01782 | val_1_rmse: 1.02394 |  0:05:54s
epoch 49 | loss: 0.96704 | val_0_rmse: 1.00568 | val_1_rmse: 1.01119 |  0:06:01s
epoch 50 | loss: 0.96795 | val_0_rmse: 1.00376 | val_1_rmse: 1.00973 |  0:06:09s
epoch 51 | loss: 0.96621 | val_0_rmse: 1.01402 | val_1_rmse: 1.01616 |  0:06:16s
epoch 52 | loss: 0.9665  | val_0_rmse: 1.01136 | val_1_rmse: 1.01792 |  0:06:23s
epoch 53 | loss: 0.96658 | val_0_rmse: 1.00986 | val_1_rmse: 1.00941 |  0:06:30s
epoch 54 | loss: 0.9665  | val_0_rmse: 1.0084  | val_1_rmse: 1.01414 |  0:06:37s
epoch 55 | loss: 0.96728 | val_0_rmse: 1.00144 | val_1_rmse: 1.00696 |  0:06:44s
epoch 56 | loss: 0.96683 | val_0_rmse: 1.00166 | val_1_rmse: 1.00579 |  0:06:52s
epoch 57 | loss: 0.96658 | val_0_rmse: 1.00756 | val_1_rmse: 1.01293 |  0:06:59s
epoch 58 | loss: 0.96695 | val_0_rmse: 1.0054  | val_1_rmse: 1.00939 |  0:07:06s
epoch 59 | loss: 0.96763 | val_0_rmse: 1.00662 | val_1_rmse: 1.01155 |  0:07:13s
epoch 60 | loss: 0.96726 | val_0_rmse: 1.01464 | val_1_rmse: 1.0212  |  0:07:20s
epoch 61 | loss: 0.96723 | val_0_rmse: 1.00506 | val_1_rmse: 1.01111 |  0:07:28s
epoch 62 | loss: 0.96674 | val_0_rmse: 1.01175 | val_1_rmse: 1.01145 |  0:07:35s
epoch 63 | loss: 0.96705 | val_0_rmse: 1.00664 | val_1_rmse: 1.01276 |  0:07:42s
epoch 64 | loss: 0.9664  | val_0_rmse: 1.00381 | val_1_rmse: 1.00992 |  0:07:49s
epoch 65 | loss: 0.96688 | val_0_rmse: 1.01837 | val_1_rmse: 1.02466 |  0:07:56s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 1.00162
Best weights from best epoch are automatically used!
ended training at: 14:40:21
Feature importance:
[('Latitude', 0.7358652615806313), ('Longitude', 0.26413473841936874)]
Mean squared error is of 22669271488.65383
Mean absolute error:120841.77006915548
MAPE:0.44408098411145125
R2 score:0.004361608272456463
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:40:22
epoch 0  | loss: 1.04197 | val_0_rmse: 0.99503 | val_1_rmse: 0.99072 |  0:00:07s
epoch 1  | loss: 0.9804  | val_0_rmse: 0.99507 | val_1_rmse: 0.99137 |  0:00:14s
epoch 2  | loss: 0.9746  | val_0_rmse: 0.99153 | val_1_rmse: 0.98794 |  0:00:21s
epoch 3  | loss: 0.97429 | val_0_rmse: 0.98878 | val_1_rmse: 0.9856  |  0:00:29s
epoch 4  | loss: 0.97232 | val_0_rmse: 0.98995 | val_1_rmse: 0.98615 |  0:00:36s
epoch 5  | loss: 0.97388 | val_0_rmse: 0.98858 | val_1_rmse: 0.98507 |  0:00:43s
epoch 6  | loss: 0.97316 | val_0_rmse: 0.99188 | val_1_rmse: 0.98764 |  0:00:50s
epoch 7  | loss: 0.972   | val_0_rmse: 0.98888 | val_1_rmse: 0.98474 |  0:00:57s
epoch 8  | loss: 0.97047 | val_0_rmse: 0.98737 | val_1_rmse: 0.98355 |  0:01:05s
epoch 9  | loss: 0.9709  | val_0_rmse: 0.98728 | val_1_rmse: 0.98326 |  0:01:12s
epoch 10 | loss: 0.97107 | val_0_rmse: 0.99221 | val_1_rmse: 0.98875 |  0:01:19s
epoch 11 | loss: 0.97215 | val_0_rmse: 0.98705 | val_1_rmse: 0.98369 |  0:01:26s
epoch 12 | loss: 0.97228 | val_0_rmse: 0.99491 | val_1_rmse: 0.9909  |  0:01:33s
epoch 13 | loss: 0.97191 | val_0_rmse: 0.98749 | val_1_rmse: 0.98402 |  0:01:41s
epoch 14 | loss: 0.97244 | val_0_rmse: 0.99476 | val_1_rmse: 0.99131 |  0:01:48s
epoch 15 | loss: 0.97206 | val_0_rmse: 0.98989 | val_1_rmse: 0.98541 |  0:01:55s
epoch 16 | loss: 0.9708  | val_0_rmse: 0.99259 | val_1_rmse: 0.98912 |  0:02:02s
epoch 17 | loss: 0.97159 | val_0_rmse: 0.98869 | val_1_rmse: 0.98554 |  0:02:09s
epoch 18 | loss: 0.97067 | val_0_rmse: 0.98928 | val_1_rmse: 0.98545 |  0:02:17s
epoch 19 | loss: 0.97125 | val_0_rmse: 0.989   | val_1_rmse: 0.9848  |  0:02:24s
epoch 20 | loss: 0.97065 | val_0_rmse: 0.9898  | val_1_rmse: 0.98643 |  0:02:31s
epoch 21 | loss: 0.97136 | val_0_rmse: 0.98645 | val_1_rmse: 0.98266 |  0:02:38s
epoch 22 | loss: 0.97043 | val_0_rmse: 0.98815 | val_1_rmse: 0.98395 |  0:02:46s
epoch 23 | loss: 0.97002 | val_0_rmse: 0.98669 | val_1_rmse: 0.98285 |  0:02:53s
epoch 24 | loss: 0.97026 | val_0_rmse: 0.98675 | val_1_rmse: 0.98267 |  0:03:00s
epoch 25 | loss: 0.97092 | val_0_rmse: 0.98512 | val_1_rmse: 0.98173 |  0:03:08s
epoch 26 | loss: 0.97059 | val_0_rmse: 0.98887 | val_1_rmse: 0.98476 |  0:03:15s
epoch 27 | loss: 0.97044 | val_0_rmse: 0.98884 | val_1_rmse: 0.98466 |  0:03:22s
epoch 28 | loss: 0.97055 | val_0_rmse: 0.98951 | val_1_rmse: 0.98567 |  0:03:29s
epoch 29 | loss: 0.97015 | val_0_rmse: 0.99052 | val_1_rmse: 0.98666 |  0:03:36s
epoch 30 | loss: 0.97052 | val_0_rmse: 0.98768 | val_1_rmse: 0.98361 |  0:03:44s
epoch 31 | loss: 0.97016 | val_0_rmse: 0.9896  | val_1_rmse: 0.985   |  0:03:51s
epoch 32 | loss: 0.97066 | val_0_rmse: 0.98928 | val_1_rmse: 0.98469 |  0:03:58s
epoch 33 | loss: 0.9704  | val_0_rmse: 0.98837 | val_1_rmse: 0.9846  |  0:04:05s
epoch 34 | loss: 0.96925 | val_0_rmse: 0.98652 | val_1_rmse: 0.98268 |  0:04:13s
epoch 35 | loss: 0.97066 | val_0_rmse: 0.99161 | val_1_rmse: 0.98847 |  0:04:20s
epoch 36 | loss: 0.96998 | val_0_rmse: 0.99423 | val_1_rmse: 0.99004 |  0:04:27s
epoch 37 | loss: 0.97064 | val_0_rmse: 0.98969 | val_1_rmse: 0.98648 |  0:04:34s
epoch 38 | loss: 0.97076 | val_0_rmse: 0.9889  | val_1_rmse: 0.98568 |  0:04:42s
epoch 39 | loss: 0.96944 | val_0_rmse: 0.98664 | val_1_rmse: 0.98347 |  0:04:49s
epoch 40 | loss: 0.97061 | val_0_rmse: 0.98829 | val_1_rmse: 1.07397 |  0:04:56s
epoch 41 | loss: 0.96934 | val_0_rmse: 0.9888  | val_1_rmse: 1.0762  |  0:05:03s
epoch 42 | loss: 0.97082 | val_0_rmse: 0.9879  | val_1_rmse: 1.0817  |  0:05:10s
epoch 43 | loss: 0.96936 | val_0_rmse: 0.98741 | val_1_rmse: 1.04002 |  0:05:18s
epoch 44 | loss: 0.9698  | val_0_rmse: 0.9894  | val_1_rmse: 0.98511 |  0:05:25s
epoch 45 | loss: 0.97075 | val_0_rmse: 0.98997 | val_1_rmse: 0.98601 |  0:05:32s
epoch 46 | loss: 0.97028 | val_0_rmse: 0.98779 | val_1_rmse: 0.98379 |  0:05:40s
epoch 47 | loss: 0.96966 | val_0_rmse: 0.98841 | val_1_rmse: 0.98541 |  0:05:47s
epoch 48 | loss: 0.96999 | val_0_rmse: 0.98873 | val_1_rmse: 0.98416 |  0:05:54s
epoch 49 | loss: 0.96915 | val_0_rmse: 0.98994 | val_1_rmse: 0.98676 |  0:06:01s
epoch 50 | loss: 0.97079 | val_0_rmse: 0.98835 | val_1_rmse: 0.98464 |  0:06:08s
epoch 51 | loss: 0.96953 | val_0_rmse: 0.98617 | val_1_rmse: 0.98194 |  0:06:16s
epoch 52 | loss: 0.97008 | val_0_rmse: 0.98869 | val_1_rmse: 0.98507 |  0:06:23s
epoch 53 | loss: 0.96939 | val_0_rmse: 0.98742 | val_1_rmse: 0.98395 |  0:06:30s
epoch 54 | loss: 0.96838 | val_0_rmse: 0.98858 | val_1_rmse: 0.98558 |  0:06:37s
epoch 55 | loss: 0.96896 | val_0_rmse: 0.98746 | val_1_rmse: 0.98421 |  0:06:45s

Early stopping occured at epoch 55 with best_epoch = 25 and best_val_1_rmse = 0.98173
Best weights from best epoch are automatically used!
ended training at: 14:47:09
Feature importance:
[('Latitude', 0.813617323925116), ('Longitude', 0.18638267607488399)]
Mean squared error is of 22070616920.1655
Mean absolute error:118161.23092640752
MAPE:0.42502950076366575
R2 score:0.028282801589059048
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:47:10
epoch 0  | loss: 0.98037 | val_0_rmse: 1.019   | val_1_rmse: 1.01158 |  0:00:07s
epoch 1  | loss: 0.81564 | val_0_rmse: 0.91417 | val_1_rmse: 0.90704 |  0:00:14s
epoch 2  | loss: 0.81394 | val_0_rmse: 0.91647 | val_1_rmse: 0.9081  |  0:00:21s
epoch 3  | loss: 0.80231 | val_0_rmse: 0.8878  | val_1_rmse: 0.8824  |  0:00:29s
epoch 4  | loss: 0.79771 | val_0_rmse: 0.92909 | val_1_rmse: 0.92091 |  0:00:36s
epoch 5  | loss: 0.8022  | val_0_rmse: 0.98728 | val_1_rmse: 0.97991 |  0:00:43s
epoch 6  | loss: 0.80243 | val_0_rmse: 0.90995 | val_1_rmse: 0.90278 |  0:00:50s
epoch 7  | loss: 0.80102 | val_0_rmse: 0.89919 | val_1_rmse: 0.89284 |  0:00:58s
epoch 8  | loss: 0.79848 | val_0_rmse: 0.93795 | val_1_rmse: 0.92892 |  0:01:05s
epoch 9  | loss: 0.79599 | val_0_rmse: 0.89529 | val_1_rmse: 0.88887 |  0:01:12s
epoch 10 | loss: 0.79769 | val_0_rmse: 0.95215 | val_1_rmse: 0.94539 |  0:01:20s
epoch 11 | loss: 0.79571 | val_0_rmse: 0.89066 | val_1_rmse: 0.88401 |  0:01:28s
epoch 12 | loss: 0.79627 | val_0_rmse: 0.90848 | val_1_rmse: 0.90269 |  0:01:35s
epoch 13 | loss: 0.80281 | val_0_rmse: 0.90494 | val_1_rmse: 0.89764 |  0:01:42s
epoch 14 | loss: 0.80433 | val_0_rmse: 0.94795 | val_1_rmse: 0.94181 |  0:01:50s
epoch 15 | loss: 0.79513 | val_0_rmse: 0.96129 | val_1_rmse: 0.95348 |  0:01:57s
epoch 16 | loss: 0.79527 | val_0_rmse: 0.96666 | val_1_rmse: 0.95871 |  0:02:04s
epoch 17 | loss: 0.7975  | val_0_rmse: 0.93374 | val_1_rmse: 0.92539 |  0:02:12s
epoch 18 | loss: 0.79304 | val_0_rmse: 0.95753 | val_1_rmse: 0.94947 |  0:02:19s
epoch 19 | loss: 0.79146 | val_0_rmse: 0.94792 | val_1_rmse: 0.93993 |  0:02:26s
epoch 20 | loss: 0.79248 | val_0_rmse: 0.8886  | val_1_rmse: 0.88337 |  0:02:33s
epoch 21 | loss: 0.79375 | val_0_rmse: 0.99515 | val_1_rmse: 0.98656 |  0:02:41s
epoch 22 | loss: 0.79354 | val_0_rmse: 0.95533 | val_1_rmse: 0.94724 |  0:02:48s
epoch 23 | loss: 0.79448 | val_0_rmse: 0.90086 | val_1_rmse: 0.89441 |  0:02:55s
epoch 24 | loss: 0.79226 | val_0_rmse: 0.97469 | val_1_rmse: 0.96554 |  0:03:02s
epoch 25 | loss: 0.79657 | val_0_rmse: 0.94316 | val_1_rmse: 0.93527 |  0:03:10s
epoch 26 | loss: 0.78898 | val_0_rmse: 0.94321 | val_1_rmse: 0.93608 |  0:03:17s
epoch 27 | loss: 0.78983 | val_0_rmse: 0.89833 | val_1_rmse: 0.8912  |  0:03:25s
epoch 28 | loss: 0.78987 | val_0_rmse: 0.94482 | val_1_rmse: 0.93621 |  0:03:32s
epoch 29 | loss: 0.79355 | val_0_rmse: 0.95496 | val_1_rmse: 0.94747 |  0:03:39s
epoch 30 | loss: 0.79037 | val_0_rmse: 0.90358 | val_1_rmse: 0.89583 |  0:03:46s
epoch 31 | loss: 0.79559 | val_0_rmse: 0.94383 | val_1_rmse: 0.93572 |  0:03:54s
epoch 32 | loss: 0.78839 | val_0_rmse: 0.88175 | val_1_rmse: 0.87652 |  0:04:01s
epoch 33 | loss: 0.79175 | val_0_rmse: 0.98467 | val_1_rmse: 0.97628 |  0:04:08s
epoch 34 | loss: 0.79044 | val_0_rmse: 0.93471 | val_1_rmse: 0.92656 |  0:04:15s
epoch 35 | loss: 0.79056 | val_0_rmse: 0.90026 | val_1_rmse: 0.89511 |  0:04:23s
epoch 36 | loss: 0.78626 | val_0_rmse: 0.94102 | val_1_rmse: 0.93242 |  0:04:30s
epoch 37 | loss: 0.78647 | val_0_rmse: 1.09102 | val_1_rmse: 1.08322 |  0:04:37s
epoch 38 | loss: 0.78571 | val_0_rmse: 0.92271 | val_1_rmse: 0.91749 |  0:04:44s
epoch 39 | loss: 0.78487 | val_0_rmse: 0.93141 | val_1_rmse: 0.92346 |  0:04:52s
epoch 40 | loss: 0.78751 | val_0_rmse: 0.8892  | val_1_rmse: 0.88415 |  0:04:59s
epoch 41 | loss: 0.78488 | val_0_rmse: 0.94182 | val_1_rmse: 0.93291 |  0:05:06s
epoch 42 | loss: 0.7855  | val_0_rmse: 0.95116 | val_1_rmse: 0.94217 |  0:05:13s
epoch 43 | loss: 0.7884  | val_0_rmse: 0.92547 | val_1_rmse: 0.91655 |  0:05:21s
epoch 44 | loss: 0.78404 | val_0_rmse: 0.9736  | val_1_rmse: 0.96567 |  0:05:28s
epoch 45 | loss: 0.79126 | val_0_rmse: 0.95888 | val_1_rmse: 0.95082 |  0:05:35s
epoch 46 | loss: 0.78863 | val_0_rmse: 0.8804  | val_1_rmse: 0.8748  |  0:05:42s
epoch 47 | loss: 0.78484 | val_0_rmse: 0.93008 | val_1_rmse: 0.92193 |  0:05:50s
epoch 48 | loss: 0.78625 | val_0_rmse: 0.89054 | val_1_rmse: 0.88632 |  0:05:57s
epoch 49 | loss: 0.78595 | val_0_rmse: 0.95372 | val_1_rmse: 0.94645 |  0:06:04s
epoch 50 | loss: 0.7888  | val_0_rmse: 0.89677 | val_1_rmse: 0.89243 |  0:06:11s
epoch 51 | loss: 0.78562 | val_0_rmse: 0.91087 | val_1_rmse: 0.90352 |  0:06:19s
epoch 52 | loss: 0.78481 | val_0_rmse: 0.92816 | val_1_rmse: 0.92042 |  0:06:26s
epoch 53 | loss: 0.78565 | val_0_rmse: 0.91344 | val_1_rmse: 0.90624 |  0:06:33s
epoch 54 | loss: 0.78387 | val_0_rmse: 0.91059 | val_1_rmse: 0.90567 |  0:06:40s
epoch 55 | loss: 0.79197 | val_0_rmse: 0.91845 | val_1_rmse: 0.91203 |  0:06:48s
epoch 56 | loss: 0.78838 | val_0_rmse: 0.93257 | val_1_rmse: 0.92376 |  0:06:55s
epoch 57 | loss: 0.78157 | val_0_rmse: 0.96191 | val_1_rmse: 0.95453 |  0:07:02s
epoch 58 | loss: 0.7847  | val_0_rmse: 0.90038 | val_1_rmse: 0.89312 |  0:07:09s
epoch 59 | loss: 0.77743 | val_0_rmse: 0.88871 | val_1_rmse: 0.88084 |  0:07:17s
epoch 60 | loss: 0.78138 | val_0_rmse: 0.88111 | val_1_rmse: 0.87589 |  0:07:24s
epoch 61 | loss: 0.78185 | val_0_rmse: 0.97059 | val_1_rmse: 0.96398 |  0:07:31s
epoch 62 | loss: 0.77816 | val_0_rmse: 0.89873 | val_1_rmse: 0.89274 |  0:07:38s
epoch 63 | loss: 0.77688 | val_0_rmse: 0.91919 | val_1_rmse: 0.91359 |  0:07:46s
epoch 64 | loss: 0.77607 | val_0_rmse: 0.88686 | val_1_rmse: 0.88108 |  0:07:53s
epoch 65 | loss: 0.77338 | val_0_rmse: 0.90785 | val_1_rmse: 0.90227 |  0:08:00s
epoch 66 | loss: 0.77435 | val_0_rmse: 0.9486  | val_1_rmse: 0.94042 |  0:08:07s
epoch 67 | loss: 0.77366 | val_0_rmse: 0.96107 | val_1_rmse: 0.95289 |  0:08:15s
epoch 68 | loss: 0.77117 | val_0_rmse: 1.02749 | val_1_rmse: 1.01988 |  0:08:22s
epoch 69 | loss: 0.77538 | val_0_rmse: 0.95817 | val_1_rmse: 0.94962 |  0:08:29s
epoch 70 | loss: 0.77209 | val_0_rmse: 0.91846 | val_1_rmse: 0.91389 |  0:08:36s
epoch 71 | loss: 0.77556 | val_0_rmse: 0.90653 | val_1_rmse: 0.89973 |  0:08:44s
epoch 72 | loss: 0.77104 | val_0_rmse: 0.91871 | val_1_rmse: 0.91065 |  0:08:51s
epoch 73 | loss: 0.7709  | val_0_rmse: 0.89837 | val_1_rmse: 0.89161 |  0:08:58s
epoch 74 | loss: 0.77062 | val_0_rmse: 1.07068 | val_1_rmse: 1.06169 |  0:09:06s
epoch 75 | loss: 0.76934 | val_0_rmse: 0.88224 | val_1_rmse: 0.87584 |  0:09:13s
epoch 76 | loss: 0.76944 | val_0_rmse: 0.91662 | val_1_rmse: 0.91019 |  0:09:20s

Early stopping occured at epoch 76 with best_epoch = 46 and best_val_1_rmse = 0.8748
Best weights from best epoch are automatically used!
ended training at: 14:56:33
Feature importance:
[('Latitude', 0.49253855450454015), ('Longitude', 0.5074614454954599)]
Mean squared error is of 5209897385.834039
Mean absolute error:56209.492844055814
MAPE:0.5468187181964749
R2 score:0.23088748857275743
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:56:34
epoch 0  | loss: 0.96086 | val_0_rmse: 0.92319 | val_1_rmse: 0.9207  |  0:00:07s
epoch 1  | loss: 0.81346 | val_0_rmse: 0.98736 | val_1_rmse: 0.98303 |  0:00:14s
epoch 2  | loss: 0.8037  | val_0_rmse: 0.90517 | val_1_rmse: 0.89999 |  0:00:21s
epoch 3  | loss: 0.80412 | val_0_rmse: 0.96861 | val_1_rmse: 0.96476 |  0:00:29s
epoch 4  | loss: 0.79992 | val_0_rmse: 0.9096  | val_1_rmse: 0.90648 |  0:00:36s
epoch 5  | loss: 0.79827 | val_0_rmse: 0.92813 | val_1_rmse: 0.92476 |  0:00:43s
epoch 6  | loss: 0.7944  | val_0_rmse: 0.93706 | val_1_rmse: 0.93415 |  0:00:50s
epoch 7  | loss: 0.79364 | val_0_rmse: 0.93617 | val_1_rmse: 0.93273 |  0:00:57s
epoch 8  | loss: 0.79259 | val_0_rmse: 0.915   | val_1_rmse: 0.91313 |  0:01:05s
epoch 9  | loss: 0.7924  | val_0_rmse: 0.94156 | val_1_rmse: 0.93824 |  0:01:12s
epoch 10 | loss: 0.79257 | val_0_rmse: 0.95374 | val_1_rmse: 0.94943 |  0:01:19s
epoch 11 | loss: 0.792   | val_0_rmse: 0.9908  | val_1_rmse: 0.99396 |  0:01:27s
epoch 12 | loss: 0.79235 | val_0_rmse: 0.92315 | val_1_rmse: 0.92078 |  0:01:34s
epoch 13 | loss: 0.79012 | val_0_rmse: 0.95078 | val_1_rmse: 0.94891 |  0:01:42s
epoch 14 | loss: 0.78896 | val_0_rmse: 0.91844 | val_1_rmse: 0.91591 |  0:01:49s
epoch 15 | loss: 0.79057 | val_0_rmse: 0.9392  | val_1_rmse: 0.93559 |  0:01:56s
epoch 16 | loss: 0.79286 | val_0_rmse: 0.912   | val_1_rmse: 0.90785 |  0:02:03s
epoch 17 | loss: 0.78938 | val_0_rmse: 0.91203 | val_1_rmse: 0.90979 |  0:02:11s
epoch 18 | loss: 0.78925 | val_0_rmse: 0.90141 | val_1_rmse: 0.89698 |  0:02:18s
epoch 19 | loss: 0.79008 | val_0_rmse: 0.9071  | val_1_rmse: 0.90429 |  0:02:25s
epoch 20 | loss: 0.78871 | val_0_rmse: 0.95506 | val_1_rmse: 0.95095 |  0:02:32s
epoch 21 | loss: 0.78884 | val_0_rmse: 0.91558 | val_1_rmse: 0.91292 |  0:02:40s
epoch 22 | loss: 0.79088 | val_0_rmse: 0.94735 | val_1_rmse: 0.94898 |  0:02:47s
epoch 23 | loss: 0.78834 | val_0_rmse: 0.93118 | val_1_rmse: 0.92911 |  0:02:54s
epoch 24 | loss: 0.78756 | val_0_rmse: 0.88288 | val_1_rmse: 0.87814 |  0:03:01s
epoch 25 | loss: 0.78728 | val_0_rmse: 1.01617 | val_1_rmse: 1.01249 |  0:03:08s
epoch 26 | loss: 0.79059 | val_0_rmse: 0.98879 | val_1_rmse: 0.99091 |  0:03:16s
epoch 27 | loss: 0.78885 | val_0_rmse: 0.90434 | val_1_rmse: 0.90104 |  0:03:23s
epoch 28 | loss: 0.78626 | val_0_rmse: 0.97564 | val_1_rmse: 0.97471 |  0:03:30s
epoch 29 | loss: 0.78675 | val_0_rmse: 0.93218 | val_1_rmse: 0.92893 |  0:03:37s
epoch 30 | loss: 0.79042 | val_0_rmse: 0.89757 | val_1_rmse: 0.89496 |  0:03:45s
epoch 31 | loss: 0.78753 | val_0_rmse: 0.88406 | val_1_rmse: 0.88048 |  0:03:52s
epoch 32 | loss: 0.78617 | val_0_rmse: 0.93715 | val_1_rmse: 0.93407 |  0:03:59s
epoch 33 | loss: 0.78652 | val_0_rmse: 0.92821 | val_1_rmse: 0.92566 |  0:04:06s
epoch 34 | loss: 0.78545 | val_0_rmse: 0.92982 | val_1_rmse: 0.92708 |  0:04:14s
epoch 35 | loss: 0.7846  | val_0_rmse: 0.88719 | val_1_rmse: 0.8839  |  0:04:21s
epoch 36 | loss: 0.78597 | val_0_rmse: 0.93355 | val_1_rmse: 0.93314 |  0:04:28s
epoch 37 | loss: 0.78637 | val_0_rmse: 0.89965 | val_1_rmse: 0.89631 |  0:04:35s
epoch 38 | loss: 0.78408 | val_0_rmse: 0.98956 | val_1_rmse: 0.98772 |  0:04:43s
epoch 39 | loss: 0.79278 | val_0_rmse: 0.94434 | val_1_rmse: 0.94285 |  0:04:50s
epoch 40 | loss: 0.78876 | val_0_rmse: 0.92795 | val_1_rmse: 0.92484 |  0:04:57s
epoch 41 | loss: 0.78553 | val_0_rmse: 0.95864 | val_1_rmse: 0.9547  |  0:05:04s
epoch 42 | loss: 0.78767 | val_0_rmse: 0.88621 | val_1_rmse: 0.88314 |  0:05:12s
epoch 43 | loss: 0.78414 | val_0_rmse: 0.91741 | val_1_rmse: 0.91294 |  0:05:19s
epoch 44 | loss: 0.78414 | val_0_rmse: 0.88166 | val_1_rmse: 0.87738 |  0:05:26s
epoch 45 | loss: 0.7844  | val_0_rmse: 0.89187 | val_1_rmse: 0.8883  |  0:05:34s
epoch 46 | loss: 0.7838  | val_0_rmse: 0.9436  | val_1_rmse: 0.93951 |  0:05:41s
epoch 47 | loss: 0.78416 | val_0_rmse: 0.95958 | val_1_rmse: 0.95538 |  0:05:48s
epoch 48 | loss: 0.78448 | val_0_rmse: 0.94944 | val_1_rmse: 0.94581 |  0:05:55s
epoch 49 | loss: 0.78301 | val_0_rmse: 0.89352 | val_1_rmse: 0.89083 |  0:06:03s
epoch 50 | loss: 0.78591 | val_0_rmse: 0.89165 | val_1_rmse: 0.88929 |  0:06:10s
epoch 51 | loss: 0.784   | val_0_rmse: 0.92949 | val_1_rmse: 0.92771 |  0:06:17s
epoch 52 | loss: 0.78517 | val_0_rmse: 0.89302 | val_1_rmse: 0.88894 |  0:06:24s
epoch 53 | loss: 0.7833  | val_0_rmse: 0.95607 | val_1_rmse: 0.95322 |  0:06:32s
epoch 54 | loss: 0.78295 | val_0_rmse: 0.96434 | val_1_rmse: 0.96522 |  0:06:39s
epoch 55 | loss: 0.78354 | val_0_rmse: 0.88966 | val_1_rmse: 0.88585 |  0:06:47s
epoch 56 | loss: 0.78254 | val_0_rmse: 0.93497 | val_1_rmse: 0.93205 |  0:06:54s
epoch 57 | loss: 0.78127 | val_0_rmse: 0.94485 | val_1_rmse: 0.94182 |  0:07:02s
epoch 58 | loss: 0.78683 | val_0_rmse: 0.93656 | val_1_rmse: 0.93402 |  0:07:09s
epoch 59 | loss: 0.78213 | val_0_rmse: 0.9077  | val_1_rmse: 0.90498 |  0:07:17s
epoch 60 | loss: 0.78546 | val_0_rmse: 0.88193 | val_1_rmse: 0.87722 |  0:07:24s
epoch 61 | loss: 0.78255 | val_0_rmse: 0.91629 | val_1_rmse: 0.91305 |  0:07:32s
epoch 62 | loss: 0.78414 | val_0_rmse: 0.90741 | val_1_rmse: 0.90462 |  0:07:40s
epoch 63 | loss: 0.78496 | val_0_rmse: 0.91056 | val_1_rmse: 0.90678 |  0:07:47s
epoch 64 | loss: 0.78128 | val_0_rmse: 0.98687 | val_1_rmse: 0.98401 |  0:07:55s
epoch 65 | loss: 0.7825  | val_0_rmse: 0.88957 | val_1_rmse: 0.88617 |  0:08:02s
epoch 66 | loss: 0.78851 | val_0_rmse: 0.99738 | val_1_rmse: 1.00073 |  0:08:09s
epoch 67 | loss: 0.78487 | val_0_rmse: 0.95637 | val_1_rmse: 0.95246 |  0:08:17s
epoch 68 | loss: 0.78278 | val_0_rmse: 0.92458 | val_1_rmse: 0.92203 |  0:08:24s
epoch 69 | loss: 0.78234 | val_0_rmse: 0.92185 | val_1_rmse: 0.91929 |  0:08:31s
epoch 70 | loss: 0.78133 | val_0_rmse: 0.88095 | val_1_rmse: 0.87591 |  0:08:39s
epoch 71 | loss: 0.78335 | val_0_rmse: 0.89409 | val_1_rmse: 0.89124 |  0:08:46s
epoch 72 | loss: 0.78134 | val_0_rmse: 0.91162 | val_1_rmse: 0.90804 |  0:08:53s
epoch 73 | loss: 0.78388 | val_0_rmse: 0.91441 | val_1_rmse: 0.91051 |  0:09:00s
epoch 74 | loss: 0.78868 | val_0_rmse: 0.91303 | val_1_rmse: 0.90953 |  0:09:08s
epoch 75 | loss: 0.80991 | val_0_rmse: 1.04032 | val_1_rmse: 1.03729 |  0:09:15s
epoch 76 | loss: 0.80548 | val_0_rmse: 0.93044 | val_1_rmse: 0.92631 |  0:09:22s
epoch 77 | loss: 0.79542 | val_0_rmse: 0.99797 | val_1_rmse: 1.00011 |  0:09:30s
epoch 78 | loss: 0.78951 | val_0_rmse: 0.96265 | val_1_rmse: 0.95909 |  0:09:37s
epoch 79 | loss: 0.78824 | val_0_rmse: 0.98203 | val_1_rmse: 0.98418 |  0:09:44s
epoch 80 | loss: 0.78547 | val_0_rmse: 0.89978 | val_1_rmse: 0.89568 |  0:09:52s
epoch 81 | loss: 0.78652 | val_0_rmse: 0.91697 | val_1_rmse: 0.91313 |  0:09:59s
epoch 82 | loss: 0.78267 | val_0_rmse: 1.0268  | val_1_rmse: 1.02416 |  0:10:06s
epoch 83 | loss: 0.78113 | val_0_rmse: 0.99038 | val_1_rmse: 0.99253 |  0:10:13s
epoch 84 | loss: 0.78002 | val_0_rmse: 0.94646 | val_1_rmse: 0.94424 |  0:10:21s
epoch 85 | loss: 0.78041 | val_0_rmse: 0.98918 | val_1_rmse: 0.9906  |  0:10:28s
epoch 86 | loss: 0.77979 | val_0_rmse: 1.03531 | val_1_rmse: 1.03422 |  0:10:35s
epoch 87 | loss: 0.78111 | val_0_rmse: 1.02848 | val_1_rmse: 1.03278 |  0:10:43s
epoch 88 | loss: 0.77406 | val_0_rmse: 1.0505  | val_1_rmse: 1.04809 |  0:10:50s
epoch 89 | loss: 0.77761 | val_0_rmse: 0.99013 | val_1_rmse: 0.99341 |  0:10:57s
epoch 90 | loss: 0.77309 | val_0_rmse: 1.047   | val_1_rmse: 1.04423 |  0:11:05s
epoch 91 | loss: 0.77103 | val_0_rmse: 0.93042 | val_1_rmse: 0.92906 |  0:11:12s
epoch 92 | loss: 0.77026 | val_0_rmse: 0.89582 | val_1_rmse: 0.89306 |  0:11:19s
epoch 93 | loss: 0.77285 | val_0_rmse: 0.95049 | val_1_rmse: 0.95083 |  0:11:27s
epoch 94 | loss: 0.76794 | val_0_rmse: 0.94439 | val_1_rmse: 0.94199 |  0:11:34s
epoch 95 | loss: 0.7694  | val_0_rmse: 0.96153 | val_1_rmse: 0.95974 |  0:11:42s
epoch 96 | loss: 0.76877 | val_0_rmse: 1.0211  | val_1_rmse: 1.01788 |  0:11:49s
epoch 97 | loss: 0.76869 | val_0_rmse: 0.95688 | val_1_rmse: 0.95719 |  0:11:56s
epoch 98 | loss: 0.7685  | val_0_rmse: 0.91526 | val_1_rmse: 0.91396 |  0:12:04s
epoch 99 | loss: 0.76546 | val_0_rmse: 0.89973 | val_1_rmse: 0.89805 |  0:12:11s
epoch 100| loss: 0.76409 | val_0_rmse: 0.95061 | val_1_rmse: 0.94901 |  0:12:18s

Early stopping occured at epoch 100 with best_epoch = 70 and best_val_1_rmse = 0.87591
Best weights from best epoch are automatically used!
ended training at: 15:08:55
Feature importance:
[('Latitude', 0.43244969956435864), ('Longitude', 0.5675503004356414)]
Mean squared error is of 5333017882.610789
Mean absolute error:56771.55947537842
MAPE:0.552799444949467
R2 score:0.2253906725553414
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:16:34
epoch 0  | loss: 0.96415 | val_0_rmse: 0.92597 | val_1_rmse: 0.91417 |  0:00:10s
epoch 1  | loss: 0.85986 | val_0_rmse: 0.92262 | val_1_rmse: 0.91149 |  0:00:19s
epoch 2  | loss: 0.85112 | val_0_rmse: 0.92216 | val_1_rmse: 0.91004 |  0:00:26s
epoch 3  | loss: 0.84427 | val_0_rmse: 0.98338 | val_1_rmse: 0.97152 |  0:00:33s
epoch 4  | loss: 0.83173 | val_0_rmse: 0.94978 | val_1_rmse: 0.93792 |  0:00:40s
epoch 5  | loss: 0.82938 | val_0_rmse: 1.01794 | val_1_rmse: 1.00727 |  0:00:48s
epoch 6  | loss: 0.8233  | val_0_rmse: 0.90982 | val_1_rmse: 0.89948 |  0:00:55s
epoch 7  | loss: 0.81599 | val_0_rmse: 0.90423 | val_1_rmse: 0.89366 |  0:01:02s
epoch 8  | loss: 0.81184 | val_0_rmse: 0.90591 | val_1_rmse: 0.89774 |  0:01:10s
epoch 9  | loss: 0.81407 | val_0_rmse: 0.89212 | val_1_rmse: 0.88307 |  0:01:17s
epoch 10 | loss: 0.81114 | val_0_rmse: 0.89697 | val_1_rmse: 0.8892  |  0:01:24s
epoch 11 | loss: 0.81322 | val_0_rmse: 1.01474 | val_1_rmse: 1.00285 |  0:01:31s
epoch 12 | loss: 0.80998 | val_0_rmse: 0.99898 | val_1_rmse: 0.98851 |  0:01:39s
epoch 13 | loss: 0.81001 | val_0_rmse: 0.92545 | val_1_rmse: 0.91333 |  0:01:46s
epoch 14 | loss: 0.80653 | val_0_rmse: 0.99521 | val_1_rmse: 0.98287 |  0:01:53s
epoch 15 | loss: 0.80502 | val_0_rmse: 0.91799 | val_1_rmse: 0.91107 |  0:02:01s
epoch 16 | loss: 0.80548 | val_0_rmse: 0.91559 | val_1_rmse: 0.90376 |  0:02:08s
epoch 17 | loss: 0.79846 | val_0_rmse: 0.92651 | val_1_rmse: 0.9135  |  0:02:16s
epoch 18 | loss: 0.79991 | val_0_rmse: 0.8935  | val_1_rmse: 0.8837  |  0:02:23s
epoch 19 | loss: 0.79807 | val_0_rmse: 0.97909 | val_1_rmse: 0.96429 |  0:02:30s
epoch 20 | loss: 0.7988  | val_0_rmse: 0.89766 | val_1_rmse: 0.8893  |  0:02:37s
epoch 21 | loss: 0.79852 | val_0_rmse: 0.94534 | val_1_rmse: 0.93495 |  0:02:45s
epoch 22 | loss: 0.7955  | val_0_rmse: 0.93304 | val_1_rmse: 0.92395 |  0:02:52s
epoch 23 | loss: 0.79583 | val_0_rmse: 0.95162 | val_1_rmse: 0.94264 |  0:02:59s
epoch 24 | loss: 0.79305 | val_0_rmse: 0.98129 | val_1_rmse: 0.96735 |  0:03:07s
epoch 25 | loss: 0.79492 | val_0_rmse: 0.98703 | val_1_rmse: 0.97505 |  0:03:14s
epoch 26 | loss: 0.807   | val_0_rmse: 0.88753 | val_1_rmse: 0.87867 |  0:03:21s
epoch 27 | loss: 0.79735 | val_0_rmse: 0.96388 | val_1_rmse: 0.95401 |  0:03:28s
epoch 28 | loss: 0.79696 | val_0_rmse: 0.8884  | val_1_rmse: 0.87964 |  0:03:36s
epoch 29 | loss: 0.79689 | val_0_rmse: 0.91524 | val_1_rmse: 0.90708 |  0:03:43s
epoch 30 | loss: 0.79873 | val_0_rmse: 0.93441 | val_1_rmse: 0.92546 |  0:03:50s
epoch 31 | loss: 0.79597 | val_0_rmse: 0.89365 | val_1_rmse: 0.8857  |  0:03:58s
epoch 32 | loss: 0.79657 | val_0_rmse: 0.97654 | val_1_rmse: 0.96775 |  0:04:05s
epoch 33 | loss: 0.82177 | val_0_rmse: 0.98772 | val_1_rmse: 0.97308 |  0:04:12s
epoch 34 | loss: 0.83549 | val_0_rmse: 0.91564 | val_1_rmse: 0.90502 |  0:04:19s
epoch 35 | loss: 0.8381  | val_0_rmse: 0.95316 | val_1_rmse: 0.94044 |  0:04:27s
epoch 36 | loss: 0.82798 | val_0_rmse: 0.92979 | val_1_rmse: 0.91956 |  0:04:34s
epoch 37 | loss: 0.82741 | val_0_rmse: 0.97694 | val_1_rmse: 0.96444 |  0:04:41s
epoch 38 | loss: 0.81175 | val_0_rmse: 0.94048 | val_1_rmse: 0.92718 |  0:04:49s
epoch 39 | loss: 0.80448 | val_0_rmse: 0.90333 | val_1_rmse: 0.89464 |  0:04:56s
epoch 40 | loss: 0.80452 | val_0_rmse: 0.91555 | val_1_rmse: 0.90653 |  0:05:03s
epoch 41 | loss: 0.80186 | val_0_rmse: 0.93637 | val_1_rmse: 0.92344 |  0:05:10s
epoch 42 | loss: 0.79893 | val_0_rmse: 0.92733 | val_1_rmse: 0.91812 |  0:05:18s
epoch 43 | loss: 0.79775 | val_0_rmse: 0.89907 | val_1_rmse: 0.88993 |  0:05:25s
epoch 44 | loss: 0.7973  | val_0_rmse: 0.93748 | val_1_rmse: 0.92859 |  0:05:32s
epoch 45 | loss: 0.79711 | val_0_rmse: 0.96987 | val_1_rmse: 0.95913 |  0:05:39s
epoch 46 | loss: 0.79708 | val_0_rmse: 0.95151 | val_1_rmse: 0.94047 |  0:05:47s
epoch 47 | loss: 0.79632 | val_0_rmse: 0.89032 | val_1_rmse: 0.88185 |  0:05:54s
epoch 48 | loss: 0.79512 | val_0_rmse: 0.88697 | val_1_rmse: 0.87669 |  0:06:01s
epoch 49 | loss: 0.79128 | val_0_rmse: 0.89162 | val_1_rmse: 0.88413 |  0:06:09s
epoch 50 | loss: 0.79493 | val_0_rmse: 0.92108 | val_1_rmse: 0.90954 |  0:06:16s
epoch 51 | loss: 0.79235 | val_0_rmse: 0.96623 | val_1_rmse: 0.95355 |  0:06:23s
epoch 52 | loss: 0.79326 | val_0_rmse: 0.91088 | val_1_rmse: 0.9028  |  0:06:31s
epoch 53 | loss: 0.7935  | val_0_rmse: 0.9207  | val_1_rmse: 0.91278 |  0:06:38s
epoch 54 | loss: 0.7916  | val_0_rmse: 0.8917  | val_1_rmse: 0.88341 |  0:06:45s
epoch 55 | loss: 0.7902  | val_0_rmse: 0.95538 | val_1_rmse: 0.94599 |  0:06:53s
epoch 56 | loss: 0.79425 | val_0_rmse: 0.88568 | val_1_rmse: 0.87529 |  0:07:00s
epoch 57 | loss: 0.79098 | val_0_rmse: 0.89932 | val_1_rmse: 0.88745 |  0:07:07s
epoch 58 | loss: 0.78908 | val_0_rmse: 0.88768 | val_1_rmse: 0.87668 |  0:07:15s
epoch 59 | loss: 0.78892 | val_0_rmse: 0.88347 | val_1_rmse: 0.87541 |  0:07:22s
epoch 60 | loss: 0.78809 | val_0_rmse: 0.90033 | val_1_rmse: 0.89215 |  0:07:29s
epoch 61 | loss: 0.79009 | val_0_rmse: 0.8865  | val_1_rmse: 0.87691 |  0:07:37s
epoch 62 | loss: 0.79208 | val_0_rmse: 0.90784 | val_1_rmse: 0.89724 |  0:07:44s
epoch 63 | loss: 0.79121 | val_0_rmse: 0.90411 | val_1_rmse: 0.89323 |  0:07:51s
epoch 64 | loss: 0.79051 | val_0_rmse: 0.88277 | val_1_rmse: 0.87317 |  0:07:58s
epoch 65 | loss: 0.78947 | val_0_rmse: 0.89954 | val_1_rmse: 0.88888 |  0:08:06s
epoch 66 | loss: 0.78802 | val_0_rmse: 0.88341 | val_1_rmse: 0.87544 |  0:08:13s
epoch 67 | loss: 0.79405 | val_0_rmse: 0.94546 | val_1_rmse: 0.93632 |  0:08:20s
epoch 68 | loss: 0.78841 | val_0_rmse: 0.91155 | val_1_rmse: 0.90071 |  0:08:28s
epoch 69 | loss: 0.78831 | val_0_rmse: 0.95213 | val_1_rmse: 0.9419  |  0:08:35s
epoch 70 | loss: 0.79117 | val_0_rmse: 0.97174 | val_1_rmse: 0.96049 |  0:08:42s
epoch 71 | loss: 0.79238 | val_0_rmse: 0.95552 | val_1_rmse: 0.94672 |  0:08:50s
epoch 72 | loss: 0.79003 | val_0_rmse: 0.89072 | val_1_rmse: 0.87958 |  0:08:57s
epoch 73 | loss: 0.78868 | val_0_rmse: 0.88537 | val_1_rmse: 0.8761  |  0:09:04s
epoch 74 | loss: 0.79078 | val_0_rmse: 0.88445 | val_1_rmse: 0.87571 |  0:09:12s
epoch 75 | loss: 0.78846 | val_0_rmse: 0.92505 | val_1_rmse: 0.91446 |  0:09:19s
epoch 76 | loss: 0.79045 | val_0_rmse: 0.92113 | val_1_rmse: 0.91021 |  0:09:26s
epoch 77 | loss: 0.7886  | val_0_rmse: 0.91473 | val_1_rmse: 0.90739 |  0:09:33s
epoch 78 | loss: 0.78912 | val_0_rmse: 0.9301  | val_1_rmse: 0.91946 |  0:09:41s
epoch 79 | loss: 0.78963 | val_0_rmse: 0.89788 | val_1_rmse: 0.88951 |  0:09:48s
epoch 80 | loss: 0.78683 | val_0_rmse: 0.93302 | val_1_rmse: 0.92226 |  0:09:55s
epoch 81 | loss: 0.78694 | val_0_rmse: 0.92114 | val_1_rmse: 0.9111  |  0:10:03s
epoch 82 | loss: 0.78865 | val_0_rmse: 0.90667 | val_1_rmse: 0.89787 |  0:10:10s
epoch 83 | loss: 0.78747 | val_0_rmse: 0.88807 | val_1_rmse: 0.87959 |  0:10:17s
epoch 84 | loss: 0.79051 | val_0_rmse: 0.8982  | val_1_rmse: 0.89003 |  0:10:24s
epoch 85 | loss: 0.78733 | val_0_rmse: 0.88106 | val_1_rmse: 0.87183 |  0:10:32s
epoch 86 | loss: 0.78756 | val_0_rmse: 0.88653 | val_1_rmse: 0.87803 |  0:10:39s
epoch 87 | loss: 0.78696 | val_0_rmse: 0.91875 | val_1_rmse: 0.90991 |  0:10:47s
epoch 88 | loss: 0.79501 | val_0_rmse: 0.96268 | val_1_rmse: 0.95249 |  0:10:54s
epoch 89 | loss: 0.78607 | val_0_rmse: 0.93674 | val_1_rmse: 0.92879 |  0:11:01s
epoch 90 | loss: 0.78679 | val_0_rmse: 0.88763 | val_1_rmse: 0.87865 |  0:11:09s
epoch 91 | loss: 0.78607 | val_0_rmse: 0.88004 | val_1_rmse: 0.87154 |  0:11:16s
epoch 92 | loss: 0.78872 | val_0_rmse: 0.8823  | val_1_rmse: 0.87292 |  0:11:23s
epoch 93 | loss: 0.78709 | val_0_rmse: 0.90379 | val_1_rmse: 0.8945  |  0:11:30s
epoch 94 | loss: 0.78741 | val_0_rmse: 0.88597 | val_1_rmse: 0.8764  |  0:11:38s
epoch 95 | loss: 0.78695 | val_0_rmse: 0.95044 | val_1_rmse: 0.94206 |  0:11:45s
epoch 96 | loss: 0.787   | val_0_rmse: 1.03551 | val_1_rmse: 1.02353 |  0:11:52s
epoch 97 | loss: 0.82373 | val_0_rmse: 0.91637 | val_1_rmse: 0.90386 |  0:12:00s
epoch 98 | loss: 0.80459 | val_0_rmse: 0.93554 | val_1_rmse: 0.91774 |  0:12:07s
epoch 99 | loss: 0.80325 | val_0_rmse: 0.89069 | val_1_rmse: 0.87924 |  0:12:14s
epoch 100| loss: 0.79595 | val_0_rmse: 0.90731 | val_1_rmse: 0.89532 |  0:12:22s
epoch 101| loss: 0.79553 | val_0_rmse: 0.90371 | val_1_rmse: 0.89214 |  0:12:29s
epoch 102| loss: 0.79609 | val_0_rmse: 0.93616 | val_1_rmse: 0.92421 |  0:12:36s
epoch 103| loss: 0.79413 | val_0_rmse: 0.90334 | val_1_rmse: 0.89122 |  0:12:44s
epoch 104| loss: 0.79252 | val_0_rmse: 0.8915  | val_1_rmse: 0.88279 |  0:12:51s
epoch 105| loss: 0.79234 | val_0_rmse: 0.9461  | val_1_rmse: 0.93688 |  0:12:58s
epoch 106| loss: 0.7905  | val_0_rmse: 0.91534 | val_1_rmse: 0.90565 |  0:13:05s
epoch 107| loss: 0.79264 | val_0_rmse: 0.89868 | val_1_rmse: 0.88745 |  0:13:13s
epoch 108| loss: 0.79254 | val_0_rmse: 0.89792 | val_1_rmse: 0.88926 |  0:13:20s
epoch 109| loss: 0.79034 | val_0_rmse: 0.89178 | val_1_rmse: 0.88005 |  0:13:27s
epoch 110| loss: 0.79185 | val_0_rmse: 0.88729 | val_1_rmse: 0.87844 |  0:13:35s
epoch 111| loss: 0.79332 | val_0_rmse: 0.89208 | val_1_rmse: 0.88283 |  0:13:42s
epoch 112| loss: 0.79088 | val_0_rmse: 0.88577 | val_1_rmse: 0.8758  |  0:13:49s
epoch 113| loss: 0.79226 | val_0_rmse: 0.88927 | val_1_rmse: 0.87969 |  0:13:57s
epoch 114| loss: 0.79094 | val_0_rmse: 0.97219 | val_1_rmse: 0.95813 |  0:14:04s
epoch 115| loss: 0.79093 | val_0_rmse: 0.9054  | val_1_rmse: 0.89365 |  0:14:11s
epoch 116| loss: 0.79192 | val_0_rmse: 0.88403 | val_1_rmse: 0.8745  |  0:14:18s
epoch 117| loss: 0.79101 | val_0_rmse: 0.9555  | val_1_rmse: 0.94568 |  0:14:26s
epoch 118| loss: 0.78989 | val_0_rmse: 0.90116 | val_1_rmse: 0.89245 |  0:14:33s
epoch 119| loss: 0.79306 | val_0_rmse: 0.89332 | val_1_rmse: 0.88488 |  0:14:40s
epoch 120| loss: 0.79287 | val_0_rmse: 0.8872  | val_1_rmse: 0.8772  |  0:14:48s
epoch 121| loss: 0.78885 | val_0_rmse: 0.91175 | val_1_rmse: 0.90372 |  0:14:55s

Early stopping occured at epoch 121 with best_epoch = 91 and best_val_1_rmse = 0.87154
Best weights from best epoch are automatically used!
ended training at: 15:31:31
Feature importance:
[('Latitude', 0.703331449418422), ('Longitude', 0.29666855058157804)]
Mean squared error is of 3064791070.4807944
Mean absolute error:42323.479057980214
MAPE:0.6404048016472789
R2 score:0.2235370817420056
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:31:33
epoch 0  | loss: 0.96916 | val_0_rmse: 0.929   | val_1_rmse: 0.92594 |  0:00:07s
epoch 1  | loss: 0.85995 | val_0_rmse: 0.92436 | val_1_rmse: 0.92148 |  0:00:14s
epoch 2  | loss: 0.85387 | val_0_rmse: 0.92083 | val_1_rmse: 0.91836 |  0:00:22s
epoch 3  | loss: 0.85355 | val_0_rmse: 0.92175 | val_1_rmse: 0.91907 |  0:00:29s
epoch 4  | loss: 0.85123 | val_0_rmse: 0.92395 | val_1_rmse: 0.92178 |  0:00:36s
epoch 5  | loss: 0.84855 | val_0_rmse: 0.92246 | val_1_rmse: 0.92083 |  0:00:43s
epoch 6  | loss: 0.84884 | val_0_rmse: 0.91578 | val_1_rmse: 0.91405 |  0:00:51s
epoch 7  | loss: 0.83875 | val_0_rmse: 0.9136  | val_1_rmse: 0.91129 |  0:00:58s
epoch 8  | loss: 0.8235  | val_0_rmse: 1.00536 | val_1_rmse: 1.00255 |  0:01:05s
epoch 9  | loss: 0.81312 | val_0_rmse: 0.89782 | val_1_rmse: 0.89319 |  0:01:13s
epoch 10 | loss: 0.80927 | val_0_rmse: 0.89156 | val_1_rmse: 0.88832 |  0:01:21s
epoch 11 | loss: 0.80802 | val_0_rmse: 0.88634 | val_1_rmse: 0.88205 |  0:01:29s
epoch 12 | loss: 0.8049  | val_0_rmse: 0.89217 | val_1_rmse: 0.88791 |  0:01:37s
epoch 13 | loss: 0.80123 | val_0_rmse: 0.88757 | val_1_rmse: 0.88373 |  0:01:46s
epoch 14 | loss: 0.79993 | val_0_rmse: 0.99457 | val_1_rmse: 0.99594 |  0:01:54s
epoch 15 | loss: 0.79755 | val_0_rmse: 0.93651 | val_1_rmse: 0.93237 |  0:02:01s
epoch 16 | loss: 0.79873 | val_0_rmse: 0.93984 | val_1_rmse: 0.94027 |  0:02:09s
epoch 17 | loss: 0.79762 | val_0_rmse: 0.91432 | val_1_rmse: 0.91091 |  0:02:16s
epoch 18 | loss: 0.7984  | val_0_rmse: 0.97451 | val_1_rmse: 0.97625 |  0:02:23s
epoch 19 | loss: 0.7958  | val_0_rmse: 0.99074 | val_1_rmse: 0.99229 |  0:02:31s
epoch 20 | loss: 0.79734 | val_0_rmse: 0.91081 | val_1_rmse: 0.9074  |  0:02:38s
epoch 21 | loss: 0.79205 | val_0_rmse: 0.93178 | val_1_rmse: 0.92836 |  0:02:45s
epoch 22 | loss: 0.79107 | val_0_rmse: 0.92917 | val_1_rmse: 0.92848 |  0:02:53s
epoch 23 | loss: 0.78992 | val_0_rmse: 0.90275 | val_1_rmse: 0.89835 |  0:03:00s
epoch 24 | loss: 0.81489 | val_0_rmse: 1.03422 | val_1_rmse: 1.03107 |  0:03:07s
epoch 25 | loss: 0.79958 | val_0_rmse: 0.99692 | val_1_rmse: 0.99866 |  0:03:14s
epoch 26 | loss: 0.79547 | val_0_rmse: 0.88861 | val_1_rmse: 0.88424 |  0:03:22s
epoch 27 | loss: 0.80764 | val_0_rmse: 0.88649 | val_1_rmse: 0.8823  |  0:03:29s
epoch 28 | loss: 0.82926 | val_0_rmse: 0.97653 | val_1_rmse: 0.97269 |  0:03:36s
epoch 29 | loss: 0.8116  | val_0_rmse: 0.89161 | val_1_rmse: 0.88682 |  0:03:44s
epoch 30 | loss: 0.80198 | val_0_rmse: 0.9672  | val_1_rmse: 0.96252 |  0:03:51s
epoch 31 | loss: 0.86726 | val_0_rmse: 0.97901 | val_1_rmse: 0.9757  |  0:03:58s
epoch 32 | loss: 0.86889 | val_0_rmse: 0.92886 | val_1_rmse: 0.92532 |  0:04:06s
epoch 33 | loss: 0.86523 | val_0_rmse: 0.94565 | val_1_rmse: 0.9433  |  0:04:13s
epoch 34 | loss: 0.88471 | val_0_rmse: 0.92908 | val_1_rmse: 0.92566 |  0:04:20s
epoch 35 | loss: 0.86334 | val_0_rmse: 0.94483 | val_1_rmse: 0.94316 |  0:04:28s
epoch 36 | loss: 0.83994 | val_0_rmse: 0.99197 | val_1_rmse: 0.99136 |  0:04:35s
epoch 37 | loss: 0.81561 | val_0_rmse: 0.91549 | val_1_rmse: 0.91049 |  0:04:42s
epoch 38 | loss: 0.80695 | val_0_rmse: 0.90459 | val_1_rmse: 0.90119 |  0:04:49s
epoch 39 | loss: 0.80707 | val_0_rmse: 1.01252 | val_1_rmse: 1.00998 |  0:04:57s
epoch 40 | loss: 0.81823 | val_0_rmse: 0.90491 | val_1_rmse: 0.89996 |  0:05:04s
epoch 41 | loss: 0.7992  | val_0_rmse: 0.88362 | val_1_rmse: 0.87996 |  0:05:11s
epoch 42 | loss: 0.8003  | val_0_rmse: 0.9533  | val_1_rmse: 0.94884 |  0:05:19s
epoch 43 | loss: 0.79578 | val_0_rmse: 0.88987 | val_1_rmse: 0.88596 |  0:05:26s
epoch 44 | loss: 0.79647 | val_0_rmse: 0.98693 | val_1_rmse: 0.98814 |  0:05:33s
epoch 45 | loss: 0.79824 | val_0_rmse: 0.89321 | val_1_rmse: 0.88921 |  0:05:41s
epoch 46 | loss: 0.79393 | val_0_rmse: 0.9497  | val_1_rmse: 0.94998 |  0:05:48s
epoch 47 | loss: 0.78984 | val_0_rmse: 0.89135 | val_1_rmse: 0.88845 |  0:05:55s
epoch 48 | loss: 0.78601 | val_0_rmse: 0.90087 | val_1_rmse: 0.90057 |  0:06:03s
epoch 49 | loss: 0.78723 | val_0_rmse: 1.01789 | val_1_rmse: 1.01933 |  0:06:10s
epoch 50 | loss: 0.79117 | val_0_rmse: 0.89481 | val_1_rmse: 0.88973 |  0:06:18s
epoch 51 | loss: 0.79062 | val_0_rmse: 0.9195  | val_1_rmse: 0.91785 |  0:06:25s
epoch 52 | loss: 0.78768 | val_0_rmse: 0.88292 | val_1_rmse: 0.88013 |  0:06:32s
epoch 53 | loss: 0.78533 | val_0_rmse: 0.92581 | val_1_rmse: 0.92271 |  0:06:40s
epoch 54 | loss: 0.78488 | val_0_rmse: 0.98943 | val_1_rmse: 0.9915  |  0:06:47s
epoch 55 | loss: 0.78562 | val_0_rmse: 0.87969 | val_1_rmse: 0.87611 |  0:06:54s
epoch 56 | loss: 0.78546 | val_0_rmse: 0.88672 | val_1_rmse: 0.8828  |  0:07:02s
epoch 57 | loss: 0.78726 | val_0_rmse: 0.88277 | val_1_rmse: 0.87981 |  0:07:09s
epoch 58 | loss: 0.78528 | val_0_rmse: 0.90046 | val_1_rmse: 0.89828 |  0:07:16s
epoch 59 | loss: 0.78664 | val_0_rmse: 0.89543 | val_1_rmse: 0.89209 |  0:07:23s
epoch 60 | loss: 0.78473 | val_0_rmse: 0.89122 | val_1_rmse: 0.888   |  0:07:31s
epoch 61 | loss: 0.78347 | val_0_rmse: 0.88355 | val_1_rmse: 0.87954 |  0:07:38s
epoch 62 | loss: 0.78302 | val_0_rmse: 0.9727  | val_1_rmse: 0.97416 |  0:07:45s
epoch 63 | loss: 0.7853  | val_0_rmse: 0.94115 | val_1_rmse: 0.93778 |  0:07:53s
epoch 64 | loss: 0.78296 | val_0_rmse: 0.88527 | val_1_rmse: 0.88205 |  0:08:00s
epoch 65 | loss: 0.78388 | val_0_rmse: 0.89078 | val_1_rmse: 0.88753 |  0:08:07s
epoch 66 | loss: 0.78263 | val_0_rmse: 0.94075 | val_1_rmse: 0.9409  |  0:08:14s
epoch 67 | loss: 0.78423 | val_0_rmse: 0.90438 | val_1_rmse: 0.90236 |  0:08:22s
epoch 68 | loss: 0.78589 | val_0_rmse: 0.92634 | val_1_rmse: 0.92235 |  0:08:29s
epoch 69 | loss: 0.79159 | val_0_rmse: 0.93875 | val_1_rmse: 0.93436 |  0:08:36s
epoch 70 | loss: 0.78572 | val_0_rmse: 0.89227 | val_1_rmse: 0.88908 |  0:08:44s
epoch 71 | loss: 0.78341 | val_0_rmse: 0.89523 | val_1_rmse: 0.89088 |  0:08:51s
epoch 72 | loss: 0.78443 | val_0_rmse: 0.87828 | val_1_rmse: 0.87516 |  0:08:58s
epoch 73 | loss: 0.78661 | val_0_rmse: 0.91742 | val_1_rmse: 0.9127  |  0:09:06s
epoch 74 | loss: 0.7884  | val_0_rmse: 1.00073 | val_1_rmse: 1.00189 |  0:09:13s
epoch 75 | loss: 0.78437 | val_0_rmse: 0.88042 | val_1_rmse: 0.87631 |  0:09:20s
epoch 76 | loss: 0.78657 | val_0_rmse: 0.8807  | val_1_rmse: 0.8762  |  0:09:28s
epoch 77 | loss: 0.78467 | val_0_rmse: 0.89687 | val_1_rmse: 0.88944 |  0:09:35s
epoch 78 | loss: 0.78278 | val_0_rmse: 0.92102 | val_1_rmse: 0.91743 |  0:09:42s
epoch 79 | loss: 0.7842  | val_0_rmse: 0.89655 | val_1_rmse: 0.89316 |  0:09:50s
epoch 80 | loss: 0.7849  | val_0_rmse: 0.93411 | val_1_rmse: 0.9326  |  0:09:57s
epoch 81 | loss: 0.78765 | val_0_rmse: 0.88945 | val_1_rmse: 0.8858  |  0:10:04s
epoch 82 | loss: 0.80431 | val_0_rmse: 0.98755 | val_1_rmse: 1.02809 |  0:10:12s
epoch 83 | loss: 0.79951 | val_0_rmse: 0.884   | val_1_rmse: 0.87996 |  0:10:19s
epoch 84 | loss: 0.84381 | val_0_rmse: 0.92951 | val_1_rmse: 0.92612 |  0:10:26s
epoch 85 | loss: 0.86342 | val_0_rmse: 0.92657 | val_1_rmse: 0.92393 |  0:10:33s
epoch 86 | loss: 0.8615  | val_0_rmse: 0.92374 | val_1_rmse: 0.92097 |  0:10:41s
epoch 87 | loss: 0.85942 | val_0_rmse: 0.92351 | val_1_rmse: 0.92093 |  0:10:48s
epoch 88 | loss: 0.85817 | val_0_rmse: 0.92229 | val_1_rmse: 0.91951 |  0:10:55s
epoch 89 | loss: 0.8571  | val_0_rmse: 0.9235  | val_1_rmse: 0.92143 |  0:11:03s
epoch 90 | loss: 0.85796 | val_0_rmse: 0.92239 | val_1_rmse: 0.91988 |  0:11:10s
epoch 91 | loss: 0.85384 | val_0_rmse: 0.92095 | val_1_rmse: 0.91909 |  0:11:17s
epoch 92 | loss: 0.8588  | val_0_rmse: 0.92462 | val_1_rmse: 0.92151 |  0:11:25s
epoch 93 | loss: 0.85944 | val_0_rmse: 0.92256 | val_1_rmse: 0.92018 |  0:11:32s
epoch 94 | loss: 0.85717 | val_0_rmse: 0.92208 | val_1_rmse: 0.91941 |  0:11:39s
epoch 95 | loss: 0.85505 | val_0_rmse: 0.9201  | val_1_rmse: 0.91698 |  0:11:47s
epoch 96 | loss: 0.8531  | val_0_rmse: 0.91833 | val_1_rmse: 0.91577 |  0:11:54s
epoch 97 | loss: 0.84228 | val_0_rmse: 0.9125  | val_1_rmse: 0.90905 |  0:12:01s
epoch 98 | loss: 0.83825 | val_0_rmse: 0.93273 | val_1_rmse: 0.9313  |  0:12:09s
epoch 99 | loss: 0.82395 | val_0_rmse: 0.93791 | val_1_rmse: 0.93629 |  0:12:16s
epoch 100| loss: 0.82046 | val_0_rmse: 0.90096 | val_1_rmse: 0.89811 |  0:12:23s
epoch 101| loss: 0.81046 | val_0_rmse: 0.89381 | val_1_rmse: 0.89036 |  0:12:31s
epoch 102| loss: 0.80948 | val_0_rmse: 0.93742 | val_1_rmse: 0.93386 |  0:12:38s

Early stopping occured at epoch 102 with best_epoch = 72 and best_val_1_rmse = 0.87516
Best weights from best epoch are automatically used!
ended training at: 15:44:13
Feature importance:
[('Latitude', 0.8195654499675864), ('Longitude', 0.18043455003241357)]
Mean squared error is of 3115211381.95912
Mean absolute error:42613.31439748934
MAPE:0.6355252600101065
R2 score:0.22930606136002296
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:49:39
epoch 0  | loss: 1.07005 | val_0_rmse: 1.01369 | val_1_rmse: 1.00569 |  0:00:07s
epoch 1  | loss: 0.97836 | val_0_rmse: 1.02451 | val_1_rmse: 1.00631 |  0:00:14s
epoch 2  | loss: 0.97826 | val_0_rmse: 1.00022 | val_1_rmse: 0.99484 |  0:00:21s
epoch 3  | loss: 0.97251 | val_0_rmse: 1.00275 | val_1_rmse: 0.99451 |  0:00:28s
epoch 4  | loss: 0.96949 | val_0_rmse: 1.00432 | val_1_rmse: 0.99581 |  0:00:36s
epoch 5  | loss: 0.96825 | val_0_rmse: 1.00085 | val_1_rmse: 0.99411 |  0:00:43s
epoch 6  | loss: 0.96725 | val_0_rmse: 1.04146 | val_1_rmse: 1.01572 |  0:00:50s
epoch 7  | loss: 0.96495 | val_0_rmse: 1.0242  | val_1_rmse: 1.0176  |  0:00:57s
epoch 8  | loss: 0.96362 | val_0_rmse: 1.08924 | val_1_rmse: 1.07744 |  0:01:05s
epoch 9  | loss: 0.96484 | val_0_rmse: 1.06251 | val_1_rmse: 1.05624 |  0:01:12s
epoch 10 | loss: 0.96451 | val_0_rmse: 1.03114 | val_1_rmse: 1.00531 |  0:01:19s
epoch 11 | loss: 0.98133 | val_0_rmse: 1.10914 | val_1_rmse: 1.07969 |  0:01:27s
epoch 12 | loss: 0.9748  | val_0_rmse: 1.00026 | val_1_rmse: 0.99403 |  0:01:34s
epoch 13 | loss: 0.96693 | val_0_rmse: 1.00093 | val_1_rmse: 0.99363 |  0:01:41s
epoch 14 | loss: 0.96593 | val_0_rmse: 1.01212 | val_1_rmse: 1.0059  |  0:01:49s
epoch 15 | loss: 0.96349 | val_0_rmse: 1.01435 | val_1_rmse: 1.00859 |  0:01:56s
epoch 16 | loss: 0.9635  | val_0_rmse: 1.04363 | val_1_rmse: 1.03841 |  0:02:03s
epoch 17 | loss: 0.96337 | val_0_rmse: 1.02922 | val_1_rmse: 1.01923 |  0:02:11s
epoch 18 | loss: 0.96104 | val_0_rmse: 1.07427 | val_1_rmse: 0.99843 |  0:02:18s
epoch 19 | loss: 0.96257 | val_0_rmse: 1.09    | val_1_rmse: 1.08243 |  0:02:25s
epoch 20 | loss: 0.9615  | val_0_rmse: 1.04358 | val_1_rmse: 0.99692 |  0:02:33s
epoch 21 | loss: 0.96268 | val_0_rmse: 1.04289 | val_1_rmse: 1.02676 |  0:02:40s
epoch 22 | loss: 0.96255 | val_0_rmse: 0.99991 | val_1_rmse: 0.99425 |  0:02:47s
epoch 23 | loss: 0.96299 | val_0_rmse: 1.02413 | val_1_rmse: 1.01164 |  0:02:55s
epoch 24 | loss: 0.96045 | val_0_rmse: 1.04924 | val_1_rmse: 0.99981 |  0:03:02s
epoch 25 | loss: 0.96356 | val_0_rmse: 1.02111 | val_1_rmse: 1.01027 |  0:03:09s
epoch 26 | loss: 0.96164 | val_0_rmse: 1.01056 | val_1_rmse: 1.00003 |  0:03:16s
epoch 27 | loss: 0.96449 | val_0_rmse: 1.00926 | val_1_rmse: 1.00155 |  0:03:24s
epoch 28 | loss: 0.96382 | val_0_rmse: 0.99886 | val_1_rmse: 0.99329 |  0:03:31s
epoch 29 | loss: 0.96026 | val_0_rmse: 1.00873 | val_1_rmse: 1.00278 |  0:03:38s
epoch 30 | loss: 0.96371 | val_0_rmse: 1.04836 | val_1_rmse: 0.99345 |  0:03:46s
epoch 31 | loss: 0.98382 | val_0_rmse: 1.01353 | val_1_rmse: 1.00153 |  0:03:53s
epoch 32 | loss: 0.98582 | val_0_rmse: 1.01264 | val_1_rmse: 1.00689 |  0:04:00s
epoch 33 | loss: 0.98161 | val_0_rmse: 1.02378 | val_1_rmse: 1.01493 |  0:04:07s
epoch 34 | loss: 0.97582 | val_0_rmse: 1.02408 | val_1_rmse: 1.01639 |  0:04:15s
epoch 35 | loss: 0.9726  | val_0_rmse: 1.10416 | val_1_rmse: 0.99495 |  0:04:22s
epoch 36 | loss: 0.97086 | val_0_rmse: 1.0039  | val_1_rmse: 0.99345 |  0:04:29s
epoch 37 | loss: 0.96913 | val_0_rmse: 1.00134 | val_1_rmse: 0.99492 |  0:04:37s
epoch 38 | loss: 0.96746 | val_0_rmse: 1.00251 | val_1_rmse: 0.99424 |  0:04:44s
epoch 39 | loss: 0.96745 | val_0_rmse: 0.99898 | val_1_rmse: 0.99324 |  0:04:51s
epoch 40 | loss: 0.96939 | val_0_rmse: 1.01445 | val_1_rmse: 1.00107 |  0:04:59s
epoch 41 | loss: 0.96659 | val_0_rmse: 1.01179 | val_1_rmse: 1.00452 |  0:05:06s
epoch 42 | loss: 0.96559 | val_0_rmse: 0.99871 | val_1_rmse: 0.99336 |  0:05:13s
epoch 43 | loss: 0.96588 | val_0_rmse: 0.99906 | val_1_rmse: 0.99339 |  0:05:21s
epoch 44 | loss: 0.96673 | val_0_rmse: 1.00617 | val_1_rmse: 0.99539 |  0:05:28s
epoch 45 | loss: 0.96664 | val_0_rmse: 1.00423 | val_1_rmse: 0.99572 |  0:05:35s
epoch 46 | loss: 0.96623 | val_0_rmse: 1.01961 | val_1_rmse: 0.99332 |  0:05:43s
epoch 47 | loss: 0.96464 | val_0_rmse: 1.00321 | val_1_rmse: 0.99509 |  0:05:50s
epoch 48 | loss: 0.96362 | val_0_rmse: 1.01136 | val_1_rmse: 0.99382 |  0:05:57s
epoch 49 | loss: 0.96082 | val_0_rmse: 0.9992  | val_1_rmse: 0.99342 |  0:06:05s
epoch 50 | loss: 0.96052 | val_0_rmse: 1.02435 | val_1_rmse: 1.00955 |  0:06:12s
epoch 51 | loss: 0.96158 | val_0_rmse: 1.0283  | val_1_rmse: 0.99991 |  0:06:19s
epoch 52 | loss: 0.96167 | val_0_rmse: 1.01053 | val_1_rmse: 1.00414 |  0:06:26s
epoch 53 | loss: 0.96102 | val_0_rmse: 1.0049  | val_1_rmse: 0.9941  |  0:06:34s
epoch 54 | loss: 0.96173 | val_0_rmse: 1.00693 | val_1_rmse: 0.99638 |  0:06:41s
epoch 55 | loss: 0.96124 | val_0_rmse: 1.02355 | val_1_rmse: 1.00903 |  0:06:49s
epoch 56 | loss: 0.96003 | val_0_rmse: 1.03066 | val_1_rmse: 1.0202  |  0:06:56s
epoch 57 | loss: 0.96078 | val_0_rmse: 1.01242 | val_1_rmse: 1.00417 |  0:07:03s
epoch 58 | loss: 0.96011 | val_0_rmse: 1.02504 | val_1_rmse: 1.00068 |  0:07:11s
epoch 59 | loss: 0.96087 | val_0_rmse: 1.01081 | val_1_rmse: 1.00532 |  0:07:18s
epoch 60 | loss: 0.9604  | val_0_rmse: 1.00201 | val_1_rmse: 0.99632 |  0:07:25s
epoch 61 | loss: 0.96054 | val_0_rmse: 0.99947 | val_1_rmse: 0.99393 |  0:07:33s
epoch 62 | loss: 0.95933 | val_0_rmse: 1.00139 | val_1_rmse: 0.99501 |  0:07:40s
epoch 63 | loss: 0.9597  | val_0_rmse: 1.04607 | val_1_rmse: 1.04065 |  0:07:47s
epoch 64 | loss: 0.95761 | val_0_rmse: 1.05401 | val_1_rmse: 1.03814 |  0:07:55s
epoch 65 | loss: 0.96008 | val_0_rmse: 1.0033  | val_1_rmse: 0.99514 |  0:08:02s
epoch 66 | loss: 0.9797  | val_0_rmse: 1.0173  | val_1_rmse: 1.00814 |  0:08:09s
epoch 67 | loss: 0.96816 | val_0_rmse: 1.03195 | val_1_rmse: 0.99818 |  0:08:17s
epoch 68 | loss: 0.96496 | val_0_rmse: 1.05686 | val_1_rmse: 0.99805 |  0:08:24s
epoch 69 | loss: 0.96608 | val_0_rmse: 0.9995  | val_1_rmse: 0.99328 |  0:08:31s

Early stopping occured at epoch 69 with best_epoch = 39 and best_val_1_rmse = 0.99324
Best weights from best epoch are automatically used!
ended training at: 15:58:13
Feature importance:
[('Latitude', 0.6322118256901111), ('Longitude', 0.3677881743098889)]
Mean squared error is of 58892578317.39189
Mean absolute error:199023.7981599227
MAPE:1.0502134061483945
R2 score:-0.00016912522325940316
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:58:14
epoch 0  | loss: 1.12209 | val_0_rmse: 1.08388 | val_1_rmse: 1.02207 |  0:00:07s
epoch 1  | loss: 0.9725  | val_0_rmse: 1.02728 | val_1_rmse: 1.00705 |  0:00:14s
epoch 2  | loss: 0.97136 | val_0_rmse: 1.04523 | val_1_rmse: 1.03706 |  0:00:21s
epoch 3  | loss: 0.97016 | val_0_rmse: 1.04421 | val_1_rmse: 1.02375 |  0:00:29s
epoch 4  | loss: 0.96955 | val_0_rmse: 1.09938 | val_1_rmse: 0.99601 |  0:00:36s
epoch 5  | loss: 0.97839 | val_0_rmse: 1.03804 | val_1_rmse: 0.9961  |  0:00:44s
epoch 6  | loss: 0.9753  | val_0_rmse: 1.06776 | val_1_rmse: 1.03094 |  0:00:51s
epoch 7  | loss: 0.97123 | val_0_rmse: 1.00404 | val_1_rmse: 0.99677 |  0:00:58s
epoch 8  | loss: 0.97233 | val_0_rmse: 1.02882 | val_1_rmse: 1.01903 |  0:01:05s
epoch 9  | loss: 0.98223 | val_0_rmse: 1.33088 | val_1_rmse: 1.00409 |  0:01:13s
epoch 10 | loss: 0.9771  | val_0_rmse: 1.01842 | val_1_rmse: 1.00134 |  0:01:20s
epoch 11 | loss: 0.97464 | val_0_rmse: 1.15981 | val_1_rmse: 1.01195 |  0:01:27s
epoch 12 | loss: 0.97041 | val_0_rmse: 1.11505 | val_1_rmse: 0.99616 |  0:01:35s
epoch 13 | loss: 0.96849 | val_0_rmse: 1.03804 | val_1_rmse: 1.03015 |  0:01:42s
epoch 14 | loss: 0.96952 | val_0_rmse: 1.0815  | val_1_rmse: 1.05833 |  0:01:49s
epoch 15 | loss: 0.96636 | val_0_rmse: 1.32921 | val_1_rmse: 1.00194 |  0:01:57s
epoch 16 | loss: 0.96593 | val_0_rmse: 2.45822 | val_1_rmse: 1.0027  |  0:02:04s
epoch 17 | loss: 0.9655  | val_0_rmse: 2.50704 | val_1_rmse: 0.99883 |  0:02:11s
epoch 18 | loss: 0.96172 | val_0_rmse: 1.00609 | val_1_rmse: 0.99826 |  0:02:19s
epoch 19 | loss: 0.9653  | val_0_rmse: 3.14843 | val_1_rmse: 0.99729 |  0:02:26s
epoch 20 | loss: 0.96396 | val_0_rmse: 1.07185 | val_1_rmse: 1.00833 |  0:02:33s
epoch 21 | loss: 0.96506 | val_0_rmse: 1.00811 | val_1_rmse: 1.00225 |  0:02:41s
epoch 22 | loss: 0.96524 | val_0_rmse: 1.08047 | val_1_rmse: 1.07249 |  0:02:48s
epoch 23 | loss: 0.96633 | val_0_rmse: 1.07849 | val_1_rmse: 1.06317 |  0:02:55s
epoch 24 | loss: 0.96448 | val_0_rmse: 1.00173 | val_1_rmse: 0.99625 |  0:03:03s
epoch 25 | loss: 0.96718 | val_0_rmse: 1.06179 | val_1_rmse: 1.03904 |  0:03:10s
epoch 26 | loss: 0.96315 | val_0_rmse: 1.02014 | val_1_rmse: 1.00718 |  0:03:17s
epoch 27 | loss: 0.97388 | val_0_rmse: 1.00602 | val_1_rmse: 0.99595 |  0:03:25s
epoch 28 | loss: 0.98994 | val_0_rmse: 1.01239 | val_1_rmse: 1.00375 |  0:03:32s
epoch 29 | loss: 0.97905 | val_0_rmse: 1.02634 | val_1_rmse: 1.01459 |  0:03:39s
epoch 30 | loss: 0.97085 | val_0_rmse: 1.08678 | val_1_rmse: 1.07603 |  0:03:47s
epoch 31 | loss: 0.97084 | val_0_rmse: 1.03021 | val_1_rmse: 1.02248 |  0:03:54s
epoch 32 | loss: 0.97113 | val_0_rmse: 1.14549 | val_1_rmse: 1.0286  |  0:04:01s
epoch 33 | loss: 0.96865 | val_0_rmse: 1.00555 | val_1_rmse: 1.00008 |  0:04:08s
epoch 34 | loss: 0.96907 | val_0_rmse: 1.11561 | val_1_rmse: 1.10898 |  0:04:16s
epoch 35 | loss: 0.96839 | val_0_rmse: 1.0865  | val_1_rmse: 1.00066 |  0:04:23s
epoch 36 | loss: 0.96537 | val_0_rmse: 1.0365  | val_1_rmse: 1.02537 |  0:04:30s
epoch 37 | loss: 0.96617 | val_0_rmse: 1.06649 | val_1_rmse: 1.00376 |  0:04:38s
epoch 38 | loss: 0.96646 | val_0_rmse: 1.09642 | val_1_rmse: 1.07807 |  0:04:45s
epoch 39 | loss: 0.96573 | val_0_rmse: 1.1195  | val_1_rmse: 1.0975  |  0:04:52s
epoch 40 | loss: 0.96308 | val_0_rmse: 1.15903 | val_1_rmse: 1.14121 |  0:05:00s
epoch 41 | loss: 0.9634  | val_0_rmse: 1.10306 | val_1_rmse: 1.07326 |  0:05:07s
epoch 42 | loss: 0.96453 | val_0_rmse: 1.05278 | val_1_rmse: 0.99666 |  0:05:14s
epoch 43 | loss: 0.96387 | val_0_rmse: 1.00151 | val_1_rmse: 0.99652 |  0:05:22s
epoch 44 | loss: 0.96352 | val_0_rmse: 1.14028 | val_1_rmse: 1.13335 |  0:05:29s
epoch 45 | loss: 0.96431 | val_0_rmse: 1.00525 | val_1_rmse: 0.99615 |  0:05:37s
epoch 46 | loss: 0.96216 | val_0_rmse: 1.00707 | val_1_rmse: 1.00078 |  0:05:44s
epoch 47 | loss: 0.96355 | val_0_rmse: 1.05012 | val_1_rmse: 1.0458  |  0:05:51s
epoch 48 | loss: 0.96308 | val_0_rmse: 1.01366 | val_1_rmse: 1.00334 |  0:05:59s
epoch 49 | loss: 0.96183 | val_0_rmse: 1.00833 | val_1_rmse: 0.99619 |  0:06:06s
epoch 50 | loss: 0.96251 | val_0_rmse: 1.0931  | val_1_rmse: 1.08091 |  0:06:13s
epoch 51 | loss: 0.96363 | val_0_rmse: 1.0949  | val_1_rmse: 1.08815 |  0:06:21s
epoch 52 | loss: 0.96292 | val_0_rmse: 1.00718 | val_1_rmse: 0.99642 |  0:06:28s
epoch 53 | loss: 0.96354 | val_0_rmse: 1.01073 | val_1_rmse: 0.99635 |  0:06:35s
epoch 54 | loss: 0.96188 | val_0_rmse: 1.07897 | val_1_rmse: 1.05185 |  0:06:43s
epoch 55 | loss: 0.96251 | val_0_rmse: 1.0844  | val_1_rmse: 1.07785 |  0:06:50s
epoch 56 | loss: 0.96205 | val_0_rmse: 1.023   | val_1_rmse: 1.01844 |  0:06:57s
epoch 57 | loss: 0.96241 | val_0_rmse: 1.00095 | val_1_rmse: 0.99619 |  0:07:05s

Early stopping occured at epoch 57 with best_epoch = 27 and best_val_1_rmse = 0.99595
Best weights from best epoch are automatically used!
ended training at: 16:05:21
Feature importance:
[('Latitude', 0.09429512301737066), ('Longitude', 0.9057048769826294)]
Mean squared error is of 57863378191.36071
Mean absolute error:196616.9839029488
MAPE:1.0123656912672305
R2 score:-0.00042498446772087917
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:14:22
epoch 0  | loss: 1.15795 | val_0_rmse: 1.03165 | val_1_rmse: 1.01605 |  0:00:07s
epoch 1  | loss: 0.97658 | val_0_rmse: 1.00356 | val_1_rmse: 1.00514 |  0:00:14s
epoch 2  | loss: 0.96736 | val_0_rmse: 1.7696  | val_1_rmse: 1.01324 |  0:00:21s
epoch 3  | loss: 0.96492 | val_0_rmse: 1.09074 | val_1_rmse: 1.02899 |  0:00:28s
epoch 4  | loss: 0.96346 | val_0_rmse: 1.05735 | val_1_rmse: 1.01313 |  0:00:35s
epoch 5  | loss: 0.96345 | val_0_rmse: 1.02549 | val_1_rmse: 1.0036  |  0:00:43s
epoch 6  | loss: 0.96438 | val_0_rmse: 1.50926 | val_1_rmse: 1.02923 |  0:00:50s
epoch 7  | loss: 0.96425 | val_0_rmse: 1.0818  | val_1_rmse: 1.02945 |  0:00:57s
epoch 8  | loss: 0.96276 | val_0_rmse: 1.04005 | val_1_rmse: 1.03264 |  0:01:04s
epoch 9  | loss: 0.96519 | val_0_rmse: 1.12967 | val_1_rmse: 1.01782 |  0:01:11s
epoch 10 | loss: 0.96229 | val_0_rmse: 1.34167 | val_1_rmse: 1.00551 |  0:01:19s
epoch 11 | loss: 0.96311 | val_0_rmse: 1.75236 | val_1_rmse: 1.01944 |  0:01:26s
epoch 12 | loss: 0.96002 | val_0_rmse: 1.04474 | val_1_rmse: 1.01395 |  0:01:33s
epoch 13 | loss: 0.96083 | val_0_rmse: 1.02845 | val_1_rmse: 1.01068 |  0:01:41s
epoch 14 | loss: 0.96255 | val_0_rmse: 1.27076 | val_1_rmse: 1.02939 |  0:01:48s
epoch 15 | loss: 0.96596 | val_0_rmse: 1.62402 | val_1_rmse: 1.0249  |  0:01:55s
epoch 16 | loss: 0.96411 | val_0_rmse: 1.14575 | val_1_rmse: 1.02217 |  0:02:03s
epoch 17 | loss: 0.9608  | val_0_rmse: 1.50377 | val_1_rmse: 1.0194  |  0:02:10s
epoch 18 | loss: 0.96281 | val_0_rmse: 1.20058 | val_1_rmse: 1.01378 |  0:02:17s
epoch 19 | loss: 0.96208 | val_0_rmse: 1.04029 | val_1_rmse: 1.02275 |  0:02:25s
epoch 20 | loss: 0.96885 | val_0_rmse: 1.11868 | val_1_rmse: 1.00636 |  0:02:32s
epoch 21 | loss: 0.9716  | val_0_rmse: 1.13105 | val_1_rmse: 1.04098 |  0:02:39s
epoch 22 | loss: 0.97374 | val_0_rmse: 1.02309 | val_1_rmse: 1.00512 |  0:02:47s
epoch 23 | loss: 0.96684 | val_0_rmse: 1.02946 | val_1_rmse: 1.02089 |  0:02:54s
epoch 24 | loss: 0.97011 | val_0_rmse: 1.87504 | val_1_rmse: 1.02423 |  0:03:01s
epoch 25 | loss: 0.96561 | val_0_rmse: 1.64106 | val_1_rmse: 1.00476 |  0:03:09s
epoch 26 | loss: 0.96202 | val_0_rmse: 1.32506 | val_1_rmse: 1.01975 |  0:03:16s
epoch 27 | loss: 0.96113 | val_0_rmse: 1.18296 | val_1_rmse: 1.04742 |  0:03:23s
epoch 28 | loss: 0.96195 | val_0_rmse: 1.11993 | val_1_rmse: 1.02036 |  0:03:31s
epoch 29 | loss: 0.95995 | val_0_rmse: 1.03487 | val_1_rmse: 1.0213  |  0:03:38s
epoch 30 | loss: 0.96014 | val_0_rmse: 1.37592 | val_1_rmse: 1.00795 |  0:03:45s
epoch 31 | loss: 0.96135 | val_0_rmse: 1.78719 | val_1_rmse: 1.02539 |  0:03:52s
epoch 32 | loss: 0.96053 | val_0_rmse: 1.52112 | val_1_rmse: 1.02332 |  0:04:00s
epoch 33 | loss: 0.96    | val_0_rmse: 1.59526 | val_1_rmse: 1.00709 |  0:04:07s
epoch 34 | loss: 0.96175 | val_0_rmse: 1.15747 | val_1_rmse: 1.03523 |  0:04:14s
epoch 35 | loss: 0.96038 | val_0_rmse: 1.03688 | val_1_rmse: 1.03746 |  0:04:22s

Early stopping occured at epoch 35 with best_epoch = 5 and best_val_1_rmse = 1.0036
Best weights from best epoch are automatically used!
ended training at: 16:18:47
Feature importance:
[('Latitude', 0.5688888845536436), ('Longitude', 0.43111111544635633)]
Mean squared error is of 30973096714.569855
Mean absolute error:143921.80791426418
MAPE:0.4437083616571541
R2 score:-0.002257218878655598
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:18:48
epoch 0  | loss: 1.19582 | val_0_rmse: 1.03149 | val_1_rmse: 1.01216 |  0:00:07s
epoch 1  | loss: 0.96441 | val_0_rmse: 1.02865 | val_1_rmse: 1.00707 |  0:00:14s
epoch 2  | loss: 0.96101 | val_0_rmse: 1.12092 | val_1_rmse: 1.0267  |  0:00:22s
epoch 3  | loss: 0.96205 | val_0_rmse: 1.23267 | val_1_rmse: 1.06141 |  0:00:29s
epoch 4  | loss: 0.96357 | val_0_rmse: 2.80704 | val_1_rmse: 1.31764 |  0:00:37s
epoch 5  | loss: 0.96186 | val_0_rmse: 1.04229 | val_1_rmse: 1.02329 |  0:00:44s
epoch 6  | loss: 0.95984 | val_0_rmse: 1.26252 | val_1_rmse: 1.0026  |  0:00:51s
epoch 7  | loss: 0.95923 | val_0_rmse: 1.10164 | val_1_rmse: 0.99916 |  0:00:59s
epoch 8  | loss: 0.96136 | val_0_rmse: 1.19172 | val_1_rmse: 1.02162 |  0:01:06s
epoch 9  | loss: 0.96007 | val_0_rmse: 1.16517 | val_1_rmse: 1.00311 |  0:01:13s
epoch 10 | loss: 0.96015 | val_0_rmse: 1.03696 | val_1_rmse: 0.99994 |  0:01:21s
epoch 11 | loss: 0.96021 | val_0_rmse: 1.0445  | val_1_rmse: 0.99877 |  0:01:28s
epoch 12 | loss: 0.96069 | val_0_rmse: 1.286   | val_1_rmse: 1.06387 |  0:01:36s
epoch 13 | loss: 0.95969 | val_0_rmse: 1.01535 | val_1_rmse: 1.00453 |  0:01:43s
epoch 14 | loss: 0.96071 | val_0_rmse: 1.01492 | val_1_rmse: 1.00509 |  0:01:50s
epoch 15 | loss: 0.95856 | val_0_rmse: 1.02054 | val_1_rmse: 1.00006 |  0:01:58s
epoch 16 | loss: 0.96522 | val_0_rmse: 1.00192 | val_1_rmse: 0.99829 |  0:02:05s
epoch 17 | loss: 0.9672  | val_0_rmse: 1.04643 | val_1_rmse: 1.0042  |  0:02:12s
epoch 18 | loss: 0.96234 | val_0_rmse: 1.91965 | val_1_rmse: 1.0085  |  0:02:20s
epoch 19 | loss: 0.96236 | val_0_rmse: 1.01729 | val_1_rmse: 0.99748 |  0:02:27s
epoch 20 | loss: 0.96161 | val_0_rmse: 1.26608 | val_1_rmse: 1.02339 |  0:02:35s
epoch 21 | loss: 0.96067 | val_0_rmse: 1.10248 | val_1_rmse: 1.01426 |  0:02:42s
epoch 22 | loss: 0.96061 | val_0_rmse: 1.10246 | val_1_rmse: 1.00711 |  0:02:49s
epoch 23 | loss: 0.96015 | val_0_rmse: 1.05691 | val_1_rmse: 1.00664 |  0:02:57s
epoch 24 | loss: 0.95985 | val_0_rmse: 1.11002 | val_1_rmse: 1.02297 |  0:03:04s
epoch 25 | loss: 0.96064 | val_0_rmse: 1.03764 | val_1_rmse: 1.01361 |  0:03:11s
epoch 26 | loss: 0.97046 | val_0_rmse: 1.00324 | val_1_rmse: 0.99651 |  0:03:19s
epoch 27 | loss: 2.59345 | val_0_rmse: 9.03811 | val_1_rmse: 3.11125 |  0:03:26s
epoch 28 | loss: 4.31632 | val_0_rmse: 79.99536| val_1_rmse: 35.70861|  0:03:33s
epoch 29 | loss: 3.14761 | val_0_rmse: 52.40697| val_1_rmse: 18.12211|  0:03:41s
epoch 30 | loss: 3.39706 | val_0_rmse: 14.1159 | val_1_rmse: 2.92204 |  0:03:48s
epoch 31 | loss: 2.29183 | val_0_rmse: 71.68418| val_1_rmse: 24.09218|  0:03:56s
epoch 32 | loss: 1.07904 | val_0_rmse: 1.25475 | val_1_rmse: 1.05289 |  0:04:03s
epoch 33 | loss: 1.01307 | val_0_rmse: 1260.32124| val_1_rmse: 415.55466|  0:04:10s
epoch 34 | loss: 1.03058 | val_0_rmse: 7516.8871| val_1_rmse: 682.59504|  0:04:18s
epoch 35 | loss: 1.04397 | val_0_rmse: 180.05665| val_1_rmse: 163.76206|  0:04:25s
epoch 36 | loss: 1.04222 | val_0_rmse: 51673.1086| val_1_rmse: 50182.26649|  0:04:32s
epoch 37 | loss: 1.02391 | val_0_rmse: 167657.1041| val_1_rmse: 50963.90869|  0:04:40s
epoch 38 | loss: 1.01612 | val_0_rmse: 97882.14314| val_1_rmse: 90337.55493|  0:04:47s
epoch 39 | loss: 1.03496 | val_0_rmse: 47043.33913| val_1_rmse: 43617.45612|  0:04:54s
epoch 40 | loss: 1.04608 | val_0_rmse: 363852.76317| val_1_rmse: 121433.36504|  0:05:01s
epoch 41 | loss: 1.01967 | val_0_rmse: 11133.73268| val_1_rmse: 10653.50671|  0:05:09s
epoch 42 | loss: 1.01959 | val_0_rmse: 954.68822| val_1_rmse: 921.75682|  0:05:16s
epoch 43 | loss: 1.04556 | val_0_rmse: 75991837.57627| val_1_rmse: 75803604.2515|  0:05:23s
epoch 44 | loss: 1.00638 | val_0_rmse: 70626.96099| val_1_rmse: 7829.0559|  0:05:31s
epoch 45 | loss: 1.01635 | val_0_rmse: 2651481072.12188| val_1_rmse: 84559561.63517|  0:05:38s
epoch 46 | loss: 1.00776 | val_0_rmse: 7235096.89696| val_1_rmse: 3427419.25407|  0:05:45s
epoch 47 | loss: 1.01141 | val_0_rmse: 512769.78221| val_1_rmse: 509769.03381|  0:05:53s
epoch 48 | loss: 1.00988 | val_0_rmse: 3247950.9092| val_1_rmse: 978456.15365|  0:06:00s
epoch 49 | loss: 1.00279 | val_0_rmse: 1546011482974.699| val_1_rmse: 2040878079.99593|  0:06:07s
epoch 50 | loss: 1.00446 | val_0_rmse: 55863533.5373| val_1_rmse: 15651860.1643|  0:06:15s
epoch 51 | loss: 1.00638 | val_0_rmse: 225580975.34071| val_1_rmse: 9612498.00407|  0:06:22s
epoch 52 | loss: 1.00675 | val_0_rmse: 2799514.15729| val_1_rmse: 885599.68386|  0:06:29s
epoch 53 | loss: 1.00895 | val_0_rmse: 96493170.63057| val_1_rmse: 26006183.50086|  0:06:37s
epoch 54 | loss: 1.0111  | val_0_rmse: 1107859.66716| val_1_rmse: 151486.6447|  0:06:44s
epoch 55 | loss: 1.02179 | val_0_rmse: 15543065.35628| val_1_rmse: 5165988.55609|  0:06:51s
epoch 56 | loss: 1.03134 | val_0_rmse: 14895634.03384| val_1_rmse: 12170272.6908|  0:06:59s

Early stopping occured at epoch 56 with best_epoch = 26 and best_val_1_rmse = 0.99651
Best weights from best epoch are automatically used!
ended training at: 16:25:49
Feature importance:
[('Latitude', 0.0308488513186352), ('Longitude', 0.9691511486813648)]
Mean squared error is of 31555411489.72477
Mean absolute error:143441.75447706142
MAPE:0.4287972442185787
R2 score:-0.0002905661066556764
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:35:34
