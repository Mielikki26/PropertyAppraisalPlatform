TabNet Logs:

Saving copy of script...
In this script only the South American datasets are used and they have two extra features - Country and ProvinceThis is done to test the possibility that the number of features being used is too low and adding more is helpful
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:04:21
epoch 0  | loss: 0.66717 | val_0_rmse: 0.72275 | val_1_rmse: 0.72443 |  0:00:09s
epoch 1  | loss: 0.44245 | val_0_rmse: 0.68251 | val_1_rmse: 0.68436 |  0:00:15s
epoch 2  | loss: 0.42263 | val_0_rmse: 0.67047 | val_1_rmse: 0.6717  |  0:00:21s
epoch 3  | loss: 0.40711 | val_0_rmse: 0.71395 | val_1_rmse: 0.71472 |  0:00:26s
epoch 4  | loss: 0.39517 | val_0_rmse: 0.70703 | val_1_rmse: 0.70903 |  0:00:32s
epoch 5  | loss: 0.39449 | val_0_rmse: 0.7664  | val_1_rmse: 0.76629 |  0:00:38s
epoch 6  | loss: 0.38775 | val_0_rmse: 0.73597 | val_1_rmse: 0.73673 |  0:00:44s
epoch 7  | loss: 0.38173 | val_0_rmse: 0.69189 | val_1_rmse: 0.69338 |  0:00:50s
epoch 8  | loss: 0.38397 | val_0_rmse: 0.66635 | val_1_rmse: 0.66879 |  0:00:56s
epoch 9  | loss: 0.37945 | val_0_rmse: 0.61563 | val_1_rmse: 0.61932 |  0:01:02s
epoch 10 | loss: 0.37638 | val_0_rmse: 0.61226 | val_1_rmse: 0.61708 |  0:01:08s
epoch 11 | loss: 0.37395 | val_0_rmse: 0.68504 | val_1_rmse: 0.68773 |  0:01:14s
epoch 12 | loss: 0.37468 | val_0_rmse: 0.64491 | val_1_rmse: 0.64852 |  0:01:21s
epoch 13 | loss: 0.37459 | val_0_rmse: 0.66964 | val_1_rmse: 0.67246 |  0:01:27s
epoch 14 | loss: 0.36498 | val_0_rmse: 0.73403 | val_1_rmse: 0.73514 |  0:01:34s
epoch 15 | loss: 0.36308 | val_0_rmse: 0.79653 | val_1_rmse: 0.79579 |  0:01:41s
epoch 16 | loss: 0.36555 | val_0_rmse: 0.59099 | val_1_rmse: 0.59357 |  0:01:48s
epoch 17 | loss: 0.36161 | val_0_rmse: 0.6028  | val_1_rmse: 0.60402 |  0:01:55s
epoch 18 | loss: 0.35951 | val_0_rmse: 0.79181 | val_1_rmse: 0.79178 |  0:02:02s
epoch 19 | loss: 0.35597 | val_0_rmse: 0.66301 | val_1_rmse: 0.66372 |  0:02:09s
epoch 20 | loss: 0.35784 | val_0_rmse: 0.63897 | val_1_rmse: 0.64028 |  0:02:17s
epoch 21 | loss: 0.35452 | val_0_rmse: 0.74743 | val_1_rmse: 0.74634 |  0:02:24s
epoch 22 | loss: 0.35486 | val_0_rmse: 0.61597 | val_1_rmse: 0.61761 |  0:02:32s
epoch 23 | loss: 0.35382 | val_0_rmse: 0.65854 | val_1_rmse: 0.65929 |  0:02:39s
epoch 24 | loss: 0.35239 | val_0_rmse: 0.6958  | val_1_rmse: 0.69642 |  0:02:47s
epoch 25 | loss: 0.35331 | val_0_rmse: 0.60297 | val_1_rmse: 0.60424 |  0:02:55s
epoch 26 | loss: 0.3509  | val_0_rmse: 0.63835 | val_1_rmse: 0.63848 |  0:03:03s
epoch 27 | loss: 0.35228 | val_0_rmse: 0.73324 | val_1_rmse: 0.73336 |  0:03:10s
epoch 28 | loss: 0.35536 | val_0_rmse: 0.61417 | val_1_rmse: 0.61559 |  0:03:18s
epoch 29 | loss: 0.35111 | val_0_rmse: 0.60122 | val_1_rmse: 0.60299 |  0:03:26s
epoch 30 | loss: 0.35084 | val_0_rmse: 0.6063  | val_1_rmse: 0.6079  |  0:03:34s
epoch 31 | loss: 0.35116 | val_0_rmse: 0.62845 | val_1_rmse: 0.6294  |  0:03:42s
epoch 32 | loss: 0.35221 | val_0_rmse: 0.63369 | val_1_rmse: 0.6345  |  0:03:50s
epoch 33 | loss: 0.35154 | val_0_rmse: 0.72592 | val_1_rmse: 0.72535 |  0:03:58s
epoch 34 | loss: 0.34896 | val_0_rmse: 0.64093 | val_1_rmse: 0.64141 |  0:04:07s
epoch 35 | loss: 0.34645 | val_0_rmse: 0.75523 | val_1_rmse: 0.75343 |  0:04:15s
epoch 36 | loss: 0.35009 | val_0_rmse: 0.59414 | val_1_rmse: 0.59326 |  0:04:23s
epoch 37 | loss: 0.35243 | val_0_rmse: 0.59474 | val_1_rmse: 0.59508 |  0:04:31s
epoch 38 | loss: 0.34882 | val_0_rmse: 0.60288 | val_1_rmse: 0.6035  |  0:04:40s
epoch 39 | loss: 0.34887 | val_0_rmse: 0.74708 | val_1_rmse: 0.74558 |  0:04:48s
epoch 40 | loss: 0.34521 | val_0_rmse: 0.60073 | val_1_rmse: 0.59963 |  0:04:57s
epoch 41 | loss: 0.34696 | val_0_rmse: 0.63169 | val_1_rmse: 0.63205 |  0:05:05s
epoch 42 | loss: 0.34483 | val_0_rmse: 0.59946 | val_1_rmse: 0.6002  |  0:05:13s
epoch 43 | loss: 0.34456 | val_0_rmse: 0.59096 | val_1_rmse: 0.59198 |  0:05:21s
epoch 44 | loss: 0.34355 | val_0_rmse: 0.59007 | val_1_rmse: 0.5909  |  0:05:30s
epoch 45 | loss: 0.34151 | val_0_rmse: 0.59089 | val_1_rmse: 0.59084 |  0:05:38s
epoch 46 | loss: 0.34468 | val_0_rmse: 0.81074 | val_1_rmse: 0.80959 |  0:05:47s
epoch 47 | loss: 0.34083 | val_0_rmse: 0.6345  | val_1_rmse: 0.63552 |  0:05:55s
epoch 48 | loss: 0.34343 | val_0_rmse: 0.67904 | val_1_rmse: 0.67854 |  0:06:03s
epoch 49 | loss: 0.33883 | val_0_rmse: 0.68985 | val_1_rmse: 0.68887 |  0:06:12s
epoch 50 | loss: 0.3429  | val_0_rmse: 0.71937 | val_1_rmse: 0.71896 |  0:06:20s
epoch 51 | loss: 0.34162 | val_0_rmse: 0.59116 | val_1_rmse: 0.59052 |  0:06:29s
epoch 52 | loss: 0.33841 | val_0_rmse: 0.66473 | val_1_rmse: 0.66573 |  0:06:38s
epoch 53 | loss: 0.33674 | val_0_rmse: 0.6167  | val_1_rmse: 0.6165  |  0:06:49s
epoch 54 | loss: 0.33907 | val_0_rmse: 0.6125  | val_1_rmse: 0.61172 |  0:06:58s
epoch 55 | loss: 0.33942 | val_0_rmse: 0.71485 | val_1_rmse: 0.71508 |  0:07:09s
epoch 56 | loss: 0.33768 | val_0_rmse: 0.62602 | val_1_rmse: 0.62807 |  0:07:18s
epoch 57 | loss: 0.3499  | val_0_rmse: 0.72955 | val_1_rmse: 0.73056 |  0:07:27s
epoch 58 | loss: 0.34138 | val_0_rmse: 0.58563 | val_1_rmse: 0.58535 |  0:07:36s
epoch 59 | loss: 0.33796 | val_0_rmse: 0.60477 | val_1_rmse: 0.60538 |  0:07:46s
epoch 60 | loss: 0.34021 | val_0_rmse: 0.5892  | val_1_rmse: 0.58765 |  0:07:56s
epoch 61 | loss: 0.34262 | val_0_rmse: 0.60261 | val_1_rmse: 0.60125 |  0:08:06s
epoch 62 | loss: 0.34363 | val_0_rmse: 0.86262 | val_1_rmse: 0.86074 |  0:08:16s
epoch 63 | loss: 0.34056 | val_0_rmse: 0.65137 | val_1_rmse: 0.65016 |  0:08:27s
epoch 64 | loss: 0.33589 | val_0_rmse: 0.69247 | val_1_rmse: 0.69141 |  0:08:37s
epoch 65 | loss: 0.33458 | val_0_rmse: 0.81485 | val_1_rmse: 0.81256 |  0:08:46s
epoch 66 | loss: 0.33333 | val_0_rmse: 0.85498 | val_1_rmse: 0.85146 |  0:08:56s
epoch 67 | loss: 0.33572 | val_0_rmse: 0.67789 | val_1_rmse: 0.6765  |  0:09:05s
epoch 68 | loss: 0.33458 | val_0_rmse: 0.61253 | val_1_rmse: 0.61186 |  0:09:14s
epoch 69 | loss: 0.33214 | val_0_rmse: 0.63626 | val_1_rmse: 0.63481 |  0:09:23s
epoch 70 | loss: 0.33448 | val_0_rmse: 0.69953 | val_1_rmse: 0.69619 |  0:09:32s
epoch 71 | loss: 0.33919 | val_0_rmse: 0.57644 | val_1_rmse: 0.57312 |  0:09:41s
epoch 72 | loss: 0.33728 | val_0_rmse: 0.63545 | val_1_rmse: 0.6346  |  0:09:50s
epoch 73 | loss: 0.33434 | val_0_rmse: 0.70732 | val_1_rmse: 0.70447 |  0:09:59s
epoch 74 | loss: 0.33257 | val_0_rmse: 0.6193  | val_1_rmse: 0.61919 |  0:10:08s
epoch 75 | loss: 0.3304  | val_0_rmse: 0.60923 | val_1_rmse: 0.60854 |  0:10:18s
epoch 76 | loss: 0.3321  | val_0_rmse: 0.60122 | val_1_rmse: 0.59891 |  0:10:27s
epoch 77 | loss: 0.33009 | val_0_rmse: 0.64734 | val_1_rmse: 0.64648 |  0:10:36s
epoch 78 | loss: 0.33265 | val_0_rmse: 0.6619  | val_1_rmse: 0.66158 |  0:10:45s
epoch 79 | loss: 0.33014 | val_0_rmse: 0.6631  | val_1_rmse: 0.66262 |  0:10:54s
epoch 80 | loss: 0.33293 | val_0_rmse: 0.82544 | val_1_rmse: 0.82297 |  0:11:04s
epoch 81 | loss: 0.33474 | val_0_rmse: 0.57123 | val_1_rmse: 0.56976 |  0:11:13s
epoch 82 | loss: 0.32841 | val_0_rmse: 0.82967 | val_1_rmse: 0.82799 |  0:11:22s
epoch 83 | loss: 0.32928 | val_0_rmse: 0.61831 | val_1_rmse: 0.6191  |  0:11:31s
epoch 84 | loss: 0.32894 | val_0_rmse: 0.61908 | val_1_rmse: 0.61594 |  0:11:40s
epoch 85 | loss: 0.32818 | val_0_rmse: 0.67455 | val_1_rmse: 0.67422 |  0:11:49s
epoch 86 | loss: 0.32747 | val_0_rmse: 0.64697 | val_1_rmse: 0.64445 |  0:11:58s
epoch 87 | loss: 0.32629 | val_0_rmse: 0.58274 | val_1_rmse: 0.5808  |  0:12:07s
epoch 88 | loss: 0.32627 | val_0_rmse: 0.60519 | val_1_rmse: 0.60186 |  0:12:16s
epoch 89 | loss: 0.33279 | val_0_rmse: 0.62374 | val_1_rmse: 0.62431 |  0:12:25s
epoch 90 | loss: 0.33967 | val_0_rmse: 0.70164 | val_1_rmse: 0.70113 |  0:12:35s
epoch 91 | loss: 0.34634 | val_0_rmse: 0.654   | val_1_rmse: 0.65523 |  0:12:44s
epoch 92 | loss: 0.34013 | val_0_rmse: 0.58755 | val_1_rmse: 0.58756 |  0:12:53s
epoch 93 | loss: 0.33459 | val_0_rmse: 0.82624 | val_1_rmse: 0.82767 |  0:13:02s
epoch 94 | loss: 0.33385 | val_0_rmse: 0.58226 | val_1_rmse: 0.58228 |  0:13:11s
epoch 95 | loss: 0.33157 | val_0_rmse: 0.59572 | val_1_rmse: 0.5963  |  0:13:20s
epoch 96 | loss: 0.33042 | val_0_rmse: 0.58095 | val_1_rmse: 0.5806  |  0:13:29s
epoch 97 | loss: 0.32803 | val_0_rmse: 0.66097 | val_1_rmse: 0.66066 |  0:13:38s
epoch 98 | loss: 0.32533 | val_0_rmse: 0.56903 | val_1_rmse: 0.57006 |  0:13:48s
epoch 99 | loss: 0.32686 | val_0_rmse: 0.67516 | val_1_rmse: 0.67239 |  0:13:57s
epoch 100| loss: 0.32894 | val_0_rmse: 0.5902  | val_1_rmse: 0.58976 |  0:14:06s
epoch 101| loss: 0.32671 | val_0_rmse: 0.59051 | val_1_rmse: 0.59048 |  0:14:15s
epoch 102| loss: 0.3288  | val_0_rmse: 0.67051 | val_1_rmse: 0.67115 |  0:14:24s
epoch 103| loss: 0.32588 | val_0_rmse: 0.59603 | val_1_rmse: 0.5958  |  0:14:33s
epoch 104| loss: 0.32446 | val_0_rmse: 0.6207  | val_1_rmse: 0.62027 |  0:14:42s
epoch 105| loss: 0.32365 | val_0_rmse: 0.56844 | val_1_rmse: 0.56921 |  0:14:51s
epoch 106| loss: 0.32252 | val_0_rmse: 0.83314 | val_1_rmse: 0.83224 |  0:15:01s
epoch 107| loss: 0.3213  | val_0_rmse: 0.63877 | val_1_rmse: 0.63955 |  0:15:10s
epoch 108| loss: 0.32125 | val_0_rmse: 0.66744 | val_1_rmse: 0.667   |  0:15:19s
epoch 109| loss: 0.32042 | val_0_rmse: 0.62092 | val_1_rmse: 0.62201 |  0:15:28s
epoch 110| loss: 0.31954 | val_0_rmse: 0.63749 | val_1_rmse: 0.63551 |  0:15:37s
epoch 111| loss: 0.31734 | val_0_rmse: 0.55036 | val_1_rmse: 0.55028 |  0:15:46s
epoch 112| loss: 0.31584 | val_0_rmse: 0.59577 | val_1_rmse: 0.59553 |  0:15:55s
epoch 113| loss: 0.32855 | val_0_rmse: 0.65461 | val_1_rmse: 0.65494 |  0:16:04s
epoch 114| loss: 0.32488 | val_0_rmse: 0.58228 | val_1_rmse: 0.58314 |  0:16:13s
epoch 115| loss: 0.32116 | val_0_rmse: 0.61066 | val_1_rmse: 0.61275 |  0:16:22s
epoch 116| loss: 0.32144 | val_0_rmse: 0.60771 | val_1_rmse: 0.60582 |  0:16:31s
epoch 117| loss: 0.31938 | val_0_rmse: 0.63267 | val_1_rmse: 0.63174 |  0:16:40s
epoch 118| loss: 0.3216  | val_0_rmse: 0.62152 | val_1_rmse: 0.61986 |  0:16:49s
epoch 119| loss: 0.3197  | val_0_rmse: 0.60363 | val_1_rmse: 0.60399 |  0:16:58s
epoch 120| loss: 0.31714 | val_0_rmse: 0.59722 | val_1_rmse: 0.59696 |  0:17:07s
epoch 121| loss: 0.31606 | val_0_rmse: 0.77838 | val_1_rmse: 0.78038 |  0:17:16s
epoch 122| loss: 0.313   | val_0_rmse: 0.57984 | val_1_rmse: 0.58057 |  0:17:25s
epoch 123| loss: 0.31469 | val_0_rmse: 0.6876  | val_1_rmse: 0.68689 |  0:17:34s
epoch 124| loss: 0.31275 | val_0_rmse: 0.68782 | val_1_rmse: 0.68682 |  0:17:43s
epoch 125| loss: 0.31582 | val_0_rmse: 0.58861 | val_1_rmse: 0.58773 |  0:17:52s
epoch 126| loss: 0.31271 | val_0_rmse: 0.56232 | val_1_rmse: 0.56385 |  0:18:01s
epoch 127| loss: 0.31355 | val_0_rmse: 0.94996 | val_1_rmse: 0.95056 |  0:18:10s
epoch 128| loss: 0.31241 | val_0_rmse: 0.56079 | val_1_rmse: 0.56219 |  0:18:19s
epoch 129| loss: 0.31188 | val_0_rmse: 0.59802 | val_1_rmse: 0.5982  |  0:18:28s
epoch 130| loss: 0.33042 | val_0_rmse: 0.82354 | val_1_rmse: 0.82089 |  0:18:38s
epoch 131| loss: 0.31916 | val_0_rmse: 0.62494 | val_1_rmse: 0.62492 |  0:18:47s
epoch 132| loss: 0.31181 | val_0_rmse: 0.63412 | val_1_rmse: 0.63718 |  0:18:56s
epoch 133| loss: 0.31447 | val_0_rmse: 0.59805 | val_1_rmse: 0.60016 |  0:19:05s
epoch 134| loss: 0.31159 | val_0_rmse: 0.66268 | val_1_rmse: 0.66343 |  0:19:14s
epoch 135| loss: 0.31103 | val_0_rmse: 0.61802 | val_1_rmse: 0.6193  |  0:19:23s
epoch 136| loss: 0.31124 | val_0_rmse: 0.63055 | val_1_rmse: 0.63122 |  0:19:32s
epoch 137| loss: 0.3095  | val_0_rmse: 0.58693 | val_1_rmse: 0.58683 |  0:19:41s
epoch 138| loss: 0.31106 | val_0_rmse: 0.58738 | val_1_rmse: 0.58947 |  0:19:50s
epoch 139| loss: 0.30914 | val_0_rmse: 0.63723 | val_1_rmse: 0.63767 |  0:19:59s
epoch 140| loss: 0.31193 | val_0_rmse: 0.5994  | val_1_rmse: 0.60024 |  0:20:08s
epoch 141| loss: 0.31017 | val_0_rmse: 0.60921 | val_1_rmse: 0.60635 |  0:20:17s

Early stopping occured at epoch 141 with best_epoch = 111 and best_val_1_rmse = 0.55028
Best weights from best epoch are automatically used!
ended training at: 05:24:41
Feature importance:
Mean squared error is of 2086218678.669193
Mean absolute error:32901.95126676029
MAPE:0.3096966771531139
R2 score:0.6895759877251693
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:24:43
epoch 0  | loss: 0.71035 | val_0_rmse: 0.71776 | val_1_rmse: 0.71808 |  0:00:09s
epoch 1  | loss: 0.45767 | val_0_rmse: 0.69161 | val_1_rmse: 0.69189 |  0:00:18s
epoch 2  | loss: 0.44589 | val_0_rmse: 0.68099 | val_1_rmse: 0.68037 |  0:00:27s
epoch 3  | loss: 0.43312 | val_0_rmse: 0.65466 | val_1_rmse: 0.65266 |  0:00:36s
epoch 4  | loss: 0.43118 | val_0_rmse: 0.64552 | val_1_rmse: 0.64289 |  0:00:45s
epoch 5  | loss: 0.42686 | val_0_rmse: 0.65064 | val_1_rmse: 0.64853 |  0:00:54s
epoch 6  | loss: 0.42998 | val_0_rmse: 0.65033 | val_1_rmse: 0.64839 |  0:01:03s
epoch 7  | loss: 0.42089 | val_0_rmse: 0.64026 | val_1_rmse: 0.63996 |  0:01:13s
epoch 8  | loss: 0.41682 | val_0_rmse: 0.63476 | val_1_rmse: 0.63533 |  0:01:22s
epoch 9  | loss: 0.41151 | val_0_rmse: 0.70945 | val_1_rmse: 0.71161 |  0:01:31s
epoch 10 | loss: 0.40868 | val_0_rmse: 0.64207 | val_1_rmse: 0.63282 |  0:01:40s
epoch 11 | loss: 0.39652 | val_0_rmse: 0.62169 | val_1_rmse: 0.6228  |  0:01:49s
epoch 12 | loss: 0.38773 | val_0_rmse: 0.61577 | val_1_rmse: 0.61758 |  0:01:58s
epoch 13 | loss: 0.38423 | val_0_rmse: 0.62304 | val_1_rmse: 0.62312 |  0:02:07s
epoch 14 | loss: 0.37656 | val_0_rmse: 0.63583 | val_1_rmse: 0.62362 |  0:02:16s
epoch 15 | loss: 0.36392 | val_0_rmse: 0.88721 | val_1_rmse: 0.88963 |  0:02:25s
epoch 16 | loss: 0.38195 | val_0_rmse: 0.66071 | val_1_rmse: 0.66532 |  0:02:34s
epoch 17 | loss: 0.37533 | val_0_rmse: 0.6377  | val_1_rmse: 0.6373  |  0:02:43s
epoch 18 | loss: 0.35685 | val_0_rmse: 0.60738 | val_1_rmse: 0.60729 |  0:02:52s
epoch 19 | loss: 0.34937 | val_0_rmse: 0.6149  | val_1_rmse: 0.61433 |  0:03:01s
epoch 20 | loss: 0.34874 | val_0_rmse: 0.60848 | val_1_rmse: 0.61254 |  0:03:11s
epoch 21 | loss: 0.34495 | val_0_rmse: 0.6067  | val_1_rmse: 0.6078  |  0:03:20s
epoch 22 | loss: 0.34043 | val_0_rmse: 0.6141  | val_1_rmse: 0.61719 |  0:03:29s
epoch 23 | loss: 0.34272 | val_0_rmse: 0.77912 | val_1_rmse: 0.778   |  0:03:38s
epoch 24 | loss: 0.34038 | val_0_rmse: 0.66763 | val_1_rmse: 0.66699 |  0:03:47s
epoch 25 | loss: 0.337   | val_0_rmse: 0.61742 | val_1_rmse: 0.61949 |  0:03:56s
epoch 26 | loss: 0.3375  | val_0_rmse: 0.58828 | val_1_rmse: 0.5922  |  0:04:05s
epoch 27 | loss: 0.33778 | val_0_rmse: 0.58857 | val_1_rmse: 0.58914 |  0:04:14s
epoch 28 | loss: 0.33825 | val_0_rmse: 0.57896 | val_1_rmse: 0.58441 |  0:04:24s
epoch 29 | loss: 0.3381  | val_0_rmse: 0.57826 | val_1_rmse: 0.58268 |  0:04:33s
epoch 30 | loss: 0.33702 | val_0_rmse: 0.6272  | val_1_rmse: 0.62815 |  0:04:42s
epoch 31 | loss: 0.33458 | val_0_rmse: 0.57715 | val_1_rmse: 0.57919 |  0:04:51s
epoch 32 | loss: 0.33497 | val_0_rmse: 0.61544 | val_1_rmse: 0.61786 |  0:05:00s
epoch 33 | loss: 0.33282 | val_0_rmse: 0.63454 | val_1_rmse: 0.63449 |  0:05:09s
epoch 34 | loss: 0.32934 | val_0_rmse: 0.57001 | val_1_rmse: 0.57392 |  0:05:18s
epoch 35 | loss: 0.33965 | val_0_rmse: 0.67209 | val_1_rmse: 0.67177 |  0:05:27s
epoch 36 | loss: 0.34194 | val_0_rmse: 0.57042 | val_1_rmse: 0.5738  |  0:05:36s
epoch 37 | loss: 0.33152 | val_0_rmse: 0.63082 | val_1_rmse: 0.62997 |  0:05:45s
epoch 38 | loss: 0.33225 | val_0_rmse: 0.58734 | val_1_rmse: 0.58826 |  0:05:55s
epoch 39 | loss: 0.33007 | val_0_rmse: 0.5978  | val_1_rmse: 0.59569 |  0:06:04s
epoch 40 | loss: 0.32688 | val_0_rmse: 0.58585 | val_1_rmse: 0.58963 |  0:06:13s
epoch 41 | loss: 0.32757 | val_0_rmse: 0.57638 | val_1_rmse: 0.57894 |  0:06:22s
epoch 42 | loss: 0.3273  | val_0_rmse: 0.5951  | val_1_rmse: 0.58848 |  0:06:31s
epoch 43 | loss: 0.32665 | val_0_rmse: 0.65504 | val_1_rmse: 0.64819 |  0:06:40s
epoch 44 | loss: 0.33449 | val_0_rmse: 0.61739 | val_1_rmse: 0.61771 |  0:06:49s
epoch 45 | loss: 0.32668 | val_0_rmse: 0.57365 | val_1_rmse: 0.5783  |  0:06:58s
epoch 46 | loss: 0.32378 | val_0_rmse: 0.64307 | val_1_rmse: 0.64802 |  0:07:07s
epoch 47 | loss: 0.32314 | val_0_rmse: 0.5875  | val_1_rmse: 0.59131 |  0:07:17s
epoch 48 | loss: 0.32532 | val_0_rmse: 0.65988 | val_1_rmse: 0.64701 |  0:07:26s
epoch 49 | loss: 0.32318 | val_0_rmse: 0.57252 | val_1_rmse: 0.57531 |  0:07:35s
epoch 50 | loss: 0.32026 | val_0_rmse: 0.57925 | val_1_rmse: 0.58339 |  0:07:44s
epoch 51 | loss: 0.31987 | val_0_rmse: 0.61122 | val_1_rmse: 0.61483 |  0:07:53s
epoch 52 | loss: 0.31957 | val_0_rmse: 0.58605 | val_1_rmse: 0.58978 |  0:08:02s
epoch 53 | loss: 0.31836 | val_0_rmse: 0.66479 | val_1_rmse: 0.66405 |  0:08:11s
epoch 54 | loss: 0.31845 | val_0_rmse: 0.70054 | val_1_rmse: 0.70177 |  0:08:21s
epoch 55 | loss: 0.31709 | val_0_rmse: 0.58656 | val_1_rmse: 0.58839 |  0:08:30s
epoch 56 | loss: 0.31636 | val_0_rmse: 0.5589  | val_1_rmse: 0.56349 |  0:08:39s
epoch 57 | loss: 0.31589 | val_0_rmse: 0.59545 | val_1_rmse: 0.59796 |  0:08:49s
epoch 58 | loss: 0.31522 | val_0_rmse: 0.6061  | val_1_rmse: 0.60811 |  0:08:59s
epoch 59 | loss: 0.31476 | val_0_rmse: 0.63375 | val_1_rmse: 0.64013 |  0:09:08s
epoch 60 | loss: 0.31503 | val_0_rmse: 0.56273 | val_1_rmse: 0.66411 |  0:09:17s
epoch 61 | loss: 0.3124  | val_0_rmse: 0.55887 | val_1_rmse: 0.62488 |  0:09:26s
epoch 62 | loss: 0.31103 | val_0_rmse: 0.57732 | val_1_rmse: 0.58117 |  0:09:35s
epoch 63 | loss: 0.31197 | val_0_rmse: 0.59664 | val_1_rmse: 0.59955 |  0:09:44s
epoch 64 | loss: 0.31439 | val_0_rmse: 0.72981 | val_1_rmse: 0.73267 |  0:09:54s
epoch 65 | loss: 0.31099 | val_0_rmse: 0.60387 | val_1_rmse: 0.61071 |  0:10:03s
epoch 66 | loss: 0.30909 | val_0_rmse: 0.55326 | val_1_rmse: 0.5596  |  0:10:12s
epoch 67 | loss: 0.30956 | val_0_rmse: 0.6299  | val_1_rmse: 0.63372 |  0:10:21s
epoch 68 | loss: 0.31035 | val_0_rmse: 0.56708 | val_1_rmse: 0.5721  |  0:10:30s
epoch 69 | loss: 0.30757 | val_0_rmse: 0.5837  | val_1_rmse: 0.58646 |  0:10:39s
epoch 70 | loss: 0.30948 | val_0_rmse: 0.58748 | val_1_rmse: 0.59078 |  0:10:49s
epoch 71 | loss: 0.3081  | val_0_rmse: 0.57532 | val_1_rmse: 0.57983 |  0:10:58s
epoch 72 | loss: 0.31051 | val_0_rmse: 0.65189 | val_1_rmse: 0.65288 |  0:11:07s
epoch 73 | loss: 0.30743 | val_0_rmse: 0.61898 | val_1_rmse: 0.62428 |  0:11:16s
epoch 74 | loss: 0.3072  | val_0_rmse: 0.61758 | val_1_rmse: 0.61945 |  0:11:25s
epoch 75 | loss: 0.30541 | val_0_rmse: 0.598   | val_1_rmse: 0.60373 |  0:11:34s
epoch 76 | loss: 0.30419 | val_0_rmse: 0.63149 | val_1_rmse: 0.63608 |  0:11:43s
epoch 77 | loss: 0.30513 | val_0_rmse: 0.62098 | val_1_rmse: 0.62144 |  0:11:52s
epoch 78 | loss: 0.30529 | val_0_rmse: 0.59098 | val_1_rmse: 0.59513 |  0:12:02s
epoch 79 | loss: 0.3043  | val_0_rmse: 0.57245 | val_1_rmse: 0.57674 |  0:12:12s
epoch 80 | loss: 0.30677 | val_0_rmse: 0.57334 | val_1_rmse: 0.58066 |  0:12:22s
epoch 81 | loss: 0.30554 | val_0_rmse: 0.64161 | val_1_rmse: 0.64442 |  0:12:31s
epoch 82 | loss: 0.30496 | val_0_rmse: 0.57164 | val_1_rmse: 0.57467 |  0:12:40s
epoch 83 | loss: 0.30318 | val_0_rmse: 0.54363 | val_1_rmse: 0.55003 |  0:12:50s
epoch 84 | loss: 0.3026  | val_0_rmse: 0.57258 | val_1_rmse: 0.57719 |  0:12:59s
epoch 85 | loss: 0.30492 | val_0_rmse: 0.60143 | val_1_rmse: 0.60371 |  0:13:08s
epoch 86 | loss: 0.30358 | val_0_rmse: 0.65846 | val_1_rmse: 0.66178 |  0:13:17s
epoch 87 | loss: 0.30422 | val_0_rmse: 0.60235 | val_1_rmse: 0.60327 |  0:13:26s
epoch 88 | loss: 0.30343 | val_0_rmse: 0.562   | val_1_rmse: 0.56755 |  0:13:35s
epoch 89 | loss: 0.30241 | val_0_rmse: 0.63232 | val_1_rmse: 0.63499 |  0:13:44s
epoch 90 | loss: 0.30138 | val_0_rmse: 0.68974 | val_1_rmse: 0.68955 |  0:13:53s
epoch 91 | loss: 0.30029 | val_0_rmse: 0.60658 | val_1_rmse: 0.60908 |  0:14:02s
epoch 92 | loss: 0.30296 | val_0_rmse: 0.69988 | val_1_rmse: 0.69638 |  0:14:11s
epoch 93 | loss: 0.30195 | val_0_rmse: 0.57511 | val_1_rmse: 0.57811 |  0:14:21s
epoch 94 | loss: 0.29854 | val_0_rmse: 0.58776 | val_1_rmse: 0.58043 |  0:14:30s
epoch 95 | loss: 0.29991 | val_0_rmse: 0.64992 | val_1_rmse: 0.65468 |  0:14:39s
epoch 96 | loss: 0.29943 | val_0_rmse: 0.60053 | val_1_rmse: 0.60773 |  0:14:48s
epoch 97 | loss: 0.29917 | val_0_rmse: 0.63218 | val_1_rmse: 0.63599 |  0:14:57s
epoch 98 | loss: 0.29886 | val_0_rmse: 0.61359 | val_1_rmse: 0.61206 |  0:15:06s
epoch 99 | loss: 0.30046 | val_0_rmse: 0.61064 | val_1_rmse: 0.61357 |  0:15:15s
epoch 100| loss: 0.29799 | val_0_rmse: 0.60193 | val_1_rmse: 0.60565 |  0:15:24s
epoch 101| loss: 0.30032 | val_0_rmse: 0.56669 | val_1_rmse: 0.5746  |  0:15:34s
epoch 102| loss: 0.29775 | val_0_rmse: 0.63456 | val_1_rmse: 0.63729 |  0:15:43s
epoch 103| loss: 0.29942 | val_0_rmse: 0.56665 | val_1_rmse: 0.57157 |  0:15:52s
epoch 104| loss: 0.3002  | val_0_rmse: 0.58503 | val_1_rmse: 0.59241 |  0:16:01s
epoch 105| loss: 0.29851 | val_0_rmse: 0.55189 | val_1_rmse: 0.56023 |  0:16:10s
epoch 106| loss: 0.29846 | val_0_rmse: 0.54916 | val_1_rmse: 0.55838 |  0:16:19s
epoch 107| loss: 0.2991  | val_0_rmse: 0.61804 | val_1_rmse: 0.62184 |  0:16:28s
epoch 108| loss: 0.30019 | val_0_rmse: 0.64228 | val_1_rmse: 0.64301 |  0:16:37s
epoch 109| loss: 0.29777 | val_0_rmse: 0.55909 | val_1_rmse: 0.56717 |  0:16:46s
epoch 110| loss: 0.29839 | val_0_rmse: 0.53998 | val_1_rmse: 0.55325 |  0:16:56s
epoch 111| loss: 0.29605 | val_0_rmse: 0.67343 | val_1_rmse: 0.67581 |  0:17:05s
epoch 112| loss: 0.29491 | val_0_rmse: 0.65592 | val_1_rmse: 0.65859 |  0:17:14s
epoch 113| loss: 0.2972  | val_0_rmse: 0.61543 | val_1_rmse: 0.62103 |  0:17:23s

Early stopping occured at epoch 113 with best_epoch = 83 and best_val_1_rmse = 0.55003
Best weights from best epoch are automatically used!
ended training at: 05:42:09
Feature importance:
Mean squared error is of 2231057029.569388
Mean absolute error:33603.544416907265
MAPE:0.3207325907254046
R2 score:0.6748089672205573
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:42:10
epoch 0  | loss: 0.63887 | val_0_rmse: 0.72401 | val_1_rmse: 0.72106 |  0:00:09s
epoch 1  | loss: 0.4362  | val_0_rmse: 0.68751 | val_1_rmse: 0.6844  |  0:00:18s
epoch 2  | loss: 0.4048  | val_0_rmse: 0.64789 | val_1_rmse: 0.64444 |  0:00:27s
epoch 3  | loss: 0.38297 | val_0_rmse: 0.76928 | val_1_rmse: 0.76823 |  0:00:36s
epoch 4  | loss: 0.36668 | val_0_rmse: 0.61181 | val_1_rmse: 0.60987 |  0:00:45s
epoch 5  | loss: 0.37141 | val_0_rmse: 0.619   | val_1_rmse: 0.62092 |  0:00:54s
epoch 6  | loss: 0.36761 | val_0_rmse: 0.70906 | val_1_rmse: 0.70922 |  0:01:03s
epoch 7  | loss: 0.36564 | val_0_rmse: 0.6129  | val_1_rmse: 0.60957 |  0:01:12s
epoch 8  | loss: 0.36535 | val_0_rmse: 0.69087 | val_1_rmse: 0.69215 |  0:01:21s
epoch 9  | loss: 0.36095 | val_0_rmse: 0.68857 | val_1_rmse: 0.68623 |  0:01:30s
epoch 10 | loss: 0.35805 | val_0_rmse: 0.62291 | val_1_rmse: 0.62318 |  0:01:39s
epoch 11 | loss: 0.3588  | val_0_rmse: 0.665   | val_1_rmse: 0.66502 |  0:01:48s
epoch 12 | loss: 0.37106 | val_0_rmse: 0.62398 | val_1_rmse: 0.62428 |  0:01:57s
epoch 13 | loss: 0.38381 | val_0_rmse: 0.80574 | val_1_rmse: 0.80078 |  0:02:06s
epoch 14 | loss: 0.36385 | val_0_rmse: 1.10902 | val_1_rmse: 1.10549 |  0:02:15s
epoch 15 | loss: 0.36401 | val_0_rmse: 0.59388 | val_1_rmse: 0.59173 |  0:02:25s
epoch 16 | loss: 0.35519 | val_0_rmse: 0.73092 | val_1_rmse: 0.72877 |  0:02:34s
epoch 17 | loss: 0.35354 | val_0_rmse: 0.60504 | val_1_rmse: 0.60333 |  0:02:43s
epoch 18 | loss: 0.35416 | val_0_rmse: 0.59465 | val_1_rmse: 0.59378 |  0:02:52s
epoch 19 | loss: 0.35131 | val_0_rmse: 0.58739 | val_1_rmse: 0.58594 |  0:03:01s
epoch 20 | loss: 0.35392 | val_0_rmse: 0.60323 | val_1_rmse: 0.60222 |  0:03:10s
epoch 21 | loss: 0.35157 | val_0_rmse: 0.5863  | val_1_rmse: 0.58539 |  0:03:19s
epoch 22 | loss: 0.34983 | val_0_rmse: 0.58425 | val_1_rmse: 0.5817  |  0:03:28s
epoch 23 | loss: 0.34886 | val_0_rmse: 0.65184 | val_1_rmse: 0.652   |  0:03:37s
epoch 24 | loss: 0.35145 | val_0_rmse: 0.59156 | val_1_rmse: 0.58916 |  0:03:46s
epoch 25 | loss: 0.34671 | val_0_rmse: 0.60105 | val_1_rmse: 0.5986  |  0:03:55s
epoch 26 | loss: 0.34732 | val_0_rmse: 0.5947  | val_1_rmse: 0.59521 |  0:04:05s
epoch 27 | loss: 0.34562 | val_0_rmse: 0.62902 | val_1_rmse: 0.62846 |  0:04:14s
epoch 28 | loss: 0.34981 | val_0_rmse: 0.60356 | val_1_rmse: 0.60407 |  0:04:23s
epoch 29 | loss: 0.34663 | val_0_rmse: 0.70045 | val_1_rmse: 0.70048 |  0:04:32s
epoch 30 | loss: 0.34644 | val_0_rmse: 0.58707 | val_1_rmse: 0.58581 |  0:04:41s
epoch 31 | loss: 0.3446  | val_0_rmse: 0.57836 | val_1_rmse: 0.57714 |  0:04:50s
epoch 32 | loss: 0.34398 | val_0_rmse: 0.58499 | val_1_rmse: 0.5842  |  0:04:59s
epoch 33 | loss: 0.34221 | val_0_rmse: 0.65445 | val_1_rmse: 0.65249 |  0:05:08s
epoch 34 | loss: 0.34156 | val_0_rmse: 0.57534 | val_1_rmse: 0.57537 |  0:05:18s
epoch 35 | loss: 0.34251 | val_0_rmse: 0.78751 | val_1_rmse: 0.78478 |  0:05:27s
epoch 36 | loss: 0.33875 | val_0_rmse: 0.6464  | val_1_rmse: 0.64295 |  0:05:36s
epoch 37 | loss: 0.33683 | val_0_rmse: 0.65364 | val_1_rmse: 0.65228 |  0:05:45s
epoch 38 | loss: 0.33731 | val_0_rmse: 0.62523 | val_1_rmse: 0.62268 |  0:05:54s
epoch 39 | loss: 0.3384  | val_0_rmse: 0.57838 | val_1_rmse: 0.57754 |  0:06:03s
epoch 40 | loss: 0.33461 | val_0_rmse: 0.61762 | val_1_rmse: 0.61799 |  0:06:12s
epoch 41 | loss: 0.34117 | val_0_rmse: 0.66158 | val_1_rmse: 0.65897 |  0:06:21s
epoch 42 | loss: 0.33709 | val_0_rmse: 0.58122 | val_1_rmse: 0.57925 |  0:06:30s
epoch 43 | loss: 0.33569 | val_0_rmse: 0.60213 | val_1_rmse: 0.59865 |  0:06:39s
epoch 44 | loss: 0.33794 | val_0_rmse: 0.82753 | val_1_rmse: 0.82847 |  0:06:48s
epoch 45 | loss: 0.34088 | val_0_rmse: 0.6593  | val_1_rmse: 0.65562 |  0:06:57s
epoch 46 | loss: 0.34328 | val_0_rmse: 0.61321 | val_1_rmse: 0.6108  |  0:07:06s
epoch 47 | loss: 0.33816 | val_0_rmse: 0.58837 | val_1_rmse: 0.58677 |  0:07:15s
epoch 48 | loss: 0.3366  | val_0_rmse: 0.60798 | val_1_rmse: 0.60563 |  0:07:24s
epoch 49 | loss: 0.33958 | val_0_rmse: 0.63114 | val_1_rmse: 0.71215 |  0:07:33s
epoch 50 | loss: 0.33521 | val_0_rmse: 0.58087 | val_1_rmse: 0.57993 |  0:07:42s
epoch 51 | loss: 0.33318 | val_0_rmse: 0.86529 | val_1_rmse: 0.86176 |  0:07:51s
epoch 52 | loss: 0.33474 | val_0_rmse: 0.61572 | val_1_rmse: 0.61233 |  0:08:00s
epoch 53 | loss: 0.33386 | val_0_rmse: 0.64513 | val_1_rmse: 0.64452 |  0:08:09s
epoch 54 | loss: 0.33236 | val_0_rmse: 0.69181 | val_1_rmse: 0.6889  |  0:08:18s
epoch 55 | loss: 0.33338 | val_0_rmse: 0.76069 | val_1_rmse: 0.76317 |  0:08:27s
epoch 56 | loss: 0.32995 | val_0_rmse: 0.69073 | val_1_rmse: 0.6893  |  0:08:36s
epoch 57 | loss: 0.32984 | val_0_rmse: 0.62473 | val_1_rmse: 0.62473 |  0:08:46s
epoch 58 | loss: 0.32843 | val_0_rmse: 0.56363 | val_1_rmse: 0.56398 |  0:08:55s
epoch 59 | loss: 0.33285 | val_0_rmse: 0.66698 | val_1_rmse: 0.66614 |  0:09:04s
epoch 60 | loss: 0.33361 | val_0_rmse: 0.61133 | val_1_rmse: 0.60963 |  0:09:13s
epoch 61 | loss: 0.32795 | val_0_rmse: 0.59415 | val_1_rmse: 0.59285 |  0:09:22s
epoch 62 | loss: 0.32929 | val_0_rmse: 0.67251 | val_1_rmse: 0.66995 |  0:09:31s
epoch 63 | loss: 0.32645 | val_0_rmse: 0.60664 | val_1_rmse: 0.60889 |  0:09:40s
epoch 64 | loss: 0.33195 | val_0_rmse: 0.6711  | val_1_rmse: 0.6677  |  0:09:49s
epoch 65 | loss: 0.32986 | val_0_rmse: 0.65455 | val_1_rmse: 0.65182 |  0:09:58s
epoch 66 | loss: 0.32933 | val_0_rmse: 0.60927 | val_1_rmse: 0.60887 |  0:10:07s
epoch 67 | loss: 0.32733 | val_0_rmse: 1.28027 | val_1_rmse: 1.29132 |  0:10:16s
epoch 68 | loss: 0.32955 | val_0_rmse: 0.6209  | val_1_rmse: 0.6185  |  0:10:25s
epoch 69 | loss: 0.33121 | val_0_rmse: 0.60636 | val_1_rmse: 0.60356 |  0:10:34s
epoch 70 | loss: 0.32574 | val_0_rmse: 0.69173 | val_1_rmse: 0.69199 |  0:10:43s
epoch 71 | loss: 0.32683 | val_0_rmse: 0.59051 | val_1_rmse: 0.58854 |  0:10:52s
epoch 72 | loss: 0.32455 | val_0_rmse: 0.60408 | val_1_rmse: 0.60318 |  0:11:01s
epoch 73 | loss: 0.32581 | val_0_rmse: 0.59457 | val_1_rmse: 0.59218 |  0:11:11s
epoch 74 | loss: 0.32575 | val_0_rmse: 0.67176 | val_1_rmse: 0.6718  |  0:11:20s
epoch 75 | loss: 0.32431 | val_0_rmse: 0.67917 | val_1_rmse: 0.89531 |  0:11:29s
epoch 76 | loss: 0.32528 | val_0_rmse: 0.66513 | val_1_rmse: 0.6624  |  0:11:38s
epoch 77 | loss: 0.32641 | val_0_rmse: 0.58551 | val_1_rmse: 0.5842  |  0:11:47s
epoch 78 | loss: 0.33063 | val_0_rmse: 1.13244 | val_1_rmse: 1.12598 |  0:11:56s
epoch 79 | loss: 0.32636 | val_0_rmse: 0.67355 | val_1_rmse: 0.67257 |  0:12:05s
epoch 80 | loss: 0.3265  | val_0_rmse: 0.639   | val_1_rmse: 0.63642 |  0:12:14s
epoch 81 | loss: 0.3244  | val_0_rmse: 0.64513 | val_1_rmse: 0.64378 |  0:12:23s
epoch 82 | loss: 0.32281 | val_0_rmse: 0.60095 | val_1_rmse: 0.59998 |  0:12:32s
epoch 83 | loss: 0.32408 | val_0_rmse: 0.60195 | val_1_rmse: 0.59953 |  0:12:41s
epoch 84 | loss: 0.32409 | val_0_rmse: 0.84691 | val_1_rmse: 0.85793 |  0:12:51s
epoch 85 | loss: 0.3251  | val_0_rmse: 0.77408 | val_1_rmse: 0.77309 |  0:13:00s
epoch 86 | loss: 0.3255  | val_0_rmse: 0.58797 | val_1_rmse: 0.58561 |  0:13:09s
epoch 87 | loss: 0.32323 | val_0_rmse: 0.57835 | val_1_rmse: 0.57738 |  0:13:18s
epoch 88 | loss: 0.32674 | val_0_rmse: 0.66121 | val_1_rmse: 0.65864 |  0:13:27s

Early stopping occured at epoch 88 with best_epoch = 58 and best_val_1_rmse = 0.56398
Best weights from best epoch are automatically used!
ended training at: 05:55:41
Feature importance:
Mean squared error is of 2177867686.419081
Mean absolute error:33446.84637744661
MAPE:0.3259832719436786
R2 score:0.6833151612860232
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:55:41
epoch 0  | loss: 0.6239  | val_0_rmse: 0.72437 | val_1_rmse: 0.73145 |  0:00:09s
epoch 1  | loss: 0.44498 | val_0_rmse: 0.69708 | val_1_rmse: 0.70297 |  0:00:18s
epoch 2  | loss: 0.4355  | val_0_rmse: 0.67446 | val_1_rmse: 0.68031 |  0:00:27s
epoch 3  | loss: 0.42564 | val_0_rmse: 0.65891 | val_1_rmse: 0.666   |  0:00:36s
epoch 4  | loss: 0.42185 | val_0_rmse: 0.65236 | val_1_rmse: 0.66317 |  0:00:45s
epoch 5  | loss: 0.41596 | val_0_rmse: 0.63033 | val_1_rmse: 0.63765 |  0:00:54s
epoch 6  | loss: 0.41171 | val_0_rmse: 0.62981 | val_1_rmse: 0.63737 |  0:01:03s
epoch 7  | loss: 0.40202 | val_0_rmse: 0.66257 | val_1_rmse: 0.67294 |  0:01:12s
epoch 8  | loss: 0.38622 | val_0_rmse: 0.60685 | val_1_rmse: 0.61782 |  0:01:22s
epoch 9  | loss: 0.37285 | val_0_rmse: 0.59944 | val_1_rmse: 0.60891 |  0:01:31s
epoch 10 | loss: 0.3619  | val_0_rmse: 0.59378 | val_1_rmse: 0.60067 |  0:01:40s
epoch 11 | loss: 0.35174 | val_0_rmse: 0.5975  | val_1_rmse: 0.60498 |  0:01:49s
epoch 12 | loss: 0.34757 | val_0_rmse: 0.62656 | val_1_rmse: 0.638   |  0:01:58s
epoch 13 | loss: 0.34739 | val_0_rmse: 0.59923 | val_1_rmse: 0.61048 |  0:02:07s
epoch 14 | loss: 0.34689 | val_0_rmse: 0.58804 | val_1_rmse: 0.59728 |  0:02:16s
epoch 15 | loss: 0.34706 | val_0_rmse: 0.77072 | val_1_rmse: 0.77362 |  0:02:25s
epoch 16 | loss: 0.34647 | val_0_rmse: 0.59804 | val_1_rmse: 0.60935 |  0:02:35s
epoch 17 | loss: 0.34246 | val_0_rmse: 0.57379 | val_1_rmse: 0.582   |  0:02:44s
epoch 18 | loss: 0.34213 | val_0_rmse: 0.60716 | val_1_rmse: 0.61307 |  0:02:53s
epoch 19 | loss: 0.3387  | val_0_rmse: 0.60343 | val_1_rmse: 0.6146  |  0:03:02s
epoch 20 | loss: 0.33785 | val_0_rmse: 0.57768 | val_1_rmse: 0.58679 |  0:03:11s
epoch 21 | loss: 0.33545 | val_0_rmse: 0.57492 | val_1_rmse: 0.58289 |  0:03:20s
epoch 22 | loss: 0.33595 | val_0_rmse: 0.58475 | val_1_rmse: 0.59327 |  0:03:29s
epoch 23 | loss: 0.33925 | val_0_rmse: 0.95763 | val_1_rmse: 0.95693 |  0:03:38s
epoch 24 | loss: 0.33721 | val_0_rmse: 0.62462 | val_1_rmse: 0.6367  |  0:03:47s
epoch 25 | loss: 0.33474 | val_0_rmse: 0.68152 | val_1_rmse: 0.68615 |  0:03:56s
epoch 26 | loss: 0.33328 | val_0_rmse: 0.74587 | val_1_rmse: 0.76278 |  0:04:06s
epoch 27 | loss: 0.33319 | val_0_rmse: 0.6742  | val_1_rmse: 0.68429 |  0:04:15s
epoch 28 | loss: 0.33553 | val_0_rmse: 0.59224 | val_1_rmse: 0.60158 |  0:04:24s
epoch 29 | loss: 0.34136 | val_0_rmse: 0.58885 | val_1_rmse: 0.58434 |  0:04:33s
epoch 30 | loss: 0.33543 | val_0_rmse: 0.57921 | val_1_rmse: 0.58742 |  0:04:42s
epoch 31 | loss: 0.33394 | val_0_rmse: 0.69834 | val_1_rmse: 0.66463 |  0:04:51s
epoch 32 | loss: 0.33323 | val_0_rmse: 0.67722 | val_1_rmse: 0.6728  |  0:05:00s
epoch 33 | loss: 0.33441 | val_0_rmse: 0.62151 | val_1_rmse: 0.62504 |  0:05:10s
epoch 34 | loss: 0.33122 | val_0_rmse: 0.58515 | val_1_rmse: 0.59623 |  0:05:19s
epoch 35 | loss: 0.32891 | val_0_rmse: 0.74596 | val_1_rmse: 0.74739 |  0:05:28s
epoch 36 | loss: 0.33296 | val_0_rmse: 0.77395 | val_1_rmse: 0.77707 |  0:05:37s
epoch 37 | loss: 0.32837 | val_0_rmse: 0.6034  | val_1_rmse: 0.6154  |  0:05:46s
epoch 38 | loss: 0.33053 | val_0_rmse: 0.68131 | val_1_rmse: 0.68526 |  0:05:55s
epoch 39 | loss: 0.32929 | val_0_rmse: 0.61194 | val_1_rmse: 0.62362 |  0:06:05s
epoch 40 | loss: 0.33009 | val_0_rmse: 0.6103  | val_1_rmse: 0.61863 |  0:06:14s
epoch 41 | loss: 0.32688 | val_0_rmse: 0.60398 | val_1_rmse: 0.60621 |  0:06:23s
epoch 42 | loss: 0.32657 | val_0_rmse: 0.63764 | val_1_rmse: 0.64347 |  0:06:32s
epoch 43 | loss: 0.32814 | val_0_rmse: 0.66519 | val_1_rmse: 0.64673 |  0:06:41s
epoch 44 | loss: 0.32601 | val_0_rmse: 0.66834 | val_1_rmse: 0.67185 |  0:06:50s
epoch 45 | loss: 0.32654 | val_0_rmse: 0.66949 | val_1_rmse: 0.68418 |  0:06:59s
epoch 46 | loss: 0.32614 | val_0_rmse: 0.62672 | val_1_rmse: 0.60204 |  0:07:08s
epoch 47 | loss: 0.32452 | val_0_rmse: 0.8139  | val_1_rmse: 0.80318 |  0:07:17s

Early stopping occured at epoch 47 with best_epoch = 17 and best_val_1_rmse = 0.582
Best weights from best epoch are automatically used!
ended training at: 06:03:02
Feature importance:
Mean squared error is of 2295018902.456206
Mean absolute error:34825.4455184685
MAPE:0.33976927533175666
R2 score:0.668520239534206
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:03:03
epoch 0  | loss: 0.60787 | val_0_rmse: 0.69992 | val_1_rmse: 0.70183 |  0:00:09s
epoch 1  | loss: 0.44766 | val_0_rmse: 0.67522 | val_1_rmse: 0.67544 |  0:00:18s
epoch 2  | loss: 0.43093 | val_0_rmse: 0.65412 | val_1_rmse: 0.65599 |  0:00:27s
epoch 3  | loss: 0.42094 | val_0_rmse: 0.64743 | val_1_rmse: 0.64967 |  0:00:36s
epoch 4  | loss: 0.41333 | val_0_rmse: 0.64595 | val_1_rmse: 0.65154 |  0:00:45s
epoch 5  | loss: 0.41201 | val_0_rmse: 0.64305 | val_1_rmse: 0.64398 |  0:00:54s
epoch 6  | loss: 0.39725 | val_0_rmse: 0.69638 | val_1_rmse: 0.70813 |  0:01:03s
epoch 7  | loss: 0.40573 | val_0_rmse: 0.66181 | val_1_rmse: 0.66747 |  0:01:12s
epoch 8  | loss: 0.37902 | val_0_rmse: 0.66611 | val_1_rmse: 0.66323 |  0:01:21s
epoch 9  | loss: 0.37704 | val_0_rmse: 0.62931 | val_1_rmse: 0.6306  |  0:01:30s
epoch 10 | loss: 0.37216 | val_0_rmse: 0.6314  | val_1_rmse: 0.63209 |  0:01:39s
epoch 11 | loss: 0.36569 | val_0_rmse: 0.64216 | val_1_rmse: 0.64025 |  0:01:48s
epoch 12 | loss: 0.36908 | val_0_rmse: 0.66222 | val_1_rmse: 0.6667  |  0:01:58s
epoch 13 | loss: 0.36968 | val_0_rmse: 0.64307 | val_1_rmse: 0.64184 |  0:02:07s
epoch 14 | loss: 0.36646 | val_0_rmse: 0.64177 | val_1_rmse: 0.64532 |  0:02:15s
epoch 15 | loss: 0.3641  | val_0_rmse: 0.58598 | val_1_rmse: 0.589   |  0:02:24s
epoch 16 | loss: 0.36087 | val_0_rmse: 0.60095 | val_1_rmse: 0.60403 |  0:02:33s
epoch 17 | loss: 0.36239 | val_0_rmse: 0.61876 | val_1_rmse: 0.62285 |  0:02:42s
epoch 18 | loss: 0.35951 | val_0_rmse: 0.61536 | val_1_rmse: 0.6162  |  0:02:50s
epoch 19 | loss: 0.35481 | val_0_rmse: 0.58618 | val_1_rmse: 0.58767 |  0:02:59s
epoch 20 | loss: 0.35237 | val_0_rmse: 0.58961 | val_1_rmse: 0.59798 |  0:03:08s
epoch 21 | loss: 0.35314 | val_0_rmse: 0.59074 | val_1_rmse: 0.5941  |  0:03:16s
epoch 22 | loss: 0.35021 | val_0_rmse: 0.59187 | val_1_rmse: 0.59366 |  0:03:25s
epoch 23 | loss: 0.34814 | val_0_rmse: 0.59369 | val_1_rmse: 0.59462 |  0:03:34s
epoch 24 | loss: 0.34754 | val_0_rmse: 0.60886 | val_1_rmse: 0.60876 |  0:03:42s
epoch 25 | loss: 0.34508 | val_0_rmse: 0.59529 | val_1_rmse: 0.59925 |  0:03:51s
epoch 26 | loss: 0.343   | val_0_rmse: 0.62278 | val_1_rmse: 0.62249 |  0:04:00s
epoch 27 | loss: 0.3409  | val_0_rmse: 0.59646 | val_1_rmse: 0.59833 |  0:04:08s
epoch 28 | loss: 0.34101 | val_0_rmse: 0.57973 | val_1_rmse: 0.58177 |  0:04:17s
epoch 29 | loss: 0.33783 | val_0_rmse: 0.62868 | val_1_rmse: 0.6287  |  0:04:26s
epoch 30 | loss: 0.33886 | val_0_rmse: 0.59482 | val_1_rmse: 0.59555 |  0:04:34s
epoch 31 | loss: 0.33972 | val_0_rmse: 0.68012 | val_1_rmse: 0.67839 |  0:04:43s
epoch 32 | loss: 0.33582 | val_0_rmse: 0.58423 | val_1_rmse: 0.58855 |  0:04:52s
epoch 33 | loss: 0.33761 | val_0_rmse: 0.573   | val_1_rmse: 0.57727 |  0:05:01s
epoch 34 | loss: 0.3334  | val_0_rmse: 0.59596 | val_1_rmse: 0.59563 |  0:05:09s
epoch 35 | loss: 0.33333 | val_0_rmse: 0.60209 | val_1_rmse: 0.60671 |  0:05:18s
epoch 36 | loss: 0.33335 | val_0_rmse: 0.62184 | val_1_rmse: 0.62244 |  0:05:26s
epoch 37 | loss: 0.33138 | val_0_rmse: 0.63418 | val_1_rmse: 0.63737 |  0:05:35s
epoch 38 | loss: 0.33265 | val_0_rmse: 0.60703 | val_1_rmse: 0.60693 |  0:05:44s
epoch 39 | loss: 0.32792 | val_0_rmse: 0.6605  | val_1_rmse: 0.65929 |  0:05:52s
epoch 40 | loss: 0.32428 | val_0_rmse: 0.58299 | val_1_rmse: 0.58849 |  0:06:01s
epoch 41 | loss: 0.3257  | val_0_rmse: 0.60038 | val_1_rmse: 0.60682 |  0:06:10s
epoch 42 | loss: 0.32492 | val_0_rmse: 0.56866 | val_1_rmse: 0.57112 |  0:06:18s
epoch 43 | loss: 0.32102 | val_0_rmse: 0.59228 | val_1_rmse: 0.59662 |  0:06:27s
epoch 44 | loss: 0.3224  | val_0_rmse: 0.57803 | val_1_rmse: 0.58381 |  0:06:35s
epoch 45 | loss: 0.32181 | val_0_rmse: 0.69679 | val_1_rmse: 0.7007  |  0:06:44s
epoch 46 | loss: 0.32182 | val_0_rmse: 0.58237 | val_1_rmse: 0.58514 |  0:06:53s
epoch 47 | loss: 0.32293 | val_0_rmse: 0.59297 | val_1_rmse: 0.59505 |  0:07:01s
epoch 48 | loss: 0.31823 | val_0_rmse: 0.59547 | val_1_rmse: 0.60073 |  0:07:10s
epoch 49 | loss: 0.31963 | val_0_rmse: 0.57244 | val_1_rmse: 0.57705 |  0:07:19s
epoch 50 | loss: 0.31889 | val_0_rmse: 0.55886 | val_1_rmse: 0.56428 |  0:07:27s
epoch 51 | loss: 0.31847 | val_0_rmse: 0.59577 | val_1_rmse: 0.59649 |  0:07:36s
epoch 52 | loss: 0.31864 | val_0_rmse: 0.6026  | val_1_rmse: 0.6096  |  0:07:45s
epoch 53 | loss: 0.31819 | val_0_rmse: 0.57544 | val_1_rmse: 0.58316 |  0:07:53s
epoch 54 | loss: 0.31402 | val_0_rmse: 0.67241 | val_1_rmse: 0.7991  |  0:08:02s
epoch 55 | loss: 0.31657 | val_0_rmse: 0.55571 | val_1_rmse: 0.56034 |  0:08:11s
epoch 56 | loss: 0.3178  | val_0_rmse: 0.66516 | val_1_rmse: 0.66485 |  0:08:19s
epoch 57 | loss: 0.31424 | val_0_rmse: 0.55051 | val_1_rmse: 0.557   |  0:08:28s
epoch 58 | loss: 0.31458 | val_0_rmse: 0.57654 | val_1_rmse: 0.58308 |  0:08:37s
epoch 59 | loss: 0.31415 | val_0_rmse: 0.62059 | val_1_rmse: 0.62816 |  0:08:45s
epoch 60 | loss: 0.31474 | val_0_rmse: 0.58312 | val_1_rmse: 0.58735 |  0:08:54s
epoch 61 | loss: 0.31319 | val_0_rmse: 0.59942 | val_1_rmse: 0.60114 |  0:09:03s
epoch 62 | loss: 0.31226 | val_0_rmse: 0.57529 | val_1_rmse: 0.57827 |  0:09:12s
epoch 63 | loss: 0.31278 | val_0_rmse: 0.59451 | val_1_rmse: 0.60282 |  0:09:20s
epoch 64 | loss: 0.31245 | val_0_rmse: 0.55359 | val_1_rmse: 0.55903 |  0:09:29s
epoch 65 | loss: 0.31161 | val_0_rmse: 0.60053 | val_1_rmse: 0.60327 |  0:09:38s
epoch 66 | loss: 0.31126 | val_0_rmse: 0.56288 | val_1_rmse: 0.57217 |  0:09:46s
epoch 67 | loss: 0.31265 | val_0_rmse: 0.56743 | val_1_rmse: 0.57375 |  0:09:55s
epoch 68 | loss: 0.30927 | val_0_rmse: 0.57079 | val_1_rmse: 0.57787 |  0:10:03s
epoch 69 | loss: 0.31148 | val_0_rmse: 0.58604 | val_1_rmse: 0.5883  |  0:10:12s
epoch 70 | loss: 0.31029 | val_0_rmse: 0.63036 | val_1_rmse: 0.63686 |  0:10:21s
epoch 71 | loss: 0.30886 | val_0_rmse: 0.57954 | val_1_rmse: 0.58504 |  0:10:30s
epoch 72 | loss: 0.30976 | val_0_rmse: 0.60903 | val_1_rmse: 0.62067 |  0:10:38s
epoch 73 | loss: 0.3088  | val_0_rmse: 0.56841 | val_1_rmse: 0.57274 |  0:10:47s
epoch 74 | loss: 0.30923 | val_0_rmse: 0.61333 | val_1_rmse: 0.61855 |  0:10:56s
epoch 75 | loss: 0.30914 | val_0_rmse: 0.539   | val_1_rmse: 0.54713 |  0:11:04s
epoch 76 | loss: 0.30835 | val_0_rmse: 0.58531 | val_1_rmse: 0.58911 |  0:11:13s
epoch 77 | loss: 0.3083  | val_0_rmse: 0.54919 | val_1_rmse: 0.55555 |  0:11:22s
epoch 78 | loss: 0.30898 | val_0_rmse: 0.61023 | val_1_rmse: 0.61768 |  0:11:30s
epoch 79 | loss: 0.30996 | val_0_rmse: 0.60789 | val_1_rmse: 0.61641 |  0:11:39s
epoch 80 | loss: 0.30702 | val_0_rmse: 0.56461 | val_1_rmse: 0.56961 |  0:11:48s
epoch 81 | loss: 0.3053  | val_0_rmse: 0.58371 | val_1_rmse: 0.58639 |  0:11:56s
epoch 82 | loss: 0.30625 | val_0_rmse: 0.64384 | val_1_rmse: 0.64941 |  0:12:05s
epoch 83 | loss: 0.30704 | val_0_rmse: 0.56924 | val_1_rmse: 0.57711 |  0:12:14s
epoch 84 | loss: 0.30431 | val_0_rmse: 0.54705 | val_1_rmse: 0.55601 |  0:12:22s
epoch 85 | loss: 0.30597 | val_0_rmse: 0.55008 | val_1_rmse: 0.55932 |  0:12:31s
epoch 86 | loss: 0.30344 | val_0_rmse: 0.57226 | val_1_rmse: 0.58093 |  0:12:40s
epoch 87 | loss: 0.30693 | val_0_rmse: 0.58707 | val_1_rmse: 0.59434 |  0:12:49s
epoch 88 | loss: 0.30511 | val_0_rmse: 0.54637 | val_1_rmse: 0.55126 |  0:12:57s
epoch 89 | loss: 0.30644 | val_0_rmse: 0.5424  | val_1_rmse: 0.55483 |  0:13:06s
epoch 90 | loss: 0.30395 | val_0_rmse: 0.57691 | val_1_rmse: 0.58369 |  0:13:15s
epoch 91 | loss: 0.30549 | val_0_rmse: 0.55633 | val_1_rmse: 0.56254 |  0:13:23s
epoch 92 | loss: 0.30434 | val_0_rmse: 0.55868 | val_1_rmse: 0.56437 |  0:13:32s
epoch 93 | loss: 0.30172 | val_0_rmse: 0.60086 | val_1_rmse: 0.60969 |  0:13:41s
epoch 94 | loss: 0.30227 | val_0_rmse: 0.54348 | val_1_rmse: 0.54887 |  0:13:49s
epoch 95 | loss: 0.30208 | val_0_rmse: 0.64514 | val_1_rmse: 0.65413 |  0:13:58s
epoch 96 | loss: 0.30367 | val_0_rmse: 0.54425 | val_1_rmse: 0.5514  |  0:14:07s
epoch 97 | loss: 0.30487 | val_0_rmse: 0.64268 | val_1_rmse: 0.64468 |  0:14:15s
epoch 98 | loss: 0.3074  | val_0_rmse: 0.54191 | val_1_rmse: 0.54831 |  0:14:24s
epoch 99 | loss: 0.30327 | val_0_rmse: 0.57513 | val_1_rmse: 0.58002 |  0:14:33s
epoch 100| loss: 0.30286 | val_0_rmse: 0.55912 | val_1_rmse: 0.56969 |  0:14:41s
epoch 101| loss: 0.301   | val_0_rmse: 0.57934 | val_1_rmse: 0.58423 |  0:14:50s
epoch 102| loss: 0.30184 | val_0_rmse: 0.60859 | val_1_rmse: 0.62088 |  0:14:59s
epoch 103| loss: 0.30211 | val_0_rmse: 0.65105 | val_1_rmse: 0.66307 |  0:15:08s
epoch 104| loss: 0.2998  | val_0_rmse: 0.55686 | val_1_rmse: 0.56327 |  0:15:16s
epoch 105| loss: 0.30022 | val_0_rmse: 0.62069 | val_1_rmse: 0.63095 |  0:15:25s

Early stopping occured at epoch 105 with best_epoch = 75 and best_val_1_rmse = 0.54713
Best weights from best epoch are automatically used!
ended training at: 06:18:31
Feature importance:
Mean squared error is of 2020178107.2503998
Mean absolute error:32328.73437463492
MAPE:0.31877688773067
R2 score:0.7045143882886853
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:18:31
epoch 0  | loss: 0.72073 | val_0_rmse: 0.72685 | val_1_rmse: 0.71815 |  0:00:08s
epoch 1  | loss: 0.46003 | val_0_rmse: 0.68477 | val_1_rmse: 0.67663 |  0:00:17s
epoch 2  | loss: 0.4442  | val_0_rmse: 0.68273 | val_1_rmse: 0.67806 |  0:00:25s
epoch 3  | loss: 0.43366 | val_0_rmse: 0.67553 | val_1_rmse: 0.6693  |  0:00:34s
epoch 4  | loss: 0.42153 | val_0_rmse: 0.63706 | val_1_rmse: 0.63583 |  0:00:43s
epoch 5  | loss: 0.4113  | val_0_rmse: 0.63438 | val_1_rmse: 0.63469 |  0:00:52s
epoch 6  | loss: 0.39875 | val_0_rmse: 0.62088 | val_1_rmse: 0.61444 |  0:01:00s
epoch 7  | loss: 0.38831 | val_0_rmse: 0.61003 | val_1_rmse: 0.60369 |  0:01:09s
epoch 8  | loss: 0.38121 | val_0_rmse: 0.61705 | val_1_rmse: 0.61025 |  0:01:18s
epoch 9  | loss: 0.37525 | val_0_rmse: 0.60097 | val_1_rmse: 0.59743 |  0:01:26s
epoch 10 | loss: 0.37539 | val_0_rmse: 0.63224 | val_1_rmse: 0.63193 |  0:01:35s
epoch 11 | loss: 0.37385 | val_0_rmse: 0.64525 | val_1_rmse: 0.64426 |  0:01:44s
epoch 12 | loss: 0.37136 | val_0_rmse: 0.63069 | val_1_rmse: 0.62334 |  0:01:52s
epoch 13 | loss: 0.3646  | val_0_rmse: 0.59373 | val_1_rmse: 0.58791 |  0:02:01s
epoch 14 | loss: 0.36278 | val_0_rmse: 0.67408 | val_1_rmse: 0.6652  |  0:02:10s
epoch 15 | loss: 0.35994 | val_0_rmse: 0.66248 | val_1_rmse: 0.65581 |  0:02:18s
epoch 16 | loss: 0.35932 | val_0_rmse: 0.68947 | val_1_rmse: 0.68111 |  0:02:27s
epoch 17 | loss: 0.3559  | val_0_rmse: 0.70515 | val_1_rmse: 0.69674 |  0:02:35s
epoch 18 | loss: 0.35449 | val_0_rmse: 0.62196 | val_1_rmse: 0.62048 |  0:02:44s
epoch 19 | loss: 0.35473 | val_0_rmse: 0.58529 | val_1_rmse: 0.57989 |  0:02:53s
epoch 20 | loss: 0.34832 | val_0_rmse: 0.61989 | val_1_rmse: 0.61282 |  0:03:01s
epoch 21 | loss: 0.34993 | val_0_rmse: 0.63986 | val_1_rmse: 0.63663 |  0:03:10s
epoch 22 | loss: 0.35651 | val_0_rmse: 0.62947 | val_1_rmse: 0.62901 |  0:03:19s
epoch 23 | loss: 0.34802 | val_0_rmse: 0.61502 | val_1_rmse: 0.61212 |  0:03:27s
epoch 24 | loss: 0.3469  | val_0_rmse: 0.64074 | val_1_rmse: 0.64089 |  0:03:36s
epoch 25 | loss: 0.34589 | val_0_rmse: 0.59581 | val_1_rmse: 0.59143 |  0:03:45s
epoch 26 | loss: 0.34628 | val_0_rmse: 0.60523 | val_1_rmse: 0.6026  |  0:03:53s
epoch 27 | loss: 0.33897 | val_0_rmse: 0.58825 | val_1_rmse: 0.5832  |  0:04:02s
epoch 28 | loss: 0.34441 | val_0_rmse: 0.62197 | val_1_rmse: 0.61549 |  0:04:11s
epoch 29 | loss: 0.3399  | val_0_rmse: 0.58878 | val_1_rmse: 0.58508 |  0:04:20s
epoch 30 | loss: 0.33811 | val_0_rmse: 0.58144 | val_1_rmse: 0.57642 |  0:04:28s
epoch 31 | loss: 0.3368  | val_0_rmse: 0.57711 | val_1_rmse: 0.57253 |  0:04:37s
epoch 32 | loss: 0.33387 | val_0_rmse: 0.56763 | val_1_rmse: 0.56119 |  0:04:46s
epoch 33 | loss: 0.33312 | val_0_rmse: 0.62464 | val_1_rmse: 0.61757 |  0:04:54s
epoch 34 | loss: 0.33359 | val_0_rmse: 0.56823 | val_1_rmse: 0.56534 |  0:05:03s
epoch 35 | loss: 0.32999 | val_0_rmse: 0.62746 | val_1_rmse: 0.62088 |  0:05:12s
epoch 36 | loss: 0.33088 | val_0_rmse: 0.68155 | val_1_rmse: 0.67363 |  0:05:20s
epoch 37 | loss: 0.32927 | val_0_rmse: 0.6995  | val_1_rmse: 0.70366 |  0:05:29s
epoch 38 | loss: 0.32978 | val_0_rmse: 0.58659 | val_1_rmse: 0.58433 |  0:05:38s
epoch 39 | loss: 0.32716 | val_0_rmse: 0.61205 | val_1_rmse: 0.60438 |  0:05:47s
epoch 40 | loss: 0.33077 | val_0_rmse: 0.6589  | val_1_rmse: 0.66034 |  0:05:55s
epoch 41 | loss: 0.3285  | val_0_rmse: 0.63529 | val_1_rmse: 0.62908 |  0:06:04s
epoch 42 | loss: 0.32667 | val_0_rmse: 0.61404 | val_1_rmse: 0.60881 |  0:06:13s
epoch 43 | loss: 0.32547 | val_0_rmse: 0.638   | val_1_rmse: 0.64041 |  0:06:21s
epoch 44 | loss: 0.32427 | val_0_rmse: 0.60242 | val_1_rmse: 0.59843 |  0:06:30s
epoch 45 | loss: 0.3229  | val_0_rmse: 0.63622 | val_1_rmse: 0.63222 |  0:06:39s
epoch 46 | loss: 0.32515 | val_0_rmse: 0.5827  | val_1_rmse: 0.57858 |  0:06:47s
epoch 47 | loss: 0.32268 | val_0_rmse: 0.72016 | val_1_rmse: 0.71138 |  0:06:56s
epoch 48 | loss: 0.32418 | val_0_rmse: 0.59152 | val_1_rmse: 0.58926 |  0:07:04s
epoch 49 | loss: 0.32227 | val_0_rmse: 0.60106 | val_1_rmse: 0.59943 |  0:07:13s
epoch 50 | loss: 0.31957 | val_0_rmse: 0.55982 | val_1_rmse: 0.55614 |  0:07:22s
epoch 51 | loss: 0.31967 | val_0_rmse: 0.57814 | val_1_rmse: 0.57481 |  0:07:30s
epoch 52 | loss: 0.31858 | val_0_rmse: 0.59831 | val_1_rmse: 0.59245 |  0:07:39s
epoch 53 | loss: 0.31854 | val_0_rmse: 0.56341 | val_1_rmse: 0.56065 |  0:07:47s
epoch 54 | loss: 0.31872 | val_0_rmse: 0.60186 | val_1_rmse: 0.60018 |  0:07:56s
epoch 55 | loss: 0.32329 | val_0_rmse: 0.65363 | val_1_rmse: 0.64858 |  0:08:05s
epoch 56 | loss: 0.3159  | val_0_rmse: 0.56251 | val_1_rmse: 0.55996 |  0:08:13s
epoch 57 | loss: 0.31469 | val_0_rmse: 0.59956 | val_1_rmse: 0.59696 |  0:08:22s
epoch 58 | loss: 0.31465 | val_0_rmse: 0.60341 | val_1_rmse: 0.59976 |  0:08:31s
epoch 59 | loss: 0.31858 | val_0_rmse: 0.56206 | val_1_rmse: 0.56175 |  0:08:39s
epoch 60 | loss: 0.31661 | val_0_rmse: 0.57765 | val_1_rmse: 0.57737 |  0:08:48s
epoch 61 | loss: 0.31692 | val_0_rmse: 0.73397 | val_1_rmse: 0.72388 |  0:08:56s
epoch 62 | loss: 0.31611 | val_0_rmse: 0.64089 | val_1_rmse: 0.64529 |  0:09:05s
epoch 63 | loss: 0.31393 | val_0_rmse: 0.56415 | val_1_rmse: 0.55976 |  0:09:14s
epoch 64 | loss: 0.31276 | val_0_rmse: 0.578   | val_1_rmse: 0.57494 |  0:09:22s
epoch 65 | loss: 0.30993 | val_0_rmse: 0.61065 | val_1_rmse: 0.60404 |  0:09:31s
epoch 66 | loss: 0.31335 | val_0_rmse: 0.60553 | val_1_rmse: 0.60104 |  0:09:40s
epoch 67 | loss: 0.31604 | val_0_rmse: 0.61085 | val_1_rmse: 0.61362 |  0:09:49s
epoch 68 | loss: 0.31392 | val_0_rmse: 0.56409 | val_1_rmse: 0.56413 |  0:09:57s
epoch 69 | loss: 0.31207 | val_0_rmse: 0.6324  | val_1_rmse: 0.63571 |  0:10:06s
epoch 70 | loss: 0.31409 | val_0_rmse: 0.58933 | val_1_rmse: 0.58431 |  0:10:15s
epoch 71 | loss: 0.31139 | val_0_rmse: 0.58173 | val_1_rmse: 0.57621 |  0:10:23s
epoch 72 | loss: 0.31362 | val_0_rmse: 0.59563 | val_1_rmse: 0.59465 |  0:10:32s
epoch 73 | loss: 0.3111  | val_0_rmse: 0.57676 | val_1_rmse: 0.57587 |  0:10:41s
epoch 74 | loss: 0.3102  | val_0_rmse: 0.59453 | val_1_rmse: 0.59304 |  0:10:49s
epoch 75 | loss: 0.31271 | val_0_rmse: 0.55523 | val_1_rmse: 0.55327 |  0:10:58s
epoch 76 | loss: 0.31357 | val_0_rmse: 0.55883 | val_1_rmse: 0.55881 |  0:11:07s
epoch 77 | loss: 0.31191 | val_0_rmse: 0.77853 | val_1_rmse: 0.77194 |  0:11:15s
epoch 78 | loss: 0.30986 | val_0_rmse: 0.6214  | val_1_rmse: 0.61406 |  0:11:24s
epoch 79 | loss: 0.30856 | val_0_rmse: 0.63685 | val_1_rmse: 0.63457 |  0:11:33s
epoch 80 | loss: 0.31213 | val_0_rmse: 0.56124 | val_1_rmse: 0.56223 |  0:11:41s
epoch 81 | loss: 0.31137 | val_0_rmse: 0.56953 | val_1_rmse: 0.56776 |  0:11:50s
epoch 82 | loss: 0.32281 | val_0_rmse: 0.60376 | val_1_rmse: 0.6028  |  0:11:59s
epoch 83 | loss: 0.31001 | val_0_rmse: 0.57001 | val_1_rmse: 0.56814 |  0:12:08s
epoch 84 | loss: 0.30848 | val_0_rmse: 0.63143 | val_1_rmse: 0.62825 |  0:12:16s
epoch 85 | loss: 0.30979 | val_0_rmse: 0.69036 | val_1_rmse: 0.69232 |  0:12:25s
epoch 86 | loss: 0.30901 | val_0_rmse: 0.6359  | val_1_rmse: 0.63025 |  0:12:34s
epoch 87 | loss: 0.30993 | val_0_rmse: 0.61313 | val_1_rmse: 0.61265 |  0:12:42s
epoch 88 | loss: 0.30882 | val_0_rmse: 0.60319 | val_1_rmse: 0.6075  |  0:12:51s
epoch 89 | loss: 0.3097  | val_0_rmse: 0.60236 | val_1_rmse: 0.6056  |  0:13:00s
epoch 90 | loss: 0.30562 | val_0_rmse: 0.56261 | val_1_rmse: 0.56523 |  0:13:08s
epoch 91 | loss: 0.30872 | val_0_rmse: 0.62935 | val_1_rmse: 0.62854 |  0:13:17s
epoch 92 | loss: 0.30862 | val_0_rmse: 0.62298 | val_1_rmse: 0.61935 |  0:13:26s
epoch 93 | loss: 0.30743 | val_0_rmse: 0.64542 | val_1_rmse: 0.64377 |  0:13:34s
epoch 94 | loss: 0.30621 | val_0_rmse: 0.5438  | val_1_rmse: 0.54506 |  0:13:43s
epoch 95 | loss: 0.30578 | val_0_rmse: 0.62255 | val_1_rmse: 0.6179  |  0:13:52s
epoch 96 | loss: 0.30531 | val_0_rmse: 0.64418 | val_1_rmse: 0.63793 |  0:14:00s
epoch 97 | loss: 0.30888 | val_0_rmse: 0.58459 | val_1_rmse: 0.5825  |  0:14:09s
epoch 98 | loss: 0.3076  | val_0_rmse: 0.58292 | val_1_rmse: 0.5796  |  0:14:18s
epoch 99 | loss: 0.30502 | val_0_rmse: 0.59926 | val_1_rmse: 0.6011  |  0:14:27s
epoch 100| loss: 0.3059  | val_0_rmse: 0.61207 | val_1_rmse: 0.60925 |  0:14:35s
epoch 101| loss: 0.30342 | val_0_rmse: 0.58486 | val_1_rmse: 0.58207 |  0:14:44s
epoch 102| loss: 0.30426 | val_0_rmse: 0.70073 | val_1_rmse: 0.69541 |  0:14:52s
epoch 103| loss: 0.30573 | val_0_rmse: 0.60147 | val_1_rmse: 0.60072 |  0:15:01s
epoch 104| loss: 0.30584 | val_0_rmse: 0.56051 | val_1_rmse: 0.55999 |  0:15:10s
epoch 105| loss: 0.30557 | val_0_rmse: 0.65097 | val_1_rmse: 0.65295 |  0:15:18s
epoch 106| loss: 0.30756 | val_0_rmse: 0.59179 | val_1_rmse: 0.58916 |  0:15:27s
epoch 107| loss: 0.30517 | val_0_rmse: 0.55168 | val_1_rmse: 0.55433 |  0:15:36s
epoch 108| loss: 0.30493 | val_0_rmse: 0.54068 | val_1_rmse: 0.5396  |  0:15:44s
epoch 109| loss: 0.30329 | val_0_rmse: 0.55383 | val_1_rmse: 0.55431 |  0:15:53s
epoch 110| loss: 0.30299 | val_0_rmse: 0.56151 | val_1_rmse: 0.56125 |  0:16:02s
epoch 111| loss: 0.30594 | val_0_rmse: 0.5984  | val_1_rmse: 0.6001  |  0:16:11s
epoch 112| loss: 0.30528 | val_0_rmse: 0.61112 | val_1_rmse: 0.60752 |  0:16:19s
epoch 113| loss: 0.30074 | val_0_rmse: 0.60323 | val_1_rmse: 0.60137 |  0:16:28s
epoch 114| loss: 0.30377 | val_0_rmse: 0.56883 | val_1_rmse: 0.57065 |  0:16:37s
epoch 115| loss: 0.30419 | val_0_rmse: 0.58262 | val_1_rmse: 0.58577 |  0:16:45s
epoch 116| loss: 0.30229 | val_0_rmse: 0.61447 | val_1_rmse: 0.61656 |  0:16:54s
epoch 117| loss: 0.30151 | val_0_rmse: 0.68509 | val_1_rmse: 0.67904 |  0:17:03s
epoch 118| loss: 0.30347 | val_0_rmse: 0.5683  | val_1_rmse: 0.56764 |  0:17:11s
epoch 119| loss: 0.30234 | val_0_rmse: 0.57408 | val_1_rmse: 0.58544 |  0:17:20s
epoch 120| loss: 0.30289 | val_0_rmse: 0.62667 | val_1_rmse: 0.62388 |  0:17:29s
epoch 121| loss: 0.30147 | val_0_rmse: 0.62339 | val_1_rmse: 0.62243 |  0:17:37s
epoch 122| loss: 0.30227 | val_0_rmse: 0.58992 | val_1_rmse: 0.58501 |  0:17:46s
epoch 123| loss: 0.30333 | val_0_rmse: 0.61769 | val_1_rmse: 0.61434 |  0:17:55s
epoch 124| loss: 0.30337 | val_0_rmse: 0.56585 | val_1_rmse: 0.56327 |  0:18:03s
epoch 125| loss: 0.30314 | val_0_rmse: 0.56399 | val_1_rmse: 0.56158 |  0:18:12s
epoch 126| loss: 0.30158 | val_0_rmse: 0.57403 | val_1_rmse: 0.57531 |  0:18:21s
epoch 127| loss: 0.30097 | val_0_rmse: 0.64737 | val_1_rmse: 0.6423  |  0:18:29s
epoch 128| loss: 0.30069 | val_0_rmse: 0.59535 | val_1_rmse: 0.6002  |  0:18:38s
epoch 129| loss: 0.30055 | val_0_rmse: 0.67612 | val_1_rmse: 0.67014 |  0:18:47s
epoch 130| loss: 0.30012 | val_0_rmse: 0.54895 | val_1_rmse: 0.54994 |  0:18:55s
epoch 131| loss: 0.30027 | val_0_rmse: 0.56751 | val_1_rmse: 0.56931 |  0:19:04s
epoch 132| loss: 0.3001  | val_0_rmse: 0.56581 | val_1_rmse: 0.56858 |  0:19:13s
epoch 133| loss: 0.30186 | val_0_rmse: 0.55073 | val_1_rmse: 0.55456 |  0:19:21s
epoch 134| loss: 0.30065 | val_0_rmse: 0.66585 | val_1_rmse: 0.67046 |  0:19:30s
epoch 135| loss: 0.30028 | val_0_rmse: 0.74765 | val_1_rmse: 0.76416 |  0:19:38s
epoch 136| loss: 0.30016 | val_0_rmse: 0.5574  | val_1_rmse: 0.55865 |  0:19:47s
epoch 137| loss: 0.30315 | val_0_rmse: 0.62694 | val_1_rmse: 0.62685 |  0:19:56s
epoch 138| loss: 0.30119 | val_0_rmse: 0.59513 | val_1_rmse: 0.59696 |  0:20:04s

Early stopping occured at epoch 138 with best_epoch = 108 and best_val_1_rmse = 0.5396
Best weights from best epoch are automatically used!
ended training at: 06:38:39
Feature importance:
Mean squared error is of 1983423667.9558268
Mean absolute error:31831.912661613635
MAPE:0.30667885959846397
R2 score:0.7068911449531768
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:38:40
epoch 0  | loss: 0.68234 | val_0_rmse: 0.71134 | val_1_rmse: 0.70496 |  0:00:08s
epoch 1  | loss: 0.45198 | val_0_rmse: 0.69172 | val_1_rmse: 0.68554 |  0:00:17s
epoch 2  | loss: 0.43885 | val_0_rmse: 0.68114 | val_1_rmse: 0.67805 |  0:00:26s
epoch 3  | loss: 0.43295 | val_0_rmse: 0.65723 | val_1_rmse: 0.65555 |  0:00:34s
epoch 4  | loss: 0.44039 | val_0_rmse: 0.65417 | val_1_rmse: 0.64836 |  0:00:43s
epoch 5  | loss: 0.44045 | val_0_rmse: 0.65109 | val_1_rmse: 0.64791 |  0:00:52s
epoch 6  | loss: 0.41917 | val_0_rmse: 0.63506 | val_1_rmse: 0.63336 |  0:01:00s
epoch 7  | loss: 0.4149  | val_0_rmse: 0.63266 | val_1_rmse: 0.63082 |  0:01:09s
epoch 8  | loss: 0.4117  | val_0_rmse: 0.6413  | val_1_rmse: 0.63878 |  0:01:18s
epoch 9  | loss: 0.40896 | val_0_rmse: 0.64294 | val_1_rmse: 0.63958 |  0:01:27s
epoch 10 | loss: 0.40719 | val_0_rmse: 0.63061 | val_1_rmse: 0.63054 |  0:01:35s
epoch 11 | loss: 0.40525 | val_0_rmse: 0.62055 | val_1_rmse: 0.62027 |  0:01:44s
epoch 12 | loss: 0.39768 | val_0_rmse: 0.62261 | val_1_rmse: 0.6235  |  0:01:53s
epoch 13 | loss: 0.39403 | val_0_rmse: 0.6324  | val_1_rmse: 0.63084 |  0:02:02s
epoch 14 | loss: 0.38109 | val_0_rmse: 0.6118  | val_1_rmse: 0.60972 |  0:02:10s
epoch 15 | loss: 0.37264 | val_0_rmse: 0.61482 | val_1_rmse: 0.61187 |  0:02:19s
epoch 16 | loss: 0.37036 | val_0_rmse: 0.61129 | val_1_rmse: 0.61163 |  0:02:27s
epoch 17 | loss: 0.36222 | val_0_rmse: 0.62557 | val_1_rmse: 0.62785 |  0:02:36s
epoch 18 | loss: 0.3594  | val_0_rmse: 0.62058 | val_1_rmse: 0.61912 |  0:02:45s
epoch 19 | loss: 0.35666 | val_0_rmse: 0.59762 | val_1_rmse: 0.59776 |  0:02:53s
epoch 20 | loss: 0.35168 | val_0_rmse: 0.60361 | val_1_rmse: 0.60465 |  0:03:02s
epoch 21 | loss: 0.34489 | val_0_rmse: 0.59201 | val_1_rmse: 0.59587 |  0:03:11s
epoch 22 | loss: 0.34376 | val_0_rmse: 0.61015 | val_1_rmse: 0.6104  |  0:03:20s
epoch 23 | loss: 0.34415 | val_0_rmse: 0.59839 | val_1_rmse: 0.59881 |  0:03:28s
epoch 24 | loss: 0.34134 | val_0_rmse: 0.57435 | val_1_rmse: 0.57582 |  0:03:37s
epoch 25 | loss: 0.33529 | val_0_rmse: 0.57302 | val_1_rmse: 0.57153 |  0:03:45s
epoch 26 | loss: 0.33912 | val_0_rmse: 0.56854 | val_1_rmse: 0.56875 |  0:03:54s
epoch 27 | loss: 0.33397 | val_0_rmse: 0.60711 | val_1_rmse: 0.61065 |  0:04:03s
epoch 28 | loss: 0.3318  | val_0_rmse: 0.59608 | val_1_rmse: 0.59233 |  0:04:11s
epoch 29 | loss: 0.33194 | val_0_rmse: 0.57603 | val_1_rmse: 0.57714 |  0:04:20s
epoch 30 | loss: 0.33256 | val_0_rmse: 0.5815  | val_1_rmse: 0.58465 |  0:04:29s
epoch 31 | loss: 0.32913 | val_0_rmse: 0.57432 | val_1_rmse: 0.57436 |  0:04:37s
epoch 32 | loss: 0.33223 | val_0_rmse: 0.60202 | val_1_rmse: 0.60374 |  0:04:46s
epoch 33 | loss: 0.32516 | val_0_rmse: 0.60079 | val_1_rmse: 0.60072 |  0:04:55s
epoch 34 | loss: 0.32868 | val_0_rmse: 0.6236  | val_1_rmse: 0.62202 |  0:05:03s
epoch 35 | loss: 0.32519 | val_0_rmse: 0.62036 | val_1_rmse: 0.62292 |  0:05:12s
epoch 36 | loss: 0.32638 | val_0_rmse: 0.57993 | val_1_rmse: 0.58262 |  0:05:21s
epoch 37 | loss: 0.32474 | val_0_rmse: 0.57033 | val_1_rmse: 0.56753 |  0:05:29s
epoch 38 | loss: 0.32439 | val_0_rmse: 0.57575 | val_1_rmse: 0.57494 |  0:05:38s
epoch 39 | loss: 0.3316  | val_0_rmse: 0.59266 | val_1_rmse: 0.59155 |  0:05:47s
epoch 40 | loss: 0.32481 | val_0_rmse: 0.60881 | val_1_rmse: 0.6078  |  0:05:55s
epoch 41 | loss: 0.32268 | val_0_rmse: 0.70812 | val_1_rmse: 0.69934 |  0:06:04s
epoch 42 | loss: 0.32428 | val_0_rmse: 0.57144 | val_1_rmse: 0.57452 |  0:06:13s
epoch 43 | loss: 0.32427 | val_0_rmse: 0.59417 | val_1_rmse: 0.59603 |  0:06:21s
epoch 44 | loss: 0.32292 | val_0_rmse: 0.70223 | val_1_rmse: 0.69466 |  0:06:30s
epoch 45 | loss: 0.32016 | val_0_rmse: 0.62863 | val_1_rmse: 0.63371 |  0:06:39s
epoch 46 | loss: 0.32006 | val_0_rmse: 0.59928 | val_1_rmse: 0.59516 |  0:06:48s
epoch 47 | loss: 0.31906 | val_0_rmse: 0.56102 | val_1_rmse: 0.56476 |  0:06:56s
epoch 48 | loss: 0.31781 | val_0_rmse: 0.57075 | val_1_rmse: 0.57242 |  0:07:05s
epoch 49 | loss: 0.31669 | val_0_rmse: 0.59352 | val_1_rmse: 0.59457 |  0:07:14s
epoch 50 | loss: 0.31803 | val_0_rmse: 0.59865 | val_1_rmse: 0.60336 |  0:07:22s
epoch 51 | loss: 0.31761 | val_0_rmse: 0.55716 | val_1_rmse: 0.56052 |  0:07:31s
epoch 52 | loss: 0.3154  | val_0_rmse: 0.64692 | val_1_rmse: 0.64353 |  0:07:40s
epoch 53 | loss: 0.3169  | val_0_rmse: 0.55265 | val_1_rmse: 0.55317 |  0:07:48s
epoch 54 | loss: 0.31526 | val_0_rmse: 0.54733 | val_1_rmse: 0.55016 |  0:07:57s
epoch 55 | loss: 0.31757 | val_0_rmse: 0.55993 | val_1_rmse: 0.56193 |  0:08:06s
epoch 56 | loss: 0.31346 | val_0_rmse: 0.57934 | val_1_rmse: 0.58387 |  0:08:14s
epoch 57 | loss: 0.3142  | val_0_rmse: 0.5978  | val_1_rmse: 0.60002 |  0:08:23s
epoch 58 | loss: 0.3139  | val_0_rmse: 0.55992 | val_1_rmse: 0.56457 |  0:08:32s
epoch 59 | loss: 0.31605 | val_0_rmse: 0.55627 | val_1_rmse: 0.55888 |  0:08:40s
epoch 60 | loss: 0.3134  | val_0_rmse: 0.63064 | val_1_rmse: 0.62821 |  0:08:49s
epoch 61 | loss: 0.31404 | val_0_rmse: 0.61256 | val_1_rmse: 0.61044 |  0:08:58s
epoch 62 | loss: 0.31237 | val_0_rmse: 0.56781 | val_1_rmse: 0.57233 |  0:09:07s
epoch 63 | loss: 0.31244 | val_0_rmse: 0.57126 | val_1_rmse: 0.57551 |  0:09:15s
epoch 64 | loss: 0.32025 | val_0_rmse: 0.73408 | val_1_rmse: 0.72168 |  0:09:24s
epoch 65 | loss: 0.31388 | val_0_rmse: 0.56566 | val_1_rmse: 0.56659 |  0:09:33s
epoch 66 | loss: 0.31133 | val_0_rmse: 0.61865 | val_1_rmse: 0.61354 |  0:09:42s
epoch 67 | loss: 0.31305 | val_0_rmse: 0.55383 | val_1_rmse: 0.55645 |  0:09:50s
epoch 68 | loss: 0.31147 | val_0_rmse: 0.54623 | val_1_rmse: 0.55135 |  0:09:59s
epoch 69 | loss: 0.31249 | val_0_rmse: 0.56856 | val_1_rmse: 0.56843 |  0:10:08s
epoch 70 | loss: 0.31026 | val_0_rmse: 0.63517 | val_1_rmse: 0.64228 |  0:10:16s
epoch 71 | loss: 0.31283 | val_0_rmse: 0.63059 | val_1_rmse: 0.6286  |  0:10:25s
epoch 72 | loss: 0.30898 | val_0_rmse: 0.60271 | val_1_rmse: 0.60266 |  0:10:34s
epoch 73 | loss: 0.31149 | val_0_rmse: 0.58018 | val_1_rmse: 0.58013 |  0:10:42s
epoch 74 | loss: 0.31056 | val_0_rmse: 0.58949 | val_1_rmse: 0.5904  |  0:10:51s
epoch 75 | loss: 0.30931 | val_0_rmse: 0.56237 | val_1_rmse: 0.5644  |  0:11:00s
epoch 76 | loss: 0.30939 | val_0_rmse: 0.57255 | val_1_rmse: 0.57738 |  0:11:08s
epoch 77 | loss: 0.3054  | val_0_rmse: 0.64587 | val_1_rmse: 0.64158 |  0:11:17s
epoch 78 | loss: 0.30847 | val_0_rmse: 0.67707 | val_1_rmse: 0.67321 |  0:11:26s
epoch 79 | loss: 0.31091 | val_0_rmse: 0.57466 | val_1_rmse: 0.57868 |  0:11:35s
epoch 80 | loss: 0.30719 | val_0_rmse: 0.59103 | val_1_rmse: 0.5934  |  0:11:43s
epoch 81 | loss: 0.3072  | val_0_rmse: 0.5897  | val_1_rmse: 0.59265 |  0:11:52s
epoch 82 | loss: 0.30688 | val_0_rmse: 0.58251 | val_1_rmse: 0.58219 |  0:12:01s
epoch 83 | loss: 0.30743 | val_0_rmse: 0.57541 | val_1_rmse: 0.58059 |  0:12:09s
epoch 84 | loss: 0.30728 | val_0_rmse: 0.59664 | val_1_rmse: 0.59579 |  0:12:18s

Early stopping occured at epoch 84 with best_epoch = 54 and best_val_1_rmse = 0.55016
Best weights from best epoch are automatically used!
ended training at: 06:51:01
Feature importance:
Mean squared error is of 2018839126.218201
Mean absolute error:32151.83358830506
MAPE:0.30768182922254944
R2 score:0.6975053440590038
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:51:02
epoch 0  | loss: 0.64948 | val_0_rmse: 0.85543 | val_1_rmse: 0.85512 |  0:00:08s
epoch 1  | loss: 0.43203 | val_0_rmse: 0.84439 | val_1_rmse: 0.84766 |  0:00:17s
epoch 2  | loss: 0.42136 | val_0_rmse: 0.67046 | val_1_rmse: 0.67553 |  0:00:26s
epoch 3  | loss: 0.40622 | val_0_rmse: 0.6292  | val_1_rmse: 0.63423 |  0:00:34s
epoch 4  | loss: 0.38319 | val_0_rmse: 0.63893 | val_1_rmse: 0.64461 |  0:00:43s
epoch 5  | loss: 0.3725  | val_0_rmse: 0.63917 | val_1_rmse: 0.64128 |  0:00:52s
epoch 6  | loss: 0.36717 | val_0_rmse: 0.60301 | val_1_rmse: 0.61053 |  0:01:00s
epoch 7  | loss: 0.36384 | val_0_rmse: 0.61018 | val_1_rmse: 0.62039 |  0:01:09s
epoch 8  | loss: 0.3593  | val_0_rmse: 0.59685 | val_1_rmse: 0.6028  |  0:01:18s
epoch 9  | loss: 0.36229 | val_0_rmse: 0.71463 | val_1_rmse: 0.71344 |  0:01:26s
epoch 10 | loss: 0.36036 | val_0_rmse: 0.71069 | val_1_rmse: 0.71342 |  0:01:35s
epoch 11 | loss: 0.35651 | val_0_rmse: 0.63866 | val_1_rmse: 0.63967 |  0:01:44s
epoch 12 | loss: 0.35873 | val_0_rmse: 0.65928 | val_1_rmse: 0.67042 |  0:01:52s
epoch 13 | loss: 0.351   | val_0_rmse: 0.61088 | val_1_rmse: 0.61925 |  0:02:01s
epoch 14 | loss: 0.3566  | val_0_rmse: 0.59302 | val_1_rmse: 0.60026 |  0:02:10s
epoch 15 | loss: 0.3541  | val_0_rmse: 0.58125 | val_1_rmse: 0.58598 |  0:02:18s
epoch 16 | loss: 0.34927 | val_0_rmse: 0.58712 | val_1_rmse: 0.59424 |  0:02:27s
epoch 17 | loss: 0.34624 | val_0_rmse: 0.59456 | val_1_rmse: 0.59992 |  0:02:36s
epoch 18 | loss: 0.34891 | val_0_rmse: 0.58983 | val_1_rmse: 0.59373 |  0:02:44s
epoch 19 | loss: 0.34826 | val_0_rmse: 0.58798 | val_1_rmse: 0.59547 |  0:02:53s
epoch 20 | loss: 0.3461  | val_0_rmse: 0.617   | val_1_rmse: 0.62883 |  0:03:02s
epoch 21 | loss: 0.34622 | val_0_rmse: 0.58606 | val_1_rmse: 0.59272 |  0:03:10s
epoch 22 | loss: 0.3524  | val_0_rmse: 0.70002 | val_1_rmse: 0.70144 |  0:03:19s
epoch 23 | loss: 0.35498 | val_0_rmse: 0.6004  | val_1_rmse: 0.60963 |  0:03:28s
epoch 24 | loss: 0.34674 | val_0_rmse: 0.67777 | val_1_rmse: 0.68248 |  0:03:36s
epoch 25 | loss: 0.3483  | val_0_rmse: 0.65138 | val_1_rmse: 0.66197 |  0:03:45s
epoch 26 | loss: 0.33984 | val_0_rmse: 0.58196 | val_1_rmse: 0.58924 |  0:03:54s
epoch 27 | loss: 0.33991 | val_0_rmse: 0.64953 | val_1_rmse: 0.66022 |  0:04:03s
epoch 28 | loss: 0.34592 | val_0_rmse: 0.60284 | val_1_rmse: 0.61088 |  0:04:11s
epoch 29 | loss: 0.34145 | val_0_rmse: 0.66314 | val_1_rmse: 0.66352 |  0:04:20s
epoch 30 | loss: 0.34303 | val_0_rmse: 0.61499 | val_1_rmse: 0.62399 |  0:04:29s
epoch 31 | loss: 0.33827 | val_0_rmse: 0.62172 | val_1_rmse: 0.63084 |  0:04:38s
epoch 32 | loss: 0.34493 | val_0_rmse: 0.62016 | val_1_rmse: 0.62816 |  0:04:46s
epoch 33 | loss: 0.35663 | val_0_rmse: 0.60107 | val_1_rmse: 0.60496 |  0:04:55s
epoch 34 | loss: 0.34327 | val_0_rmse: 0.61211 | val_1_rmse: 0.61605 |  0:05:04s
epoch 35 | loss: 0.34276 | val_0_rmse: 0.60794 | val_1_rmse: 0.61674 |  0:05:12s
epoch 36 | loss: 0.34334 | val_0_rmse: 0.6722  | val_1_rmse: 0.67661 |  0:05:21s
epoch 37 | loss: 0.34196 | val_0_rmse: 0.59471 | val_1_rmse: 0.59716 |  0:05:30s
epoch 38 | loss: 0.34165 | val_0_rmse: 0.58993 | val_1_rmse: 0.59834 |  0:05:39s
epoch 39 | loss: 0.3391  | val_0_rmse: 0.65637 | val_1_rmse: 0.66738 |  0:05:48s
epoch 40 | loss: 0.34061 | val_0_rmse: 0.59679 | val_1_rmse: 0.60422 |  0:05:56s
epoch 41 | loss: 0.33747 | val_0_rmse: 0.76688 | val_1_rmse: 0.77929 |  0:06:05s
epoch 42 | loss: 0.33677 | val_0_rmse: 0.6059  | val_1_rmse: 0.60849 |  0:06:13s
epoch 43 | loss: 0.33787 | val_0_rmse: 0.62862 | val_1_rmse: 0.62805 |  0:06:22s
epoch 44 | loss: 0.34127 | val_0_rmse: 0.60653 | val_1_rmse: 0.60958 |  0:06:31s
epoch 45 | loss: 0.33565 | val_0_rmse: 0.64209 | val_1_rmse: 0.64422 |  0:06:39s

Early stopping occured at epoch 45 with best_epoch = 15 and best_val_1_rmse = 0.58598
Best weights from best epoch are automatically used!
ended training at: 06:57:44
Feature importance:
Mean squared error is of 2324720773.653647
Mean absolute error:34706.73159393531
MAPE:0.3357259970004753
R2 score:0.6613213389927128
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:57:45
epoch 0  | loss: 0.67927 | val_0_rmse: 0.70694 | val_1_rmse: 0.70521 |  0:00:08s
epoch 1  | loss: 0.45527 | val_0_rmse: 0.67921 | val_1_rmse: 0.67735 |  0:00:17s
epoch 2  | loss: 0.43618 | val_0_rmse: 0.66042 | val_1_rmse: 0.65716 |  0:00:26s
epoch 3  | loss: 0.43084 | val_0_rmse: 0.65323 | val_1_rmse: 0.64918 |  0:00:34s
epoch 4  | loss: 0.43457 | val_0_rmse: 0.65378 | val_1_rmse: 0.65066 |  0:00:43s
epoch 5  | loss: 0.42713 | val_0_rmse: 0.64759 | val_1_rmse: 0.64534 |  0:00:52s
epoch 6  | loss: 0.42138 | val_0_rmse: 0.64466 | val_1_rmse: 0.64312 |  0:01:00s
epoch 7  | loss: 0.41388 | val_0_rmse: 0.67186 | val_1_rmse: 0.66003 |  0:01:09s
epoch 8  | loss: 0.40353 | val_0_rmse: 0.64862 | val_1_rmse: 0.64837 |  0:01:18s
epoch 9  | loss: 0.39256 | val_0_rmse: 0.63459 | val_1_rmse: 0.62549 |  0:01:26s
epoch 10 | loss: 0.37697 | val_0_rmse: 0.72757 | val_1_rmse: 0.72717 |  0:01:35s
epoch 11 | loss: 0.37616 | val_0_rmse: 0.63704 | val_1_rmse: 0.6367  |  0:01:44s
epoch 12 | loss: 0.36631 | val_0_rmse: 0.62077 | val_1_rmse: 0.6219  |  0:01:52s
epoch 13 | loss: 0.36745 | val_0_rmse: 0.67683 | val_1_rmse: 0.67695 |  0:02:01s
epoch 14 | loss: 0.36114 | val_0_rmse: 0.6187  | val_1_rmse: 0.6184  |  0:02:10s
epoch 15 | loss: 0.36903 | val_0_rmse: 0.59491 | val_1_rmse: 0.59579 |  0:02:18s
epoch 16 | loss: 0.35981 | val_0_rmse: 0.60942 | val_1_rmse: 0.61082 |  0:02:27s
epoch 17 | loss: 0.35799 | val_0_rmse: 0.6196  | val_1_rmse: 0.61865 |  0:02:35s
epoch 18 | loss: 0.35808 | val_0_rmse: 0.62776 | val_1_rmse: 0.62749 |  0:02:44s
epoch 19 | loss: 0.35103 | val_0_rmse: 0.66924 | val_1_rmse: 0.66984 |  0:02:53s
epoch 20 | loss: 0.355   | val_0_rmse: 0.61006 | val_1_rmse: 0.60868 |  0:03:01s
epoch 21 | loss: 0.34717 | val_0_rmse: 0.61469 | val_1_rmse: 0.6112  |  0:03:10s
epoch 22 | loss: 0.34816 | val_0_rmse: 0.66712 | val_1_rmse: 0.66892 |  0:03:19s
epoch 23 | loss: 0.34442 | val_0_rmse: 0.62794 | val_1_rmse: 0.62394 |  0:03:27s
epoch 24 | loss: 0.34383 | val_0_rmse: 0.64876 | val_1_rmse: 0.6513  |  0:03:36s
epoch 25 | loss: 0.34512 | val_0_rmse: 0.72807 | val_1_rmse: 0.69227 |  0:03:45s
epoch 26 | loss: 0.34009 | val_0_rmse: 0.5861  | val_1_rmse: 0.58872 |  0:03:53s
epoch 27 | loss: 0.33715 | val_0_rmse: 0.57249 | val_1_rmse: 0.5728  |  0:04:02s
epoch 28 | loss: 0.33686 | val_0_rmse: 0.61208 | val_1_rmse: 0.61104 |  0:04:11s
epoch 29 | loss: 0.33614 | val_0_rmse: 0.62265 | val_1_rmse: 0.62218 |  0:04:20s
epoch 30 | loss: 0.332   | val_0_rmse: 0.61609 | val_1_rmse: 0.61358 |  0:04:28s
epoch 31 | loss: 0.32972 | val_0_rmse: 0.59638 | val_1_rmse: 0.59858 |  0:04:37s
epoch 32 | loss: 0.32858 | val_0_rmse: 0.57071 | val_1_rmse: 0.56982 |  0:04:46s
epoch 33 | loss: 0.32993 | val_0_rmse: 0.58748 | val_1_rmse: 0.58757 |  0:04:54s
epoch 34 | loss: 0.32844 | val_0_rmse: 0.60831 | val_1_rmse: 0.60742 |  0:05:03s
epoch 35 | loss: 0.32534 | val_0_rmse: 0.58227 | val_1_rmse: 0.5819  |  0:05:12s
epoch 36 | loss: 0.32778 | val_0_rmse: 0.63338 | val_1_rmse: 0.63306 |  0:05:20s
epoch 37 | loss: 0.3225  | val_0_rmse: 0.58896 | val_1_rmse: 0.59063 |  0:05:29s
epoch 38 | loss: 0.32425 | val_0_rmse: 0.60386 | val_1_rmse: 0.60256 |  0:05:38s
epoch 39 | loss: 0.3296  | val_0_rmse: 0.65392 | val_1_rmse: 0.6553  |  0:05:46s
epoch 40 | loss: 0.32282 | val_0_rmse: 0.58387 | val_1_rmse: 0.58378 |  0:05:55s
epoch 41 | loss: 0.3217  | val_0_rmse: 0.55678 | val_1_rmse: 0.55731 |  0:06:04s
epoch 42 | loss: 0.32085 | val_0_rmse: 0.58903 | val_1_rmse: 0.58813 |  0:06:12s
epoch 43 | loss: 0.32157 | val_0_rmse: 0.61531 | val_1_rmse: 0.61639 |  0:06:21s
epoch 44 | loss: 0.31939 | val_0_rmse: 0.60347 | val_1_rmse: 0.60222 |  0:06:30s
epoch 45 | loss: 0.31999 | val_0_rmse: 0.59418 | val_1_rmse: 0.59365 |  0:06:38s
epoch 46 | loss: 0.31603 | val_0_rmse: 0.59991 | val_1_rmse: 0.60022 |  0:06:47s
epoch 47 | loss: 0.3158  | val_0_rmse: 0.59651 | val_1_rmse: 0.59733 |  0:06:56s
epoch 48 | loss: 0.31796 | val_0_rmse: 0.59555 | val_1_rmse: 0.59368 |  0:07:04s
epoch 49 | loss: 0.31507 | val_0_rmse: 0.64838 | val_1_rmse: 0.64803 |  0:07:13s
epoch 50 | loss: 0.31426 | val_0_rmse: 0.67078 | val_1_rmse: 0.67174 |  0:07:22s
epoch 51 | loss: 0.31536 | val_0_rmse: 0.63171 | val_1_rmse: 0.62791 |  0:07:30s
epoch 52 | loss: 0.31679 | val_0_rmse: 0.62144 | val_1_rmse: 0.62256 |  0:07:39s
epoch 53 | loss: 0.3147  | val_0_rmse: 0.56814 | val_1_rmse: 0.57151 |  0:07:48s
epoch 54 | loss: 0.31175 | val_0_rmse: 0.60573 | val_1_rmse: 0.60445 |  0:07:56s
epoch 55 | loss: 0.31567 | val_0_rmse: 0.55007 | val_1_rmse: 0.5509  |  0:08:05s
epoch 56 | loss: 0.31154 | val_0_rmse: 0.63482 | val_1_rmse: 0.63344 |  0:08:14s
epoch 57 | loss: 0.31162 | val_0_rmse: 0.62651 | val_1_rmse: 0.62842 |  0:08:22s
epoch 58 | loss: 0.31401 | val_0_rmse: 0.66602 | val_1_rmse: 0.66432 |  0:08:31s
epoch 59 | loss: 0.31297 | val_0_rmse: 0.61323 | val_1_rmse: 0.61341 |  0:08:40s
epoch 60 | loss: 0.3098  | val_0_rmse: 0.61099 | val_1_rmse: 0.6083  |  0:08:49s
epoch 61 | loss: 0.31072 | val_0_rmse: 0.60661 | val_1_rmse: 0.61106 |  0:08:57s
epoch 62 | loss: 0.31186 | val_0_rmse: 0.619   | val_1_rmse: 0.61715 |  0:09:06s
epoch 63 | loss: 0.31079 | val_0_rmse: 0.60166 | val_1_rmse: 0.59896 |  0:09:15s
epoch 64 | loss: 0.30976 | val_0_rmse: 0.60927 | val_1_rmse: 0.61148 |  0:09:24s
epoch 65 | loss: 0.31268 | val_0_rmse: 0.65356 | val_1_rmse: 0.65639 |  0:09:32s
epoch 66 | loss: 0.31575 | val_0_rmse: 0.56638 | val_1_rmse: 0.5665  |  0:09:41s
epoch 67 | loss: 0.3114  | val_0_rmse: 0.63467 | val_1_rmse: 0.63848 |  0:09:49s
epoch 68 | loss: 0.30936 | val_0_rmse: 0.60112 | val_1_rmse: 0.59957 |  0:09:58s
epoch 69 | loss: 0.32799 | val_0_rmse: 0.55532 | val_1_rmse: 0.55695 |  0:10:07s
epoch 70 | loss: 0.31492 | val_0_rmse: 0.64274 | val_1_rmse: 0.61295 |  0:10:16s
epoch 71 | loss: 0.31679 | val_0_rmse: 0.628   | val_1_rmse: 0.56264 |  0:10:24s
epoch 72 | loss: 0.31676 | val_0_rmse: 1.06611 | val_1_rmse: 0.65062 |  0:10:33s
epoch 73 | loss: 0.32841 | val_0_rmse: 1.05143 | val_1_rmse: 0.78305 |  0:10:42s
epoch 74 | loss: 0.3403  | val_0_rmse: 0.67217 | val_1_rmse: 0.67425 |  0:10:50s
epoch 75 | loss: 0.32583 | val_0_rmse: 0.63465 | val_1_rmse: 0.58585 |  0:10:59s
epoch 76 | loss: 0.32565 | val_0_rmse: 0.59909 | val_1_rmse: 0.5824  |  0:11:07s
epoch 77 | loss: 0.3229  | val_0_rmse: 0.90849 | val_1_rmse: 0.72258 |  0:11:16s
epoch 78 | loss: 0.32089 | val_0_rmse: 0.6865  | val_1_rmse: 0.6464  |  0:11:25s
epoch 79 | loss: 0.31329 | val_0_rmse: 0.59252 | val_1_rmse: 0.58645 |  0:11:34s
epoch 80 | loss: 0.30968 | val_0_rmse: 0.57338 | val_1_rmse: 0.5735  |  0:11:42s
epoch 81 | loss: 0.31017 | val_0_rmse: 0.60438 | val_1_rmse: 0.57973 |  0:11:51s
epoch 82 | loss: 0.30793 | val_0_rmse: 0.65959 | val_1_rmse: 0.63713 |  0:12:00s
epoch 83 | loss: 0.30768 | val_0_rmse: 0.62168 | val_1_rmse: 0.60628 |  0:12:08s
epoch 84 | loss: 0.30866 | val_0_rmse: 0.62296 | val_1_rmse: 0.57693 |  0:12:17s
epoch 85 | loss: 0.31054 | val_0_rmse: 0.62268 | val_1_rmse: 0.59449 |  0:12:25s

Early stopping occured at epoch 85 with best_epoch = 55 and best_val_1_rmse = 0.5509
Best weights from best epoch are automatically used!
ended training at: 07:10:13
Feature importance:
Mean squared error is of 2059686815.7012475
Mean absolute error:32770.745851571206
MAPE:0.3221764108457918
R2 score:0.7023977550240161
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:10:14
epoch 0  | loss: 0.71244 | val_0_rmse: 0.73529 | val_1_rmse: 0.72923 |  0:00:08s
epoch 1  | loss: 0.44989 | val_0_rmse: 0.67994 | val_1_rmse: 0.68118 |  0:00:17s
epoch 2  | loss: 0.43263 | val_0_rmse: 0.66411 | val_1_rmse: 0.66439 |  0:00:25s
epoch 3  | loss: 0.42332 | val_0_rmse: 0.64978 | val_1_rmse: 0.65021 |  0:00:34s
epoch 4  | loss: 0.4171  | val_0_rmse: 0.64698 | val_1_rmse: 0.64945 |  0:00:43s
epoch 5  | loss: 0.41583 | val_0_rmse: 0.63382 | val_1_rmse: 0.63424 |  0:00:52s
epoch 6  | loss: 0.40971 | val_0_rmse: 0.64199 | val_1_rmse: 0.64236 |  0:01:01s
epoch 7  | loss: 0.40677 | val_0_rmse: 0.63511 | val_1_rmse: 0.63511 |  0:01:09s
epoch 8  | loss: 0.40098 | val_0_rmse: 0.64655 | val_1_rmse: 0.65269 |  0:01:18s
epoch 9  | loss: 0.3983  | val_0_rmse: 0.61865 | val_1_rmse: 0.62187 |  0:01:27s
epoch 10 | loss: 0.38798 | val_0_rmse: 0.64286 | val_1_rmse: 0.63975 |  0:01:35s
epoch 11 | loss: 0.37663 | val_0_rmse: 0.63908 | val_1_rmse: 0.63631 |  0:01:44s
epoch 12 | loss: 0.37108 | val_0_rmse: 0.60185 | val_1_rmse: 0.60596 |  0:01:53s
epoch 13 | loss: 0.36592 | val_0_rmse: 0.6165  | val_1_rmse: 0.62183 |  0:02:02s
epoch 14 | loss: 0.36629 | val_0_rmse: 0.63513 | val_1_rmse: 0.62206 |  0:02:10s
epoch 15 | loss: 0.36382 | val_0_rmse: 0.63281 | val_1_rmse: 0.63422 |  0:02:19s
epoch 16 | loss: 0.35923 | val_0_rmse: 0.59224 | val_1_rmse: 0.59426 |  0:02:28s
epoch 17 | loss: 0.35549 | val_0_rmse: 0.67609 | val_1_rmse: 0.67128 |  0:02:37s
epoch 18 | loss: 0.35343 | val_0_rmse: 0.60148 | val_1_rmse: 0.74311 |  0:02:45s
epoch 19 | loss: 0.35263 | val_0_rmse: 0.63685 | val_1_rmse: 1.04723 |  0:02:54s
epoch 20 | loss: 0.34439 | val_0_rmse: 0.57723 | val_1_rmse: 0.5794  |  0:03:03s
epoch 21 | loss: 0.34087 | val_0_rmse: 0.58307 | val_1_rmse: 0.60142 |  0:03:11s
epoch 22 | loss: 0.33733 | val_0_rmse: 0.58571 | val_1_rmse: 0.58749 |  0:03:20s
epoch 23 | loss: 0.33478 | val_0_rmse: 0.57182 | val_1_rmse: 0.57252 |  0:03:29s
epoch 24 | loss: 0.33281 | val_0_rmse: 0.69839 | val_1_rmse: 0.70604 |  0:03:37s
epoch 25 | loss: 0.33394 | val_0_rmse: 0.68819 | val_1_rmse: 0.68861 |  0:03:46s
epoch 26 | loss: 0.33346 | val_0_rmse: 0.6061  | val_1_rmse: 0.61311 |  0:03:55s
epoch 27 | loss: 0.33085 | val_0_rmse: 0.58765 | val_1_rmse: 0.59113 |  0:04:03s
epoch 28 | loss: 0.32863 | val_0_rmse: 0.60167 | val_1_rmse: 0.60263 |  0:04:12s
epoch 29 | loss: 0.32925 | val_0_rmse: 0.60924 | val_1_rmse: 0.60895 |  0:04:21s
epoch 30 | loss: 0.32872 | val_0_rmse: 0.57143 | val_1_rmse: 0.57374 |  0:04:30s
epoch 31 | loss: 0.32537 | val_0_rmse: 0.61591 | val_1_rmse: 0.63681 |  0:04:38s
epoch 32 | loss: 0.32593 | val_0_rmse: 0.63218 | val_1_rmse: 0.63315 |  0:04:47s
epoch 33 | loss: 0.32568 | val_0_rmse: 0.61654 | val_1_rmse: 0.62191 |  0:04:56s
epoch 34 | loss: 0.32193 | val_0_rmse: 0.63939 | val_1_rmse: 0.63997 |  0:05:04s
epoch 35 | loss: 0.32084 | val_0_rmse: 0.69042 | val_1_rmse: 0.68929 |  0:05:13s
epoch 36 | loss: 0.32271 | val_0_rmse: 0.5737  | val_1_rmse: 0.57746 |  0:05:22s
epoch 37 | loss: 0.32147 | val_0_rmse: 0.61432 | val_1_rmse: 0.61562 |  0:05:31s
epoch 38 | loss: 0.3216  | val_0_rmse: 0.58367 | val_1_rmse: 0.58625 |  0:05:39s
epoch 39 | loss: 0.31973 | val_0_rmse: 0.5844  | val_1_rmse: 0.58499 |  0:05:48s
epoch 40 | loss: 0.32186 | val_0_rmse: 0.61089 | val_1_rmse: 0.61148 |  0:05:57s
epoch 41 | loss: 0.31756 | val_0_rmse: 0.60462 | val_1_rmse: 0.61433 |  0:06:05s
epoch 42 | loss: 0.31756 | val_0_rmse: 0.66522 | val_1_rmse: 0.66583 |  0:06:14s
epoch 43 | loss: 0.32177 | val_0_rmse: 0.60349 | val_1_rmse: 0.60332 |  0:06:23s
epoch 44 | loss: 0.31887 | val_0_rmse: 0.68577 | val_1_rmse: 0.68234 |  0:06:32s
epoch 45 | loss: 0.31564 | val_0_rmse: 0.57823 | val_1_rmse: 0.58044 |  0:06:40s
epoch 46 | loss: 0.31385 | val_0_rmse: 0.57978 | val_1_rmse: 0.58195 |  0:06:49s
epoch 47 | loss: 0.31452 | val_0_rmse: 0.70646 | val_1_rmse: 0.70549 |  0:06:58s
epoch 48 | loss: 0.31746 | val_0_rmse: 0.55675 | val_1_rmse: 0.5631  |  0:07:06s
epoch 49 | loss: 0.31479 | val_0_rmse: 0.64279 | val_1_rmse: 0.64305 |  0:07:15s
epoch 50 | loss: 0.31414 | val_0_rmse: 0.56179 | val_1_rmse: 0.56322 |  0:07:24s
epoch 51 | loss: 0.31502 | val_0_rmse: 0.57872 | val_1_rmse: 0.5825  |  0:07:32s
epoch 52 | loss: 0.31297 | val_0_rmse: 0.56005 | val_1_rmse: 0.5645  |  0:07:41s
epoch 53 | loss: 0.31272 | val_0_rmse: 0.57539 | val_1_rmse: 0.58174 |  0:07:50s
epoch 54 | loss: 0.31151 | val_0_rmse: 0.57469 | val_1_rmse: 0.5832  |  0:07:58s
epoch 55 | loss: 0.31341 | val_0_rmse: 0.57214 | val_1_rmse: 0.57715 |  0:08:07s
epoch 56 | loss: 0.31455 | val_0_rmse: 0.58335 | val_1_rmse: 0.58711 |  0:08:16s
epoch 57 | loss: 0.3113  | val_0_rmse: 0.57573 | val_1_rmse: 0.58077 |  0:08:24s
epoch 58 | loss: 0.31414 | val_0_rmse: 0.54784 | val_1_rmse: 0.55277 |  0:08:33s
epoch 59 | loss: 0.31325 | val_0_rmse: 0.54819 | val_1_rmse: 0.55426 |  0:08:42s
epoch 60 | loss: 0.31212 | val_0_rmse: 0.61963 | val_1_rmse: 0.62435 |  0:08:51s
epoch 61 | loss: 0.30943 | val_0_rmse: 0.56135 | val_1_rmse: 0.56641 |  0:08:59s
epoch 62 | loss: 0.30936 | val_0_rmse: 0.57128 | val_1_rmse: 0.57637 |  0:09:08s
epoch 63 | loss: 0.3081  | val_0_rmse: 0.57722 | val_1_rmse: 0.58092 |  0:09:17s
epoch 64 | loss: 0.30849 | val_0_rmse: 0.69311 | val_1_rmse: 0.69464 |  0:09:25s
epoch 65 | loss: 0.30982 | val_0_rmse: 0.54192 | val_1_rmse: 0.55209 |  0:09:34s
epoch 66 | loss: 0.30692 | val_0_rmse: 0.58294 | val_1_rmse: 0.58915 |  0:09:43s
epoch 67 | loss: 0.30839 | val_0_rmse: 0.59003 | val_1_rmse: 0.59654 |  0:09:52s
epoch 68 | loss: 0.30701 | val_0_rmse: 0.54627 | val_1_rmse: 0.55667 |  0:10:00s
epoch 69 | loss: 0.30769 | val_0_rmse: 0.7375  | val_1_rmse: 0.73795 |  0:10:09s
epoch 70 | loss: 0.30798 | val_0_rmse: 0.64022 | val_1_rmse: 0.64344 |  0:10:18s
epoch 71 | loss: 0.30532 | val_0_rmse: 0.63261 | val_1_rmse: 0.63828 |  0:10:26s
epoch 72 | loss: 0.31178 | val_0_rmse: 0.63461 | val_1_rmse: 0.63477 |  0:10:35s
epoch 73 | loss: 0.31215 | val_0_rmse: 0.54795 | val_1_rmse: 0.55563 |  0:10:44s
epoch 74 | loss: 0.30956 | val_0_rmse: 0.64608 | val_1_rmse: 0.65215 |  0:10:53s
epoch 75 | loss: 0.31101 | val_0_rmse: 0.61497 | val_1_rmse: 0.61794 |  0:11:01s
epoch 76 | loss: 0.30953 | val_0_rmse: 0.55917 | val_1_rmse: 0.54745 |  0:11:10s
epoch 77 | loss: 0.30411 | val_0_rmse: 0.59842 | val_1_rmse: 0.60153 |  0:11:19s
epoch 78 | loss: 0.30766 | val_0_rmse: 0.83164 | val_1_rmse: 0.57526 |  0:11:27s
epoch 79 | loss: 0.30408 | val_0_rmse: 0.90906 | val_1_rmse: 0.59284 |  0:11:36s
epoch 80 | loss: 0.30471 | val_0_rmse: 0.63957 | val_1_rmse: 0.6437  |  0:11:45s
epoch 81 | loss: 0.30443 | val_0_rmse: 0.68911 | val_1_rmse: 0.64427 |  0:11:53s
epoch 82 | loss: 0.30327 | val_0_rmse: 0.61    | val_1_rmse: 0.61241 |  0:12:02s
epoch 83 | loss: 0.30382 | val_0_rmse: 0.56195 | val_1_rmse: 0.56729 |  0:12:10s
epoch 84 | loss: 0.30266 | val_0_rmse: 0.56881 | val_1_rmse: 0.57619 |  0:12:19s
epoch 85 | loss: 0.30309 | val_0_rmse: 0.61609 | val_1_rmse: 0.61897 |  0:12:28s
epoch 86 | loss: 0.30285 | val_0_rmse: 0.58182 | val_1_rmse: 0.58818 |  0:12:36s
epoch 87 | loss: 0.30212 | val_0_rmse: 0.56343 | val_1_rmse: 0.56667 |  0:12:45s
epoch 88 | loss: 0.30128 | val_0_rmse: 0.62908 | val_1_rmse: 0.57141 |  0:12:53s
epoch 89 | loss: 0.30327 | val_0_rmse: 0.71285 | val_1_rmse: 0.58573 |  0:13:02s
epoch 90 | loss: 0.30257 | val_0_rmse: 0.57201 | val_1_rmse: 0.57637 |  0:13:11s
epoch 91 | loss: 0.30384 | val_0_rmse: 0.64985 | val_1_rmse: 0.59749 |  0:13:19s
epoch 92 | loss: 0.30072 | val_0_rmse: 0.56776 | val_1_rmse: 0.57578 |  0:13:28s
epoch 93 | loss: 0.30167 | val_0_rmse: 1.05544 | val_1_rmse: 0.6264  |  0:13:36s
epoch 94 | loss: 0.30062 | val_0_rmse: 1.01835 | val_1_rmse: 0.5625  |  0:13:45s
epoch 95 | loss: 0.29938 | val_0_rmse: 0.56319 | val_1_rmse: 0.5699  |  0:13:53s
epoch 96 | loss: 0.30216 | val_0_rmse: 0.54668 | val_1_rmse: 0.55789 |  0:14:02s
epoch 97 | loss: 0.30119 | val_0_rmse: 0.65583 | val_1_rmse: 0.56222 |  0:14:11s
epoch 98 | loss: 0.30204 | val_0_rmse: 0.59222 | val_1_rmse: 0.59823 |  0:14:19s
epoch 99 | loss: 0.30027 | val_0_rmse: 0.55023 | val_1_rmse: 0.55823 |  0:14:28s
epoch 100| loss: 0.30006 | val_0_rmse: 0.65932 | val_1_rmse: 0.65616 |  0:14:36s
epoch 101| loss: 0.30073 | val_0_rmse: 0.5405  | val_1_rmse: 0.5479  |  0:14:45s
epoch 102| loss: 0.30119 | val_0_rmse: 0.60739 | val_1_rmse: 0.61571 |  0:14:53s
epoch 103| loss: 0.29856 | val_0_rmse: 0.59507 | val_1_rmse: 0.60435 |  0:15:02s
epoch 104| loss: 0.30042 | val_0_rmse: 0.55498 | val_1_rmse: 0.56278 |  0:15:11s
epoch 105| loss: 0.29811 | val_0_rmse: 0.56869 | val_1_rmse: 0.57933 |  0:15:19s
epoch 106| loss: 0.30012 | val_0_rmse: 0.64752 | val_1_rmse: 0.65473 |  0:15:28s

Early stopping occured at epoch 106 with best_epoch = 76 and best_val_1_rmse = 0.54745
Best weights from best epoch are automatically used!
ended training at: 07:25:45
Feature importance:
Mean squared error is of 2063500116.330578
Mean absolute error:33131.28617536536
MAPE:0.328530816871277
R2 score:0.7006269370008762
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:25:46
epoch 0  | loss: 0.83729 | val_0_rmse: 0.71856 | val_1_rmse: 0.72241 |  0:00:03s
epoch 1  | loss: 0.39562 | val_0_rmse: 0.63    | val_1_rmse: 0.63558 |  0:00:06s
epoch 2  | loss: 0.34383 | val_0_rmse: 0.60827 | val_1_rmse: 0.61799 |  0:00:09s
epoch 3  | loss: 0.32141 | val_0_rmse: 0.63478 | val_1_rmse: 0.64367 |  0:00:12s
epoch 4  | loss: 0.31919 | val_0_rmse: 0.59115 | val_1_rmse: 0.60167 |  0:00:15s
epoch 5  | loss: 0.30331 | val_0_rmse: 0.59446 | val_1_rmse: 0.60458 |  0:00:18s
epoch 6  | loss: 0.3026  | val_0_rmse: 0.57416 | val_1_rmse: 0.58296 |  0:00:22s
epoch 7  | loss: 0.29822 | val_0_rmse: 0.56916 | val_1_rmse: 0.57762 |  0:00:25s
epoch 8  | loss: 0.29442 | val_0_rmse: 0.56095 | val_1_rmse: 0.56938 |  0:00:28s
epoch 9  | loss: 0.29151 | val_0_rmse: 0.53992 | val_1_rmse: 0.54832 |  0:00:31s
epoch 10 | loss: 0.29177 | val_0_rmse: 0.53819 | val_1_rmse: 0.54675 |  0:00:34s
epoch 11 | loss: 0.28514 | val_0_rmse: 0.52919 | val_1_rmse: 0.53839 |  0:00:37s
epoch 12 | loss: 0.28732 | val_0_rmse: 0.52721 | val_1_rmse: 0.53567 |  0:00:40s
epoch 13 | loss: 0.28215 | val_0_rmse: 0.53414 | val_1_rmse: 0.54305 |  0:00:44s
epoch 14 | loss: 0.28363 | val_0_rmse: 0.52292 | val_1_rmse: 0.53215 |  0:00:47s
epoch 15 | loss: 0.28301 | val_0_rmse: 0.53585 | val_1_rmse: 0.5458  |  0:00:50s
epoch 16 | loss: 0.28348 | val_0_rmse: 0.52252 | val_1_rmse: 0.53386 |  0:00:53s
epoch 17 | loss: 0.2801  | val_0_rmse: 0.52997 | val_1_rmse: 0.54175 |  0:00:56s
epoch 18 | loss: 0.28288 | val_0_rmse: 0.51861 | val_1_rmse: 0.52735 |  0:00:59s
epoch 19 | loss: 0.27957 | val_0_rmse: 0.52364 | val_1_rmse: 0.53681 |  0:01:02s
epoch 20 | loss: 0.27972 | val_0_rmse: 0.51961 | val_1_rmse: 0.52919 |  0:01:06s
epoch 21 | loss: 0.28101 | val_0_rmse: 0.52552 | val_1_rmse: 0.53918 |  0:01:09s
epoch 22 | loss: 0.2823  | val_0_rmse: 0.5255  | val_1_rmse: 0.53498 |  0:01:12s
epoch 23 | loss: 0.27777 | val_0_rmse: 0.51558 | val_1_rmse: 0.52637 |  0:01:15s
epoch 24 | loss: 0.28002 | val_0_rmse: 0.51843 | val_1_rmse: 0.52732 |  0:01:18s
epoch 25 | loss: 0.27488 | val_0_rmse: 0.53127 | val_1_rmse: 0.54218 |  0:01:21s
epoch 26 | loss: 0.27799 | val_0_rmse: 0.51765 | val_1_rmse: 0.52378 |  0:01:24s
epoch 27 | loss: 0.26948 | val_0_rmse: 0.51523 | val_1_rmse: 0.52419 |  0:01:27s
epoch 28 | loss: 0.26706 | val_0_rmse: 0.56923 | val_1_rmse: 0.57651 |  0:01:30s
epoch 29 | loss: 0.27094 | val_0_rmse: 0.59981 | val_1_rmse: 0.60827 |  0:01:34s
epoch 30 | loss: 0.26648 | val_0_rmse: 0.56559 | val_1_rmse: 0.57485 |  0:01:37s
epoch 31 | loss: 0.2646  | val_0_rmse: 0.51008 | val_1_rmse: 0.51748 |  0:01:40s
epoch 32 | loss: 0.25826 | val_0_rmse: 0.52166 | val_1_rmse: 0.5296  |  0:01:43s
epoch 33 | loss: 0.26093 | val_0_rmse: 0.50586 | val_1_rmse: 0.51185 |  0:01:46s
epoch 34 | loss: 0.26159 | val_0_rmse: 0.5284  | val_1_rmse: 0.53751 |  0:01:49s
epoch 35 | loss: 0.26037 | val_0_rmse: 0.49963 | val_1_rmse: 0.50939 |  0:01:52s
epoch 36 | loss: 0.25725 | val_0_rmse: 0.51532 | val_1_rmse: 0.52479 |  0:01:55s
epoch 37 | loss: 0.25404 | val_0_rmse: 0.55095 | val_1_rmse: 0.55894 |  0:01:58s
epoch 38 | loss: 0.25413 | val_0_rmse: 0.49812 | val_1_rmse: 0.50807 |  0:02:02s
epoch 39 | loss: 0.25647 | val_0_rmse: 0.52215 | val_1_rmse: 0.53097 |  0:02:05s
epoch 40 | loss: 0.25334 | val_0_rmse: 0.49482 | val_1_rmse: 0.50359 |  0:02:08s
epoch 41 | loss: 0.25268 | val_0_rmse: 0.50696 | val_1_rmse: 0.51358 |  0:02:11s
epoch 42 | loss: 0.25299 | val_0_rmse: 0.53878 | val_1_rmse: 0.54962 |  0:02:14s
epoch 43 | loss: 0.25066 | val_0_rmse: 0.50376 | val_1_rmse: 0.51314 |  0:02:17s
epoch 44 | loss: 0.25637 | val_0_rmse: 0.51093 | val_1_rmse: 0.51936 |  0:02:20s
epoch 45 | loss: 0.2514  | val_0_rmse: 0.57689 | val_1_rmse: 0.58391 |  0:02:23s
epoch 46 | loss: 0.25276 | val_0_rmse: 0.50563 | val_1_rmse: 0.51475 |  0:02:27s
epoch 47 | loss: 0.25275 | val_0_rmse: 0.49369 | val_1_rmse: 0.50157 |  0:02:30s
epoch 48 | loss: 0.25199 | val_0_rmse: 0.50144 | val_1_rmse: 0.51178 |  0:02:33s
epoch 49 | loss: 0.25577 | val_0_rmse: 0.5958  | val_1_rmse: 0.60477 |  0:02:36s
epoch 50 | loss: 0.2502  | val_0_rmse: 0.48961 | val_1_rmse: 0.50186 |  0:02:39s
epoch 51 | loss: 0.25085 | val_0_rmse: 0.49368 | val_1_rmse: 0.50354 |  0:02:42s
epoch 52 | loss: 0.25171 | val_0_rmse: 0.54372 | val_1_rmse: 0.5535  |  0:02:45s
epoch 53 | loss: 0.24961 | val_0_rmse: 0.48675 | val_1_rmse: 0.49934 |  0:02:48s
epoch 54 | loss: 0.24966 | val_0_rmse: 0.52084 | val_1_rmse: 0.53003 |  0:02:51s
epoch 55 | loss: 0.25047 | val_0_rmse: 0.53935 | val_1_rmse: 0.55062 |  0:02:55s
epoch 56 | loss: 0.24823 | val_0_rmse: 0.50519 | val_1_rmse: 0.51557 |  0:02:58s
epoch 57 | loss: 0.24489 | val_0_rmse: 0.48793 | val_1_rmse: 0.49921 |  0:03:01s
epoch 58 | loss: 0.24891 | val_0_rmse: 0.5056  | val_1_rmse: 0.5154  |  0:03:04s
epoch 59 | loss: 0.24852 | val_0_rmse: 0.50412 | val_1_rmse: 0.51371 |  0:03:07s
epoch 60 | loss: 0.24915 | val_0_rmse: 0.51024 | val_1_rmse: 0.52145 |  0:03:10s
epoch 61 | loss: 0.2475  | val_0_rmse: 0.50761 | val_1_rmse: 0.51493 |  0:03:13s
epoch 62 | loss: 0.24538 | val_0_rmse: 0.57137 | val_1_rmse: 0.58092 |  0:03:16s
epoch 63 | loss: 0.2481  | val_0_rmse: 0.49253 | val_1_rmse: 0.50124 |  0:03:19s
epoch 64 | loss: 0.25065 | val_0_rmse: 0.50911 | val_1_rmse: 0.51861 |  0:03:23s
epoch 65 | loss: 0.24705 | val_0_rmse: 0.50065 | val_1_rmse: 0.50932 |  0:03:26s
epoch 66 | loss: 0.2503  | val_0_rmse: 0.50589 | val_1_rmse: 0.51841 |  0:03:29s
epoch 67 | loss: 0.24959 | val_0_rmse: 0.53547 | val_1_rmse: 0.54444 |  0:03:32s
epoch 68 | loss: 0.24911 | val_0_rmse: 0.49249 | val_1_rmse: 0.50546 |  0:03:35s
epoch 69 | loss: 0.24728 | val_0_rmse: 0.54628 | val_1_rmse: 0.55705 |  0:03:38s
epoch 70 | loss: 0.24594 | val_0_rmse: 0.48702 | val_1_rmse: 0.49922 |  0:03:41s
epoch 71 | loss: 0.24475 | val_0_rmse: 0.48888 | val_1_rmse: 0.50377 |  0:03:44s
epoch 72 | loss: 0.24588 | val_0_rmse: 0.51938 | val_1_rmse: 0.53168 |  0:03:48s
epoch 73 | loss: 0.24598 | val_0_rmse: 0.49304 | val_1_rmse: 0.5068  |  0:03:51s
epoch 74 | loss: 0.24699 | val_0_rmse: 0.50446 | val_1_rmse: 0.51542 |  0:03:54s
epoch 75 | loss: 0.24212 | val_0_rmse: 0.48191 | val_1_rmse: 0.49533 |  0:03:57s
epoch 76 | loss: 0.24608 | val_0_rmse: 0.50028 | val_1_rmse: 0.51305 |  0:04:00s
epoch 77 | loss: 0.24251 | val_0_rmse: 0.5305  | val_1_rmse: 0.5415  |  0:04:03s
epoch 78 | loss: 0.24688 | val_0_rmse: 0.52639 | val_1_rmse: 0.53841 |  0:04:06s
epoch 79 | loss: 0.24982 | val_0_rmse: 0.57487 | val_1_rmse: 0.58578 |  0:04:10s
epoch 80 | loss: 0.25114 | val_0_rmse: 0.52628 | val_1_rmse: 0.53772 |  0:04:13s
epoch 81 | loss: 0.24626 | val_0_rmse: 0.49574 | val_1_rmse: 0.5078  |  0:04:16s
epoch 82 | loss: 0.24584 | val_0_rmse: 0.49414 | val_1_rmse: 0.50545 |  0:04:19s
epoch 83 | loss: 0.2429  | val_0_rmse: 0.51533 | val_1_rmse: 0.52782 |  0:04:22s
epoch 84 | loss: 0.24179 | val_0_rmse: 0.48804 | val_1_rmse: 0.49911 |  0:04:25s
epoch 85 | loss: 0.2456  | val_0_rmse: 0.54844 | val_1_rmse: 0.55718 |  0:04:28s
epoch 86 | loss: 0.24603 | val_0_rmse: 0.49336 | val_1_rmse: 0.50818 |  0:04:31s
epoch 87 | loss: 0.24162 | val_0_rmse: 0.58537 | val_1_rmse: 0.59417 |  0:04:35s
epoch 88 | loss: 0.24544 | val_0_rmse: 0.49734 | val_1_rmse: 0.50932 |  0:04:38s
epoch 89 | loss: 0.2427  | val_0_rmse: 0.54834 | val_1_rmse: 0.55853 |  0:04:41s
epoch 90 | loss: 0.24241 | val_0_rmse: 0.4956  | val_1_rmse: 0.50894 |  0:04:44s
epoch 91 | loss: 0.24159 | val_0_rmse: 0.50452 | val_1_rmse: 0.5187  |  0:04:47s
epoch 92 | loss: 0.24438 | val_0_rmse: 0.53775 | val_1_rmse: 0.54714 |  0:04:50s
epoch 93 | loss: 0.24207 | val_0_rmse: 0.52028 | val_1_rmse: 0.53218 |  0:04:53s
epoch 94 | loss: 0.24154 | val_0_rmse: 0.50102 | val_1_rmse: 0.51098 |  0:04:57s
epoch 95 | loss: 0.23806 | val_0_rmse: 0.53906 | val_1_rmse: 0.55599 |  0:05:00s
epoch 96 | loss: 0.24365 | val_0_rmse: 0.49868 | val_1_rmse: 0.51281 |  0:05:03s
epoch 97 | loss: 0.24258 | val_0_rmse: 0.51245 | val_1_rmse: 0.52705 |  0:05:06s
epoch 98 | loss: 0.23974 | val_0_rmse: 0.51932 | val_1_rmse: 0.5314  |  0:05:09s
epoch 99 | loss: 0.23734 | val_0_rmse: 0.56007 | val_1_rmse: 0.57482 |  0:05:12s
epoch 100| loss: 0.23773 | val_0_rmse: 0.54521 | val_1_rmse: 0.55709 |  0:05:15s
epoch 101| loss: 0.2407  | val_0_rmse: 0.5235  | val_1_rmse: 0.54057 |  0:05:18s
epoch 102| loss: 0.23742 | val_0_rmse: 0.50653 | val_1_rmse: 0.52423 |  0:05:22s
epoch 103| loss: 0.23851 | val_0_rmse: 0.48683 | val_1_rmse: 0.49987 |  0:05:25s
epoch 104| loss: 0.23511 | val_0_rmse: 0.50281 | val_1_rmse: 0.5198  |  0:05:28s
epoch 105| loss: 0.23376 | val_0_rmse: 0.48575 | val_1_rmse: 0.50345 |  0:05:31s

Early stopping occured at epoch 105 with best_epoch = 75 and best_val_1_rmse = 0.49533
Best weights from best epoch are automatically used!
ended training at: 07:31:19
Feature importance:
Mean squared error is of 925162008.1201854
Mean absolute error:20688.36481497042
MAPE:0.25878141502884977
R2 score:0.7661176006819077
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:31:19
epoch 0  | loss: 0.80066 | val_0_rmse: 0.69342 | val_1_rmse: 0.69535 |  0:00:03s
epoch 1  | loss: 0.37846 | val_0_rmse: 0.60588 | val_1_rmse: 0.61753 |  0:00:06s
epoch 2  | loss: 0.33484 | val_0_rmse: 0.59636 | val_1_rmse: 0.60552 |  0:00:09s
epoch 3  | loss: 0.32244 | val_0_rmse: 0.68238 | val_1_rmse: 0.6929  |  0:00:12s
epoch 4  | loss: 0.30514 | val_0_rmse: 0.61836 | val_1_rmse: 0.62958 |  0:00:15s
epoch 5  | loss: 0.30427 | val_0_rmse: 0.5527  | val_1_rmse: 0.56617 |  0:00:18s
epoch 6  | loss: 0.28898 | val_0_rmse: 0.55448 | val_1_rmse: 0.56626 |  0:00:21s
epoch 7  | loss: 0.28407 | val_0_rmse: 0.55617 | val_1_rmse: 0.5638  |  0:00:24s
epoch 8  | loss: 0.28201 | val_0_rmse: 0.53213 | val_1_rmse: 0.53747 |  0:00:28s
epoch 9  | loss: 0.27666 | val_0_rmse: 0.58378 | val_1_rmse: 0.59273 |  0:00:31s
epoch 10 | loss: 0.27657 | val_0_rmse: 0.5659  | val_1_rmse: 0.57513 |  0:00:34s
epoch 11 | loss: 0.27744 | val_0_rmse: 0.51423 | val_1_rmse: 0.52573 |  0:00:37s
epoch 12 | loss: 0.26615 | val_0_rmse: 0.53223 | val_1_rmse: 0.54091 |  0:00:40s
epoch 13 | loss: 0.26531 | val_0_rmse: 0.68742 | val_1_rmse: 0.6939  |  0:00:43s
epoch 14 | loss: 0.27031 | val_0_rmse: 0.51733 | val_1_rmse: 0.52572 |  0:00:46s
epoch 15 | loss: 0.26671 | val_0_rmse: 0.51443 | val_1_rmse: 0.52443 |  0:00:49s
epoch 16 | loss: 0.26491 | val_0_rmse: 0.5318  | val_1_rmse: 0.54039 |  0:00:53s
epoch 17 | loss: 0.26854 | val_0_rmse: 0.50811 | val_1_rmse: 0.51917 |  0:00:56s
epoch 18 | loss: 0.26634 | val_0_rmse: 0.58023 | val_1_rmse: 0.59194 |  0:00:59s
epoch 19 | loss: 0.3044  | val_0_rmse: 0.80864 | val_1_rmse: 0.81075 |  0:01:02s
epoch 20 | loss: 0.28541 | val_0_rmse: 0.54967 | val_1_rmse: 0.55852 |  0:01:05s
epoch 21 | loss: 0.28147 | val_0_rmse: 0.54836 | val_1_rmse: 0.56075 |  0:01:08s
epoch 22 | loss: 0.28286 | val_0_rmse: 0.53069 | val_1_rmse: 0.54139 |  0:01:11s
epoch 23 | loss: 0.2754  | val_0_rmse: 0.59184 | val_1_rmse: 0.60292 |  0:01:14s
epoch 24 | loss: 0.27588 | val_0_rmse: 0.57584 | val_1_rmse: 0.58032 |  0:01:17s
epoch 25 | loss: 0.26882 | val_0_rmse: 0.50908 | val_1_rmse: 0.52247 |  0:01:21s
epoch 26 | loss: 0.26569 | val_0_rmse: 0.51154 | val_1_rmse: 0.52241 |  0:01:24s
epoch 27 | loss: 0.26882 | val_0_rmse: 0.54081 | val_1_rmse: 0.55232 |  0:01:27s
epoch 28 | loss: 0.26507 | val_0_rmse: 0.52388 | val_1_rmse: 0.53709 |  0:01:30s
epoch 29 | loss: 0.26528 | val_0_rmse: 0.50687 | val_1_rmse: 0.52088 |  0:01:33s
epoch 30 | loss: 0.26573 | val_0_rmse: 0.51086 | val_1_rmse: 0.52336 |  0:01:36s
epoch 31 | loss: 0.26063 | val_0_rmse: 0.50882 | val_1_rmse: 0.52247 |  0:01:39s
epoch 32 | loss: 0.26269 | val_0_rmse: 0.51224 | val_1_rmse: 0.52509 |  0:01:42s
epoch 33 | loss: 0.26537 | val_0_rmse: 0.50431 | val_1_rmse: 0.5223  |  0:01:45s
epoch 34 | loss: 0.26086 | val_0_rmse: 0.5516  | val_1_rmse: 0.56479 |  0:01:49s
epoch 35 | loss: 0.26059 | val_0_rmse: 0.50654 | val_1_rmse: 0.51894 |  0:01:52s
epoch 36 | loss: 0.26064 | val_0_rmse: 0.52712 | val_1_rmse: 0.53984 |  0:01:55s
epoch 37 | loss: 0.26061 | val_0_rmse: 0.5119  | val_1_rmse: 0.52451 |  0:01:58s
epoch 38 | loss: 0.25802 | val_0_rmse: 0.50316 | val_1_rmse: 0.52905 |  0:02:01s
epoch 39 | loss: 0.25682 | val_0_rmse: 0.52037 | val_1_rmse: 0.54115 |  0:02:04s
epoch 40 | loss: 0.25698 | val_0_rmse: 0.52039 | val_1_rmse: 0.59443 |  0:02:07s
epoch 41 | loss: 0.25821 | val_0_rmse: 0.51219 | val_1_rmse: 0.5491  |  0:02:11s
epoch 42 | loss: 0.25801 | val_0_rmse: 0.50317 | val_1_rmse: 0.55718 |  0:02:14s
epoch 43 | loss: 0.26466 | val_0_rmse: 0.52985 | val_1_rmse: 0.56698 |  0:02:17s
epoch 44 | loss: 0.25841 | val_0_rmse: 0.52541 | val_1_rmse: 0.58503 |  0:02:20s
epoch 45 | loss: 0.25526 | val_0_rmse: 0.55448 | val_1_rmse: 0.73948 |  0:02:23s
epoch 46 | loss: 0.25507 | val_0_rmse: 0.51784 | val_1_rmse: 0.7287  |  0:02:26s
epoch 47 | loss: 0.25219 | val_0_rmse: 0.52104 | val_1_rmse: 0.72843 |  0:02:29s
epoch 48 | loss: 0.25423 | val_0_rmse: 0.5371  | val_1_rmse: 0.66502 |  0:02:33s
epoch 49 | loss: 0.25445 | val_0_rmse: 0.50832 | val_1_rmse: 0.79144 |  0:02:36s
epoch 50 | loss: 0.2576  | val_0_rmse: 0.49796 | val_1_rmse: 0.6671  |  0:02:39s
epoch 51 | loss: 0.25698 | val_0_rmse: 0.5119  | val_1_rmse: 0.8549  |  0:02:42s
epoch 52 | loss: 0.2566  | val_0_rmse: 0.52305 | val_1_rmse: 0.63929 |  0:02:45s
epoch 53 | loss: 0.25424 | val_0_rmse: 0.50287 | val_1_rmse: 0.78338 |  0:02:48s
epoch 54 | loss: 0.25375 | val_0_rmse: 0.49193 | val_1_rmse: 1.08664 |  0:02:51s
epoch 55 | loss: 0.25275 | val_0_rmse: 0.50811 | val_1_rmse: 0.80984 |  0:02:54s
epoch 56 | loss: 0.25476 | val_0_rmse: 0.50581 | val_1_rmse: 0.851   |  0:02:57s
epoch 57 | loss: 0.25178 | val_0_rmse: 0.50177 | val_1_rmse: 0.69965 |  0:03:01s
epoch 58 | loss: 0.25306 | val_0_rmse: 0.50956 | val_1_rmse: 0.74328 |  0:03:04s
epoch 59 | loss: 0.25477 | val_0_rmse: 0.49649 | val_1_rmse: 0.87386 |  0:03:07s
epoch 60 | loss: 0.25808 | val_0_rmse: 0.52014 | val_1_rmse: 0.66769 |  0:03:10s
epoch 61 | loss: 0.25292 | val_0_rmse: 0.5187  | val_1_rmse: 0.75613 |  0:03:13s
epoch 62 | loss: 0.2533  | val_0_rmse: 0.49562 | val_1_rmse: 0.68343 |  0:03:16s
epoch 63 | loss: 0.24977 | val_0_rmse: 0.48891 | val_1_rmse: 0.67867 |  0:03:19s
epoch 64 | loss: 0.25126 | val_0_rmse: 0.50884 | val_1_rmse: 0.74259 |  0:03:22s
epoch 65 | loss: 0.25513 | val_0_rmse: 0.51764 | val_1_rmse: 0.89207 |  0:03:25s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.51894
Best weights from best epoch are automatically used!
ended training at: 07:34:46
Feature importance:
Mean squared error is of 1003040071.7798717
Mean absolute error:21821.396547394685
MAPE:0.26690446414772284
R2 score:0.7434053354952371
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:34:47
epoch 0  | loss: 0.7779  | val_0_rmse: 0.7404  | val_1_rmse: 0.74425 |  0:00:03s
epoch 1  | loss: 0.36496 | val_0_rmse: 0.64435 | val_1_rmse: 0.64917 |  0:00:06s
epoch 2  | loss: 0.33866 | val_0_rmse: 0.60763 | val_1_rmse: 0.61921 |  0:00:09s
epoch 3  | loss: 0.32571 | val_0_rmse: 0.5865  | val_1_rmse: 0.59387 |  0:00:12s
epoch 4  | loss: 0.31916 | val_0_rmse: 0.58172 | val_1_rmse: 0.59376 |  0:00:15s
epoch 5  | loss: 0.31279 | val_0_rmse: 0.56638 | val_1_rmse: 0.57494 |  0:00:18s
epoch 6  | loss: 0.30789 | val_0_rmse: 0.56252 | val_1_rmse: 0.57491 |  0:00:21s
epoch 7  | loss: 0.30466 | val_0_rmse: 0.55162 | val_1_rmse: 0.56216 |  0:00:24s
epoch 8  | loss: 0.30145 | val_0_rmse: 0.54872 | val_1_rmse: 0.55854 |  0:00:28s
epoch 9  | loss: 0.30162 | val_0_rmse: 0.55881 | val_1_rmse: 0.5696  |  0:00:31s
epoch 10 | loss: 0.29933 | val_0_rmse: 0.54418 | val_1_rmse: 0.55337 |  0:00:34s
epoch 11 | loss: 0.29934 | val_0_rmse: 0.54437 | val_1_rmse: 0.55551 |  0:00:37s
epoch 12 | loss: 0.29329 | val_0_rmse: 0.5516  | val_1_rmse: 0.56415 |  0:00:40s
epoch 13 | loss: 0.29463 | val_0_rmse: 0.53472 | val_1_rmse: 0.54847 |  0:00:43s
epoch 14 | loss: 0.29037 | val_0_rmse: 0.53396 | val_1_rmse: 0.54622 |  0:00:46s
epoch 15 | loss: 0.28932 | val_0_rmse: 0.52957 | val_1_rmse: 0.54255 |  0:00:50s
epoch 16 | loss: 0.28784 | val_0_rmse: 0.52706 | val_1_rmse: 0.53662 |  0:00:53s
epoch 17 | loss: 0.28639 | val_0_rmse: 0.5289  | val_1_rmse: 0.5406  |  0:00:56s
epoch 18 | loss: 0.2848  | val_0_rmse: 0.52875 | val_1_rmse: 0.54078 |  0:00:59s
epoch 19 | loss: 0.29578 | val_0_rmse: 0.53851 | val_1_rmse: 0.54943 |  0:01:02s
epoch 20 | loss: 0.28966 | val_0_rmse: 0.53349 | val_1_rmse: 0.54748 |  0:01:05s
epoch 21 | loss: 0.28488 | val_0_rmse: 0.53075 | val_1_rmse: 0.54603 |  0:01:09s
epoch 22 | loss: 0.2904  | val_0_rmse: 0.5423  | val_1_rmse: 0.55189 |  0:01:12s
epoch 23 | loss: 0.28468 | val_0_rmse: 0.52387 | val_1_rmse: 0.5385  |  0:01:15s
epoch 24 | loss: 0.28261 | val_0_rmse: 0.53193 | val_1_rmse: 0.54448 |  0:01:18s
epoch 25 | loss: 0.28343 | val_0_rmse: 0.53768 | val_1_rmse: 0.55078 |  0:01:21s
epoch 26 | loss: 0.28335 | val_0_rmse: 0.52237 | val_1_rmse: 0.53634 |  0:01:24s
epoch 27 | loss: 0.28438 | val_0_rmse: 0.52072 | val_1_rmse: 0.53502 |  0:01:27s
epoch 28 | loss: 0.27898 | val_0_rmse: 0.51959 | val_1_rmse: 0.53417 |  0:01:31s
epoch 29 | loss: 0.27603 | val_0_rmse: 0.5125  | val_1_rmse: 0.52655 |  0:01:34s
epoch 30 | loss: 0.2783  | val_0_rmse: 0.52268 | val_1_rmse: 0.53664 |  0:01:37s
epoch 31 | loss: 0.26973 | val_0_rmse: 0.53293 | val_1_rmse: 0.54367 |  0:01:40s
epoch 32 | loss: 0.27672 | val_0_rmse: 0.51346 | val_1_rmse: 0.52584 |  0:01:43s
epoch 33 | loss: 0.26956 | val_0_rmse: 0.52096 | val_1_rmse: 0.53357 |  0:01:46s
epoch 34 | loss: 0.27374 | val_0_rmse: 0.51961 | val_1_rmse: 0.52707 |  0:01:50s
epoch 35 | loss: 0.26488 | val_0_rmse: 0.50174 | val_1_rmse: 0.51337 |  0:01:53s
epoch 36 | loss: 0.2616  | val_0_rmse: 0.54066 | val_1_rmse: 0.55356 |  0:01:56s
epoch 37 | loss: 0.26697 | val_0_rmse: 0.50792 | val_1_rmse: 0.52136 |  0:01:59s
epoch 38 | loss: 0.26922 | val_0_rmse: 0.54424 | val_1_rmse: 0.55919 |  0:02:02s
epoch 39 | loss: 0.26267 | val_0_rmse: 0.53798 | val_1_rmse: 0.54232 |  0:02:05s
epoch 40 | loss: 0.25988 | val_0_rmse: 0.53104 | val_1_rmse: 0.54387 |  0:02:09s
epoch 41 | loss: 0.2585  | val_0_rmse: 0.52602 | val_1_rmse: 0.53341 |  0:02:12s
epoch 42 | loss: 0.26719 | val_0_rmse: 0.51609 | val_1_rmse: 0.5289  |  0:02:15s
epoch 43 | loss: 0.27154 | val_0_rmse: 0.69429 | val_1_rmse: 0.69578 |  0:02:18s
epoch 44 | loss: 0.29058 | val_0_rmse: 0.59099 | val_1_rmse: 0.54129 |  0:02:21s
epoch 45 | loss: 0.26426 | val_0_rmse: 0.73203 | val_1_rmse: 0.57488 |  0:02:24s
epoch 46 | loss: 0.26063 | val_0_rmse: 0.5627  | val_1_rmse: 0.52335 |  0:02:27s
epoch 47 | loss: 0.26062 | val_0_rmse: 0.72468 | val_1_rmse: 0.64236 |  0:02:31s
epoch 48 | loss: 0.26345 | val_0_rmse: 0.49741 | val_1_rmse: 0.51024 |  0:02:34s
epoch 49 | loss: 0.25691 | val_0_rmse: 0.51267 | val_1_rmse: 0.52673 |  0:02:37s
epoch 50 | loss: 0.25516 | val_0_rmse: 0.49927 | val_1_rmse: 0.52376 |  0:02:40s
epoch 51 | loss: 0.25386 | val_0_rmse: 0.52808 | val_1_rmse: 0.53502 |  0:02:43s
epoch 52 | loss: 0.25532 | val_0_rmse: 0.54066 | val_1_rmse: 0.5544  |  0:02:46s
epoch 53 | loss: 0.25132 | val_0_rmse: 0.4961  | val_1_rmse: 0.50723 |  0:02:49s
epoch 54 | loss: 0.2481  | val_0_rmse: 0.50065 | val_1_rmse: 0.51439 |  0:02:53s
epoch 55 | loss: 0.25146 | val_0_rmse: 0.52906 | val_1_rmse: 0.53868 |  0:02:56s
epoch 56 | loss: 0.25403 | val_0_rmse: 0.49423 | val_1_rmse: 0.50575 |  0:02:59s
epoch 57 | loss: 0.25545 | val_0_rmse: 0.50183 | val_1_rmse: 0.5343  |  0:03:02s
epoch 58 | loss: 0.25691 | val_0_rmse: 0.51248 | val_1_rmse: 0.53322 |  0:03:05s
epoch 59 | loss: 0.24797 | val_0_rmse: 0.54462 | val_1_rmse: 0.5491  |  0:03:08s
epoch 60 | loss: 0.25154 | val_0_rmse: 0.51809 | val_1_rmse: 0.59136 |  0:03:11s
epoch 61 | loss: 0.25208 | val_0_rmse: 0.53036 | val_1_rmse: 0.58753 |  0:03:14s
epoch 62 | loss: 0.25596 | val_0_rmse: 0.51345 | val_1_rmse: 0.56285 |  0:03:18s
epoch 63 | loss: 0.25091 | val_0_rmse: 0.51948 | val_1_rmse: 0.58815 |  0:03:21s
epoch 64 | loss: 0.25179 | val_0_rmse: 0.5302  | val_1_rmse: 0.58069 |  0:03:24s
epoch 65 | loss: 0.24913 | val_0_rmse: 0.50516 | val_1_rmse: 0.59706 |  0:03:27s
epoch 66 | loss: 0.24918 | val_0_rmse: 0.50963 | val_1_rmse: 0.53995 |  0:03:30s
epoch 67 | loss: 0.25331 | val_0_rmse: 0.5099  | val_1_rmse: 0.52052 |  0:03:33s
epoch 68 | loss: 0.24973 | val_0_rmse: 0.5112  | val_1_rmse: 0.53347 |  0:03:36s
epoch 69 | loss: 0.2555  | val_0_rmse: 0.55832 | val_1_rmse: 0.61868 |  0:03:40s
epoch 70 | loss: 0.25056 | val_0_rmse: 0.5142  | val_1_rmse: 0.52827 |  0:03:43s
epoch 71 | loss: 0.24996 | val_0_rmse: 0.50991 | val_1_rmse: 0.56175 |  0:03:46s
epoch 72 | loss: 0.25166 | val_0_rmse: 0.53113 | val_1_rmse: 0.53703 |  0:03:49s
epoch 73 | loss: 0.24846 | val_0_rmse: 0.53445 | val_1_rmse: 0.55079 |  0:03:52s
epoch 74 | loss: 0.2503  | val_0_rmse: 0.49771 | val_1_rmse: 0.51076 |  0:03:55s
epoch 75 | loss: 0.25498 | val_0_rmse: 0.66711 | val_1_rmse: 0.78383 |  0:03:58s
epoch 76 | loss: 0.26506 | val_0_rmse: 0.56343 | val_1_rmse: 0.57576 |  0:04:01s
epoch 77 | loss: 0.26974 | val_0_rmse: 0.53353 | val_1_rmse: 0.54087 |  0:04:05s
epoch 78 | loss: 0.26235 | val_0_rmse: 0.57508 | val_1_rmse: 0.58797 |  0:04:08s
epoch 79 | loss: 0.25218 | val_0_rmse: 0.52705 | val_1_rmse: 0.53759 |  0:04:11s
epoch 80 | loss: 0.2557  | val_0_rmse: 0.50589 | val_1_rmse: 0.52069 |  0:04:14s
epoch 81 | loss: 0.24877 | val_0_rmse: 0.49973 | val_1_rmse: 0.50914 |  0:04:17s
epoch 82 | loss: 0.25024 | val_0_rmse: 0.51401 | val_1_rmse: 0.52628 |  0:04:20s
epoch 83 | loss: 0.25158 | val_0_rmse: 0.72532 | val_1_rmse: 0.73259 |  0:04:23s
epoch 84 | loss: 0.32466 | val_0_rmse: 0.62126 | val_1_rmse: 0.62843 |  0:04:27s
epoch 85 | loss: 0.32591 | val_0_rmse: 0.54556 | val_1_rmse: 0.55569 |  0:04:30s
epoch 86 | loss: 0.3012  | val_0_rmse: 0.53222 | val_1_rmse: 0.53952 |  0:04:33s

Early stopping occured at epoch 86 with best_epoch = 56 and best_val_1_rmse = 0.50575
Best weights from best epoch are automatically used!
ended training at: 07:39:21
Feature importance:
Mean squared error is of 7593621258.823131
Mean absolute error:22623.88502708968
MAPE:0.29751515209029616
R2 score:-0.8802516553787687
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:39:21
epoch 0  | loss: 0.73533 | val_0_rmse: 0.70323 | val_1_rmse: 0.70562 |  0:00:03s
epoch 1  | loss: 0.37903 | val_0_rmse: 0.7373  | val_1_rmse: 0.74517 |  0:00:06s
epoch 2  | loss: 0.34085 | val_0_rmse: 0.64883 | val_1_rmse: 0.64967 |  0:00:09s
epoch 3  | loss: 0.32821 | val_0_rmse: 0.65634 | val_1_rmse: 0.65922 |  0:00:12s
epoch 4  | loss: 0.32739 | val_0_rmse: 0.67101 | val_1_rmse: 0.67702 |  0:00:15s
epoch 5  | loss: 0.32369 | val_0_rmse: 0.6531  | val_1_rmse: 0.65392 |  0:00:18s
epoch 6  | loss: 0.31184 | val_0_rmse: 0.60076 | val_1_rmse: 0.60055 |  0:00:21s
epoch 7  | loss: 0.29951 | val_0_rmse: 0.5649  | val_1_rmse: 0.56685 |  0:00:24s
epoch 8  | loss: 0.30065 | val_0_rmse: 0.56554 | val_1_rmse: 0.56446 |  0:00:28s
epoch 9  | loss: 0.30014 | val_0_rmse: 0.55924 | val_1_rmse: 0.55625 |  0:00:31s
epoch 10 | loss: 0.30345 | val_0_rmse: 0.55919 | val_1_rmse: 0.55907 |  0:00:34s
epoch 11 | loss: 0.29913 | val_0_rmse: 0.55492 | val_1_rmse: 0.55224 |  0:00:37s
epoch 12 | loss: 0.29511 | val_0_rmse: 0.54315 | val_1_rmse: 0.54027 |  0:00:40s
epoch 13 | loss: 0.29432 | val_0_rmse: 0.54473 | val_1_rmse: 0.5432  |  0:00:43s
epoch 14 | loss: 0.28961 | val_0_rmse: 0.53092 | val_1_rmse: 0.52942 |  0:00:46s
epoch 15 | loss: 0.28901 | val_0_rmse: 0.53294 | val_1_rmse: 0.53335 |  0:00:50s
epoch 16 | loss: 0.2893  | val_0_rmse: 0.54516 | val_1_rmse: 0.54437 |  0:00:53s
epoch 17 | loss: 0.28729 | val_0_rmse: 0.54475 | val_1_rmse: 0.54104 |  0:00:56s
epoch 18 | loss: 0.2896  | val_0_rmse: 0.54532 | val_1_rmse: 0.54442 |  0:00:59s
epoch 19 | loss: 0.28716 | val_0_rmse: 0.52359 | val_1_rmse: 0.52038 |  0:01:02s
epoch 20 | loss: 0.28116 | val_0_rmse: 0.52992 | val_1_rmse: 0.52656 |  0:01:05s
epoch 21 | loss: 0.28607 | val_0_rmse: 0.52116 | val_1_rmse: 0.51261 |  0:01:08s
epoch 22 | loss: 0.28358 | val_0_rmse: 0.52763 | val_1_rmse: 0.52079 |  0:01:12s
epoch 23 | loss: 0.27926 | val_0_rmse: 0.54864 | val_1_rmse: 0.54789 |  0:01:15s
epoch 24 | loss: 0.27621 | val_0_rmse: 0.5238  | val_1_rmse: 0.5194  |  0:01:18s
epoch 25 | loss: 0.2776  | val_0_rmse: 0.53336 | val_1_rmse: 0.53229 |  0:01:21s
epoch 26 | loss: 0.27493 | val_0_rmse: 0.51149 | val_1_rmse: 0.50442 |  0:01:24s
epoch 27 | loss: 0.2682  | val_0_rmse: 0.62339 | val_1_rmse: 0.62383 |  0:01:27s
epoch 28 | loss: 0.26834 | val_0_rmse: 0.51373 | val_1_rmse: 0.50745 |  0:01:31s
epoch 29 | loss: 0.26612 | val_0_rmse: 0.51719 | val_1_rmse: 0.50849 |  0:01:34s
epoch 30 | loss: 0.26445 | val_0_rmse: 0.51998 | val_1_rmse: 0.51491 |  0:01:37s
epoch 31 | loss: 0.26135 | val_0_rmse: 0.52979 | val_1_rmse: 0.52631 |  0:01:40s
epoch 32 | loss: 0.25979 | val_0_rmse: 0.51143 | val_1_rmse: 0.50477 |  0:01:43s
epoch 33 | loss: 0.26074 | val_0_rmse: 0.52346 | val_1_rmse: 0.52063 |  0:01:46s
epoch 34 | loss: 0.25772 | val_0_rmse: 0.55832 | val_1_rmse: 0.55182 |  0:01:49s
epoch 35 | loss: 0.25674 | val_0_rmse: 0.53385 | val_1_rmse: 0.52709 |  0:01:52s
epoch 36 | loss: 0.26738 | val_0_rmse: 0.50773 | val_1_rmse: 0.505   |  0:01:56s
epoch 37 | loss: 0.26544 | val_0_rmse: 0.55562 | val_1_rmse: 0.52228 |  0:01:59s
epoch 38 | loss: 0.27565 | val_0_rmse: 0.51554 | val_1_rmse: 0.50559 |  0:02:02s
epoch 39 | loss: 0.30488 | val_0_rmse: 0.67336 | val_1_rmse: 0.66081 |  0:02:05s
epoch 40 | loss: 0.31604 | val_0_rmse: 0.64385 | val_1_rmse: 0.62961 |  0:02:08s
epoch 41 | loss: 0.28797 | val_0_rmse: 0.61143 | val_1_rmse: 0.61149 |  0:02:11s
epoch 42 | loss: 0.27742 | val_0_rmse: 0.5167  | val_1_rmse: 0.5074  |  0:02:14s
epoch 43 | loss: 0.27353 | val_0_rmse: 0.54586 | val_1_rmse: 0.54203 |  0:02:18s
epoch 44 | loss: 0.27544 | val_0_rmse: 0.60035 | val_1_rmse: 0.58692 |  0:02:21s
epoch 45 | loss: 0.27165 | val_0_rmse: 0.51065 | val_1_rmse: 0.50001 |  0:02:24s
epoch 46 | loss: 0.2705  | val_0_rmse: 0.53562 | val_1_rmse: 0.52543 |  0:02:27s
epoch 47 | loss: 0.26667 | val_0_rmse: 0.53033 | val_1_rmse: 0.5242  |  0:02:30s
epoch 48 | loss: 0.26152 | val_0_rmse: 0.5437  | val_1_rmse: 0.53148 |  0:02:33s
epoch 49 | loss: 0.26906 | val_0_rmse: 0.81352 | val_1_rmse: 0.81455 |  0:02:36s
epoch 50 | loss: 0.27274 | val_0_rmse: 0.50884 | val_1_rmse: 0.50164 |  0:02:40s
epoch 51 | loss: 0.26211 | val_0_rmse: 0.51952 | val_1_rmse: 0.50628 |  0:02:43s
epoch 52 | loss: 0.26148 | val_0_rmse: 0.49871 | val_1_rmse: 0.48662 |  0:02:46s
epoch 53 | loss: 0.26114 | val_0_rmse: 0.60213 | val_1_rmse: 0.60229 |  0:02:49s
epoch 54 | loss: 0.26274 | val_0_rmse: 0.53962 | val_1_rmse: 0.53144 |  0:02:52s
epoch 55 | loss: 0.25776 | val_0_rmse: 0.50055 | val_1_rmse: 0.49562 |  0:02:55s
epoch 56 | loss: 0.25989 | val_0_rmse: 0.50546 | val_1_rmse: 0.49741 |  0:02:59s
epoch 57 | loss: 0.2621  | val_0_rmse: 0.54011 | val_1_rmse: 0.53662 |  0:03:02s
epoch 58 | loss: 0.25658 | val_0_rmse: 0.50412 | val_1_rmse: 0.49817 |  0:03:05s
epoch 59 | loss: 0.25747 | val_0_rmse: 0.50537 | val_1_rmse: 0.499   |  0:03:08s
epoch 60 | loss: 0.2567  | val_0_rmse: 0.55158 | val_1_rmse: 0.53962 |  0:03:11s
epoch 61 | loss: 0.27568 | val_0_rmse: 0.55677 | val_1_rmse: 0.55332 |  0:03:14s
epoch 62 | loss: 0.27897 | val_0_rmse: 0.53559 | val_1_rmse: 0.52933 |  0:03:17s
epoch 63 | loss: 0.26637 | val_0_rmse: 0.57219 | val_1_rmse: 0.55808 |  0:03:21s
epoch 64 | loss: 0.26272 | val_0_rmse: 0.52387 | val_1_rmse: 0.4991  |  0:03:24s
epoch 65 | loss: 0.26297 | val_0_rmse: 0.53847 | val_1_rmse: 0.53223 |  0:03:27s
epoch 66 | loss: 0.26068 | val_0_rmse: 0.55772 | val_1_rmse: 0.5491  |  0:03:30s
epoch 67 | loss: 0.25688 | val_0_rmse: 0.49644 | val_1_rmse: 0.48892 |  0:03:33s
epoch 68 | loss: 0.25807 | val_0_rmse: 0.53354 | val_1_rmse: 0.54447 |  0:03:36s
epoch 69 | loss: 0.25349 | val_0_rmse: 0.56872 | val_1_rmse: 0.8717  |  0:03:40s
epoch 70 | loss: 0.25553 | val_0_rmse: 0.50169 | val_1_rmse: 0.49202 |  0:03:43s
epoch 71 | loss: 0.25476 | val_0_rmse: 0.51203 | val_1_rmse: 0.4997  |  0:03:46s
epoch 72 | loss: 0.25494 | val_0_rmse: 0.50825 | val_1_rmse: 0.49795 |  0:03:49s
epoch 73 | loss: 0.25217 | val_0_rmse: 0.51747 | val_1_rmse: 0.51201 |  0:03:52s
epoch 74 | loss: 0.2523  | val_0_rmse: 0.50852 | val_1_rmse: 0.50089 |  0:03:55s
epoch 75 | loss: 0.251   | val_0_rmse: 0.49272 | val_1_rmse: 0.48502 |  0:03:59s
epoch 76 | loss: 0.25333 | val_0_rmse: 0.53869 | val_1_rmse: 0.53232 |  0:04:02s
epoch 77 | loss: 0.25048 | val_0_rmse: 0.49738 | val_1_rmse: 0.4869  |  0:04:05s
epoch 78 | loss: 0.25224 | val_0_rmse: 0.52521 | val_1_rmse: 0.52122 |  0:04:08s
epoch 79 | loss: 0.25395 | val_0_rmse: 0.50427 | val_1_rmse: 0.49881 |  0:04:11s
epoch 80 | loss: 0.24987 | val_0_rmse: 0.52139 | val_1_rmse: 0.51464 |  0:04:14s
epoch 81 | loss: 0.24957 | val_0_rmse: 0.51225 | val_1_rmse: 0.5054  |  0:04:17s
epoch 82 | loss: 0.24486 | val_0_rmse: 0.50321 | val_1_rmse: 0.57589 |  0:04:20s
epoch 83 | loss: 0.24682 | val_0_rmse: 0.48497 | val_1_rmse: 0.51246 |  0:04:24s
epoch 84 | loss: 0.24793 | val_0_rmse: 0.50188 | val_1_rmse: 0.59596 |  0:04:27s
epoch 85 | loss: 0.24889 | val_0_rmse: 0.65625 | val_1_rmse: 0.71604 |  0:04:30s
epoch 86 | loss: 0.25084 | val_0_rmse: 0.52756 | val_1_rmse: 0.64327 |  0:04:33s
epoch 87 | loss: 0.25054 | val_0_rmse: 0.5046  | val_1_rmse: 0.5101  |  0:04:36s
epoch 88 | loss: 0.24993 | val_0_rmse: 0.50773 | val_1_rmse: 0.49636 |  0:04:39s
epoch 89 | loss: 0.24755 | val_0_rmse: 0.50753 | val_1_rmse: 0.50168 |  0:04:43s
epoch 90 | loss: 0.24749 | val_0_rmse: 0.48584 | val_1_rmse: 0.48587 |  0:04:46s
epoch 91 | loss: 0.24766 | val_0_rmse: 0.4883  | val_1_rmse: 0.4821  |  0:04:49s
epoch 92 | loss: 0.24769 | val_0_rmse: 0.48761 | val_1_rmse: 0.48005 |  0:04:52s
epoch 93 | loss: 0.24639 | val_0_rmse: 0.55287 | val_1_rmse: 0.6687  |  0:04:55s
epoch 94 | loss: 0.24454 | val_0_rmse: 0.54099 | val_1_rmse: 0.55749 |  0:04:58s
epoch 95 | loss: 0.24441 | val_0_rmse: 0.5942  | val_1_rmse: 0.58298 |  0:05:02s
epoch 96 | loss: 0.24621 | val_0_rmse: 0.56438 | val_1_rmse: 0.56465 |  0:05:05s
epoch 97 | loss: 0.24645 | val_0_rmse: 0.49251 | val_1_rmse: 0.48409 |  0:05:08s
epoch 98 | loss: 0.24491 | val_0_rmse: 0.5062  | val_1_rmse: 0.50173 |  0:05:11s
epoch 99 | loss: 0.24447 | val_0_rmse: 0.49765 | val_1_rmse: 0.48712 |  0:05:14s
epoch 100| loss: 0.25017 | val_0_rmse: 0.52011 | val_1_rmse: 0.51267 |  0:05:17s
epoch 101| loss: 0.24648 | val_0_rmse: 0.4844  | val_1_rmse: 0.47721 |  0:05:20s
epoch 102| loss: 0.24391 | val_0_rmse: 0.50702 | val_1_rmse: 0.49709 |  0:05:24s
epoch 103| loss: 0.24248 | val_0_rmse: 0.49111 | val_1_rmse: 0.48526 |  0:05:27s
epoch 104| loss: 0.24246 | val_0_rmse: 0.50147 | val_1_rmse: 0.49519 |  0:05:30s
epoch 105| loss: 0.24739 | val_0_rmse: 0.50151 | val_1_rmse: 0.49499 |  0:05:33s
epoch 106| loss: 0.24297 | val_0_rmse: 0.49875 | val_1_rmse: 0.48948 |  0:05:36s
epoch 107| loss: 0.25353 | val_0_rmse: 0.59097 | val_1_rmse: 0.64055 |  0:05:39s
epoch 108| loss: 0.26177 | val_0_rmse: 0.51136 | val_1_rmse: 0.52516 |  0:05:42s
epoch 109| loss: 0.25451 | val_0_rmse: 0.51452 | val_1_rmse: 0.5441  |  0:05:46s
epoch 110| loss: 0.25303 | val_0_rmse: 0.69985 | val_1_rmse: 0.7889  |  0:05:49s
epoch 111| loss: 0.25052 | val_0_rmse: 0.54937 | val_1_rmse: 0.70877 |  0:05:52s
epoch 112| loss: 0.24399 | val_0_rmse: 0.53024 | val_1_rmse: 0.66506 |  0:05:55s
epoch 113| loss: 0.24558 | val_0_rmse: 0.51689 | val_1_rmse: 0.65801 |  0:05:58s
epoch 114| loss: 0.24384 | val_0_rmse: 0.50458 | val_1_rmse: 0.61757 |  0:06:01s
epoch 115| loss: 0.24238 | val_0_rmse: 0.5047  | val_1_rmse: 0.55401 |  0:06:04s
epoch 116| loss: 0.24667 | val_0_rmse: 0.50791 | val_1_rmse: 0.59075 |  0:06:08s
epoch 117| loss: 0.24778 | val_0_rmse: 0.54656 | val_1_rmse: 0.55846 |  0:06:11s
epoch 118| loss: 0.24923 | val_0_rmse: 0.54939 | val_1_rmse: 0.54598 |  0:06:14s
epoch 119| loss: 0.24681 | val_0_rmse: 0.48658 | val_1_rmse: 0.47678 |  0:06:17s
epoch 120| loss: 0.24212 | val_0_rmse: 0.514   | val_1_rmse: 0.50842 |  0:06:20s
epoch 121| loss: 0.24073 | val_0_rmse: 0.55885 | val_1_rmse: 0.82343 |  0:06:23s
epoch 122| loss: 0.24087 | val_0_rmse: 0.52718 | val_1_rmse: 0.76557 |  0:06:26s
epoch 123| loss: 0.24236 | val_0_rmse: 0.53115 | val_1_rmse: 0.60394 |  0:06:30s
epoch 124| loss: 0.24083 | val_0_rmse: 0.5047  | val_1_rmse: 0.519   |  0:06:33s
epoch 125| loss: 0.24043 | val_0_rmse: 0.4936  | val_1_rmse: 0.54691 |  0:06:36s
epoch 126| loss: 0.24306 | val_0_rmse: 0.50099 | val_1_rmse: 0.55634 |  0:06:39s
epoch 127| loss: 0.24062 | val_0_rmse: 0.55893 | val_1_rmse: 0.56843 |  0:06:42s
epoch 128| loss: 0.24318 | val_0_rmse: 0.49882 | val_1_rmse: 0.54874 |  0:06:45s
epoch 129| loss: 0.24131 | val_0_rmse: 0.49832 | val_1_rmse: 0.52629 |  0:06:48s
epoch 130| loss: 0.24001 | val_0_rmse: 0.54526 | val_1_rmse: 0.57707 |  0:06:52s
epoch 131| loss: 0.24006 | val_0_rmse: 0.53279 | val_1_rmse: 0.74908 |  0:06:55s
epoch 132| loss: 0.23991 | val_0_rmse: 0.53657 | val_1_rmse: 0.74441 |  0:06:58s
epoch 133| loss: 0.23882 | val_0_rmse: 0.51919 | val_1_rmse: 0.67331 |  0:07:01s
epoch 134| loss: 0.23956 | val_0_rmse: 0.54546 | val_1_rmse: 0.81024 |  0:07:04s
epoch 135| loss: 0.24406 | val_0_rmse: 0.66915 | val_1_rmse: 0.96466 |  0:07:07s
epoch 136| loss: 0.24086 | val_0_rmse: 0.55327 | val_1_rmse: 0.81538 |  0:07:10s
epoch 137| loss: 0.24126 | val_0_rmse: 0.58185 | val_1_rmse: 0.64682 |  0:07:13s
epoch 138| loss: 0.23999 | val_0_rmse: 0.50756 | val_1_rmse: 0.52981 |  0:07:17s
epoch 139| loss: 0.24214 | val_0_rmse: 0.50537 | val_1_rmse: 0.51092 |  0:07:20s
epoch 140| loss: 0.24102 | val_0_rmse: 0.50638 | val_1_rmse: 0.52112 |  0:07:23s
epoch 141| loss: 0.2417  | val_0_rmse: 0.52482 | val_1_rmse: 0.53486 |  0:07:26s
epoch 142| loss: 0.24169 | val_0_rmse: 0.53972 | val_1_rmse: 0.57314 |  0:07:29s
epoch 143| loss: 0.24146 | val_0_rmse: 0.60269 | val_1_rmse: 0.8055  |  0:07:32s
epoch 144| loss: 0.23967 | val_0_rmse: 0.5074  | val_1_rmse: 0.64922 |  0:07:35s
epoch 145| loss: 0.2386  | val_0_rmse: 0.54342 | val_1_rmse: 0.82072 |  0:07:39s
epoch 146| loss: 0.23951 | val_0_rmse: 0.52988 | val_1_rmse: 0.71015 |  0:07:42s
epoch 147| loss: 0.23887 | val_0_rmse: 0.55682 | val_1_rmse: 0.61942 |  0:07:45s
epoch 148| loss: 0.24335 | val_0_rmse: 0.51894 | val_1_rmse: 0.66816 |  0:07:48s
epoch 149| loss: 0.2383  | val_0_rmse: 0.55674 | val_1_rmse: 0.7977  |  0:07:51s

Early stopping occured at epoch 149 with best_epoch = 119 and best_val_1_rmse = 0.47678
Best weights from best epoch are automatically used!
ended training at: 07:47:14
Feature importance:
Mean squared error is of 955014669.0791931
Mean absolute error:20985.524759683252
MAPE:0.2589552068641354
R2 score:0.7633438292720622
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:47:14
epoch 0  | loss: 0.79039 | val_0_rmse: 0.70985 | val_1_rmse: 0.70547 |  0:00:03s
epoch 1  | loss: 0.35623 | val_0_rmse: 0.61421 | val_1_rmse: 0.61217 |  0:00:06s
epoch 2  | loss: 0.32302 | val_0_rmse: 0.61054 | val_1_rmse: 0.59764 |  0:00:09s
epoch 3  | loss: 0.31538 | val_0_rmse: 0.57384 | val_1_rmse: 0.56966 |  0:00:12s
epoch 4  | loss: 0.31657 | val_0_rmse: 0.55719 | val_1_rmse: 0.5542  |  0:00:15s
epoch 5  | loss: 0.30772 | val_0_rmse: 0.5506  | val_1_rmse: 0.54815 |  0:00:18s
epoch 6  | loss: 0.30022 | val_0_rmse: 0.54078 | val_1_rmse: 0.53628 |  0:00:22s
epoch 7  | loss: 0.29654 | val_0_rmse: 0.5382  | val_1_rmse: 0.53221 |  0:00:25s
epoch 8  | loss: 0.29401 | val_0_rmse: 0.53277 | val_1_rmse: 0.52591 |  0:00:28s
epoch 9  | loss: 0.29426 | val_0_rmse: 0.5423  | val_1_rmse: 0.53597 |  0:00:31s
epoch 10 | loss: 0.28916 | val_0_rmse: 0.53413 | val_1_rmse: 0.52802 |  0:00:34s
epoch 11 | loss: 0.28953 | val_0_rmse: 0.52547 | val_1_rmse: 0.51945 |  0:00:37s
epoch 12 | loss: 0.28642 | val_0_rmse: 0.52391 | val_1_rmse: 0.51692 |  0:00:40s
epoch 13 | loss: 0.28916 | val_0_rmse: 0.53116 | val_1_rmse: 0.52989 |  0:00:44s
epoch 14 | loss: 0.29049 | val_0_rmse: 0.54654 | val_1_rmse: 0.54284 |  0:00:47s
epoch 15 | loss: 0.29499 | val_0_rmse: 0.53083 | val_1_rmse: 0.52393 |  0:00:50s
epoch 16 | loss: 0.29176 | val_0_rmse: 0.53507 | val_1_rmse: 0.53241 |  0:00:53s
epoch 17 | loss: 0.28479 | val_0_rmse: 0.52439 | val_1_rmse: 0.51652 |  0:00:56s
epoch 18 | loss: 0.28529 | val_0_rmse: 0.54027 | val_1_rmse: 0.53723 |  0:00:59s
epoch 19 | loss: 0.28344 | val_0_rmse: 0.52574 | val_1_rmse: 0.52087 |  0:01:02s
epoch 20 | loss: 0.28359 | val_0_rmse: 0.53156 | val_1_rmse: 0.52654 |  0:01:05s
epoch 21 | loss: 0.28668 | val_0_rmse: 0.53536 | val_1_rmse: 0.53229 |  0:01:09s
epoch 22 | loss: 0.28609 | val_0_rmse: 0.57385 | val_1_rmse: 0.56965 |  0:01:12s
epoch 23 | loss: 0.30009 | val_0_rmse: 0.52832 | val_1_rmse: 0.52154 |  0:01:15s
epoch 24 | loss: 0.28474 | val_0_rmse: 0.53082 | val_1_rmse: 0.52397 |  0:01:18s
epoch 25 | loss: 0.28518 | val_0_rmse: 0.53212 | val_1_rmse: 0.5275  |  0:01:21s
epoch 26 | loss: 0.28219 | val_0_rmse: 0.66633 | val_1_rmse: 0.66174 |  0:01:24s
epoch 27 | loss: 0.27322 | val_0_rmse: 0.52747 | val_1_rmse: 0.52269 |  0:01:27s
epoch 28 | loss: 0.27017 | val_0_rmse: 0.51776 | val_1_rmse: 0.51512 |  0:01:30s
epoch 29 | loss: 0.26888 | val_0_rmse: 0.50007 | val_1_rmse: 0.49722 |  0:01:34s
epoch 30 | loss: 0.26301 | val_0_rmse: 0.50801 | val_1_rmse: 0.50419 |  0:01:37s
epoch 31 | loss: 0.2609  | val_0_rmse: 0.50319 | val_1_rmse: 0.49856 |  0:01:40s
epoch 32 | loss: 0.26193 | val_0_rmse: 0.52255 | val_1_rmse: 0.51722 |  0:01:43s
epoch 33 | loss: 0.27002 | val_0_rmse: 0.51465 | val_1_rmse: 0.50915 |  0:01:46s
epoch 34 | loss: 0.26675 | val_0_rmse: 0.51995 | val_1_rmse: 0.51481 |  0:01:49s
epoch 35 | loss: 0.26347 | val_0_rmse: 0.56295 | val_1_rmse: 0.56492 |  0:01:53s
epoch 36 | loss: 0.25746 | val_0_rmse: 0.50248 | val_1_rmse: 0.50179 |  0:01:56s
epoch 37 | loss: 0.25878 | val_0_rmse: 0.52437 | val_1_rmse: 0.52114 |  0:01:59s
epoch 38 | loss: 0.25681 | val_0_rmse: 0.51802 | val_1_rmse: 0.49868 |  0:02:02s
epoch 39 | loss: 0.2556  | val_0_rmse: 0.51227 | val_1_rmse: 0.49396 |  0:02:05s
epoch 40 | loss: 0.25604 | val_0_rmse: 0.51673 | val_1_rmse: 0.5191  |  0:02:08s
epoch 41 | loss: 0.25375 | val_0_rmse: 0.50492 | val_1_rmse: 0.49764 |  0:02:11s
epoch 42 | loss: 0.25301 | val_0_rmse: 0.4939  | val_1_rmse: 0.49282 |  0:02:14s
epoch 43 | loss: 0.25434 | val_0_rmse: 0.50836 | val_1_rmse: 0.49313 |  0:02:17s
epoch 44 | loss: 0.25331 | val_0_rmse: 0.49986 | val_1_rmse: 0.50004 |  0:02:21s
epoch 45 | loss: 0.25614 | val_0_rmse: 0.53497 | val_1_rmse: 0.53414 |  0:02:24s
epoch 46 | loss: 0.25723 | val_0_rmse: 0.5076  | val_1_rmse: 0.50635 |  0:02:27s
epoch 47 | loss: 0.25391 | val_0_rmse: 0.51498 | val_1_rmse: 0.51718 |  0:02:30s
epoch 48 | loss: 0.26332 | val_0_rmse: 0.60122 | val_1_rmse: 0.51889 |  0:02:33s
epoch 49 | loss: 0.25969 | val_0_rmse: 0.52237 | val_1_rmse: 0.5182  |  0:02:36s
epoch 50 | loss: 0.25965 | val_0_rmse: 0.53389 | val_1_rmse: 0.53371 |  0:02:39s
epoch 51 | loss: 0.25457 | val_0_rmse: 0.65573 | val_1_rmse: 0.54392 |  0:02:43s
epoch 52 | loss: 0.25403 | val_0_rmse: 0.64444 | val_1_rmse: 0.57869 |  0:02:46s
epoch 53 | loss: 0.25452 | val_0_rmse: 0.61709 | val_1_rmse: 0.54494 |  0:02:49s
epoch 54 | loss: 0.25125 | val_0_rmse: 0.54211 | val_1_rmse: 0.49992 |  0:02:52s
epoch 55 | loss: 0.25174 | val_0_rmse: 0.53279 | val_1_rmse: 0.49377 |  0:02:55s
epoch 56 | loss: 0.25085 | val_0_rmse: 0.56152 | val_1_rmse: 0.55318 |  0:02:58s
epoch 57 | loss: 0.25187 | val_0_rmse: 0.50564 | val_1_rmse: 0.50924 |  0:03:01s
epoch 58 | loss: 0.25024 | val_0_rmse: 0.49395 | val_1_rmse: 0.49175 |  0:03:04s
epoch 59 | loss: 0.25153 | val_0_rmse: 0.56013 | val_1_rmse: 0.55457 |  0:03:08s
epoch 60 | loss: 0.25095 | val_0_rmse: 0.56086 | val_1_rmse: 0.52876 |  0:03:11s
epoch 61 | loss: 0.24578 | val_0_rmse: 0.63751 | val_1_rmse: 0.54297 |  0:03:14s
epoch 62 | loss: 0.25127 | val_0_rmse: 0.49478 | val_1_rmse: 0.49526 |  0:03:17s
epoch 63 | loss: 0.24868 | val_0_rmse: 0.51669 | val_1_rmse: 0.49612 |  0:03:20s
epoch 64 | loss: 0.24909 | val_0_rmse: 0.49504 | val_1_rmse: 0.49643 |  0:03:23s
epoch 65 | loss: 0.25162 | val_0_rmse: 0.51158 | val_1_rmse: 0.51794 |  0:03:26s
epoch 66 | loss: 0.25379 | val_0_rmse: 0.53608 | val_1_rmse: 0.5048  |  0:03:29s
epoch 67 | loss: 0.24958 | val_0_rmse: 0.81099 | val_1_rmse: 0.49653 |  0:03:32s
epoch 68 | loss: 0.24936 | val_0_rmse: 0.69216 | val_1_rmse: 0.51084 |  0:03:36s
epoch 69 | loss: 0.24808 | val_0_rmse: 0.52201 | val_1_rmse: 0.51656 |  0:03:39s
epoch 70 | loss: 0.24599 | val_0_rmse: 0.49672 | val_1_rmse: 0.49809 |  0:03:42s
epoch 71 | loss: 0.25057 | val_0_rmse: 0.56617 | val_1_rmse: 0.57057 |  0:03:45s
epoch 72 | loss: 0.25002 | val_0_rmse: 0.50238 | val_1_rmse: 0.50761 |  0:03:48s
epoch 73 | loss: 0.24703 | val_0_rmse: 0.52392 | val_1_rmse: 0.52892 |  0:03:51s
epoch 74 | loss: 0.24797 | val_0_rmse: 0.5269  | val_1_rmse: 0.53057 |  0:03:54s
epoch 75 | loss: 0.24578 | val_0_rmse: 0.51158 | val_1_rmse: 0.51203 |  0:03:57s
epoch 76 | loss: 0.2474  | val_0_rmse: 0.50434 | val_1_rmse: 0.50126 |  0:04:01s
epoch 77 | loss: 0.24725 | val_0_rmse: 0.51648 | val_1_rmse: 0.51996 |  0:04:04s
epoch 78 | loss: 0.24481 | val_0_rmse: 0.53256 | val_1_rmse: 0.53807 |  0:04:07s
epoch 79 | loss: 0.24943 | val_0_rmse: 0.5076  | val_1_rmse: 0.511   |  0:04:10s
epoch 80 | loss: 0.24929 | val_0_rmse: 0.61284 | val_1_rmse: 0.6182  |  0:04:13s
epoch 81 | loss: 0.24507 | val_0_rmse: 0.51929 | val_1_rmse: 0.51984 |  0:04:17s
epoch 82 | loss: 0.24803 | val_0_rmse: 0.55608 | val_1_rmse: 0.56404 |  0:04:20s
epoch 83 | loss: 0.24813 | val_0_rmse: 0.50591 | val_1_rmse: 0.50839 |  0:04:23s
epoch 84 | loss: 0.247   | val_0_rmse: 0.54673 | val_1_rmse: 0.55123 |  0:04:26s
epoch 85 | loss: 0.24405 | val_0_rmse: 0.56128 | val_1_rmse: 0.56958 |  0:04:29s
epoch 86 | loss: 0.25049 | val_0_rmse: 0.51344 | val_1_rmse: 0.51495 |  0:04:32s
epoch 87 | loss: 0.24539 | val_0_rmse: 0.53388 | val_1_rmse: 0.53753 |  0:04:35s
epoch 88 | loss: 0.24732 | val_0_rmse: 0.59439 | val_1_rmse: 0.59545 |  0:04:38s

Early stopping occured at epoch 88 with best_epoch = 58 and best_val_1_rmse = 0.49175
Best weights from best epoch are automatically used!
ended training at: 07:51:54
Feature importance:
Mean squared error is of 996668082.3946414
Mean absolute error:20963.575431507
MAPE:0.24558583099245063
R2 score:0.7498242673162507
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:51:54
epoch 0  | loss: 0.80936 | val_0_rmse: 0.7866  | val_1_rmse: 0.78907 |  0:00:03s
epoch 1  | loss: 0.37482 | val_0_rmse: 0.63212 | val_1_rmse: 0.62605 |  0:00:06s
epoch 2  | loss: 0.32473 | val_0_rmse: 0.58827 | val_1_rmse: 0.58673 |  0:00:09s
epoch 3  | loss: 0.32462 | val_0_rmse: 0.58268 | val_1_rmse: 0.57845 |  0:00:12s
epoch 4  | loss: 0.30861 | val_0_rmse: 0.57503 | val_1_rmse: 0.5761  |  0:00:15s
epoch 5  | loss: 0.30379 | val_0_rmse: 0.57837 | val_1_rmse: 0.56889 |  0:00:18s
epoch 6  | loss: 0.30048 | val_0_rmse: 0.5652  | val_1_rmse: 0.56087 |  0:00:22s
epoch 7  | loss: 0.29703 | val_0_rmse: 0.54827 | val_1_rmse: 0.5466  |  0:00:25s
epoch 8  | loss: 0.30067 | val_0_rmse: 0.54759 | val_1_rmse: 0.54841 |  0:00:28s
epoch 9  | loss: 0.29522 | val_0_rmse: 0.53759 | val_1_rmse: 0.54146 |  0:00:31s
epoch 10 | loss: 0.29431 | val_0_rmse: 0.5398  | val_1_rmse: 0.53664 |  0:00:34s
epoch 11 | loss: 0.29418 | val_0_rmse: 0.54109 | val_1_rmse: 0.54102 |  0:00:37s
epoch 12 | loss: 0.29039 | val_0_rmse: 0.52927 | val_1_rmse: 0.53132 |  0:00:40s
epoch 13 | loss: 0.29262 | val_0_rmse: 0.53414 | val_1_rmse: 0.53448 |  0:00:44s
epoch 14 | loss: 0.29857 | val_0_rmse: 0.53568 | val_1_rmse: 0.53463 |  0:00:47s
epoch 15 | loss: 0.29144 | val_0_rmse: 0.53153 | val_1_rmse: 0.52981 |  0:00:50s
epoch 16 | loss: 0.2875  | val_0_rmse: 0.52971 | val_1_rmse: 0.52994 |  0:00:53s
epoch 17 | loss: 0.28697 | val_0_rmse: 0.53393 | val_1_rmse: 0.53431 |  0:00:56s
epoch 18 | loss: 0.2832  | val_0_rmse: 0.52092 | val_1_rmse: 0.52094 |  0:00:59s
epoch 19 | loss: 0.28319 | val_0_rmse: 0.52407 | val_1_rmse: 0.52082 |  0:01:03s
epoch 20 | loss: 0.28144 | val_0_rmse: 0.52151 | val_1_rmse: 0.51944 |  0:01:06s
epoch 21 | loss: 0.27956 | val_0_rmse: 0.52513 | val_1_rmse: 0.52334 |  0:01:09s
epoch 22 | loss: 0.27322 | val_0_rmse: 0.51839 | val_1_rmse: 0.51596 |  0:01:12s
epoch 23 | loss: 0.26881 | val_0_rmse: 0.59277 | val_1_rmse: 0.59734 |  0:01:15s
epoch 24 | loss: 0.27107 | val_0_rmse: 0.51852 | val_1_rmse: 0.51694 |  0:01:18s
epoch 25 | loss: 0.26232 | val_0_rmse: 0.50337 | val_1_rmse: 0.49763 |  0:01:22s
epoch 26 | loss: 0.2639  | val_0_rmse: 0.50487 | val_1_rmse: 0.50001 |  0:01:25s
epoch 27 | loss: 0.26128 | val_0_rmse: 0.53816 | val_1_rmse: 0.52882 |  0:01:28s
epoch 28 | loss: 0.26245 | val_0_rmse: 0.51742 | val_1_rmse: 0.51179 |  0:01:31s
epoch 29 | loss: 0.26931 | val_0_rmse: 0.52628 | val_1_rmse: 0.52747 |  0:01:34s
epoch 30 | loss: 0.25983 | val_0_rmse: 0.55808 | val_1_rmse: 0.55253 |  0:01:37s
epoch 31 | loss: 0.26008 | val_0_rmse: 0.69177 | val_1_rmse: 0.67361 |  0:01:40s
epoch 32 | loss: 0.25946 | val_0_rmse: 0.499   | val_1_rmse: 0.49909 |  0:01:43s
epoch 33 | loss: 0.25815 | val_0_rmse: 0.49497 | val_1_rmse: 0.49264 |  0:01:47s
epoch 34 | loss: 0.2584  | val_0_rmse: 0.50445 | val_1_rmse: 0.50052 |  0:01:50s
epoch 35 | loss: 0.25654 | val_0_rmse: 0.63338 | val_1_rmse: 0.62005 |  0:01:53s
epoch 36 | loss: 0.25955 | val_0_rmse: 0.50543 | val_1_rmse: 0.49998 |  0:01:56s
epoch 37 | loss: 0.25755 | val_0_rmse: 0.55121 | val_1_rmse: 0.54407 |  0:01:59s
epoch 38 | loss: 0.25459 | val_0_rmse: 0.61451 | val_1_rmse: 0.60299 |  0:02:02s
epoch 39 | loss: 0.25635 | val_0_rmse: 0.50117 | val_1_rmse: 0.49844 |  0:02:05s
epoch 40 | loss: 0.25475 | val_0_rmse: 0.59391 | val_1_rmse: 0.58578 |  0:02:09s
epoch 41 | loss: 0.2557  | val_0_rmse: 0.52935 | val_1_rmse: 0.53194 |  0:02:12s
epoch 42 | loss: 0.25742 | val_0_rmse: 0.50575 | val_1_rmse: 0.49926 |  0:02:15s
epoch 43 | loss: 0.25471 | val_0_rmse: 0.5182  | val_1_rmse: 0.51291 |  0:02:18s
epoch 44 | loss: 0.25439 | val_0_rmse: 0.50774 | val_1_rmse: 0.50686 |  0:02:21s
epoch 45 | loss: 0.25144 | val_0_rmse: 0.49706 | val_1_rmse: 0.49809 |  0:02:24s
epoch 46 | loss: 0.25106 | val_0_rmse: 0.51262 | val_1_rmse: 0.50608 |  0:02:27s
epoch 47 | loss: 0.25224 | val_0_rmse: 0.51804 | val_1_rmse: 0.52231 |  0:02:30s
epoch 48 | loss: 0.25839 | val_0_rmse: 0.57555 | val_1_rmse: 0.5698  |  0:02:34s
epoch 49 | loss: 0.25766 | val_0_rmse: 0.50602 | val_1_rmse: 0.50187 |  0:02:37s
epoch 50 | loss: 0.25306 | val_0_rmse: 0.5217  | val_1_rmse: 0.51751 |  0:02:40s
epoch 51 | loss: 0.25382 | val_0_rmse: 0.54868 | val_1_rmse: 0.55044 |  0:02:43s
epoch 52 | loss: 0.25033 | val_0_rmse: 0.49544 | val_1_rmse: 0.49241 |  0:02:46s
epoch 53 | loss: 0.2531  | val_0_rmse: 0.4947  | val_1_rmse: 0.49341 |  0:02:49s
epoch 54 | loss: 0.25353 | val_0_rmse: 0.56837 | val_1_rmse: 0.56193 |  0:02:52s
epoch 55 | loss: 0.25665 | val_0_rmse: 0.5584  | val_1_rmse: 0.56669 |  0:02:56s
epoch 56 | loss: 0.24964 | val_0_rmse: 0.5102  | val_1_rmse: 0.50485 |  0:02:59s
epoch 57 | loss: 0.24898 | val_0_rmse: 0.52596 | val_1_rmse: 0.52377 |  0:03:02s
epoch 58 | loss: 0.25376 | val_0_rmse: 0.52127 | val_1_rmse: 0.52401 |  0:03:05s
epoch 59 | loss: 0.25151 | val_0_rmse: 0.49848 | val_1_rmse: 0.50191 |  0:03:08s
epoch 60 | loss: 0.25661 | val_0_rmse: 0.48922 | val_1_rmse: 0.49022 |  0:03:11s
epoch 61 | loss: 0.24834 | val_0_rmse: 0.48717 | val_1_rmse: 0.48517 |  0:03:14s
epoch 62 | loss: 0.24769 | val_0_rmse: 0.49008 | val_1_rmse: 0.49438 |  0:03:17s
epoch 63 | loss: 0.24694 | val_0_rmse: 0.49034 | val_1_rmse: 0.49192 |  0:03:21s
epoch 64 | loss: 0.24994 | val_0_rmse: 0.51737 | val_1_rmse: 0.51664 |  0:03:24s
epoch 65 | loss: 0.25177 | val_0_rmse: 0.49325 | val_1_rmse: 0.49597 |  0:03:27s
epoch 66 | loss: 0.24949 | val_0_rmse: 0.49126 | val_1_rmse: 0.49425 |  0:03:30s
epoch 67 | loss: 0.24966 | val_0_rmse: 0.49031 | val_1_rmse: 0.493   |  0:03:33s
epoch 68 | loss: 0.24618 | val_0_rmse: 0.48866 | val_1_rmse: 0.49103 |  0:03:36s
epoch 69 | loss: 0.24544 | val_0_rmse: 0.49099 | val_1_rmse: 0.49219 |  0:03:39s
epoch 70 | loss: 0.2471  | val_0_rmse: 0.49617 | val_1_rmse: 0.495   |  0:03:43s
epoch 71 | loss: 0.24976 | val_0_rmse: 0.51795 | val_1_rmse: 0.52262 |  0:03:46s
epoch 72 | loss: 0.24938 | val_0_rmse: 0.51234 | val_1_rmse: 0.50837 |  0:03:49s
epoch 73 | loss: 0.24848 | val_0_rmse: 0.49223 | val_1_rmse: 0.49073 |  0:03:52s
epoch 74 | loss: 0.24594 | val_0_rmse: 0.49613 | val_1_rmse: 0.49218 |  0:03:55s
epoch 75 | loss: 0.24581 | val_0_rmse: 0.49978 | val_1_rmse: 0.50401 |  0:03:58s
epoch 76 | loss: 0.24772 | val_0_rmse: 0.5085  | val_1_rmse: 0.51031 |  0:04:01s
epoch 77 | loss: 0.25062 | val_0_rmse: 0.50981 | val_1_rmse: 0.50894 |  0:04:04s
epoch 78 | loss: 0.24833 | val_0_rmse: 0.53441 | val_1_rmse: 0.52822 |  0:04:08s
epoch 79 | loss: 0.24544 | val_0_rmse: 0.49562 | val_1_rmse: 0.49535 |  0:04:11s
epoch 80 | loss: 0.24659 | val_0_rmse: 0.49595 | val_1_rmse: 0.49655 |  0:04:14s
epoch 81 | loss: 0.24325 | val_0_rmse: 0.49049 | val_1_rmse: 0.49424 |  0:04:17s
epoch 82 | loss: 0.2449  | val_0_rmse: 0.52    | val_1_rmse: 0.51859 |  0:04:20s
epoch 83 | loss: 0.24412 | val_0_rmse: 0.4884  | val_1_rmse: 0.48782 |  0:04:23s
epoch 84 | loss: 0.2444  | val_0_rmse: 0.49571 | val_1_rmse: 0.50099 |  0:04:26s
epoch 85 | loss: 0.24488 | val_0_rmse: 0.531   | val_1_rmse: 0.53958 |  0:04:30s
epoch 86 | loss: 0.24352 | val_0_rmse: 0.49367 | val_1_rmse: 0.50139 |  0:04:33s
epoch 87 | loss: 0.24424 | val_0_rmse: 0.5114  | val_1_rmse: 0.5132  |  0:04:36s
epoch 88 | loss: 0.24233 | val_0_rmse: 0.55383 | val_1_rmse: 0.54911 |  0:04:39s
epoch 89 | loss: 0.24744 | val_0_rmse: 0.50146 | val_1_rmse: 0.50282 |  0:04:42s
epoch 90 | loss: 0.24415 | val_0_rmse: 0.56377 | val_1_rmse: 0.57539 |  0:04:45s
epoch 91 | loss: 0.24395 | val_0_rmse: 0.55874 | val_1_rmse: 0.55336 |  0:04:48s

Early stopping occured at epoch 91 with best_epoch = 61 and best_val_1_rmse = 0.48517
Best weights from best epoch are automatically used!
ended training at: 07:56:44
Feature importance:
Mean squared error is of 969358169.2180187
Mean absolute error:20936.315068389027
MAPE:0.2498880727547681
R2 score:0.7527621117718124
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:56:44
epoch 0  | loss: 0.76141 | val_0_rmse: 0.74484 | val_1_rmse: 0.74734 |  0:00:03s
epoch 1  | loss: 0.34045 | val_0_rmse: 0.55864 | val_1_rmse: 0.55934 |  0:00:06s
epoch 2  | loss: 0.313   | val_0_rmse: 0.56199 | val_1_rmse: 0.56633 |  0:00:09s
epoch 3  | loss: 0.30799 | val_0_rmse: 0.56863 | val_1_rmse: 0.56964 |  0:00:12s
epoch 4  | loss: 0.30759 | val_0_rmse: 0.54954 | val_1_rmse: 0.55917 |  0:00:15s
epoch 5  | loss: 0.31664 | val_0_rmse: 0.55346 | val_1_rmse: 0.55933 |  0:00:18s
epoch 6  | loss: 0.31062 | val_0_rmse: 0.55674 | val_1_rmse: 0.56085 |  0:00:21s
epoch 7  | loss: 0.3017  | val_0_rmse: 0.55926 | val_1_rmse: 0.5567  |  0:00:24s
epoch 8  | loss: 0.29742 | val_0_rmse: 0.53322 | val_1_rmse: 0.53998 |  0:00:28s
epoch 9  | loss: 0.29176 | val_0_rmse: 0.5416  | val_1_rmse: 0.55033 |  0:00:31s
epoch 10 | loss: 0.29478 | val_0_rmse: 0.54181 | val_1_rmse: 0.55507 |  0:00:34s
epoch 11 | loss: 0.29237 | val_0_rmse: 0.54404 | val_1_rmse: 0.55483 |  0:00:37s
epoch 12 | loss: 0.28829 | val_0_rmse: 0.52918 | val_1_rmse: 0.53905 |  0:00:40s
epoch 13 | loss: 0.29083 | val_0_rmse: 0.53509 | val_1_rmse: 0.5447  |  0:00:43s
epoch 14 | loss: 0.29399 | val_0_rmse: 0.5553  | val_1_rmse: 0.56066 |  0:00:46s
epoch 15 | loss: 0.29854 | val_0_rmse: 0.53864 | val_1_rmse: 0.54638 |  0:00:50s
epoch 16 | loss: 0.29475 | val_0_rmse: 0.54003 | val_1_rmse: 0.54147 |  0:00:53s
epoch 17 | loss: 0.29296 | val_0_rmse: 0.53891 | val_1_rmse: 0.54901 |  0:00:56s
epoch 18 | loss: 0.33822 | val_0_rmse: 0.72689 | val_1_rmse: 0.72952 |  0:00:59s
epoch 19 | loss: 0.3449  | val_0_rmse: 0.59098 | val_1_rmse: 0.59899 |  0:01:02s
epoch 20 | loss: 0.31204 | val_0_rmse: 0.5471  | val_1_rmse: 0.55446 |  0:01:05s
epoch 21 | loss: 0.30876 | val_0_rmse: 0.55622 | val_1_rmse: 0.56104 |  0:01:08s
epoch 22 | loss: 0.3108  | val_0_rmse: 0.55951 | val_1_rmse: 0.56816 |  0:01:12s
epoch 23 | loss: 0.30096 | val_0_rmse: 0.53346 | val_1_rmse: 0.54076 |  0:01:15s
epoch 24 | loss: 0.34669 | val_0_rmse: 0.59707 | val_1_rmse: 0.59961 |  0:01:18s
epoch 25 | loss: 0.34256 | val_0_rmse: 0.57492 | val_1_rmse: 0.58651 |  0:01:21s
epoch 26 | loss: 0.32005 | val_0_rmse: 0.5593  | val_1_rmse: 0.57379 |  0:01:24s
epoch 27 | loss: 0.31155 | val_0_rmse: 0.5634  | val_1_rmse: 0.57317 |  0:01:27s
epoch 28 | loss: 0.30299 | val_0_rmse: 0.53969 | val_1_rmse: 0.5493  |  0:01:30s
epoch 29 | loss: 0.29394 | val_0_rmse: 0.54234 | val_1_rmse: 0.54928 |  0:01:34s
epoch 30 | loss: 0.29259 | val_0_rmse: 0.52952 | val_1_rmse: 0.53871 |  0:01:37s
epoch 31 | loss: 0.29343 | val_0_rmse: 0.53889 | val_1_rmse: 0.54856 |  0:01:40s
epoch 32 | loss: 0.2855  | val_0_rmse: 0.53203 | val_1_rmse: 0.54301 |  0:01:43s
epoch 33 | loss: 0.2971  | val_0_rmse: 0.5392  | val_1_rmse: 0.54547 |  0:01:46s
epoch 34 | loss: 0.28988 | val_0_rmse: 0.53153 | val_1_rmse: 0.54359 |  0:01:49s
epoch 35 | loss: 0.28706 | val_0_rmse: 0.52683 | val_1_rmse: 0.53603 |  0:01:52s
epoch 36 | loss: 0.28932 | val_0_rmse: 0.52385 | val_1_rmse: 0.53268 |  0:01:56s
epoch 37 | loss: 0.2887  | val_0_rmse: 0.52628 | val_1_rmse: 0.53655 |  0:01:59s
epoch 38 | loss: 0.2906  | val_0_rmse: 0.5694  | val_1_rmse: 0.57649 |  0:02:02s
epoch 39 | loss: 0.29232 | val_0_rmse: 0.54282 | val_1_rmse: 0.55091 |  0:02:05s
epoch 40 | loss: 0.2941  | val_0_rmse: 0.53522 | val_1_rmse: 0.54378 |  0:02:08s
epoch 41 | loss: 0.28789 | val_0_rmse: 0.53257 | val_1_rmse: 0.54125 |  0:02:11s
epoch 42 | loss: 0.28591 | val_0_rmse: 0.52678 | val_1_rmse: 0.53309 |  0:02:14s
epoch 43 | loss: 0.2833  | val_0_rmse: 0.52524 | val_1_rmse: 0.53364 |  0:02:18s
epoch 44 | loss: 0.28172 | val_0_rmse: 0.52549 | val_1_rmse: 0.5334  |  0:02:21s
epoch 45 | loss: 0.28223 | val_0_rmse: 0.51763 | val_1_rmse: 0.52648 |  0:02:24s
epoch 46 | loss: 0.2844  | val_0_rmse: 0.52974 | val_1_rmse: 0.54063 |  0:02:27s
epoch 47 | loss: 0.28626 | val_0_rmse: 0.52491 | val_1_rmse: 0.53215 |  0:02:30s
epoch 48 | loss: 0.28237 | val_0_rmse: 0.52664 | val_1_rmse: 0.5376  |  0:02:33s
epoch 49 | loss: 0.28486 | val_0_rmse: 0.52697 | val_1_rmse: 0.53368 |  0:02:36s
epoch 50 | loss: 0.28429 | val_0_rmse: 0.52259 | val_1_rmse: 0.53449 |  0:02:40s
epoch 51 | loss: 0.2832  | val_0_rmse: 0.52668 | val_1_rmse: 0.53333 |  0:02:43s
epoch 52 | loss: 0.28123 | val_0_rmse: 0.52146 | val_1_rmse: 0.53039 |  0:02:46s
epoch 53 | loss: 0.28385 | val_0_rmse: 0.52938 | val_1_rmse: 0.53737 |  0:02:49s
epoch 54 | loss: 0.28699 | val_0_rmse: 0.52603 | val_1_rmse: 0.53487 |  0:02:52s
epoch 55 | loss: 0.27704 | val_0_rmse: 0.52776 | val_1_rmse: 0.53568 |  0:02:55s
epoch 56 | loss: 0.28067 | val_0_rmse: 0.52054 | val_1_rmse: 0.52958 |  0:02:58s
epoch 57 | loss: 0.28102 | val_0_rmse: 0.52302 | val_1_rmse: 0.53134 |  0:03:01s
epoch 58 | loss: 0.28757 | val_0_rmse: 0.54791 | val_1_rmse: 0.55571 |  0:03:05s
epoch 59 | loss: 0.28028 | val_0_rmse: 0.51944 | val_1_rmse: 0.52667 |  0:03:08s
epoch 60 | loss: 0.27974 | val_0_rmse: 0.52683 | val_1_rmse: 0.53663 |  0:03:11s
epoch 61 | loss: 0.28044 | val_0_rmse: 0.52407 | val_1_rmse: 0.52982 |  0:03:14s
epoch 62 | loss: 0.27897 | val_0_rmse: 0.51786 | val_1_rmse: 0.52516 |  0:03:17s
epoch 63 | loss: 0.27881 | val_0_rmse: 0.52647 | val_1_rmse: 0.53386 |  0:03:20s
epoch 64 | loss: 0.27814 | val_0_rmse: 0.52054 | val_1_rmse: 0.53077 |  0:03:23s
epoch 65 | loss: 0.27475 | val_0_rmse: 0.52972 | val_1_rmse: 0.53568 |  0:03:27s
epoch 66 | loss: 0.27579 | val_0_rmse: 0.53023 | val_1_rmse: 0.53978 |  0:03:30s
epoch 67 | loss: 0.28101 | val_0_rmse: 0.53348 | val_1_rmse: 0.54307 |  0:03:33s
epoch 68 | loss: 0.2796  | val_0_rmse: 0.53298 | val_1_rmse: 0.5407  |  0:03:36s
epoch 69 | loss: 0.2811  | val_0_rmse: 0.52719 | val_1_rmse: 0.53478 |  0:03:39s
epoch 70 | loss: 0.27926 | val_0_rmse: 0.53654 | val_1_rmse: 0.53919 |  0:03:42s
epoch 71 | loss: 0.28001 | val_0_rmse: 0.52409 | val_1_rmse: 0.52915 |  0:03:45s
epoch 72 | loss: 0.27845 | val_0_rmse: 0.51987 | val_1_rmse: 0.52523 |  0:03:48s
epoch 73 | loss: 0.27195 | val_0_rmse: 0.52591 | val_1_rmse: 0.53099 |  0:03:51s
epoch 74 | loss: 0.27747 | val_0_rmse: 0.53045 | val_1_rmse: 0.53774 |  0:03:55s
epoch 75 | loss: 0.27058 | val_0_rmse: 0.57847 | val_1_rmse: 0.5896  |  0:03:58s
epoch 76 | loss: 0.27107 | val_0_rmse: 0.62525 | val_1_rmse: 0.62764 |  0:04:01s
epoch 77 | loss: 0.27264 | val_0_rmse: 0.565   | val_1_rmse: 0.57394 |  0:04:04s
epoch 78 | loss: 0.28419 | val_0_rmse: 0.53048 | val_1_rmse: 0.53589 |  0:04:07s
epoch 79 | loss: 0.28232 | val_0_rmse: 0.55084 | val_1_rmse: 0.56248 |  0:04:10s
epoch 80 | loss: 0.27981 | val_0_rmse: 0.57269 | val_1_rmse: 0.5821  |  0:04:13s
epoch 81 | loss: 0.27162 | val_0_rmse: 0.60393 | val_1_rmse: 0.6097  |  0:04:17s
epoch 82 | loss: 0.26839 | val_0_rmse: 0.60133 | val_1_rmse: 0.60307 |  0:04:20s
epoch 83 | loss: 0.26896 | val_0_rmse: 0.51631 | val_1_rmse: 0.52681 |  0:04:23s
epoch 84 | loss: 0.26151 | val_0_rmse: 0.5053  | val_1_rmse: 0.51386 |  0:04:26s
epoch 85 | loss: 0.26066 | val_0_rmse: 0.51848 | val_1_rmse: 0.52181 |  0:04:29s
epoch 86 | loss: 0.27193 | val_0_rmse: 0.55072 | val_1_rmse: 0.56202 |  0:04:32s
epoch 87 | loss: 0.2647  | val_0_rmse: 0.55203 | val_1_rmse: 0.56121 |  0:04:35s
epoch 88 | loss: 0.2639  | val_0_rmse: 0.5094  | val_1_rmse: 0.51775 |  0:04:38s
epoch 89 | loss: 0.26141 | val_0_rmse: 0.49915 | val_1_rmse: 0.50628 |  0:04:42s
epoch 90 | loss: 0.26495 | val_0_rmse: 0.66925 | val_1_rmse: 0.67193 |  0:04:45s
epoch 91 | loss: 0.26295 | val_0_rmse: 0.53509 | val_1_rmse: 0.53873 |  0:04:48s
epoch 92 | loss: 0.26123 | val_0_rmse: 0.50166 | val_1_rmse: 0.50971 |  0:04:51s
epoch 93 | loss: 0.25768 | val_0_rmse: 0.50396 | val_1_rmse: 0.51202 |  0:04:54s
epoch 94 | loss: 0.2613  | val_0_rmse: 0.52469 | val_1_rmse: 0.53232 |  0:04:57s
epoch 95 | loss: 0.27292 | val_0_rmse: 0.51285 | val_1_rmse: 0.5199  |  0:05:00s
epoch 96 | loss: 0.26349 | val_0_rmse: 0.55294 | val_1_rmse: 0.55989 |  0:05:03s
epoch 97 | loss: 0.26139 | val_0_rmse: 0.49628 | val_1_rmse: 0.50394 |  0:05:07s
epoch 98 | loss: 0.25744 | val_0_rmse: 0.50022 | val_1_rmse: 0.5076  |  0:05:10s
epoch 99 | loss: 0.25794 | val_0_rmse: 0.52889 | val_1_rmse: 0.53648 |  0:05:13s
epoch 100| loss: 0.2592  | val_0_rmse: 0.51647 | val_1_rmse: 0.51722 |  0:05:16s
epoch 101| loss: 0.25827 | val_0_rmse: 0.5089  | val_1_rmse: 0.51727 |  0:05:19s
epoch 102| loss: 0.25643 | val_0_rmse: 0.51732 | val_1_rmse: 0.52651 |  0:05:22s
epoch 103| loss: 0.25707 | val_0_rmse: 0.49897 | val_1_rmse: 0.50703 |  0:05:25s
epoch 104| loss: 0.25659 | val_0_rmse: 0.50544 | val_1_rmse: 0.51361 |  0:05:28s
epoch 105| loss: 0.25711 | val_0_rmse: 0.52176 | val_1_rmse: 0.53258 |  0:05:32s
epoch 106| loss: 0.2586  | val_0_rmse: 0.51385 | val_1_rmse: 0.51941 |  0:05:35s
epoch 107| loss: 0.25584 | val_0_rmse: 0.53698 | val_1_rmse: 0.54428 |  0:05:38s
epoch 108| loss: 0.26139 | val_0_rmse: 0.49752 | val_1_rmse: 0.50653 |  0:05:41s
epoch 109| loss: 0.25858 | val_0_rmse: 0.54731 | val_1_rmse: 0.5563  |  0:05:44s
epoch 110| loss: 0.26231 | val_0_rmse: 0.51399 | val_1_rmse: 0.52746 |  0:05:47s
epoch 111| loss: 0.25765 | val_0_rmse: 0.49795 | val_1_rmse: 0.50814 |  0:05:50s
epoch 112| loss: 0.2554  | val_0_rmse: 0.50919 | val_1_rmse: 0.51871 |  0:05:54s
epoch 113| loss: 0.25886 | val_0_rmse: 0.51719 | val_1_rmse: 0.53142 |  0:05:57s
epoch 114| loss: 0.25649 | val_0_rmse: 0.49664 | val_1_rmse: 0.50461 |  0:06:00s
epoch 115| loss: 0.25553 | val_0_rmse: 0.49494 | val_1_rmse: 0.50528 |  0:06:03s
epoch 116| loss: 0.25522 | val_0_rmse: 0.50182 | val_1_rmse: 0.50944 |  0:06:06s
epoch 117| loss: 0.25221 | val_0_rmse: 0.50288 | val_1_rmse: 0.51166 |  0:06:09s
epoch 118| loss: 0.25415 | val_0_rmse: 0.53148 | val_1_rmse: 0.5411  |  0:06:12s
epoch 119| loss: 0.25286 | val_0_rmse: 0.51475 | val_1_rmse: 0.52074 |  0:06:15s
epoch 120| loss: 0.2571  | val_0_rmse: 0.50707 | val_1_rmse: 0.51797 |  0:06:19s
epoch 121| loss: 0.25612 | val_0_rmse: 0.49843 | val_1_rmse: 0.50686 |  0:06:22s
epoch 122| loss: 0.25515 | val_0_rmse: 0.51462 | val_1_rmse: 0.52528 |  0:06:25s
epoch 123| loss: 0.25379 | val_0_rmse: 0.49807 | val_1_rmse: 0.50939 |  0:06:28s
epoch 124| loss: 0.25504 | val_0_rmse: 0.54343 | val_1_rmse: 0.55418 |  0:06:31s
epoch 125| loss: 0.25701 | val_0_rmse: 0.52062 | val_1_rmse: 0.53439 |  0:06:34s
epoch 126| loss: 0.25442 | val_0_rmse: 0.51603 | val_1_rmse: 0.52779 |  0:06:37s
epoch 127| loss: 0.25097 | val_0_rmse: 0.49967 | val_1_rmse: 0.51048 |  0:06:41s

Early stopping occured at epoch 127 with best_epoch = 97 and best_val_1_rmse = 0.50394
Best weights from best epoch are automatically used!
ended training at: 08:03:26
Feature importance:
Mean squared error is of 991910204.8026838
Mean absolute error:21620.93761705387
MAPE:0.2714341390921833
R2 score:0.7501389479966183
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:03:27
epoch 0  | loss: 0.75067 | val_0_rmse: 0.67632 | val_1_rmse: 0.66843 |  0:00:03s
epoch 1  | loss: 0.36199 | val_0_rmse: 0.59672 | val_1_rmse: 0.58761 |  0:00:06s
epoch 2  | loss: 0.32895 | val_0_rmse: 0.58361 | val_1_rmse: 0.57758 |  0:00:09s
epoch 3  | loss: 0.31745 | val_0_rmse: 0.60421 | val_1_rmse: 0.5919  |  0:00:12s
epoch 4  | loss: 0.32041 | val_0_rmse: 0.5933  | val_1_rmse: 0.58538 |  0:00:15s
epoch 5  | loss: 0.31862 | val_0_rmse: 0.55652 | val_1_rmse: 0.54973 |  0:00:18s
epoch 6  | loss: 0.3112  | val_0_rmse: 0.57882 | val_1_rmse: 0.57101 |  0:00:21s
epoch 7  | loss: 0.30942 | val_0_rmse: 0.56453 | val_1_rmse: 0.56014 |  0:00:25s
epoch 8  | loss: 0.30033 | val_0_rmse: 0.54596 | val_1_rmse: 0.53961 |  0:00:28s
epoch 9  | loss: 0.29661 | val_0_rmse: 0.55689 | val_1_rmse: 0.5547  |  0:00:31s
epoch 10 | loss: 0.29274 | val_0_rmse: 0.53877 | val_1_rmse: 0.53182 |  0:00:34s
epoch 11 | loss: 0.29093 | val_0_rmse: 0.54114 | val_1_rmse: 0.53982 |  0:00:37s
epoch 12 | loss: 0.28545 | val_0_rmse: 0.54824 | val_1_rmse: 0.54171 |  0:00:40s
epoch 13 | loss: 0.28351 | val_0_rmse: 0.52891 | val_1_rmse: 0.52368 |  0:00:43s
epoch 14 | loss: 0.28763 | val_0_rmse: 0.53999 | val_1_rmse: 0.5319  |  0:00:46s
epoch 15 | loss: 0.28419 | val_0_rmse: 0.58463 | val_1_rmse: 0.57383 |  0:00:50s
epoch 16 | loss: 0.2833  | val_0_rmse: 0.52356 | val_1_rmse: 0.51762 |  0:00:53s
epoch 17 | loss: 0.29885 | val_0_rmse: 0.65089 | val_1_rmse: 0.64388 |  0:00:56s
epoch 18 | loss: 0.29438 | val_0_rmse: 0.57871 | val_1_rmse: 0.56377 |  0:00:59s
epoch 19 | loss: 0.3107  | val_0_rmse: 0.62561 | val_1_rmse: 0.62079 |  0:01:02s
epoch 20 | loss: 0.30409 | val_0_rmse: 0.5475  | val_1_rmse: 0.54468 |  0:01:05s
epoch 21 | loss: 0.29347 | val_0_rmse: 0.53294 | val_1_rmse: 0.52803 |  0:01:08s
epoch 22 | loss: 0.2858  | val_0_rmse: 0.52562 | val_1_rmse: 0.51939 |  0:01:12s
epoch 23 | loss: 0.28213 | val_0_rmse: 0.5213  | val_1_rmse: 0.51656 |  0:01:15s
epoch 24 | loss: 0.28947 | val_0_rmse: 0.69884 | val_1_rmse: 0.68925 |  0:01:18s
epoch 25 | loss: 0.295   | val_0_rmse: 0.64645 | val_1_rmse: 0.63587 |  0:01:21s
epoch 26 | loss: 0.28108 | val_0_rmse: 0.54371 | val_1_rmse: 0.53679 |  0:01:24s
epoch 27 | loss: 0.27762 | val_0_rmse: 0.74779 | val_1_rmse: 1.11621 |  0:01:27s
epoch 28 | loss: 0.28315 | val_0_rmse: 0.60514 | val_1_rmse: 0.60218 |  0:01:31s
epoch 29 | loss: 0.28394 | val_0_rmse: 0.5603  | val_1_rmse: 0.55433 |  0:01:34s
epoch 30 | loss: 0.27503 | val_0_rmse: 0.53051 | val_1_rmse: 0.52477 |  0:01:37s
epoch 31 | loss: 0.27421 | val_0_rmse: 0.52301 | val_1_rmse: 0.51595 |  0:01:40s
epoch 32 | loss: 0.27392 | val_0_rmse: 0.52025 | val_1_rmse: 0.51451 |  0:01:43s
epoch 33 | loss: 0.27335 | val_0_rmse: 0.51344 | val_1_rmse: 0.50616 |  0:01:46s
epoch 34 | loss: 0.26815 | val_0_rmse: 0.51545 | val_1_rmse: 0.50838 |  0:01:49s
epoch 35 | loss: 0.269   | val_0_rmse: 0.50938 | val_1_rmse: 0.50301 |  0:01:53s
epoch 36 | loss: 0.26878 | val_0_rmse: 0.54085 | val_1_rmse: 0.53497 |  0:01:56s
epoch 37 | loss: 0.26712 | val_0_rmse: 0.51419 | val_1_rmse: 0.50858 |  0:01:59s
epoch 38 | loss: 0.26882 | val_0_rmse: 0.51248 | val_1_rmse: 0.50564 |  0:02:02s
epoch 39 | loss: 0.26829 | val_0_rmse: 0.52508 | val_1_rmse: 0.5195  |  0:02:05s
epoch 40 | loss: 0.27081 | val_0_rmse: 0.51155 | val_1_rmse: 0.50481 |  0:02:08s
epoch 41 | loss: 0.2683  | val_0_rmse: 0.51364 | val_1_rmse: 0.50499 |  0:02:11s
epoch 42 | loss: 0.26814 | val_0_rmse: 0.54293 | val_1_rmse: 0.53649 |  0:02:14s
epoch 43 | loss: 0.26849 | val_0_rmse: 0.50529 | val_1_rmse: 0.4991  |  0:02:17s
epoch 44 | loss: 0.26534 | val_0_rmse: 0.5243  | val_1_rmse: 0.51614 |  0:02:21s
epoch 45 | loss: 0.26975 | val_0_rmse: 0.50962 | val_1_rmse: 0.50335 |  0:02:24s
epoch 46 | loss: 0.26525 | val_0_rmse: 0.50892 | val_1_rmse: 0.49991 |  0:02:27s
epoch 47 | loss: 0.2672  | val_0_rmse: 0.51798 | val_1_rmse: 0.51256 |  0:02:30s
epoch 48 | loss: 0.26678 | val_0_rmse: 0.52614 | val_1_rmse: 0.521   |  0:02:33s
epoch 49 | loss: 0.26293 | val_0_rmse: 0.52229 | val_1_rmse: 0.51124 |  0:02:36s
epoch 50 | loss: 0.26298 | val_0_rmse: 0.50809 | val_1_rmse: 0.49959 |  0:02:39s
epoch 51 | loss: 0.26259 | val_0_rmse: 0.51557 | val_1_rmse: 0.50905 |  0:02:42s
epoch 52 | loss: 0.26123 | val_0_rmse: 0.50627 | val_1_rmse: 0.49847 |  0:02:45s
epoch 53 | loss: 0.26484 | val_0_rmse: 0.49851 | val_1_rmse: 0.4915  |  0:02:49s
epoch 54 | loss: 0.25959 | val_0_rmse: 0.50105 | val_1_rmse: 0.49507 |  0:02:52s
epoch 55 | loss: 0.2618  | val_0_rmse: 0.53944 | val_1_rmse: 0.53416 |  0:02:55s
epoch 56 | loss: 0.26446 | val_0_rmse: 0.50148 | val_1_rmse: 0.4938  |  0:02:58s
epoch 57 | loss: 0.2637  | val_0_rmse: 0.50049 | val_1_rmse: 0.49251 |  0:03:01s
epoch 58 | loss: 0.26194 | val_0_rmse: 0.51124 | val_1_rmse: 0.50374 |  0:03:04s
epoch 59 | loss: 0.26275 | val_0_rmse: 0.50106 | val_1_rmse: 0.49498 |  0:03:08s
epoch 60 | loss: 0.26255 | val_0_rmse: 0.4979  | val_1_rmse: 0.49259 |  0:03:11s
epoch 61 | loss: 0.25969 | val_0_rmse: 0.50279 | val_1_rmse: 0.49542 |  0:03:14s
epoch 62 | loss: 0.25853 | val_0_rmse: 0.50069 | val_1_rmse: 0.49153 |  0:03:17s
epoch 63 | loss: 0.26226 | val_0_rmse: 0.51571 | val_1_rmse: 0.50987 |  0:03:20s
epoch 64 | loss: 0.25953 | val_0_rmse: 0.50143 | val_1_rmse: 0.49585 |  0:03:23s
epoch 65 | loss: 0.26017 | val_0_rmse: 0.52332 | val_1_rmse: 0.51944 |  0:03:26s
epoch 66 | loss: 0.25894 | val_0_rmse: 0.49993 | val_1_rmse: 0.49469 |  0:03:29s
epoch 67 | loss: 0.26235 | val_0_rmse: 0.51063 | val_1_rmse: 0.50343 |  0:03:33s
epoch 68 | loss: 0.25763 | val_0_rmse: 0.49588 | val_1_rmse: 0.48963 |  0:03:36s
epoch 69 | loss: 0.25666 | val_0_rmse: 0.50299 | val_1_rmse: 0.49906 |  0:03:39s
epoch 70 | loss: 0.25762 | val_0_rmse: 0.50534 | val_1_rmse: 0.50049 |  0:03:42s
epoch 71 | loss: 0.25971 | val_0_rmse: 0.50198 | val_1_rmse: 0.49489 |  0:03:45s
epoch 72 | loss: 0.25832 | val_0_rmse: 0.73784 | val_1_rmse: 1.18154 |  0:03:48s
epoch 73 | loss: 0.26147 | val_0_rmse: 0.5363  | val_1_rmse: 1.17655 |  0:03:51s
epoch 74 | loss: 0.26148 | val_0_rmse: 0.66563 | val_1_rmse: 1.00345 |  0:03:54s
epoch 75 | loss: 0.25628 | val_0_rmse: 0.81788 | val_1_rmse: 1.38144 |  0:03:58s
epoch 76 | loss: 0.2579  | val_0_rmse: 0.70209 | val_1_rmse: 1.08111 |  0:04:01s
epoch 77 | loss: 0.25956 | val_0_rmse: 0.7242  | val_1_rmse: 1.15827 |  0:04:04s
epoch 78 | loss: 0.25714 | val_0_rmse: 0.77201 | val_1_rmse: 1.28194 |  0:04:07s
epoch 79 | loss: 0.27775 | val_0_rmse: 1.08436 | val_1_rmse: 1.67156 |  0:04:10s
epoch 80 | loss: 0.2685  | val_0_rmse: 0.86168 | val_1_rmse: 1.46851 |  0:04:13s
epoch 81 | loss: 0.26858 | val_0_rmse: 0.88777 | val_1_rmse: 1.52382 |  0:04:16s
epoch 82 | loss: 0.26536 | val_0_rmse: 0.99329 | val_1_rmse: 1.77093 |  0:04:19s
epoch 83 | loss: 0.26249 | val_0_rmse: 1.04116 | val_1_rmse: 1.87576 |  0:04:23s
epoch 84 | loss: 0.25924 | val_0_rmse: 0.53383 | val_1_rmse: 0.51614 |  0:04:26s
epoch 85 | loss: 0.26693 | val_0_rmse: 0.52393 | val_1_rmse: 0.51964 |  0:04:29s
epoch 86 | loss: 0.26058 | val_0_rmse: 0.51059 | val_1_rmse: 0.50582 |  0:04:32s
epoch 87 | loss: 0.26057 | val_0_rmse: 0.50667 | val_1_rmse: 0.50095 |  0:04:35s
epoch 88 | loss: 0.26029 | val_0_rmse: 0.50213 | val_1_rmse: 0.49771 |  0:04:38s
epoch 89 | loss: 0.2601  | val_0_rmse: 0.50515 | val_1_rmse: 0.50106 |  0:04:42s
epoch 90 | loss: 0.26028 | val_0_rmse: 0.50695 | val_1_rmse: 0.50506 |  0:04:45s
epoch 91 | loss: 0.2576  | val_0_rmse: 0.4987  | val_1_rmse: 0.49563 |  0:04:48s
epoch 92 | loss: 0.25432 | val_0_rmse: 0.57481 | val_1_rmse: 0.57333 |  0:04:51s
epoch 93 | loss: 0.256   | val_0_rmse: 0.50989 | val_1_rmse: 0.50898 |  0:04:54s
epoch 94 | loss: 0.25572 | val_0_rmse: 0.50389 | val_1_rmse: 0.50158 |  0:04:57s
epoch 95 | loss: 0.25527 | val_0_rmse: 0.50191 | val_1_rmse: 0.49954 |  0:05:00s
epoch 96 | loss: 0.25138 | val_0_rmse: 0.52249 | val_1_rmse: 0.51819 |  0:05:04s
epoch 97 | loss: 0.25407 | val_0_rmse: 0.50295 | val_1_rmse: 0.49828 |  0:05:07s
epoch 98 | loss: 0.25556 | val_0_rmse: 0.51314 | val_1_rmse: 0.51052 |  0:05:10s

Early stopping occured at epoch 98 with best_epoch = 68 and best_val_1_rmse = 0.48963
Best weights from best epoch are automatically used!
ended training at: 08:08:38
Feature importance:
Mean squared error is of 1020804006.9343184
Mean absolute error:21676.65818623953
MAPE:0.2570508059621585
R2 score:0.7471732731005792
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:08:38
epoch 0  | loss: 0.74681 | val_0_rmse: 0.68306 | val_1_rmse: 0.68836 |  0:00:03s
epoch 1  | loss: 0.38628 | val_0_rmse: 0.63613 | val_1_rmse: 0.63794 |  0:00:06s
epoch 2  | loss: 0.36487 | val_0_rmse: 0.64528 | val_1_rmse: 0.64588 |  0:00:09s
epoch 3  | loss: 0.34014 | val_0_rmse: 0.5931  | val_1_rmse: 0.59326 |  0:00:12s
epoch 4  | loss: 0.32856 | val_0_rmse: 0.57708 | val_1_rmse: 0.57272 |  0:00:15s
epoch 5  | loss: 0.32083 | val_0_rmse: 0.56839 | val_1_rmse: 0.5665  |  0:00:18s
epoch 6  | loss: 0.31486 | val_0_rmse: 0.56883 | val_1_rmse: 0.56974 |  0:00:22s
epoch 7  | loss: 0.30582 | val_0_rmse: 0.56493 | val_1_rmse: 0.56485 |  0:00:25s
epoch 8  | loss: 0.31588 | val_0_rmse: 0.5698  | val_1_rmse: 0.57022 |  0:00:28s
epoch 9  | loss: 0.31128 | val_0_rmse: 0.55527 | val_1_rmse: 0.55113 |  0:00:31s
epoch 10 | loss: 0.30324 | val_0_rmse: 0.55021 | val_1_rmse: 0.55268 |  0:00:34s
epoch 11 | loss: 0.29826 | val_0_rmse: 0.53915 | val_1_rmse: 0.54052 |  0:00:37s
epoch 12 | loss: 0.29967 | val_0_rmse: 0.54289 | val_1_rmse: 0.54162 |  0:00:40s
epoch 13 | loss: 0.29359 | val_0_rmse: 0.5561  | val_1_rmse: 0.55419 |  0:00:44s
epoch 14 | loss: 0.29798 | val_0_rmse: 0.56536 | val_1_rmse: 0.55837 |  0:00:47s
epoch 15 | loss: 0.30919 | val_0_rmse: 0.55173 | val_1_rmse: 0.54486 |  0:00:50s
epoch 16 | loss: 0.30398 | val_0_rmse: 0.52898 | val_1_rmse: 0.52602 |  0:00:53s
epoch 17 | loss: 0.29404 | val_0_rmse: 0.52586 | val_1_rmse: 0.52267 |  0:00:56s
epoch 18 | loss: 0.29253 | val_0_rmse: 0.54908 | val_1_rmse: 0.54701 |  0:00:59s
epoch 19 | loss: 0.29339 | val_0_rmse: 0.53063 | val_1_rmse: 0.52334 |  0:01:03s
epoch 20 | loss: 0.29363 | val_0_rmse: 0.52606 | val_1_rmse: 0.52543 |  0:01:06s
epoch 21 | loss: 0.2818  | val_0_rmse: 0.55593 | val_1_rmse: 0.55313 |  0:01:09s
epoch 22 | loss: 0.27866 | val_0_rmse: 0.53708 | val_1_rmse: 0.53081 |  0:01:12s
epoch 23 | loss: 0.28035 | val_0_rmse: 0.62221 | val_1_rmse: 0.62527 |  0:01:15s
epoch 24 | loss: 0.27399 | val_0_rmse: 0.54799 | val_1_rmse: 0.55104 |  0:01:18s
epoch 25 | loss: 0.27843 | val_0_rmse: 0.57584 | val_1_rmse: 0.56606 |  0:01:21s
epoch 26 | loss: 0.27427 | val_0_rmse: 0.55057 | val_1_rmse: 0.55145 |  0:01:25s
epoch 27 | loss: 0.27377 | val_0_rmse: 0.52602 | val_1_rmse: 0.52546 |  0:01:28s
epoch 28 | loss: 0.26967 | val_0_rmse: 0.52725 | val_1_rmse: 0.52327 |  0:01:31s
epoch 29 | loss: 0.28241 | val_0_rmse: 0.54362 | val_1_rmse: 0.53814 |  0:01:34s
epoch 30 | loss: 0.30217 | val_0_rmse: 0.58029 | val_1_rmse: 0.58628 |  0:01:37s
epoch 31 | loss: 0.29242 | val_0_rmse: 0.54827 | val_1_rmse: 0.54252 |  0:01:40s
epoch 32 | loss: 0.27765 | val_0_rmse: 0.51799 | val_1_rmse: 0.51859 |  0:01:43s
epoch 33 | loss: 0.28642 | val_0_rmse: 0.56877 | val_1_rmse: 0.57528 |  0:01:46s
epoch 34 | loss: 0.28696 | val_0_rmse: 0.57565 | val_1_rmse: 0.5781  |  0:01:50s
epoch 35 | loss: 0.28512 | val_0_rmse: 0.53103 | val_1_rmse: 0.53089 |  0:01:53s
epoch 36 | loss: 0.2855  | val_0_rmse: 0.52281 | val_1_rmse: 0.51874 |  0:01:56s
epoch 37 | loss: 0.26783 | val_0_rmse: 0.52077 | val_1_rmse: 0.51454 |  0:01:59s
epoch 38 | loss: 0.26591 | val_0_rmse: 0.50879 | val_1_rmse: 0.50839 |  0:02:02s
epoch 39 | loss: 0.26758 | val_0_rmse: 0.51584 | val_1_rmse: 0.52007 |  0:02:05s
epoch 40 | loss: 0.2642  | val_0_rmse: 0.50483 | val_1_rmse: 0.50546 |  0:02:09s
epoch 41 | loss: 0.27087 | val_0_rmse: 0.55082 | val_1_rmse: 0.54778 |  0:02:12s
epoch 42 | loss: 0.29094 | val_0_rmse: 0.56399 | val_1_rmse: 0.56359 |  0:02:15s
epoch 43 | loss: 0.31093 | val_0_rmse: 0.64582 | val_1_rmse: 0.63808 |  0:02:18s
epoch 44 | loss: 0.31843 | val_0_rmse: 0.56287 | val_1_rmse: 0.56041 |  0:02:21s
epoch 45 | loss: 0.30451 | val_0_rmse: 0.53606 | val_1_rmse: 0.53652 |  0:02:24s
epoch 46 | loss: 0.28123 | val_0_rmse: 0.77001 | val_1_rmse: 0.77757 |  0:02:27s
epoch 47 | loss: 0.27421 | val_0_rmse: 0.55745 | val_1_rmse: 0.56035 |  0:02:30s
epoch 48 | loss: 0.27465 | val_0_rmse: 0.50663 | val_1_rmse: 0.5061  |  0:02:34s
epoch 49 | loss: 0.27041 | val_0_rmse: 0.5901  | val_1_rmse: 0.58681 |  0:02:37s
epoch 50 | loss: 0.26785 | val_0_rmse: 0.5609  | val_1_rmse: 0.56601 |  0:02:40s
epoch 51 | loss: 0.2792  | val_0_rmse: 0.63758 | val_1_rmse: 0.63694 |  0:02:43s
epoch 52 | loss: 0.28564 | val_0_rmse: 0.54365 | val_1_rmse: 0.54589 |  0:02:46s
epoch 53 | loss: 0.27641 | val_0_rmse: 0.51451 | val_1_rmse: 0.51836 |  0:02:49s
epoch 54 | loss: 0.28203 | val_0_rmse: 0.51889 | val_1_rmse: 0.51939 |  0:02:52s
epoch 55 | loss: 0.2671  | val_0_rmse: 0.50907 | val_1_rmse: 0.50667 |  0:02:56s
epoch 56 | loss: 0.26417 | val_0_rmse: 0.53038 | val_1_rmse: 0.52854 |  0:02:59s
epoch 57 | loss: 0.26453 | val_0_rmse: 0.56248 | val_1_rmse: 0.56773 |  0:03:02s
epoch 58 | loss: 0.25963 | val_0_rmse: 0.49935 | val_1_rmse: 0.50134 |  0:03:05s
epoch 59 | loss: 0.25899 | val_0_rmse: 0.50988 | val_1_rmse: 0.50623 |  0:03:08s
epoch 60 | loss: 0.2553  | val_0_rmse: 0.52452 | val_1_rmse: 0.5225  |  0:03:11s
epoch 61 | loss: 0.25583 | val_0_rmse: 0.49282 | val_1_rmse: 0.49184 |  0:03:15s
epoch 62 | loss: 0.25772 | val_0_rmse: 0.55327 | val_1_rmse: 0.55441 |  0:03:18s
epoch 63 | loss: 0.25442 | val_0_rmse: 0.52588 | val_1_rmse: 0.52315 |  0:03:21s
epoch 64 | loss: 0.25845 | val_0_rmse: 0.55368 | val_1_rmse: 0.55853 |  0:03:24s
epoch 65 | loss: 0.26357 | val_0_rmse: 0.49606 | val_1_rmse: 0.49549 |  0:03:27s
epoch 66 | loss: 0.25288 | val_0_rmse: 0.53576 | val_1_rmse: 0.54117 |  0:03:30s
epoch 67 | loss: 0.25402 | val_0_rmse: 0.55023 | val_1_rmse: 0.55649 |  0:03:33s
epoch 68 | loss: 0.26041 | val_0_rmse: 0.5536  | val_1_rmse: 0.56133 |  0:03:36s
epoch 69 | loss: 0.25502 | val_0_rmse: 0.49546 | val_1_rmse: 0.49753 |  0:03:40s
epoch 70 | loss: 0.25259 | val_0_rmse: 0.55458 | val_1_rmse: 0.56093 |  0:03:43s
epoch 71 | loss: 0.26199 | val_0_rmse: 0.50892 | val_1_rmse: 0.51374 |  0:03:46s
epoch 72 | loss: 0.25376 | val_0_rmse: 0.53057 | val_1_rmse: 0.53695 |  0:03:49s
epoch 73 | loss: 0.25035 | val_0_rmse: 0.66777 | val_1_rmse: 0.66679 |  0:03:52s
epoch 74 | loss: 0.25356 | val_0_rmse: 0.52153 | val_1_rmse: 0.52789 |  0:03:55s
epoch 75 | loss: 0.26121 | val_0_rmse: 0.71958 | val_1_rmse: 0.72304 |  0:03:58s
epoch 76 | loss: 0.25675 | val_0_rmse: 0.50487 | val_1_rmse: 0.50481 |  0:04:02s
epoch 77 | loss: 0.25166 | val_0_rmse: 0.50597 | val_1_rmse: 0.51098 |  0:04:05s
epoch 78 | loss: 0.24859 | val_0_rmse: 0.56656 | val_1_rmse: 0.57191 |  0:04:08s
epoch 79 | loss: 0.25015 | val_0_rmse: 0.50446 | val_1_rmse: 0.50509 |  0:04:11s
epoch 80 | loss: 0.2485  | val_0_rmse: 0.56314 | val_1_rmse: 0.56723 |  0:04:14s
epoch 81 | loss: 0.24845 | val_0_rmse: 0.49961 | val_1_rmse: 0.49772 |  0:04:17s
epoch 82 | loss: 0.2485  | val_0_rmse: 0.50537 | val_1_rmse: 0.50985 |  0:04:20s
epoch 83 | loss: 0.25405 | val_0_rmse: 0.54948 | val_1_rmse: 0.55562 |  0:04:23s
epoch 84 | loss: 0.24942 | val_0_rmse: 0.49151 | val_1_rmse: 0.49239 |  0:04:27s
epoch 85 | loss: 0.2535  | val_0_rmse: 0.53727 | val_1_rmse: 0.54081 |  0:04:30s
epoch 86 | loss: 0.25295 | val_0_rmse: 0.50654 | val_1_rmse: 0.51158 |  0:04:33s
epoch 87 | loss: 0.25054 | val_0_rmse: 0.57455 | val_1_rmse: 0.57592 |  0:04:36s
epoch 88 | loss: 0.24513 | val_0_rmse: 0.61284 | val_1_rmse: 0.61742 |  0:04:39s
epoch 89 | loss: 0.2468  | val_0_rmse: 0.52246 | val_1_rmse: 0.52319 |  0:04:42s
epoch 90 | loss: 0.24716 | val_0_rmse: 0.53837 | val_1_rmse: 0.54234 |  0:04:45s
epoch 91 | loss: 0.24501 | val_0_rmse: 0.53103 | val_1_rmse: 0.53472 |  0:04:48s

Early stopping occured at epoch 91 with best_epoch = 61 and best_val_1_rmse = 0.49184
Best weights from best epoch are automatically used!
ended training at: 08:13:28
Feature importance:
Mean squared error is of 1011834653.7946473
Mean absolute error:21795.42144720412
MAPE:0.27312761157815263
R2 score:0.7504806363708978
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:13:28
epoch 0  | loss: 0.86842 | val_0_rmse: 0.70407 | val_1_rmse: 0.70648 |  0:00:03s
epoch 1  | loss: 0.36051 | val_0_rmse: 0.62386 | val_1_rmse: 0.62977 |  0:00:06s
epoch 2  | loss: 0.32878 | val_0_rmse: 0.57712 | val_1_rmse: 0.58204 |  0:00:09s
epoch 3  | loss: 0.31844 | val_0_rmse: 0.55694 | val_1_rmse: 0.56538 |  0:00:12s
epoch 4  | loss: 0.31137 | val_0_rmse: 0.54868 | val_1_rmse: 0.55109 |  0:00:15s
epoch 5  | loss: 0.30662 | val_0_rmse: 0.54343 | val_1_rmse: 0.54591 |  0:00:18s
epoch 6  | loss: 0.30711 | val_0_rmse: 0.61282 | val_1_rmse: 0.61838 |  0:00:21s
epoch 7  | loss: 0.30163 | val_0_rmse: 0.54995 | val_1_rmse: 0.55109 |  0:00:25s
epoch 8  | loss: 0.30373 | val_0_rmse: 0.54724 | val_1_rmse: 0.54917 |  0:00:28s
epoch 9  | loss: 0.29495 | val_0_rmse: 0.53792 | val_1_rmse: 0.53727 |  0:00:31s
epoch 10 | loss: 0.29291 | val_0_rmse: 0.55335 | val_1_rmse: 0.55233 |  0:00:34s
epoch 11 | loss: 0.29435 | val_0_rmse: 0.5399  | val_1_rmse: 0.54036 |  0:00:37s
epoch 12 | loss: 0.28461 | val_0_rmse: 0.55119 | val_1_rmse: 0.54979 |  0:00:40s
epoch 13 | loss: 0.28356 | val_0_rmse: 0.52619 | val_1_rmse: 0.52493 |  0:00:43s
epoch 14 | loss: 0.28092 | val_0_rmse: 0.58687 | val_1_rmse: 0.60004 |  0:00:47s
epoch 15 | loss: 0.27915 | val_0_rmse: 0.55461 | val_1_rmse: 0.56977 |  0:00:50s
epoch 16 | loss: 0.28172 | val_0_rmse: 0.53329 | val_1_rmse: 0.53967 |  0:00:53s
epoch 17 | loss: 0.27254 | val_0_rmse: 0.52668 | val_1_rmse: 0.52862 |  0:00:56s
epoch 18 | loss: 0.27246 | val_0_rmse: 0.51876 | val_1_rmse: 0.51699 |  0:00:59s
epoch 19 | loss: 0.27147 | val_0_rmse: 0.51608 | val_1_rmse: 0.52263 |  0:01:02s
epoch 20 | loss: 0.26845 | val_0_rmse: 0.50455 | val_1_rmse: 0.52769 |  0:01:05s
epoch 21 | loss: 0.26746 | val_0_rmse: 0.51144 | val_1_rmse: 0.51418 |  0:01:09s
epoch 22 | loss: 0.273   | val_0_rmse: 0.51323 | val_1_rmse: 0.51747 |  0:01:12s
epoch 23 | loss: 0.27072 | val_0_rmse: 0.50198 | val_1_rmse: 0.5035  |  0:01:15s
epoch 24 | loss: 0.26169 | val_0_rmse: 0.50612 | val_1_rmse: 0.51008 |  0:01:18s
epoch 25 | loss: 0.26302 | val_0_rmse: 0.50515 | val_1_rmse: 0.50797 |  0:01:21s
epoch 26 | loss: 0.25864 | val_0_rmse: 0.52307 | val_1_rmse: 0.52287 |  0:01:24s
epoch 27 | loss: 0.25821 | val_0_rmse: 0.50226 | val_1_rmse: 0.50209 |  0:01:27s
epoch 28 | loss: 0.26275 | val_0_rmse: 0.55195 | val_1_rmse: 0.55318 |  0:01:31s
epoch 29 | loss: 0.26326 | val_0_rmse: 0.50953 | val_1_rmse: 0.51547 |  0:01:34s
epoch 30 | loss: 0.2638  | val_0_rmse: 0.51657 | val_1_rmse: 0.51779 |  0:01:37s
epoch 31 | loss: 0.26057 | val_0_rmse: 0.51056 | val_1_rmse: 0.51151 |  0:01:40s
epoch 32 | loss: 0.2613  | val_0_rmse: 0.52517 | val_1_rmse: 0.52676 |  0:01:43s
epoch 33 | loss: 0.25821 | val_0_rmse: 0.5562  | val_1_rmse: 0.55533 |  0:01:46s
epoch 34 | loss: 0.25838 | val_0_rmse: 0.54384 | val_1_rmse: 0.54396 |  0:01:49s
epoch 35 | loss: 0.25695 | val_0_rmse: 0.49406 | val_1_rmse: 0.49809 |  0:01:53s
epoch 36 | loss: 0.25253 | val_0_rmse: 0.5087  | val_1_rmse: 0.51274 |  0:01:56s
epoch 37 | loss: 0.2586  | val_0_rmse: 0.5391  | val_1_rmse: 0.53854 |  0:01:59s
epoch 38 | loss: 0.25609 | val_0_rmse: 0.64119 | val_1_rmse: 0.6449  |  0:02:02s
epoch 39 | loss: 0.25868 | val_0_rmse: 0.49614 | val_1_rmse: 0.49885 |  0:02:05s
epoch 40 | loss: 0.25558 | val_0_rmse: 0.49463 | val_1_rmse: 0.69703 |  0:02:08s
epoch 41 | loss: 0.25344 | val_0_rmse: 0.60449 | val_1_rmse: 0.6062  |  0:02:11s
epoch 42 | loss: 0.26974 | val_0_rmse: 0.50089 | val_1_rmse: 0.50315 |  0:02:14s
epoch 43 | loss: 0.25742 | val_0_rmse: 0.51118 | val_1_rmse: 0.51201 |  0:02:18s
epoch 44 | loss: 0.25653 | val_0_rmse: 0.50287 | val_1_rmse: 0.50721 |  0:02:21s
epoch 45 | loss: 0.25378 | val_0_rmse: 0.49726 | val_1_rmse: 0.49964 |  0:02:24s
epoch 46 | loss: 0.25544 | val_0_rmse: 0.52627 | val_1_rmse: 0.52495 |  0:02:27s
epoch 47 | loss: 0.2547  | val_0_rmse: 0.5201  | val_1_rmse: 0.52329 |  0:02:30s
epoch 48 | loss: 0.26349 | val_0_rmse: 0.52441 | val_1_rmse: 2.3288  |  0:02:33s
epoch 49 | loss: 0.25784 | val_0_rmse: 0.51258 | val_1_rmse: 1.66004 |  0:02:36s
epoch 50 | loss: 0.25994 | val_0_rmse: 0.56091 | val_1_rmse: 2.43782 |  0:02:39s
epoch 51 | loss: 0.26018 | val_0_rmse: 0.57551 | val_1_rmse: 2.34434 |  0:02:42s
epoch 52 | loss: 0.25733 | val_0_rmse: 0.60052 | val_1_rmse: 1.97479 |  0:02:45s
epoch 53 | loss: 0.25511 | val_0_rmse: 0.51379 | val_1_rmse: 1.75097 |  0:02:49s
epoch 54 | loss: 0.25322 | val_0_rmse: 0.51877 | val_1_rmse: 1.84549 |  0:02:52s
epoch 55 | loss: 0.25757 | val_0_rmse: 0.50831 | val_1_rmse: 2.01258 |  0:02:55s
epoch 56 | loss: 0.25085 | val_0_rmse: 0.50029 | val_1_rmse: 1.93636 |  0:02:58s
epoch 57 | loss: 0.25271 | val_0_rmse: 0.49736 | val_1_rmse: 1.73554 |  0:03:01s
epoch 58 | loss: 0.25294 | val_0_rmse: 0.51379 | val_1_rmse: 1.78117 |  0:03:04s
epoch 59 | loss: 0.25039 | val_0_rmse: 0.52754 | val_1_rmse: 1.74206 |  0:03:07s
epoch 60 | loss: 0.2499  | val_0_rmse: 0.52185 | val_1_rmse: 1.92118 |  0:03:10s
epoch 61 | loss: 0.25506 | val_0_rmse: 0.58044 | val_1_rmse: 1.95394 |  0:03:13s
epoch 62 | loss: 0.25055 | val_0_rmse: 0.49105 | val_1_rmse: 2.18784 |  0:03:16s
epoch 63 | loss: 0.24955 | val_0_rmse: 0.54525 | val_1_rmse: 1.87373 |  0:03:19s
epoch 64 | loss: 0.24916 | val_0_rmse: 0.48906 | val_1_rmse: 1.66805 |  0:03:23s
epoch 65 | loss: 0.24688 | val_0_rmse: 0.58211 | val_1_rmse: 1.91413 |  0:03:26s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.49809
Best weights from best epoch are automatically used!
ended training at: 08:16:55
Feature importance:
Mean squared error is of 1016284184.175138
Mean absolute error:21640.758458414362
MAPE:0.2694418785875136
R2 score:0.7496445632722791
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:16:55
epoch 0  | loss: 3.37743 | val_0_rmse: 1.1979  | val_1_rmse: 1.14585 |  0:00:00s
epoch 1  | loss: 2.21319 | val_0_rmse: 1.08251 | val_1_rmse: 1.00056 |  0:00:00s
epoch 2  | loss: 1.73448 | val_0_rmse: 1.00315 | val_1_rmse: 0.89705 |  0:00:00s
epoch 3  | loss: 1.23551 | val_0_rmse: 0.99076 | val_1_rmse: 0.93098 |  0:00:00s
epoch 4  | loss: 0.89978 | val_0_rmse: 1.05682 | val_1_rmse: 0.956   |  0:00:01s
epoch 5  | loss: 0.91715 | val_0_rmse: 1.01832 | val_1_rmse: 0.9385  |  0:00:01s
epoch 6  | loss: 0.89964 | val_0_rmse: 0.97672 | val_1_rmse: 0.90511 |  0:00:01s
epoch 7  | loss: 0.73282 | val_0_rmse: 0.93476 | val_1_rmse: 0.86478 |  0:00:01s
epoch 8  | loss: 0.67349 | val_0_rmse: 0.81244 | val_1_rmse: 0.75869 |  0:00:01s
epoch 9  | loss: 0.72884 | val_0_rmse: 0.79498 | val_1_rmse: 0.75062 |  0:00:02s
epoch 10 | loss: 0.69023 | val_0_rmse: 0.76471 | val_1_rmse: 0.7173  |  0:00:02s
epoch 11 | loss: 0.629   | val_0_rmse: 0.74201 | val_1_rmse: 0.68831 |  0:00:02s
epoch 12 | loss: 0.60676 | val_0_rmse: 0.75528 | val_1_rmse: 0.68801 |  0:00:02s
epoch 13 | loss: 0.53595 | val_0_rmse: 0.77503 | val_1_rmse: 0.70178 |  0:00:02s
epoch 14 | loss: 0.55265 | val_0_rmse: 0.77402 | val_1_rmse: 0.69057 |  0:00:03s
epoch 15 | loss: 0.57798 | val_0_rmse: 0.76593 | val_1_rmse: 0.6848  |  0:00:03s
epoch 16 | loss: 0.52194 | val_0_rmse: 0.7822  | val_1_rmse: 0.70204 |  0:00:03s
epoch 17 | loss: 0.50557 | val_0_rmse: 0.78975 | val_1_rmse: 0.7086  |  0:00:03s
epoch 18 | loss: 0.5262  | val_0_rmse: 0.74283 | val_1_rmse: 0.66278 |  0:00:03s
epoch 19 | loss: 0.51322 | val_0_rmse: 0.74583 | val_1_rmse: 0.67126 |  0:00:04s
epoch 20 | loss: 0.51854 | val_0_rmse: 0.73888 | val_1_rmse: 0.6746  |  0:00:04s
epoch 21 | loss: 0.51166 | val_0_rmse: 0.71485 | val_1_rmse: 0.65584 |  0:00:04s
epoch 22 | loss: 0.50412 | val_0_rmse: 0.71125 | val_1_rmse: 0.65406 |  0:00:04s
epoch 23 | loss: 0.50366 | val_0_rmse: 0.71457 | val_1_rmse: 0.66081 |  0:00:04s
epoch 24 | loss: 0.51153 | val_0_rmse: 0.70514 | val_1_rmse: 0.6496  |  0:00:05s
epoch 25 | loss: 0.49582 | val_0_rmse: 0.70541 | val_1_rmse: 0.65595 |  0:00:05s
epoch 26 | loss: 0.49054 | val_0_rmse: 0.71864 | val_1_rmse: 0.67172 |  0:00:05s
epoch 27 | loss: 0.49944 | val_0_rmse: 0.71037 | val_1_rmse: 0.65311 |  0:00:05s
epoch 28 | loss: 0.47916 | val_0_rmse: 0.71783 | val_1_rmse: 0.65612 |  0:00:05s
epoch 29 | loss: 0.48994 | val_0_rmse: 0.69965 | val_1_rmse: 0.64047 |  0:00:06s
epoch 30 | loss: 0.47437 | val_0_rmse: 0.69427 | val_1_rmse: 0.63068 |  0:00:06s
epoch 31 | loss: 0.486   | val_0_rmse: 0.68812 | val_1_rmse: 0.61734 |  0:00:06s
epoch 32 | loss: 0.46706 | val_0_rmse: 0.68739 | val_1_rmse: 0.61865 |  0:00:06s
epoch 33 | loss: 0.46613 | val_0_rmse: 0.68274 | val_1_rmse: 0.61621 |  0:00:07s
epoch 34 | loss: 0.48459 | val_0_rmse: 0.68184 | val_1_rmse: 0.61233 |  0:00:07s
epoch 35 | loss: 0.44645 | val_0_rmse: 0.68176 | val_1_rmse: 0.6122  |  0:00:07s
epoch 36 | loss: 0.45169 | val_0_rmse: 0.6825  | val_1_rmse: 0.61065 |  0:00:07s
epoch 37 | loss: 0.44893 | val_0_rmse: 0.67763 | val_1_rmse: 0.60182 |  0:00:07s
epoch 38 | loss: 0.44658 | val_0_rmse: 0.67893 | val_1_rmse: 0.59894 |  0:00:08s
epoch 39 | loss: 0.42987 | val_0_rmse: 0.68087 | val_1_rmse: 0.60124 |  0:00:08s
epoch 40 | loss: 0.41277 | val_0_rmse: 0.67614 | val_1_rmse: 0.59547 |  0:00:08s
epoch 41 | loss: 0.4154  | val_0_rmse: 0.66095 | val_1_rmse: 0.58604 |  0:00:08s
epoch 42 | loss: 0.41413 | val_0_rmse: 0.66615 | val_1_rmse: 0.59814 |  0:00:08s
epoch 43 | loss: 0.40302 | val_0_rmse: 0.68076 | val_1_rmse: 0.60366 |  0:00:09s
epoch 44 | loss: 0.39984 | val_0_rmse: 0.7026  | val_1_rmse: 0.61991 |  0:00:09s
epoch 45 | loss: 0.4091  | val_0_rmse: 0.67792 | val_1_rmse: 0.5963  |  0:00:09s
epoch 46 | loss: 0.4366  | val_0_rmse: 0.67197 | val_1_rmse: 0.58853 |  0:00:09s
epoch 47 | loss: 0.42757 | val_0_rmse: 0.6853  | val_1_rmse: 0.60128 |  0:00:09s
epoch 48 | loss: 0.42482 | val_0_rmse: 0.71145 | val_1_rmse: 0.6324  |  0:00:10s
epoch 49 | loss: 0.41658 | val_0_rmse: 0.73039 | val_1_rmse: 0.65363 |  0:00:10s
epoch 50 | loss: 0.42913 | val_0_rmse: 0.72295 | val_1_rmse: 0.65105 |  0:00:10s
epoch 51 | loss: 0.42077 | val_0_rmse: 0.66662 | val_1_rmse: 0.58766 |  0:00:10s
epoch 52 | loss: 0.41449 | val_0_rmse: 0.65451 | val_1_rmse: 0.57258 |  0:00:10s
epoch 53 | loss: 0.40281 | val_0_rmse: 0.78025 | val_1_rmse: 0.73376 |  0:00:11s
epoch 54 | loss: 0.40972 | val_0_rmse: 0.74611 | val_1_rmse: 0.70024 |  0:00:11s
epoch 55 | loss: 0.39956 | val_0_rmse: 0.67541 | val_1_rmse: 0.60519 |  0:00:11s
epoch 56 | loss: 0.39636 | val_0_rmse: 0.65111 | val_1_rmse: 0.57073 |  0:00:11s
epoch 57 | loss: 0.40623 | val_0_rmse: 0.70935 | val_1_rmse: 0.65274 |  0:00:11s
epoch 58 | loss: 0.39561 | val_0_rmse: 0.86816 | val_1_rmse: 0.85771 |  0:00:12s
epoch 59 | loss: 0.3792  | val_0_rmse: 0.94385 | val_1_rmse: 0.938   |  0:00:12s
epoch 60 | loss: 0.36762 | val_0_rmse: 0.86665 | val_1_rmse: 0.84582 |  0:00:12s
epoch 61 | loss: 0.36908 | val_0_rmse: 0.80136 | val_1_rmse: 0.75863 |  0:00:12s
epoch 62 | loss: 0.37516 | val_0_rmse: 0.80927 | val_1_rmse: 0.77195 |  0:00:13s
epoch 63 | loss: 0.38025 | val_0_rmse: 0.75903 | val_1_rmse: 0.70675 |  0:00:13s
epoch 64 | loss: 0.366   | val_0_rmse: 0.66592 | val_1_rmse: 0.61183 |  0:00:13s
epoch 65 | loss: 0.37405 | val_0_rmse: 0.63481 | val_1_rmse: 0.57207 |  0:00:13s
epoch 66 | loss: 0.37953 | val_0_rmse: 0.66982 | val_1_rmse: 0.60516 |  0:00:13s
epoch 67 | loss: 0.38762 | val_0_rmse: 0.67015 | val_1_rmse: 0.59862 |  0:00:14s
epoch 68 | loss: 0.37448 | val_0_rmse: 0.64642 | val_1_rmse: 0.57696 |  0:00:14s
epoch 69 | loss: 0.37649 | val_0_rmse: 0.65507 | val_1_rmse: 0.59503 |  0:00:14s
epoch 70 | loss: 0.3655  | val_0_rmse: 0.61542 | val_1_rmse: 0.55018 |  0:00:14s
epoch 71 | loss: 0.36951 | val_0_rmse: 0.611   | val_1_rmse: 0.55555 |  0:00:14s
epoch 72 | loss: 0.37425 | val_0_rmse: 0.7525  | val_1_rmse: 0.71453 |  0:00:15s
epoch 73 | loss: 0.38013 | val_0_rmse: 0.90881 | val_1_rmse: 0.87763 |  0:00:15s
epoch 74 | loss: 0.38581 | val_0_rmse: 0.8394  | val_1_rmse: 0.78396 |  0:00:15s
epoch 75 | loss: 0.37563 | val_0_rmse: 0.78585 | val_1_rmse: 0.72983 |  0:00:15s
epoch 76 | loss: 0.37697 | val_0_rmse: 0.7646  | val_1_rmse: 0.72919 |  0:00:15s
epoch 77 | loss: 0.36919 | val_0_rmse: 0.73402 | val_1_rmse: 0.67928 |  0:00:16s
epoch 78 | loss: 0.37343 | val_0_rmse: 0.7495  | val_1_rmse: 0.69457 |  0:00:16s
epoch 79 | loss: 0.38123 | val_0_rmse: 0.75369 | val_1_rmse: 0.69584 |  0:00:16s
epoch 80 | loss: 0.37695 | val_0_rmse: 0.71912 | val_1_rmse: 0.67004 |  0:00:16s
epoch 81 | loss: 0.36269 | val_0_rmse: 0.71459 | val_1_rmse: 0.66939 |  0:00:16s
epoch 82 | loss: 0.36378 | val_0_rmse: 0.70547 | val_1_rmse: 0.65086 |  0:00:17s
epoch 83 | loss: 0.36218 | val_0_rmse: 0.72533 | val_1_rmse: 0.66824 |  0:00:17s
epoch 84 | loss: 0.3705  | val_0_rmse: 0.65352 | val_1_rmse: 0.59461 |  0:00:17s
epoch 85 | loss: 0.37512 | val_0_rmse: 0.64317 | val_1_rmse: 0.58472 |  0:00:17s
epoch 86 | loss: 0.37229 | val_0_rmse: 0.62542 | val_1_rmse: 0.56659 |  0:00:17s
epoch 87 | loss: 0.36742 | val_0_rmse: 0.61967 | val_1_rmse: 0.55508 |  0:00:18s
epoch 88 | loss: 0.36947 | val_0_rmse: 0.6372  | val_1_rmse: 0.57237 |  0:00:18s
epoch 89 | loss: 0.36421 | val_0_rmse: 0.66063 | val_1_rmse: 0.60439 |  0:00:18s
epoch 90 | loss: 0.37103 | val_0_rmse: 0.65591 | val_1_rmse: 0.57971 |  0:00:18s
epoch 91 | loss: 0.36418 | val_0_rmse: 0.65794 | val_1_rmse: 0.57937 |  0:00:18s
epoch 92 | loss: 0.37796 | val_0_rmse: 0.65208 | val_1_rmse: 0.57902 |  0:00:19s
epoch 93 | loss: 0.36022 | val_0_rmse: 0.67824 | val_1_rmse: 0.60706 |  0:00:19s
epoch 94 | loss: 0.37072 | val_0_rmse: 0.71072 | val_1_rmse: 0.6507  |  0:00:19s
epoch 95 | loss: 0.36813 | val_0_rmse: 0.67257 | val_1_rmse: 0.60447 |  0:00:19s
epoch 96 | loss: 0.36855 | val_0_rmse: 0.61471 | val_1_rmse: 0.53963 |  0:00:19s
epoch 97 | loss: 0.36549 | val_0_rmse: 0.61387 | val_1_rmse: 0.54758 |  0:00:20s
epoch 98 | loss: 0.35487 | val_0_rmse: 0.60888 | val_1_rmse: 0.54463 |  0:00:20s
epoch 99 | loss: 0.34568 | val_0_rmse: 0.61903 | val_1_rmse: 0.55751 |  0:00:20s
epoch 100| loss: 0.36981 | val_0_rmse: 0.59349 | val_1_rmse: 0.53588 |  0:00:20s
epoch 101| loss: 0.35143 | val_0_rmse: 0.58585 | val_1_rmse: 0.54294 |  0:00:20s
epoch 102| loss: 0.34888 | val_0_rmse: 0.58992 | val_1_rmse: 0.54618 |  0:00:21s
epoch 103| loss: 0.36193 | val_0_rmse: 0.60846 | val_1_rmse: 0.55327 |  0:00:21s
epoch 104| loss: 0.35365 | val_0_rmse: 0.6466  | val_1_rmse: 0.58818 |  0:00:21s
epoch 105| loss: 0.35939 | val_0_rmse: 0.67438 | val_1_rmse: 0.62545 |  0:00:21s
epoch 106| loss: 0.34163 | val_0_rmse: 0.63742 | val_1_rmse: 0.58351 |  0:00:21s
epoch 107| loss: 0.34558 | val_0_rmse: 0.581   | val_1_rmse: 0.53954 |  0:00:22s
epoch 108| loss: 0.3357  | val_0_rmse: 0.59132 | val_1_rmse: 0.54788 |  0:00:22s
epoch 109| loss: 0.34398 | val_0_rmse: 0.60159 | val_1_rmse: 0.54285 |  0:00:22s
epoch 110| loss: 0.35509 | val_0_rmse: 0.58512 | val_1_rmse: 0.52664 |  0:00:22s
epoch 111| loss: 0.3398  | val_0_rmse: 0.60569 | val_1_rmse: 0.55022 |  0:00:22s
epoch 112| loss: 0.35075 | val_0_rmse: 0.62378 | val_1_rmse: 0.55368 |  0:00:23s
epoch 113| loss: 0.33558 | val_0_rmse: 0.62111 | val_1_rmse: 0.55234 |  0:00:23s
epoch 114| loss: 0.34975 | val_0_rmse: 0.62534 | val_1_rmse: 0.5654  |  0:00:23s
epoch 115| loss: 0.32832 | val_0_rmse: 0.62239 | val_1_rmse: 0.56505 |  0:00:23s
epoch 116| loss: 0.3356  | val_0_rmse: 0.60942 | val_1_rmse: 0.53371 |  0:00:23s
epoch 117| loss: 0.33857 | val_0_rmse: 0.62136 | val_1_rmse: 0.53922 |  0:00:24s
epoch 118| loss: 0.34736 | val_0_rmse: 0.65121 | val_1_rmse: 0.57987 |  0:00:24s
epoch 119| loss: 0.34728 | val_0_rmse: 0.66161 | val_1_rmse: 0.5961  |  0:00:24s
epoch 120| loss: 0.34038 | val_0_rmse: 0.64352 | val_1_rmse: 0.56541 |  0:00:24s
epoch 121| loss: 0.3277  | val_0_rmse: 0.66223 | val_1_rmse: 0.58229 |  0:00:24s
epoch 122| loss: 0.33622 | val_0_rmse: 0.66298 | val_1_rmse: 0.58343 |  0:00:25s
epoch 123| loss: 0.34084 | val_0_rmse: 0.66349 | val_1_rmse: 0.58473 |  0:00:25s
epoch 124| loss: 0.3377  | val_0_rmse: 0.65069 | val_1_rmse: 0.56945 |  0:00:25s
epoch 125| loss: 0.33614 | val_0_rmse: 0.63808 | val_1_rmse: 0.55712 |  0:00:25s
epoch 126| loss: 0.33306 | val_0_rmse: 0.63702 | val_1_rmse: 0.55088 |  0:00:25s
epoch 127| loss: 0.32657 | val_0_rmse: 0.60732 | val_1_rmse: 0.52879 |  0:00:26s
epoch 128| loss: 0.32326 | val_0_rmse: 0.60977 | val_1_rmse: 0.53149 |  0:00:26s
epoch 129| loss: 0.33436 | val_0_rmse: 0.61475 | val_1_rmse: 0.54225 |  0:00:26s
epoch 130| loss: 0.3429  | val_0_rmse: 0.61637 | val_1_rmse: 0.54568 |  0:00:26s
epoch 131| loss: 0.33259 | val_0_rmse: 0.61861 | val_1_rmse: 0.54202 |  0:00:26s
epoch 132| loss: 0.34069 | val_0_rmse: 0.61651 | val_1_rmse: 0.53773 |  0:00:27s
epoch 133| loss: 0.3318  | val_0_rmse: 0.60753 | val_1_rmse: 0.54382 |  0:00:27s
epoch 134| loss: 0.34329 | val_0_rmse: 0.60963 | val_1_rmse: 0.53836 |  0:00:27s
epoch 135| loss: 0.32588 | val_0_rmse: 0.62761 | val_1_rmse: 0.54091 |  0:00:27s
epoch 136| loss: 0.33859 | val_0_rmse: 0.64822 | val_1_rmse: 0.56272 |  0:00:27s
epoch 137| loss: 0.33011 | val_0_rmse: 0.67585 | val_1_rmse: 0.59144 |  0:00:28s
epoch 138| loss: 0.3206  | val_0_rmse: 0.68744 | val_1_rmse: 0.61826 |  0:00:28s
epoch 139| loss: 0.33036 | val_0_rmse: 0.64679 | val_1_rmse: 0.58698 |  0:00:28s
epoch 140| loss: 0.32962 | val_0_rmse: 0.60942 | val_1_rmse: 0.56127 |  0:00:28s

Early stopping occured at epoch 140 with best_epoch = 110 and best_val_1_rmse = 0.52664
Best weights from best epoch are automatically used!
ended training at: 08:17:24
Feature importance:
Mean squared error is of 3181055968.014233
Mean absolute error:39661.38680961538
MAPE:0.3649497861369837
R2 score:0.5693689814025321
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:17:25
epoch 0  | loss: 3.39595 | val_0_rmse: 1.02548 | val_1_rmse: 0.99703 |  0:00:00s
epoch 1  | loss: 2.11908 | val_0_rmse: 0.93321 | val_1_rmse: 0.95277 |  0:00:00s
epoch 2  | loss: 1.25701 | val_0_rmse: 0.82026 | val_1_rmse: 0.84187 |  0:00:00s
epoch 3  | loss: 0.88918 | val_0_rmse: 0.85906 | val_1_rmse: 0.8897  |  0:00:00s
epoch 4  | loss: 0.71996 | val_0_rmse: 0.91861 | val_1_rmse: 0.94306 |  0:00:01s
epoch 5  | loss: 0.69653 | val_0_rmse: 0.87485 | val_1_rmse: 0.89385 |  0:00:01s
epoch 6  | loss: 0.57178 | val_0_rmse: 0.86567 | val_1_rmse: 0.92534 |  0:00:01s
epoch 7  | loss: 0.55086 | val_0_rmse: 0.79794 | val_1_rmse: 0.79803 |  0:00:01s
epoch 8  | loss: 0.52592 | val_0_rmse: 0.76406 | val_1_rmse: 0.71878 |  0:00:01s
epoch 9  | loss: 0.50261 | val_0_rmse: 0.7495  | val_1_rmse: 0.73858 |  0:00:02s
epoch 10 | loss: 0.50977 | val_0_rmse: 0.73391 | val_1_rmse: 0.74855 |  0:00:02s
epoch 11 | loss: 0.48058 | val_0_rmse: 0.70568 | val_1_rmse: 0.72648 |  0:00:02s
epoch 12 | loss: 0.47589 | val_0_rmse: 0.71217 | val_1_rmse: 0.74308 |  0:00:02s
epoch 13 | loss: 0.48468 | val_0_rmse: 0.70077 | val_1_rmse: 0.71498 |  0:00:02s
epoch 14 | loss: 0.47907 | val_0_rmse: 0.70687 | val_1_rmse: 0.70038 |  0:00:03s
epoch 15 | loss: 0.47863 | val_0_rmse: 0.71794 | val_1_rmse: 0.7216  |  0:00:03s
epoch 16 | loss: 0.47743 | val_0_rmse: 0.69347 | val_1_rmse: 0.68191 |  0:00:03s
epoch 17 | loss: 0.48272 | val_0_rmse: 0.6848  | val_1_rmse: 0.67364 |  0:00:03s
epoch 18 | loss: 0.4756  | val_0_rmse: 0.67521 | val_1_rmse: 0.67261 |  0:00:03s
epoch 19 | loss: 0.46519 | val_0_rmse: 0.68241 | val_1_rmse: 0.67236 |  0:00:04s
epoch 20 | loss: 0.45729 | val_0_rmse: 0.68682 | val_1_rmse: 0.65822 |  0:00:04s
epoch 21 | loss: 0.46849 | val_0_rmse: 0.68885 | val_1_rmse: 0.6702  |  0:00:04s
epoch 22 | loss: 0.47577 | val_0_rmse: 0.68926 | val_1_rmse: 0.65782 |  0:00:04s
epoch 23 | loss: 0.46937 | val_0_rmse: 0.68665 | val_1_rmse: 0.65552 |  0:00:04s
epoch 24 | loss: 0.45984 | val_0_rmse: 0.698   | val_1_rmse: 0.64915 |  0:00:05s
epoch 25 | loss: 0.4813  | val_0_rmse: 0.70421 | val_1_rmse: 0.6424  |  0:00:05s
epoch 26 | loss: 0.45701 | val_0_rmse: 0.71252 | val_1_rmse: 0.65211 |  0:00:05s
epoch 27 | loss: 0.46687 | val_0_rmse: 0.70321 | val_1_rmse: 0.6379  |  0:00:05s
epoch 28 | loss: 0.48385 | val_0_rmse: 0.70604 | val_1_rmse: 0.63806 |  0:00:05s
epoch 29 | loss: 0.49171 | val_0_rmse: 0.71176 | val_1_rmse: 0.65103 |  0:00:06s
epoch 30 | loss: 0.4946  | val_0_rmse: 0.69754 | val_1_rmse: 0.65441 |  0:00:06s
epoch 31 | loss: 0.47913 | val_0_rmse: 0.69288 | val_1_rmse: 0.66187 |  0:00:06s
epoch 32 | loss: 0.47543 | val_0_rmse: 0.69819 | val_1_rmse: 0.672   |  0:00:06s
epoch 33 | loss: 0.4752  | val_0_rmse: 0.69419 | val_1_rmse: 0.67329 |  0:00:06s
epoch 34 | loss: 0.48445 | val_0_rmse: 0.69965 | val_1_rmse: 0.6768  |  0:00:07s
epoch 35 | loss: 0.47239 | val_0_rmse: 0.70013 | val_1_rmse: 0.66757 |  0:00:07s
epoch 36 | loss: 0.481   | val_0_rmse: 0.6894  | val_1_rmse: 0.67101 |  0:00:07s
epoch 37 | loss: 0.49569 | val_0_rmse: 0.69566 | val_1_rmse: 0.68934 |  0:00:07s
epoch 38 | loss: 0.49464 | val_0_rmse: 0.70517 | val_1_rmse: 0.69654 |  0:00:07s
epoch 39 | loss: 0.48811 | val_0_rmse: 0.6885  | val_1_rmse: 0.66572 |  0:00:08s
epoch 40 | loss: 0.48176 | val_0_rmse: 0.72015 | val_1_rmse: 0.69426 |  0:00:08s
epoch 41 | loss: 0.4895  | val_0_rmse: 0.70676 | val_1_rmse: 0.68178 |  0:00:08s
epoch 42 | loss: 0.4771  | val_0_rmse: 0.68864 | val_1_rmse: 0.65395 |  0:00:08s
epoch 43 | loss: 0.46967 | val_0_rmse: 0.69026 | val_1_rmse: 0.66387 |  0:00:09s
epoch 44 | loss: 0.47175 | val_0_rmse: 0.6954  | val_1_rmse: 0.67707 |  0:00:09s
epoch 45 | loss: 0.47574 | val_0_rmse: 0.68157 | val_1_rmse: 0.65834 |  0:00:09s
epoch 46 | loss: 0.46551 | val_0_rmse: 0.67984 | val_1_rmse: 0.6539  |  0:00:09s
epoch 47 | loss: 0.46636 | val_0_rmse: 0.6825  | val_1_rmse: 0.65622 |  0:00:09s
epoch 48 | loss: 0.46226 | val_0_rmse: 0.67548 | val_1_rmse: 0.65536 |  0:00:09s
epoch 49 | loss: 0.46878 | val_0_rmse: 0.66683 | val_1_rmse: 0.65307 |  0:00:10s
epoch 50 | loss: 0.4592  | val_0_rmse: 0.66564 | val_1_rmse: 0.65598 |  0:00:10s
epoch 51 | loss: 0.46263 | val_0_rmse: 0.65827 | val_1_rmse: 0.64623 |  0:00:10s
epoch 52 | loss: 0.46025 | val_0_rmse: 0.65993 | val_1_rmse: 0.63847 |  0:00:10s
epoch 53 | loss: 0.47663 | val_0_rmse: 0.66148 | val_1_rmse: 0.64386 |  0:00:10s
epoch 54 | loss: 0.45696 | val_0_rmse: 0.66913 | val_1_rmse: 0.65844 |  0:00:11s
epoch 55 | loss: 0.4619  | val_0_rmse: 0.65692 | val_1_rmse: 0.64758 |  0:00:11s
epoch 56 | loss: 0.44649 | val_0_rmse: 0.65319 | val_1_rmse: 0.64158 |  0:00:11s
epoch 57 | loss: 0.46195 | val_0_rmse: 0.65327 | val_1_rmse: 0.64748 |  0:00:11s

Early stopping occured at epoch 57 with best_epoch = 27 and best_val_1_rmse = 0.6379
Best weights from best epoch are automatically used!
ended training at: 08:17:37
Feature importance:
Mean squared error is of 3443460768.772947
Mean absolute error:44481.64285618132
MAPE:0.4814363933625152
R2 score:0.5420563404375515
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:17:37
epoch 0  | loss: 2.77814 | val_0_rmse: 1.40741 | val_1_rmse: 0.99501 |  0:00:00s
epoch 1  | loss: 1.68025 | val_0_rmse: 0.98462 | val_1_rmse: 0.94305 |  0:00:00s
epoch 2  | loss: 1.1528  | val_0_rmse: 0.8234  | val_1_rmse: 0.83874 |  0:00:00s
epoch 3  | loss: 0.8983  | val_0_rmse: 0.80775 | val_1_rmse: 0.82272 |  0:00:00s
epoch 4  | loss: 0.87697 | val_0_rmse: 1.02832 | val_1_rmse: 0.9193  |  0:00:01s
epoch 5  | loss: 0.6872  | val_0_rmse: 0.78203 | val_1_rmse: 0.79824 |  0:00:01s
epoch 6  | loss: 0.58616 | val_0_rmse: 0.75585 | val_1_rmse: 0.77838 |  0:00:01s
epoch 7  | loss: 0.57406 | val_0_rmse: 0.74415 | val_1_rmse: 0.75427 |  0:00:01s
epoch 8  | loss: 0.55787 | val_0_rmse: 0.71884 | val_1_rmse: 0.73429 |  0:00:01s
epoch 9  | loss: 0.53053 | val_0_rmse: 0.75358 | val_1_rmse: 0.74077 |  0:00:02s
epoch 10 | loss: 0.51515 | val_0_rmse: 0.74275 | val_1_rmse: 0.75176 |  0:00:02s
epoch 11 | loss: 0.49569 | val_0_rmse: 0.72576 | val_1_rmse: 0.75507 |  0:00:02s
epoch 12 | loss: 0.51389 | val_0_rmse: 0.73131 | val_1_rmse: 0.7428  |  0:00:02s
epoch 13 | loss: 0.50716 | val_0_rmse: 0.72268 | val_1_rmse: 0.73081 |  0:00:02s
epoch 14 | loss: 0.49872 | val_0_rmse: 0.70397 | val_1_rmse: 0.72301 |  0:00:03s
epoch 15 | loss: 0.48268 | val_0_rmse: 0.7047  | val_1_rmse: 0.73009 |  0:00:03s
epoch 16 | loss: 0.47635 | val_0_rmse: 0.71553 | val_1_rmse: 0.73311 |  0:00:03s
epoch 17 | loss: 0.46787 | val_0_rmse: 0.70728 | val_1_rmse: 0.72702 |  0:00:03s
epoch 18 | loss: 0.46527 | val_0_rmse: 0.7024  | val_1_rmse: 0.71659 |  0:00:03s
epoch 19 | loss: 0.452   | val_0_rmse: 0.7043  | val_1_rmse: 0.72058 |  0:00:04s
epoch 20 | loss: 0.46404 | val_0_rmse: 0.70662 | val_1_rmse: 0.72517 |  0:00:04s
epoch 21 | loss: 0.46747 | val_0_rmse: 0.69862 | val_1_rmse: 0.73323 |  0:00:04s
epoch 22 | loss: 0.45779 | val_0_rmse: 0.68928 | val_1_rmse: 0.71976 |  0:00:04s
epoch 23 | loss: 0.46549 | val_0_rmse: 0.71047 | val_1_rmse: 0.72813 |  0:00:04s
epoch 24 | loss: 0.47195 | val_0_rmse: 0.70829 | val_1_rmse: 0.72545 |  0:00:05s
epoch 25 | loss: 0.4602  | val_0_rmse: 0.69401 | val_1_rmse: 0.7117  |  0:00:05s
epoch 26 | loss: 0.45548 | val_0_rmse: 0.67837 | val_1_rmse: 0.69937 |  0:00:05s
epoch 27 | loss: 0.44315 | val_0_rmse: 0.71445 | val_1_rmse: 0.72349 |  0:00:05s
epoch 28 | loss: 0.45129 | val_0_rmse: 0.72422 | val_1_rmse: 0.7317  |  0:00:05s
epoch 29 | loss: 0.4496  | val_0_rmse: 0.68857 | val_1_rmse: 0.70727 |  0:00:06s
epoch 30 | loss: 0.45086 | val_0_rmse: 0.68798 | val_1_rmse: 0.70887 |  0:00:06s
epoch 31 | loss: 0.45773 | val_0_rmse: 0.74273 | val_1_rmse: 0.75408 |  0:00:06s
epoch 32 | loss: 0.44102 | val_0_rmse: 0.76911 | val_1_rmse: 0.77748 |  0:00:06s
epoch 33 | loss: 0.44341 | val_0_rmse: 0.70329 | val_1_rmse: 0.72706 |  0:00:06s
epoch 34 | loss: 0.45839 | val_0_rmse: 0.6916  | val_1_rmse: 0.71695 |  0:00:07s
epoch 35 | loss: 0.44563 | val_0_rmse: 0.73367 | val_1_rmse: 0.75326 |  0:00:07s
epoch 36 | loss: 0.45056 | val_0_rmse: 0.7309  | val_1_rmse: 0.75491 |  0:00:07s
epoch 37 | loss: 0.43636 | val_0_rmse: 0.71112 | val_1_rmse: 0.74631 |  0:00:07s
epoch 38 | loss: 0.44419 | val_0_rmse: 0.7032  | val_1_rmse: 0.74124 |  0:00:07s
epoch 39 | loss: 0.4351  | val_0_rmse: 0.69623 | val_1_rmse: 0.73141 |  0:00:08s
epoch 40 | loss: 0.42614 | val_0_rmse: 0.69399 | val_1_rmse: 0.72685 |  0:00:08s
epoch 41 | loss: 0.43501 | val_0_rmse: 0.66746 | val_1_rmse: 0.70782 |  0:00:08s
epoch 42 | loss: 0.42212 | val_0_rmse: 0.65746 | val_1_rmse: 0.70264 |  0:00:08s
epoch 43 | loss: 0.42795 | val_0_rmse: 0.65552 | val_1_rmse: 0.69809 |  0:00:08s
epoch 44 | loss: 0.41449 | val_0_rmse: 0.66039 | val_1_rmse: 0.69785 |  0:00:09s
epoch 45 | loss: 0.42962 | val_0_rmse: 0.65515 | val_1_rmse: 0.69312 |  0:00:09s
epoch 46 | loss: 0.43739 | val_0_rmse: 0.65667 | val_1_rmse: 0.69606 |  0:00:09s
epoch 47 | loss: 0.42886 | val_0_rmse: 0.66671 | val_1_rmse: 0.7027  |  0:00:09s
epoch 48 | loss: 0.43046 | val_0_rmse: 0.66334 | val_1_rmse: 0.70535 |  0:00:09s
epoch 49 | loss: 0.42624 | val_0_rmse: 0.65935 | val_1_rmse: 0.70257 |  0:00:10s
epoch 50 | loss: 0.43003 | val_0_rmse: 0.6558  | val_1_rmse: 0.69859 |  0:00:10s
epoch 51 | loss: 0.42072 | val_0_rmse: 0.66528 | val_1_rmse: 0.70057 |  0:00:10s
epoch 52 | loss: 0.42452 | val_0_rmse: 0.65545 | val_1_rmse: 0.69768 |  0:00:10s
epoch 53 | loss: 0.41891 | val_0_rmse: 0.65196 | val_1_rmse: 0.69475 |  0:00:10s
epoch 54 | loss: 0.42616 | val_0_rmse: 0.6493  | val_1_rmse: 0.68974 |  0:00:11s
epoch 55 | loss: 0.42091 | val_0_rmse: 0.65063 | val_1_rmse: 0.68683 |  0:00:11s
epoch 56 | loss: 0.42178 | val_0_rmse: 0.65839 | val_1_rmse: 0.69062 |  0:00:11s
epoch 57 | loss: 0.42859 | val_0_rmse: 0.65778 | val_1_rmse: 0.69366 |  0:00:11s
epoch 58 | loss: 0.42068 | val_0_rmse: 0.65694 | val_1_rmse: 0.69536 |  0:00:12s
epoch 59 | loss: 0.42182 | val_0_rmse: 0.65222 | val_1_rmse: 0.68695 |  0:00:12s
epoch 60 | loss: 0.42428 | val_0_rmse: 0.65694 | val_1_rmse: 0.68982 |  0:00:12s
epoch 61 | loss: 0.41676 | val_0_rmse: 0.65079 | val_1_rmse: 0.69169 |  0:00:12s
epoch 62 | loss: 0.41794 | val_0_rmse: 0.64974 | val_1_rmse: 0.699   |  0:00:12s
epoch 63 | loss: 0.42288 | val_0_rmse: 0.65873 | val_1_rmse: 0.69695 |  0:00:12s
epoch 64 | loss: 0.41186 | val_0_rmse: 0.66917 | val_1_rmse: 0.7005  |  0:00:13s
epoch 65 | loss: 0.41715 | val_0_rmse: 0.67249 | val_1_rmse: 0.70361 |  0:00:13s
epoch 66 | loss: 0.4114  | val_0_rmse: 0.66209 | val_1_rmse: 0.69706 |  0:00:13s
epoch 67 | loss: 0.41508 | val_0_rmse: 0.66231 | val_1_rmse: 0.70197 |  0:00:13s
epoch 68 | loss: 0.41203 | val_0_rmse: 0.68574 | val_1_rmse: 0.71792 |  0:00:14s
epoch 69 | loss: 0.41261 | val_0_rmse: 0.67794 | val_1_rmse: 0.71355 |  0:00:14s
epoch 70 | loss: 0.41985 | val_0_rmse: 0.6611  | val_1_rmse: 0.70734 |  0:00:14s
epoch 71 | loss: 0.42123 | val_0_rmse: 0.65304 | val_1_rmse: 0.69837 |  0:00:14s
epoch 72 | loss: 0.40718 | val_0_rmse: 0.65813 | val_1_rmse: 0.70768 |  0:00:14s
epoch 73 | loss: 0.41053 | val_0_rmse: 0.67157 | val_1_rmse: 0.71667 |  0:00:15s
epoch 74 | loss: 0.41796 | val_0_rmse: 0.67467 | val_1_rmse: 0.70537 |  0:00:15s
epoch 75 | loss: 0.4137  | val_0_rmse: 0.6704  | val_1_rmse: 0.69689 |  0:00:15s
epoch 76 | loss: 0.42203 | val_0_rmse: 0.67237 | val_1_rmse: 0.70529 |  0:00:15s
epoch 77 | loss: 0.42248 | val_0_rmse: 0.66525 | val_1_rmse: 0.71131 |  0:00:15s
epoch 78 | loss: 0.41592 | val_0_rmse: 0.67068 | val_1_rmse: 0.72269 |  0:00:16s
epoch 79 | loss: 0.40853 | val_0_rmse: 0.67076 | val_1_rmse: 0.72493 |  0:00:16s
epoch 80 | loss: 0.40873 | val_0_rmse: 0.67281 | val_1_rmse: 0.72519 |  0:00:16s
epoch 81 | loss: 0.41226 | val_0_rmse: 0.668   | val_1_rmse: 0.72655 |  0:00:16s
epoch 82 | loss: 0.40706 | val_0_rmse: 0.673   | val_1_rmse: 0.7299  |  0:00:16s
epoch 83 | loss: 0.41062 | val_0_rmse: 0.68343 | val_1_rmse: 0.73218 |  0:00:17s
epoch 84 | loss: 0.40034 | val_0_rmse: 0.67097 | val_1_rmse: 0.72681 |  0:00:17s
epoch 85 | loss: 0.4019  | val_0_rmse: 0.6529  | val_1_rmse: 0.71569 |  0:00:17s

Early stopping occured at epoch 85 with best_epoch = 55 and best_val_1_rmse = 0.68683
Best weights from best epoch are automatically used!
ended training at: 08:17:54
Feature importance:
Mean squared error is of 3251325004.118777
Mean absolute error:41423.6111337912
MAPE:0.4127788369549251
R2 score:0.5667994913391201
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:17:54
epoch 0  | loss: 3.63778 | val_0_rmse: 1.0653  | val_1_rmse: 1.01971 |  0:00:00s
epoch 1  | loss: 1.40817 | val_0_rmse: 1.03725 | val_1_rmse: 0.99093 |  0:00:00s
epoch 2  | loss: 1.22304 | val_0_rmse: 0.93308 | val_1_rmse: 0.91889 |  0:00:00s
epoch 3  | loss: 1.09578 | val_0_rmse: 0.81055 | val_1_rmse: 0.80311 |  0:00:00s
epoch 4  | loss: 0.84038 | val_0_rmse: 0.81454 | val_1_rmse: 0.83608 |  0:00:01s
epoch 5  | loss: 0.82731 | val_0_rmse: 0.7678  | val_1_rmse: 0.78927 |  0:00:01s
epoch 6  | loss: 0.6781  | val_0_rmse: 0.80336 | val_1_rmse: 0.83001 |  0:00:01s
epoch 7  | loss: 0.5876  | val_0_rmse: 0.91002 | val_1_rmse: 0.8321  |  0:00:01s
epoch 8  | loss: 0.5399  | val_0_rmse: 0.87593 | val_1_rmse: 0.80506 |  0:00:01s
epoch 9  | loss: 0.50554 | val_0_rmse: 0.83033 | val_1_rmse: 0.85065 |  0:00:02s
epoch 10 | loss: 0.50813 | val_0_rmse: 0.85923 | val_1_rmse: 0.85041 |  0:00:02s
epoch 11 | loss: 0.4894  | val_0_rmse: 0.80448 | val_1_rmse: 0.78157 |  0:00:02s
epoch 12 | loss: 0.49187 | val_0_rmse: 0.83474 | val_1_rmse: 0.78057 |  0:00:02s
epoch 13 | loss: 0.45984 | val_0_rmse: 0.74858 | val_1_rmse: 0.73375 |  0:00:02s
epoch 14 | loss: 0.47035 | val_0_rmse: 0.71451 | val_1_rmse: 0.72439 |  0:00:03s
epoch 15 | loss: 0.47402 | val_0_rmse: 0.71505 | val_1_rmse: 0.709   |  0:00:03s
epoch 16 | loss: 0.43752 | val_0_rmse: 0.72133 | val_1_rmse: 0.73383 |  0:00:03s
epoch 17 | loss: 0.45225 | val_0_rmse: 0.70869 | val_1_rmse: 0.71828 |  0:00:03s
epoch 18 | loss: 0.43983 | val_0_rmse: 0.70734 | val_1_rmse: 0.71452 |  0:00:03s
epoch 19 | loss: 0.43814 | val_0_rmse: 0.71938 | val_1_rmse: 0.71343 |  0:00:04s
epoch 20 | loss: 0.44239 | val_0_rmse: 0.74397 | val_1_rmse: 0.72526 |  0:00:04s
epoch 21 | loss: 0.44104 | val_0_rmse: 0.73637 | val_1_rmse: 0.73735 |  0:00:04s
epoch 22 | loss: 0.44008 | val_0_rmse: 0.73219 | val_1_rmse: 0.73741 |  0:00:04s
epoch 23 | loss: 0.42301 | val_0_rmse: 0.72308 | val_1_rmse: 0.73313 |  0:00:04s
epoch 24 | loss: 0.41659 | val_0_rmse: 0.72239 | val_1_rmse: 0.73008 |  0:00:05s
epoch 25 | loss: 0.42262 | val_0_rmse: 0.71857 | val_1_rmse: 0.71984 |  0:00:05s
epoch 26 | loss: 0.42628 | val_0_rmse: 0.70946 | val_1_rmse: 0.7293  |  0:00:05s
epoch 27 | loss: 0.42384 | val_0_rmse: 0.6847  | val_1_rmse: 0.72555 |  0:00:05s
epoch 28 | loss: 0.41987 | val_0_rmse: 0.67595 | val_1_rmse: 0.72743 |  0:00:05s
epoch 29 | loss: 0.41806 | val_0_rmse: 0.68099 | val_1_rmse: 0.72028 |  0:00:06s
epoch 30 | loss: 0.4222  | val_0_rmse: 0.6832  | val_1_rmse: 0.71184 |  0:00:06s
epoch 31 | loss: 0.42329 | val_0_rmse: 0.67925 | val_1_rmse: 0.7016  |  0:00:06s
epoch 32 | loss: 0.43165 | val_0_rmse: 0.67564 | val_1_rmse: 0.70727 |  0:00:06s
epoch 33 | loss: 0.41421 | val_0_rmse: 0.68383 | val_1_rmse: 0.73289 |  0:00:06s
epoch 34 | loss: 0.42241 | val_0_rmse: 0.6763  | val_1_rmse: 0.73445 |  0:00:07s
epoch 35 | loss: 0.42931 | val_0_rmse: 0.66284 | val_1_rmse: 0.70382 |  0:00:07s
epoch 36 | loss: 0.42294 | val_0_rmse: 0.66546 | val_1_rmse: 0.69745 |  0:00:07s
epoch 37 | loss: 0.4159  | val_0_rmse: 0.66472 | val_1_rmse: 0.70473 |  0:00:07s
epoch 38 | loss: 0.41659 | val_0_rmse: 0.65468 | val_1_rmse: 0.71259 |  0:00:07s
epoch 39 | loss: 0.4129  | val_0_rmse: 0.64828 | val_1_rmse: 0.71638 |  0:00:08s
epoch 40 | loss: 0.41163 | val_0_rmse: 0.6434  | val_1_rmse: 0.70718 |  0:00:08s
epoch 41 | loss: 0.40352 | val_0_rmse: 0.6423  | val_1_rmse: 0.70278 |  0:00:08s
epoch 42 | loss: 0.4066  | val_0_rmse: 0.64168 | val_1_rmse: 0.70227 |  0:00:08s
epoch 43 | loss: 0.40643 | val_0_rmse: 0.6447  | val_1_rmse: 0.70667 |  0:00:08s
epoch 44 | loss: 0.40835 | val_0_rmse: 0.64352 | val_1_rmse: 0.70576 |  0:00:09s
epoch 45 | loss: 0.41728 | val_0_rmse: 0.63749 | val_1_rmse: 0.70496 |  0:00:09s
epoch 46 | loss: 0.40033 | val_0_rmse: 0.63544 | val_1_rmse: 0.70862 |  0:00:09s
epoch 47 | loss: 0.40808 | val_0_rmse: 0.63577 | val_1_rmse: 0.70939 |  0:00:09s
epoch 48 | loss: 0.40641 | val_0_rmse: 0.63843 | val_1_rmse: 0.71321 |  0:00:09s
epoch 49 | loss: 0.39827 | val_0_rmse: 0.64026 | val_1_rmse: 0.71798 |  0:00:10s
epoch 50 | loss: 0.40935 | val_0_rmse: 0.63688 | val_1_rmse: 0.71109 |  0:00:10s
epoch 51 | loss: 0.40643 | val_0_rmse: 0.63629 | val_1_rmse: 0.70144 |  0:00:10s
epoch 52 | loss: 0.40924 | val_0_rmse: 0.63363 | val_1_rmse: 0.70169 |  0:00:10s
epoch 53 | loss: 0.40166 | val_0_rmse: 0.63298 | val_1_rmse: 0.69988 |  0:00:10s
epoch 54 | loss: 0.39993 | val_0_rmse: 0.63322 | val_1_rmse: 0.70272 |  0:00:11s
epoch 55 | loss: 0.39853 | val_0_rmse: 0.63058 | val_1_rmse: 0.69967 |  0:00:11s
epoch 56 | loss: 0.3961  | val_0_rmse: 0.63106 | val_1_rmse: 0.70052 |  0:00:11s
epoch 57 | loss: 0.39153 | val_0_rmse: 0.62734 | val_1_rmse: 0.6949  |  0:00:11s
epoch 58 | loss: 0.40016 | val_0_rmse: 0.62926 | val_1_rmse: 0.69068 |  0:00:11s
epoch 59 | loss: 0.4022  | val_0_rmse: 0.62771 | val_1_rmse: 0.69137 |  0:00:12s
epoch 60 | loss: 0.39804 | val_0_rmse: 0.62672 | val_1_rmse: 0.70027 |  0:00:12s
epoch 61 | loss: 0.39108 | val_0_rmse: 0.63035 | val_1_rmse: 0.7012  |  0:00:12s
epoch 62 | loss: 0.39221 | val_0_rmse: 0.62984 | val_1_rmse: 0.69715 |  0:00:12s
epoch 63 | loss: 0.39557 | val_0_rmse: 0.62251 | val_1_rmse: 0.69844 |  0:00:12s
epoch 64 | loss: 0.39738 | val_0_rmse: 0.62339 | val_1_rmse: 0.70886 |  0:00:13s
epoch 65 | loss: 0.40568 | val_0_rmse: 0.62413 | val_1_rmse: 0.70585 |  0:00:13s
epoch 66 | loss: 0.39836 | val_0_rmse: 0.63219 | val_1_rmse: 0.69175 |  0:00:13s
epoch 67 | loss: 0.39766 | val_0_rmse: 0.63334 | val_1_rmse: 0.69256 |  0:00:13s
epoch 68 | loss: 0.39642 | val_0_rmse: 0.62726 | val_1_rmse: 0.69551 |  0:00:13s
epoch 69 | loss: 0.40203 | val_0_rmse: 0.63584 | val_1_rmse: 0.69744 |  0:00:14s
epoch 70 | loss: 0.40418 | val_0_rmse: 0.64154 | val_1_rmse: 0.69587 |  0:00:14s
epoch 71 | loss: 0.40478 | val_0_rmse: 0.63531 | val_1_rmse: 0.69627 |  0:00:14s
epoch 72 | loss: 0.40451 | val_0_rmse: 0.63487 | val_1_rmse: 0.69424 |  0:00:14s
epoch 73 | loss: 0.40651 | val_0_rmse: 0.63532 | val_1_rmse: 0.68727 |  0:00:14s
epoch 74 | loss: 0.41557 | val_0_rmse: 0.63406 | val_1_rmse: 0.68861 |  0:00:15s
epoch 75 | loss: 0.41898 | val_0_rmse: 0.63321 | val_1_rmse: 0.68808 |  0:00:15s
epoch 76 | loss: 0.40826 | val_0_rmse: 0.63847 | val_1_rmse: 0.69982 |  0:00:15s
epoch 77 | loss: 0.43052 | val_0_rmse: 0.64492 | val_1_rmse: 0.71468 |  0:00:15s
epoch 78 | loss: 0.40543 | val_0_rmse: 0.6432  | val_1_rmse: 0.70814 |  0:00:15s
epoch 79 | loss: 0.41211 | val_0_rmse: 0.6428  | val_1_rmse: 0.6944  |  0:00:16s
epoch 80 | loss: 0.41539 | val_0_rmse: 0.64265 | val_1_rmse: 0.69067 |  0:00:16s
epoch 81 | loss: 0.40517 | val_0_rmse: 0.63203 | val_1_rmse: 0.69074 |  0:00:16s
epoch 82 | loss: 0.40709 | val_0_rmse: 0.63035 | val_1_rmse: 0.69334 |  0:00:16s
epoch 83 | loss: 0.40196 | val_0_rmse: 0.63247 | val_1_rmse: 0.68574 |  0:00:16s
epoch 84 | loss: 0.40152 | val_0_rmse: 0.63538 | val_1_rmse: 0.68061 |  0:00:17s
epoch 85 | loss: 0.40072 | val_0_rmse: 0.62725 | val_1_rmse: 0.6871  |  0:00:17s
epoch 86 | loss: 0.39492 | val_0_rmse: 0.62803 | val_1_rmse: 0.69868 |  0:00:17s
epoch 87 | loss: 0.40211 | val_0_rmse: 0.62585 | val_1_rmse: 0.68608 |  0:00:17s
epoch 88 | loss: 0.40252 | val_0_rmse: 0.6314  | val_1_rmse: 0.68511 |  0:00:17s
epoch 89 | loss: 0.40787 | val_0_rmse: 0.62631 | val_1_rmse: 0.69013 |  0:00:18s
epoch 90 | loss: 0.39542 | val_0_rmse: 0.63673 | val_1_rmse: 0.7106  |  0:00:18s
epoch 91 | loss: 0.40736 | val_0_rmse: 0.62619 | val_1_rmse: 0.6942  |  0:00:18s
epoch 92 | loss: 0.39173 | val_0_rmse: 0.62483 | val_1_rmse: 0.67959 |  0:00:18s
epoch 93 | loss: 0.3893  | val_0_rmse: 0.62971 | val_1_rmse: 0.68439 |  0:00:18s
epoch 94 | loss: 0.40323 | val_0_rmse: 0.62873 | val_1_rmse: 0.69641 |  0:00:19s
epoch 95 | loss: 0.39269 | val_0_rmse: 0.62843 | val_1_rmse: 0.69945 |  0:00:19s
epoch 96 | loss: 0.39589 | val_0_rmse: 0.62597 | val_1_rmse: 0.6929  |  0:00:19s
epoch 97 | loss: 0.3899  | val_0_rmse: 0.62969 | val_1_rmse: 0.69451 |  0:00:19s
epoch 98 | loss: 0.39451 | val_0_rmse: 0.63272 | val_1_rmse: 0.69507 |  0:00:19s
epoch 99 | loss: 0.39021 | val_0_rmse: 0.63558 | val_1_rmse: 0.6995  |  0:00:20s
epoch 100| loss: 0.39073 | val_0_rmse: 0.63233 | val_1_rmse: 0.69737 |  0:00:20s
epoch 101| loss: 0.39632 | val_0_rmse: 0.6266  | val_1_rmse: 0.68944 |  0:00:20s
epoch 102| loss: 0.38791 | val_0_rmse: 0.62656 | val_1_rmse: 0.68356 |  0:00:20s
epoch 103| loss: 0.38815 | val_0_rmse: 0.62966 | val_1_rmse: 0.68485 |  0:00:20s
epoch 104| loss: 0.38612 | val_0_rmse: 0.62378 | val_1_rmse: 0.69162 |  0:00:21s
epoch 105| loss: 0.37735 | val_0_rmse: 0.62759 | val_1_rmse: 0.70295 |  0:00:21s
epoch 106| loss: 0.39734 | val_0_rmse: 0.631   | val_1_rmse: 0.70252 |  0:00:21s
epoch 107| loss: 0.40184 | val_0_rmse: 0.63507 | val_1_rmse: 0.70424 |  0:00:21s
epoch 108| loss: 0.40086 | val_0_rmse: 0.63953 | val_1_rmse: 0.70662 |  0:00:21s
epoch 109| loss: 0.40918 | val_0_rmse: 0.63854 | val_1_rmse: 0.71067 |  0:00:22s
epoch 110| loss: 0.4014  | val_0_rmse: 0.64685 | val_1_rmse: 0.726   |  0:00:22s
epoch 111| loss: 0.40181 | val_0_rmse: 0.65467 | val_1_rmse: 0.73513 |  0:00:22s
epoch 112| loss: 0.40158 | val_0_rmse: 0.62989 | val_1_rmse: 0.70763 |  0:00:22s
epoch 113| loss: 0.404   | val_0_rmse: 0.62605 | val_1_rmse: 0.69493 |  0:00:22s
epoch 114| loss: 0.39225 | val_0_rmse: 0.63823 | val_1_rmse: 0.70346 |  0:00:23s
epoch 115| loss: 0.40203 | val_0_rmse: 0.64291 | val_1_rmse: 0.71189 |  0:00:23s
epoch 116| loss: 0.3921  | val_0_rmse: 0.63372 | val_1_rmse: 0.70718 |  0:00:23s
epoch 117| loss: 0.40018 | val_0_rmse: 0.63073 | val_1_rmse: 0.70102 |  0:00:23s
epoch 118| loss: 0.39018 | val_0_rmse: 0.63017 | val_1_rmse: 0.69623 |  0:00:23s
epoch 119| loss: 0.39238 | val_0_rmse: 0.62802 | val_1_rmse: 0.6934  |  0:00:24s
epoch 120| loss: 0.39788 | val_0_rmse: 0.62969 | val_1_rmse: 0.6991  |  0:00:24s
epoch 121| loss: 0.3882  | val_0_rmse: 0.62416 | val_1_rmse: 0.70047 |  0:00:24s
epoch 122| loss: 0.39616 | val_0_rmse: 0.6252  | val_1_rmse: 0.69664 |  0:00:24s

Early stopping occured at epoch 122 with best_epoch = 92 and best_val_1_rmse = 0.67959
Best weights from best epoch are automatically used!
ended training at: 08:18:19
Feature importance:
Mean squared error is of 3620637931.5247383
Mean absolute error:44461.636879670325
MAPE:0.4425533316874342
R2 score:0.4926961900618678
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:18:19
epoch 0  | loss: 3.47212 | val_0_rmse: 1.17005 | val_1_rmse: 1.1245  |  0:00:00s
epoch 1  | loss: 2.63517 | val_0_rmse: 0.99954 | val_1_rmse: 0.91835 |  0:00:00s
epoch 2  | loss: 1.33732 | val_0_rmse: 1.05996 | val_1_rmse: 1.0334  |  0:00:00s
epoch 3  | loss: 1.00082 | val_0_rmse: 1.02949 | val_1_rmse: 0.99896 |  0:00:00s
epoch 4  | loss: 0.64668 | val_0_rmse: 0.88622 | val_1_rmse: 0.91547 |  0:00:01s
epoch 5  | loss: 0.65086 | val_0_rmse: 0.79777 | val_1_rmse: 0.8254  |  0:00:01s
epoch 6  | loss: 0.63405 | val_0_rmse: 0.81262 | val_1_rmse: 0.82209 |  0:00:01s
epoch 7  | loss: 0.56427 | val_0_rmse: 0.81959 | val_1_rmse: 0.81659 |  0:00:01s
epoch 8  | loss: 0.55632 | val_0_rmse: 0.79393 | val_1_rmse: 0.76433 |  0:00:01s
epoch 9  | loss: 0.55556 | val_0_rmse: 0.78037 | val_1_rmse: 0.73548 |  0:00:02s
epoch 10 | loss: 0.52775 | val_0_rmse: 0.77535 | val_1_rmse: 0.73656 |  0:00:02s
epoch 11 | loss: 0.52552 | val_0_rmse: 0.76187 | val_1_rmse: 0.73474 |  0:00:02s
epoch 12 | loss: 0.5111  | val_0_rmse: 0.76138 | val_1_rmse: 0.72688 |  0:00:02s
epoch 13 | loss: 0.50968 | val_0_rmse: 0.77124 | val_1_rmse: 0.72624 |  0:00:02s
epoch 14 | loss: 0.52716 | val_0_rmse: 0.74216 | val_1_rmse: 0.70123 |  0:00:03s
epoch 15 | loss: 0.51127 | val_0_rmse: 0.76394 | val_1_rmse: 0.72523 |  0:00:03s
epoch 16 | loss: 0.51586 | val_0_rmse: 0.76513 | val_1_rmse: 0.74028 |  0:00:03s
epoch 17 | loss: 0.49428 | val_0_rmse: 0.74491 | val_1_rmse: 0.71569 |  0:00:03s
epoch 18 | loss: 0.49415 | val_0_rmse: 0.73492 | val_1_rmse: 0.71021 |  0:00:03s
epoch 19 | loss: 0.47538 | val_0_rmse: 0.75907 | val_1_rmse: 0.7481  |  0:00:04s
epoch 20 | loss: 0.48399 | val_0_rmse: 0.71111 | val_1_rmse: 0.70747 |  0:00:04s
epoch 21 | loss: 0.47488 | val_0_rmse: 0.70504 | val_1_rmse: 0.67414 |  0:00:04s
epoch 22 | loss: 0.47935 | val_0_rmse: 0.69046 | val_1_rmse: 0.6715  |  0:00:04s
epoch 23 | loss: 0.47454 | val_0_rmse: 0.68878 | val_1_rmse: 0.68107 |  0:00:04s
epoch 24 | loss: 0.45078 | val_0_rmse: 0.67651 | val_1_rmse: 0.66252 |  0:00:05s
epoch 25 | loss: 0.461   | val_0_rmse: 0.67595 | val_1_rmse: 0.65837 |  0:00:05s
epoch 26 | loss: 0.45071 | val_0_rmse: 0.68535 | val_1_rmse: 0.67307 |  0:00:05s
epoch 27 | loss: 0.43898 | val_0_rmse: 0.71536 | val_1_rmse: 0.71642 |  0:00:05s
epoch 28 | loss: 0.43889 | val_0_rmse: 0.72009 | val_1_rmse: 0.71272 |  0:00:05s
epoch 29 | loss: 0.45363 | val_0_rmse: 0.6761  | val_1_rmse: 0.65969 |  0:00:06s
epoch 30 | loss: 0.43956 | val_0_rmse: 0.66248 | val_1_rmse: 0.63964 |  0:00:06s
epoch 31 | loss: 0.43117 | val_0_rmse: 0.66184 | val_1_rmse: 0.64183 |  0:00:06s
epoch 32 | loss: 0.44122 | val_0_rmse: 0.65207 | val_1_rmse: 0.63766 |  0:00:06s
epoch 33 | loss: 0.42079 | val_0_rmse: 0.65189 | val_1_rmse: 0.63821 |  0:00:06s
epoch 34 | loss: 0.43475 | val_0_rmse: 0.65489 | val_1_rmse: 0.64994 |  0:00:07s
epoch 35 | loss: 0.43097 | val_0_rmse: 0.64996 | val_1_rmse: 0.64635 |  0:00:07s
epoch 36 | loss: 0.43508 | val_0_rmse: 0.64837 | val_1_rmse: 0.64326 |  0:00:07s
epoch 37 | loss: 0.43593 | val_0_rmse: 0.65183 | val_1_rmse: 0.63119 |  0:00:07s
epoch 38 | loss: 0.42625 | val_0_rmse: 0.65695 | val_1_rmse: 0.63174 |  0:00:07s
epoch 39 | loss: 0.42882 | val_0_rmse: 0.65617 | val_1_rmse: 0.63135 |  0:00:08s
epoch 40 | loss: 0.42418 | val_0_rmse: 0.6493  | val_1_rmse: 0.62469 |  0:00:08s
epoch 41 | loss: 0.41498 | val_0_rmse: 0.65805 | val_1_rmse: 0.64268 |  0:00:08s
epoch 42 | loss: 0.43721 | val_0_rmse: 0.65903 | val_1_rmse: 0.64352 |  0:00:08s
epoch 43 | loss: 0.42292 | val_0_rmse: 0.64879 | val_1_rmse: 0.63005 |  0:00:08s
epoch 44 | loss: 0.42018 | val_0_rmse: 0.65648 | val_1_rmse: 0.6378  |  0:00:09s
epoch 45 | loss: 0.42719 | val_0_rmse: 0.64634 | val_1_rmse: 0.62932 |  0:00:09s
epoch 46 | loss: 0.41414 | val_0_rmse: 0.63442 | val_1_rmse: 0.62345 |  0:00:09s
epoch 47 | loss: 0.43013 | val_0_rmse: 0.63214 | val_1_rmse: 0.61788 |  0:00:09s
epoch 48 | loss: 0.41422 | val_0_rmse: 0.63901 | val_1_rmse: 0.6253  |  0:00:09s
epoch 49 | loss: 0.42558 | val_0_rmse: 0.63757 | val_1_rmse: 0.62218 |  0:00:10s
epoch 50 | loss: 0.42569 | val_0_rmse: 0.63708 | val_1_rmse: 0.62339 |  0:00:10s
epoch 51 | loss: 0.41149 | val_0_rmse: 0.66518 | val_1_rmse: 0.64426 |  0:00:10s
epoch 52 | loss: 0.40598 | val_0_rmse: 0.68372 | val_1_rmse: 0.65867 |  0:00:10s
epoch 53 | loss: 0.40718 | val_0_rmse: 0.66388 | val_1_rmse: 0.64148 |  0:00:10s
epoch 54 | loss: 0.3892  | val_0_rmse: 0.63915 | val_1_rmse: 0.61732 |  0:00:11s
epoch 55 | loss: 0.39678 | val_0_rmse: 0.64103 | val_1_rmse: 0.62437 |  0:00:11s
epoch 56 | loss: 0.39398 | val_0_rmse: 0.62489 | val_1_rmse: 0.61203 |  0:00:11s
epoch 57 | loss: 0.39538 | val_0_rmse: 0.6294  | val_1_rmse: 0.62079 |  0:00:11s
epoch 58 | loss: 0.39133 | val_0_rmse: 0.66313 | val_1_rmse: 0.66077 |  0:00:11s
epoch 59 | loss: 0.38456 | val_0_rmse: 0.64577 | val_1_rmse: 0.64269 |  0:00:12s
epoch 60 | loss: 0.37719 | val_0_rmse: 0.61224 | val_1_rmse: 0.60594 |  0:00:12s
epoch 61 | loss: 0.40359 | val_0_rmse: 0.62643 | val_1_rmse: 0.62545 |  0:00:12s
epoch 62 | loss: 0.37201 | val_0_rmse: 0.6572  | val_1_rmse: 0.66133 |  0:00:12s
epoch 63 | loss: 0.41243 | val_0_rmse: 0.63923 | val_1_rmse: 0.62807 |  0:00:12s
epoch 64 | loss: 0.39452 | val_0_rmse: 0.64836 | val_1_rmse: 0.62841 |  0:00:13s
epoch 65 | loss: 0.39211 | val_0_rmse: 0.69561 | val_1_rmse: 0.67684 |  0:00:13s
epoch 66 | loss: 0.3933  | val_0_rmse: 0.70086 | val_1_rmse: 0.6819  |  0:00:13s
epoch 67 | loss: 0.38486 | val_0_rmse: 0.65757 | val_1_rmse: 0.64601 |  0:00:13s
epoch 68 | loss: 0.37171 | val_0_rmse: 0.65552 | val_1_rmse: 0.64895 |  0:00:13s
epoch 69 | loss: 0.36847 | val_0_rmse: 0.64544 | val_1_rmse: 0.6326  |  0:00:14s
epoch 70 | loss: 0.37805 | val_0_rmse: 0.62664 | val_1_rmse: 0.61789 |  0:00:14s
epoch 71 | loss: 0.35802 | val_0_rmse: 0.62223 | val_1_rmse: 0.61456 |  0:00:14s
epoch 72 | loss: 0.36132 | val_0_rmse: 0.6452  | val_1_rmse: 0.6372  |  0:00:14s
epoch 73 | loss: 0.37785 | val_0_rmse: 0.62466 | val_1_rmse: 0.61058 |  0:00:14s
epoch 74 | loss: 0.3759  | val_0_rmse: 0.62407 | val_1_rmse: 0.61322 |  0:00:15s
epoch 75 | loss: 0.37314 | val_0_rmse: 0.64723 | val_1_rmse: 0.6228  |  0:00:15s
epoch 76 | loss: 0.37232 | val_0_rmse: 0.64213 | val_1_rmse: 0.6231  |  0:00:15s
epoch 77 | loss: 0.37853 | val_0_rmse: 0.60669 | val_1_rmse: 0.59887 |  0:00:15s
epoch 78 | loss: 0.35659 | val_0_rmse: 0.60807 | val_1_rmse: 0.5888  |  0:00:15s
epoch 79 | loss: 0.35491 | val_0_rmse: 0.6233  | val_1_rmse: 0.60337 |  0:00:16s
epoch 80 | loss: 0.36484 | val_0_rmse: 0.62142 | val_1_rmse: 0.60763 |  0:00:16s
epoch 81 | loss: 0.35534 | val_0_rmse: 0.59735 | val_1_rmse: 0.58129 |  0:00:16s
epoch 82 | loss: 0.36203 | val_0_rmse: 0.59475 | val_1_rmse: 0.57832 |  0:00:16s
epoch 83 | loss: 0.34888 | val_0_rmse: 0.60122 | val_1_rmse: 0.59018 |  0:00:16s
epoch 84 | loss: 0.36843 | val_0_rmse: 0.61536 | val_1_rmse: 0.59782 |  0:00:17s
epoch 85 | loss: 0.35139 | val_0_rmse: 0.63039 | val_1_rmse: 0.61319 |  0:00:17s
epoch 86 | loss: 0.36125 | val_0_rmse: 0.63619 | val_1_rmse: 0.62117 |  0:00:17s
epoch 87 | loss: 0.35094 | val_0_rmse: 0.63286 | val_1_rmse: 0.62373 |  0:00:17s
epoch 88 | loss: 0.35153 | val_0_rmse: 0.60402 | val_1_rmse: 0.60574 |  0:00:17s
epoch 89 | loss: 0.35249 | val_0_rmse: 0.58754 | val_1_rmse: 0.58925 |  0:00:18s
epoch 90 | loss: 0.35005 | val_0_rmse: 0.5843  | val_1_rmse: 0.57913 |  0:00:18s
epoch 91 | loss: 0.34359 | val_0_rmse: 0.59132 | val_1_rmse: 0.58033 |  0:00:18s
epoch 92 | loss: 0.34826 | val_0_rmse: 0.58534 | val_1_rmse: 0.57923 |  0:00:18s
epoch 93 | loss: 0.33898 | val_0_rmse: 0.5771  | val_1_rmse: 0.57231 |  0:00:18s
epoch 94 | loss: 0.33614 | val_0_rmse: 0.58274 | val_1_rmse: 0.58066 |  0:00:19s
epoch 95 | loss: 0.35217 | val_0_rmse: 0.6104  | val_1_rmse: 0.60257 |  0:00:19s
epoch 96 | loss: 0.34338 | val_0_rmse: 0.6194  | val_1_rmse: 0.60392 |  0:00:19s
epoch 97 | loss: 0.34314 | val_0_rmse: 0.60595 | val_1_rmse: 0.60026 |  0:00:19s
epoch 98 | loss: 0.3428  | val_0_rmse: 0.59606 | val_1_rmse: 0.59345 |  0:00:19s
epoch 99 | loss: 0.34319 | val_0_rmse: 0.59268 | val_1_rmse: 0.59095 |  0:00:20s
epoch 100| loss: 0.3432  | val_0_rmse: 0.604   | val_1_rmse: 0.60261 |  0:00:20s
epoch 101| loss: 0.3558  | val_0_rmse: 0.60665 | val_1_rmse: 0.6066  |  0:00:20s
epoch 102| loss: 0.35086 | val_0_rmse: 0.59735 | val_1_rmse: 0.58634 |  0:00:20s
epoch 103| loss: 0.34661 | val_0_rmse: 0.59306 | val_1_rmse: 0.58225 |  0:00:20s
epoch 104| loss: 0.34056 | val_0_rmse: 0.58272 | val_1_rmse: 0.58459 |  0:00:21s
epoch 105| loss: 0.33448 | val_0_rmse: 0.58775 | val_1_rmse: 0.59068 |  0:00:21s
epoch 106| loss: 0.33265 | val_0_rmse: 0.589   | val_1_rmse: 0.5825  |  0:00:21s
epoch 107| loss: 0.32811 | val_0_rmse: 0.58614 | val_1_rmse: 0.58455 |  0:00:21s
epoch 108| loss: 0.34036 | val_0_rmse: 0.5726  | val_1_rmse: 0.57484 |  0:00:21s
epoch 109| loss: 0.3331  | val_0_rmse: 0.56436 | val_1_rmse: 0.56883 |  0:00:22s
epoch 110| loss: 0.32169 | val_0_rmse: 0.58206 | val_1_rmse: 0.58319 |  0:00:22s
epoch 111| loss: 0.33116 | val_0_rmse: 0.60945 | val_1_rmse: 0.60213 |  0:00:22s
epoch 112| loss: 0.33906 | val_0_rmse: 0.62867 | val_1_rmse: 0.60866 |  0:00:22s
epoch 113| loss: 0.3212  | val_0_rmse: 0.61548 | val_1_rmse: 0.59455 |  0:00:22s
epoch 114| loss: 0.31745 | val_0_rmse: 0.58831 | val_1_rmse: 0.57034 |  0:00:23s
epoch 115| loss: 0.34052 | val_0_rmse: 0.59601 | val_1_rmse: 0.57808 |  0:00:23s
epoch 116| loss: 0.33008 | val_0_rmse: 0.59222 | val_1_rmse: 0.57892 |  0:00:23s
epoch 117| loss: 0.33855 | val_0_rmse: 0.59482 | val_1_rmse: 0.58302 |  0:00:23s
epoch 118| loss: 0.32642 | val_0_rmse: 0.60761 | val_1_rmse: 0.59627 |  0:00:23s
epoch 119| loss: 0.33636 | val_0_rmse: 0.61624 | val_1_rmse: 0.60247 |  0:00:24s
epoch 120| loss: 0.33227 | val_0_rmse: 0.60389 | val_1_rmse: 0.58832 |  0:00:24s
epoch 121| loss: 0.32835 | val_0_rmse: 0.62741 | val_1_rmse: 0.60891 |  0:00:24s
epoch 122| loss: 0.32033 | val_0_rmse: 0.62454 | val_1_rmse: 0.61318 |  0:00:24s
epoch 123| loss: 0.31726 | val_0_rmse: 0.61337 | val_1_rmse: 0.6048  |  0:00:24s
epoch 124| loss: 0.33166 | val_0_rmse: 0.62    | val_1_rmse: 0.59581 |  0:00:25s
epoch 125| loss: 0.31606 | val_0_rmse: 0.61593 | val_1_rmse: 0.5844  |  0:00:25s
epoch 126| loss: 0.3418  | val_0_rmse: 0.58312 | val_1_rmse: 0.55753 |  0:00:25s
epoch 127| loss: 0.32817 | val_0_rmse: 0.59602 | val_1_rmse: 0.57515 |  0:00:25s
epoch 128| loss: 0.31859 | val_0_rmse: 0.69272 | val_1_rmse: 0.66839 |  0:00:25s
epoch 129| loss: 0.32508 | val_0_rmse: 0.74247 | val_1_rmse: 0.71294 |  0:00:26s
epoch 130| loss: 0.31407 | val_0_rmse: 0.67111 | val_1_rmse: 0.64589 |  0:00:26s
epoch 131| loss: 0.31648 | val_0_rmse: 0.60123 | val_1_rmse: 0.5822  |  0:00:26s
epoch 132| loss: 0.3056  | val_0_rmse: 0.59024 | val_1_rmse: 0.56604 |  0:00:26s
epoch 133| loss: 0.31845 | val_0_rmse: 0.57659 | val_1_rmse: 0.55876 |  0:00:26s
epoch 134| loss: 0.31355 | val_0_rmse: 0.56735 | val_1_rmse: 0.56022 |  0:00:27s
epoch 135| loss: 0.30935 | val_0_rmse: 0.56805 | val_1_rmse: 0.56705 |  0:00:27s
epoch 136| loss: 0.32717 | val_0_rmse: 0.55752 | val_1_rmse: 0.55813 |  0:00:27s
epoch 137| loss: 0.33612 | val_0_rmse: 0.5576  | val_1_rmse: 0.56498 |  0:00:27s
epoch 138| loss: 0.3188  | val_0_rmse: 0.59139 | val_1_rmse: 0.59983 |  0:00:27s
epoch 139| loss: 0.31112 | val_0_rmse: 0.56511 | val_1_rmse: 0.57018 |  0:00:28s
epoch 140| loss: 0.31556 | val_0_rmse: 0.57336 | val_1_rmse: 0.58021 |  0:00:28s
epoch 141| loss: 0.32092 | val_0_rmse: 0.59023 | val_1_rmse: 0.60121 |  0:00:28s
epoch 142| loss: 0.31962 | val_0_rmse: 0.56901 | val_1_rmse: 0.5759  |  0:00:28s
epoch 143| loss: 0.30353 | val_0_rmse: 0.55585 | val_1_rmse: 0.55274 |  0:00:28s
epoch 144| loss: 0.3028  | val_0_rmse: 0.55423 | val_1_rmse: 0.54911 |  0:00:29s
epoch 145| loss: 0.31313 | val_0_rmse: 0.56055 | val_1_rmse: 0.55979 |  0:00:29s
epoch 146| loss: 0.31184 | val_0_rmse: 0.56807 | val_1_rmse: 0.56878 |  0:00:29s
epoch 147| loss: 0.30848 | val_0_rmse: 0.56758 | val_1_rmse: 0.56671 |  0:00:29s
epoch 148| loss: 0.31182 | val_0_rmse: 0.56072 | val_1_rmse: 0.56394 |  0:00:29s
epoch 149| loss: 0.30454 | val_0_rmse: 0.55623 | val_1_rmse: 0.55801 |  0:00:30s
Stop training because you reached max_epochs = 150 with best_epoch = 144 and best_val_1_rmse = 0.54911
Best weights from best epoch are automatically used!
ended training at: 08:18:49
Feature importance:
Mean squared error is of 2671346082.942364
Mean absolute error:37491.702360096155
MAPE:0.30494524822745667
R2 score:0.6705304289418789
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:18:49
epoch 0  | loss: 3.31736 | val_0_rmse: 1.01504 | val_1_rmse: 0.9922  |  0:00:00s
epoch 1  | loss: 2.34365 | val_0_rmse: 1.00961 | val_1_rmse: 1.02315 |  0:00:00s
epoch 2  | loss: 1.38042 | val_0_rmse: 1.0593  | val_1_rmse: 1.07737 |  0:00:00s
epoch 3  | loss: 1.08403 | val_0_rmse: 0.94679 | val_1_rmse: 0.93887 |  0:00:00s
epoch 4  | loss: 0.79536 | val_0_rmse: 0.86252 | val_1_rmse: 0.85734 |  0:00:01s
epoch 5  | loss: 0.71358 | val_0_rmse: 0.80069 | val_1_rmse: 0.80387 |  0:00:01s
epoch 6  | loss: 0.62916 | val_0_rmse: 0.7938  | val_1_rmse: 0.82256 |  0:00:01s
epoch 7  | loss: 0.59797 | val_0_rmse: 0.83466 | val_1_rmse: 0.89141 |  0:00:01s
epoch 8  | loss: 0.5916  | val_0_rmse: 0.83918 | val_1_rmse: 0.87758 |  0:00:01s
epoch 9  | loss: 0.55307 | val_0_rmse: 0.80324 | val_1_rmse: 0.82614 |  0:00:02s
epoch 10 | loss: 0.52336 | val_0_rmse: 1.02006 | val_1_rmse: 1.07803 |  0:00:02s
epoch 11 | loss: 0.52373 | val_0_rmse: 1.01469 | val_1_rmse: 1.07236 |  0:00:02s
epoch 12 | loss: 0.496   | val_0_rmse: 0.87676 | val_1_rmse: 0.92692 |  0:00:02s
epoch 13 | loss: 0.49123 | val_0_rmse: 0.7622  | val_1_rmse: 0.8054  |  0:00:02s
epoch 14 | loss: 0.48718 | val_0_rmse: 0.7667  | val_1_rmse: 0.80247 |  0:00:03s
epoch 15 | loss: 0.47874 | val_0_rmse: 0.79836 | val_1_rmse: 0.82274 |  0:00:03s
epoch 16 | loss: 0.46387 | val_0_rmse: 0.73762 | val_1_rmse: 0.77635 |  0:00:03s
epoch 17 | loss: 0.46533 | val_0_rmse: 0.71301 | val_1_rmse: 0.7588  |  0:00:03s
epoch 18 | loss: 0.48309 | val_0_rmse: 0.70495 | val_1_rmse: 0.74354 |  0:00:03s
epoch 19 | loss: 0.44637 | val_0_rmse: 0.7012  | val_1_rmse: 0.73165 |  0:00:04s
epoch 20 | loss: 0.46514 | val_0_rmse: 0.72623 | val_1_rmse: 0.7677  |  0:00:04s
epoch 21 | loss: 0.45739 | val_0_rmse: 0.76268 | val_1_rmse: 0.80781 |  0:00:04s
epoch 22 | loss: 0.45559 | val_0_rmse: 0.70743 | val_1_rmse: 0.73678 |  0:00:04s
epoch 23 | loss: 0.45597 | val_0_rmse: 0.71491 | val_1_rmse: 0.74496 |  0:00:04s
epoch 24 | loss: 0.45823 | val_0_rmse: 0.74406 | val_1_rmse: 0.78223 |  0:00:05s
epoch 25 | loss: 0.46126 | val_0_rmse: 0.71492 | val_1_rmse: 0.73973 |  0:00:05s
epoch 26 | loss: 0.44122 | val_0_rmse: 0.69713 | val_1_rmse: 0.71454 |  0:00:05s
epoch 27 | loss: 0.44023 | val_0_rmse: 0.70859 | val_1_rmse: 0.73574 |  0:00:05s
epoch 28 | loss: 0.43663 | val_0_rmse: 0.72122 | val_1_rmse: 0.75446 |  0:00:05s
epoch 29 | loss: 0.45052 | val_0_rmse: 0.69633 | val_1_rmse: 0.72    |  0:00:06s
epoch 30 | loss: 0.44113 | val_0_rmse: 0.6936  | val_1_rmse: 0.71762 |  0:00:06s
epoch 31 | loss: 0.42789 | val_0_rmse: 0.69463 | val_1_rmse: 0.72504 |  0:00:06s
epoch 32 | loss: 0.42742 | val_0_rmse: 0.6913  | val_1_rmse: 0.72234 |  0:00:06s
epoch 33 | loss: 0.43723 | val_0_rmse: 0.68377 | val_1_rmse: 0.70824 |  0:00:06s
epoch 34 | loss: 0.42499 | val_0_rmse: 0.68096 | val_1_rmse: 0.69863 |  0:00:07s
epoch 35 | loss: 0.42854 | val_0_rmse: 0.68007 | val_1_rmse: 0.69562 |  0:00:07s
epoch 36 | loss: 0.41868 | val_0_rmse: 0.68186 | val_1_rmse: 0.69975 |  0:00:07s
epoch 37 | loss: 0.41795 | val_0_rmse: 0.67788 | val_1_rmse: 0.69943 |  0:00:07s
epoch 38 | loss: 0.43585 | val_0_rmse: 0.67369 | val_1_rmse: 0.70666 |  0:00:07s
epoch 39 | loss: 0.4324  | val_0_rmse: 0.67777 | val_1_rmse: 0.71519 |  0:00:08s
epoch 40 | loss: 0.43726 | val_0_rmse: 0.68581 | val_1_rmse: 0.72337 |  0:00:08s
epoch 41 | loss: 0.42446 | val_0_rmse: 0.71176 | val_1_rmse: 0.74489 |  0:00:08s
epoch 42 | loss: 0.44865 | val_0_rmse: 0.7119  | val_1_rmse: 0.74507 |  0:00:08s
epoch 43 | loss: 0.44333 | val_0_rmse: 0.69659 | val_1_rmse: 0.73646 |  0:00:08s
epoch 44 | loss: 0.43237 | val_0_rmse: 0.68655 | val_1_rmse: 0.72274 |  0:00:09s
epoch 45 | loss: 0.42217 | val_0_rmse: 0.68253 | val_1_rmse: 0.72123 |  0:00:09s
epoch 46 | loss: 0.42926 | val_0_rmse: 0.68534 | val_1_rmse: 0.71951 |  0:00:09s
epoch 47 | loss: 0.42709 | val_0_rmse: 0.68032 | val_1_rmse: 0.71158 |  0:00:09s
epoch 48 | loss: 0.42864 | val_0_rmse: 0.67755 | val_1_rmse: 0.69432 |  0:00:09s
epoch 49 | loss: 0.42892 | val_0_rmse: 0.68243 | val_1_rmse: 0.70435 |  0:00:10s
epoch 50 | loss: 0.43234 | val_0_rmse: 0.67769 | val_1_rmse: 0.71064 |  0:00:10s
epoch 51 | loss: 0.43257 | val_0_rmse: 0.68204 | val_1_rmse: 0.70952 |  0:00:10s
epoch 52 | loss: 0.44658 | val_0_rmse: 0.6698  | val_1_rmse: 0.68899 |  0:00:10s
epoch 53 | loss: 0.42915 | val_0_rmse: 0.67598 | val_1_rmse: 0.67715 |  0:00:10s
epoch 54 | loss: 0.43656 | val_0_rmse: 0.67233 | val_1_rmse: 0.66212 |  0:00:11s
epoch 55 | loss: 0.42821 | val_0_rmse: 0.67164 | val_1_rmse: 0.65651 |  0:00:11s
epoch 56 | loss: 0.43765 | val_0_rmse: 0.67354 | val_1_rmse: 0.65752 |  0:00:11s
epoch 57 | loss: 0.43098 | val_0_rmse: 0.67351 | val_1_rmse: 0.65883 |  0:00:11s
epoch 58 | loss: 0.43368 | val_0_rmse: 0.66717 | val_1_rmse: 0.65169 |  0:00:11s
epoch 59 | loss: 0.44037 | val_0_rmse: 0.66426 | val_1_rmse: 0.65517 |  0:00:12s
epoch 60 | loss: 0.42763 | val_0_rmse: 0.66918 | val_1_rmse: 0.66179 |  0:00:12s
epoch 61 | loss: 0.42823 | val_0_rmse: 0.6784  | val_1_rmse: 0.67363 |  0:00:12s
epoch 62 | loss: 0.42213 | val_0_rmse: 0.72394 | val_1_rmse: 0.71997 |  0:00:12s
epoch 63 | loss: 0.42823 | val_0_rmse: 0.76385 | val_1_rmse: 0.75118 |  0:00:12s
epoch 64 | loss: 0.43729 | val_0_rmse: 0.74613 | val_1_rmse: 0.7367  |  0:00:12s
epoch 65 | loss: 0.43245 | val_0_rmse: 0.72537 | val_1_rmse: 0.71071 |  0:00:13s
epoch 66 | loss: 0.44711 | val_0_rmse: 0.71373 | val_1_rmse: 0.69613 |  0:00:13s
epoch 67 | loss: 0.42722 | val_0_rmse: 0.69796 | val_1_rmse: 0.6836  |  0:00:13s
epoch 68 | loss: 0.4397  | val_0_rmse: 0.67936 | val_1_rmse: 0.66974 |  0:00:13s
epoch 69 | loss: 0.42629 | val_0_rmse: 0.6694  | val_1_rmse: 0.66423 |  0:00:13s
epoch 70 | loss: 0.42891 | val_0_rmse: 0.66875 | val_1_rmse: 0.66755 |  0:00:14s
epoch 71 | loss: 0.43591 | val_0_rmse: 0.66222 | val_1_rmse: 0.66161 |  0:00:14s
epoch 72 | loss: 0.43333 | val_0_rmse: 0.662   | val_1_rmse: 0.65835 |  0:00:14s
epoch 73 | loss: 0.43437 | val_0_rmse: 0.66629 | val_1_rmse: 0.66476 |  0:00:14s
epoch 74 | loss: 0.43079 | val_0_rmse: 0.66288 | val_1_rmse: 0.66706 |  0:00:14s
epoch 75 | loss: 0.42084 | val_0_rmse: 0.65971 | val_1_rmse: 0.66736 |  0:00:15s
epoch 76 | loss: 0.42822 | val_0_rmse: 0.66229 | val_1_rmse: 0.66878 |  0:00:15s
epoch 77 | loss: 0.42384 | val_0_rmse: 0.66027 | val_1_rmse: 0.659   |  0:00:15s
epoch 78 | loss: 0.42037 | val_0_rmse: 0.65996 | val_1_rmse: 0.65382 |  0:00:15s
epoch 79 | loss: 0.42236 | val_0_rmse: 0.66099 | val_1_rmse: 0.65544 |  0:00:15s
epoch 80 | loss: 0.42569 | val_0_rmse: 0.66066 | val_1_rmse: 0.65385 |  0:00:16s
epoch 81 | loss: 0.42688 | val_0_rmse: 0.66056 | val_1_rmse: 0.65398 |  0:00:16s
epoch 82 | loss: 0.4223  | val_0_rmse: 0.66214 | val_1_rmse: 0.65368 |  0:00:16s
epoch 83 | loss: 0.42498 | val_0_rmse: 0.65755 | val_1_rmse: 0.65056 |  0:00:16s
epoch 84 | loss: 0.42362 | val_0_rmse: 0.66138 | val_1_rmse: 0.6565  |  0:00:16s
epoch 85 | loss: 0.414   | val_0_rmse: 0.66284 | val_1_rmse: 0.66101 |  0:00:17s
epoch 86 | loss: 0.43229 | val_0_rmse: 0.65857 | val_1_rmse: 0.65891 |  0:00:17s
epoch 87 | loss: 0.42046 | val_0_rmse: 0.66038 | val_1_rmse: 0.66054 |  0:00:17s
epoch 88 | loss: 0.42198 | val_0_rmse: 0.6545  | val_1_rmse: 0.65821 |  0:00:17s
epoch 89 | loss: 0.4171  | val_0_rmse: 0.65795 | val_1_rmse: 0.65512 |  0:00:17s
epoch 90 | loss: 0.42236 | val_0_rmse: 0.65786 | val_1_rmse: 0.64518 |  0:00:18s
epoch 91 | loss: 0.4175  | val_0_rmse: 0.66074 | val_1_rmse: 0.64383 |  0:00:18s
epoch 92 | loss: 0.41988 | val_0_rmse: 0.66737 | val_1_rmse: 0.6519  |  0:00:18s
epoch 93 | loss: 0.41269 | val_0_rmse: 0.66038 | val_1_rmse: 0.65078 |  0:00:18s
epoch 94 | loss: 0.41811 | val_0_rmse: 0.64915 | val_1_rmse: 0.64746 |  0:00:19s
epoch 95 | loss: 0.41143 | val_0_rmse: 0.64984 | val_1_rmse: 0.65465 |  0:00:19s
epoch 96 | loss: 0.41834 | val_0_rmse: 0.65749 | val_1_rmse: 0.66542 |  0:00:19s
epoch 97 | loss: 0.41093 | val_0_rmse: 0.6573  | val_1_rmse: 0.66848 |  0:00:19s
epoch 98 | loss: 0.41641 | val_0_rmse: 0.6537  | val_1_rmse: 0.67135 |  0:00:19s
epoch 99 | loss: 0.41075 | val_0_rmse: 0.64551 | val_1_rmse: 0.66042 |  0:00:19s
epoch 100| loss: 0.40611 | val_0_rmse: 0.63905 | val_1_rmse: 0.65321 |  0:00:20s
epoch 101| loss: 0.42152 | val_0_rmse: 0.64259 | val_1_rmse: 0.6498  |  0:00:20s
epoch 102| loss: 0.40677 | val_0_rmse: 0.64225 | val_1_rmse: 0.64566 |  0:00:20s
epoch 103| loss: 0.42357 | val_0_rmse: 0.64972 | val_1_rmse: 0.65301 |  0:00:20s
epoch 104| loss: 0.43012 | val_0_rmse: 0.64356 | val_1_rmse: 0.65236 |  0:00:21s
epoch 105| loss: 0.41222 | val_0_rmse: 0.64009 | val_1_rmse: 0.65055 |  0:00:21s
epoch 106| loss: 0.41981 | val_0_rmse: 0.64012 | val_1_rmse: 0.65029 |  0:00:21s
epoch 107| loss: 0.41441 | val_0_rmse: 0.64506 | val_1_rmse: 0.65178 |  0:00:21s
epoch 108| loss: 0.40606 | val_0_rmse: 0.64876 | val_1_rmse: 0.65415 |  0:00:21s
epoch 109| loss: 0.40397 | val_0_rmse: 0.64659 | val_1_rmse: 0.65812 |  0:00:22s
epoch 110| loss: 0.41615 | val_0_rmse: 0.64402 | val_1_rmse: 0.66083 |  0:00:22s
epoch 111| loss: 0.4054  | val_0_rmse: 0.64192 | val_1_rmse: 0.66192 |  0:00:22s
epoch 112| loss: 0.41514 | val_0_rmse: 0.64818 | val_1_rmse: 0.66755 |  0:00:22s
epoch 113| loss: 0.4024  | val_0_rmse: 0.65381 | val_1_rmse: 0.67162 |  0:00:22s
epoch 114| loss: 0.40955 | val_0_rmse: 0.6561  | val_1_rmse: 0.67431 |  0:00:23s
epoch 115| loss: 0.40483 | val_0_rmse: 0.65115 | val_1_rmse: 0.67226 |  0:00:23s
epoch 116| loss: 0.40578 | val_0_rmse: 0.64685 | val_1_rmse: 0.6703  |  0:00:23s
epoch 117| loss: 0.40726 | val_0_rmse: 0.64034 | val_1_rmse: 0.66913 |  0:00:23s
epoch 118| loss: 0.40969 | val_0_rmse: 0.63326 | val_1_rmse: 0.6691  |  0:00:23s
epoch 119| loss: 0.40963 | val_0_rmse: 0.63313 | val_1_rmse: 0.67114 |  0:00:24s
epoch 120| loss: 0.40962 | val_0_rmse: 0.63608 | val_1_rmse: 0.6687  |  0:00:24s
epoch 121| loss: 0.41019 | val_0_rmse: 0.62923 | val_1_rmse: 0.66306 |  0:00:24s

Early stopping occured at epoch 121 with best_epoch = 91 and best_val_1_rmse = 0.64383
Best weights from best epoch are automatically used!
ended training at: 08:19:14
Feature importance:
Mean squared error is of 3266659195.121214
Mean absolute error:41516.699704464285
MAPE:0.3706659934876625
R2 score:0.5611130474155427
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:19:14
epoch 0  | loss: 3.09923 | val_0_rmse: 1.18302 | val_1_rmse: 1.04508 |  0:00:00s
epoch 1  | loss: 1.41449 | val_0_rmse: 1.05292 | val_1_rmse: 1.03663 |  0:00:00s
epoch 2  | loss: 1.14069 | val_0_rmse: 0.94663 | val_1_rmse: 0.96183 |  0:00:00s
epoch 3  | loss: 0.93003 | val_0_rmse: 0.82676 | val_1_rmse: 0.84749 |  0:00:00s
epoch 4  | loss: 0.76268 | val_0_rmse: 0.79708 | val_1_rmse: 0.80657 |  0:00:01s
epoch 5  | loss: 0.69424 | val_0_rmse: 0.74096 | val_1_rmse: 0.73858 |  0:00:01s
epoch 6  | loss: 0.62228 | val_0_rmse: 0.74345 | val_1_rmse: 0.71832 |  0:00:01s
epoch 7  | loss: 0.57198 | val_0_rmse: 0.70085 | val_1_rmse: 0.66665 |  0:00:01s
epoch 8  | loss: 0.53564 | val_0_rmse: 0.72357 | val_1_rmse: 0.70228 |  0:00:01s
epoch 9  | loss: 0.55423 | val_0_rmse: 0.72567 | val_1_rmse: 0.70544 |  0:00:02s
epoch 10 | loss: 0.50825 | val_0_rmse: 0.71833 | val_1_rmse: 0.6892  |  0:00:02s
epoch 11 | loss: 0.50784 | val_0_rmse: 0.72088 | val_1_rmse: 0.69778 |  0:00:02s
epoch 12 | loss: 0.4971  | val_0_rmse: 0.72366 | val_1_rmse: 0.70192 |  0:00:02s
epoch 13 | loss: 0.4904  | val_0_rmse: 0.70902 | val_1_rmse: 0.70008 |  0:00:02s
epoch 14 | loss: 0.48192 | val_0_rmse: 0.697   | val_1_rmse: 0.69662 |  0:00:03s
epoch 15 | loss: 0.48705 | val_0_rmse: 0.71243 | val_1_rmse: 0.71251 |  0:00:03s
epoch 16 | loss: 0.46493 | val_0_rmse: 0.7287  | val_1_rmse: 0.71296 |  0:00:03s
epoch 17 | loss: 0.46643 | val_0_rmse: 0.70721 | val_1_rmse: 0.69285 |  0:00:03s
epoch 18 | loss: 0.46982 | val_0_rmse: 0.70572 | val_1_rmse: 0.69686 |  0:00:03s
epoch 19 | loss: 0.46149 | val_0_rmse: 0.70429 | val_1_rmse: 0.697   |  0:00:04s
epoch 20 | loss: 0.47557 | val_0_rmse: 0.69266 | val_1_rmse: 0.68961 |  0:00:04s
epoch 21 | loss: 0.45819 | val_0_rmse: 0.68049 | val_1_rmse: 0.68562 |  0:00:04s
epoch 22 | loss: 0.45567 | val_0_rmse: 0.68984 | val_1_rmse: 0.67585 |  0:00:04s
epoch 23 | loss: 0.45642 | val_0_rmse: 0.70245 | val_1_rmse: 0.67913 |  0:00:04s
epoch 24 | loss: 0.46486 | val_0_rmse: 0.69426 | val_1_rmse: 0.67466 |  0:00:05s
epoch 25 | loss: 0.45881 | val_0_rmse: 0.69441 | val_1_rmse: 0.6881  |  0:00:05s
epoch 26 | loss: 0.45449 | val_0_rmse: 0.70004 | val_1_rmse: 0.69788 |  0:00:05s
epoch 27 | loss: 0.45343 | val_0_rmse: 0.71572 | val_1_rmse: 0.70646 |  0:00:05s
epoch 28 | loss: 0.45748 | val_0_rmse: 0.70463 | val_1_rmse: 0.69295 |  0:00:05s
epoch 29 | loss: 0.46285 | val_0_rmse: 0.70265 | val_1_rmse: 0.69967 |  0:00:06s
epoch 30 | loss: 0.4598  | val_0_rmse: 0.7131  | val_1_rmse: 0.71321 |  0:00:06s
epoch 31 | loss: 0.46457 | val_0_rmse: 0.70205 | val_1_rmse: 0.69961 |  0:00:06s
epoch 32 | loss: 0.46108 | val_0_rmse: 0.70389 | val_1_rmse: 0.69842 |  0:00:06s
epoch 33 | loss: 0.45247 | val_0_rmse: 0.70979 | val_1_rmse: 0.70311 |  0:00:06s
epoch 34 | loss: 0.45604 | val_0_rmse: 0.6901  | val_1_rmse: 0.68982 |  0:00:07s
epoch 35 | loss: 0.4523  | val_0_rmse: 0.68837 | val_1_rmse: 0.69685 |  0:00:07s
epoch 36 | loss: 0.45049 | val_0_rmse: 0.69099 | val_1_rmse: 0.69257 |  0:00:07s
epoch 37 | loss: 0.46067 | val_0_rmse: 0.68248 | val_1_rmse: 0.68015 |  0:00:07s

Early stopping occured at epoch 37 with best_epoch = 7 and best_val_1_rmse = 0.66665
Best weights from best epoch are automatically used!
ended training at: 08:19:22
Feature importance:
Mean squared error is of 4041078704.0499306
Mean absolute error:48152.940678296705
MAPE:0.4909800159451488
R2 score:0.4702879722651052
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:19:22
epoch 0  | loss: 3.20868 | val_0_rmse: 0.95578 | val_1_rmse: 0.91637 |  0:00:00s
epoch 1  | loss: 1.30767 | val_0_rmse: 0.81699 | val_1_rmse: 0.77511 |  0:00:00s
epoch 2  | loss: 0.99291 | val_0_rmse: 0.85749 | val_1_rmse: 0.79544 |  0:00:00s
epoch 3  | loss: 0.64281 | val_0_rmse: 0.89604 | val_1_rmse: 0.87453 |  0:00:00s
epoch 4  | loss: 0.71326 | val_0_rmse: 0.78255 | val_1_rmse: 0.80625 |  0:00:01s
epoch 5  | loss: 0.59708 | val_0_rmse: 0.80485 | val_1_rmse: 0.82113 |  0:00:01s
epoch 6  | loss: 0.64235 | val_0_rmse: 0.78761 | val_1_rmse: 0.79398 |  0:00:01s
epoch 7  | loss: 0.55511 | val_0_rmse: 0.76104 | val_1_rmse: 0.75065 |  0:00:01s
epoch 8  | loss: 0.55999 | val_0_rmse: 0.77728 | val_1_rmse: 0.76583 |  0:00:01s
epoch 9  | loss: 0.56888 | val_0_rmse: 1.02295 | val_1_rmse: 0.73385 |  0:00:02s
epoch 10 | loss: 0.58189 | val_0_rmse: 0.94457 | val_1_rmse: 0.72607 |  0:00:02s
epoch 11 | loss: 0.51041 | val_0_rmse: 0.75116 | val_1_rmse: 0.74941 |  0:00:02s
epoch 12 | loss: 0.53328 | val_0_rmse: 0.73987 | val_1_rmse: 0.73568 |  0:00:02s
epoch 13 | loss: 0.49133 | val_0_rmse: 0.72604 | val_1_rmse: 0.74628 |  0:00:02s
epoch 14 | loss: 0.47037 | val_0_rmse: 0.73045 | val_1_rmse: 0.75047 |  0:00:03s
epoch 15 | loss: 0.48893 | val_0_rmse: 0.7242  | val_1_rmse: 0.7318  |  0:00:03s
epoch 16 | loss: 0.47181 | val_0_rmse: 0.69947 | val_1_rmse: 0.70037 |  0:00:03s
epoch 17 | loss: 0.47166 | val_0_rmse: 0.69108 | val_1_rmse: 0.68188 |  0:00:03s
epoch 18 | loss: 0.47131 | val_0_rmse: 0.69956 | val_1_rmse: 0.68746 |  0:00:03s
epoch 19 | loss: 0.47764 | val_0_rmse: 0.71542 | val_1_rmse: 0.67687 |  0:00:04s
epoch 20 | loss: 0.44481 | val_0_rmse: 0.74214 | val_1_rmse: 0.69727 |  0:00:04s
epoch 21 | loss: 0.46061 | val_0_rmse: 0.72598 | val_1_rmse: 0.69777 |  0:00:04s
epoch 22 | loss: 0.44837 | val_0_rmse: 0.69338 | val_1_rmse: 0.6934  |  0:00:04s
epoch 23 | loss: 0.43793 | val_0_rmse: 0.6807  | val_1_rmse: 0.70725 |  0:00:04s
epoch 24 | loss: 0.44628 | val_0_rmse: 0.67643 | val_1_rmse: 0.7288  |  0:00:05s
epoch 25 | loss: 0.44502 | val_0_rmse: 0.6792  | val_1_rmse: 0.71606 |  0:00:05s
epoch 26 | loss: 0.44755 | val_0_rmse: 0.67828 | val_1_rmse: 0.7116  |  0:00:05s
epoch 27 | loss: 0.43699 | val_0_rmse: 0.67312 | val_1_rmse: 0.71067 |  0:00:05s
epoch 28 | loss: 0.4369  | val_0_rmse: 0.67365 | val_1_rmse: 0.70747 |  0:00:05s
epoch 29 | loss: 0.45878 | val_0_rmse: 0.67885 | val_1_rmse: 0.6972  |  0:00:06s
epoch 30 | loss: 0.43525 | val_0_rmse: 0.67988 | val_1_rmse: 0.6913  |  0:00:06s
epoch 31 | loss: 0.4386  | val_0_rmse: 0.684   | val_1_rmse: 0.6921  |  0:00:06s
epoch 32 | loss: 0.44584 | val_0_rmse: 0.67384 | val_1_rmse: 0.69059 |  0:00:06s
epoch 33 | loss: 0.43075 | val_0_rmse: 0.67079 | val_1_rmse: 0.70959 |  0:00:06s
epoch 34 | loss: 0.44233 | val_0_rmse: 0.66009 | val_1_rmse: 0.69335 |  0:00:07s
epoch 35 | loss: 0.43063 | val_0_rmse: 0.65952 | val_1_rmse: 0.69826 |  0:00:07s
epoch 36 | loss: 0.43423 | val_0_rmse: 0.65405 | val_1_rmse: 0.68625 |  0:00:07s
epoch 37 | loss: 0.432   | val_0_rmse: 0.66784 | val_1_rmse: 0.69589 |  0:00:07s
epoch 38 | loss: 0.42877 | val_0_rmse: 0.66129 | val_1_rmse: 0.69606 |  0:00:07s
epoch 39 | loss: 0.42823 | val_0_rmse: 0.66003 | val_1_rmse: 0.69798 |  0:00:08s
epoch 40 | loss: 0.4155  | val_0_rmse: 0.65885 | val_1_rmse: 0.6918  |  0:00:08s
epoch 41 | loss: 0.41948 | val_0_rmse: 0.66496 | val_1_rmse: 0.69811 |  0:00:08s
epoch 42 | loss: 0.43221 | val_0_rmse: 0.67749 | val_1_rmse: 0.71201 |  0:00:08s
epoch 43 | loss: 0.44339 | val_0_rmse: 0.68698 | val_1_rmse: 0.71126 |  0:00:08s
epoch 44 | loss: 0.44601 | val_0_rmse: 0.67486 | val_1_rmse: 0.67624 |  0:00:09s
epoch 45 | loss: 0.45228 | val_0_rmse: 0.66515 | val_1_rmse: 0.67118 |  0:00:09s
epoch 46 | loss: 0.43119 | val_0_rmse: 0.68034 | val_1_rmse: 0.71698 |  0:00:09s
epoch 47 | loss: 0.44353 | val_0_rmse: 0.67026 | val_1_rmse: 0.7116  |  0:00:09s
epoch 48 | loss: 0.44623 | val_0_rmse: 0.65985 | val_1_rmse: 0.6931  |  0:00:09s
epoch 49 | loss: 0.43971 | val_0_rmse: 0.67333 | val_1_rmse: 0.69563 |  0:00:10s
epoch 50 | loss: 0.45833 | val_0_rmse: 0.66863 | val_1_rmse: 0.69716 |  0:00:10s
epoch 51 | loss: 0.44129 | val_0_rmse: 0.67107 | val_1_rmse: 0.70904 |  0:00:10s
epoch 52 | loss: 0.44446 | val_0_rmse: 0.66307 | val_1_rmse: 0.70081 |  0:00:10s
epoch 53 | loss: 0.43735 | val_0_rmse: 0.67818 | val_1_rmse: 0.70369 |  0:00:10s
epoch 54 | loss: 0.45032 | val_0_rmse: 0.67763 | val_1_rmse: 0.71163 |  0:00:11s
epoch 55 | loss: 0.4329  | val_0_rmse: 0.67065 | val_1_rmse: 0.71666 |  0:00:11s
epoch 56 | loss: 0.43128 | val_0_rmse: 0.6727  | val_1_rmse: 0.71165 |  0:00:11s
epoch 57 | loss: 0.42838 | val_0_rmse: 0.68299 | val_1_rmse: 0.71058 |  0:00:11s
epoch 58 | loss: 0.43126 | val_0_rmse: 0.67859 | val_1_rmse: 0.71591 |  0:00:11s
epoch 59 | loss: 0.41829 | val_0_rmse: 0.66389 | val_1_rmse: 0.72066 |  0:00:11s
epoch 60 | loss: 0.43224 | val_0_rmse: 0.67409 | val_1_rmse: 0.73953 |  0:00:12s
epoch 61 | loss: 0.42572 | val_0_rmse: 0.68723 | val_1_rmse: 0.73612 |  0:00:12s
epoch 62 | loss: 0.43496 | val_0_rmse: 0.68119 | val_1_rmse: 0.72302 |  0:00:12s
epoch 63 | loss: 0.43399 | val_0_rmse: 0.65093 | val_1_rmse: 0.71296 |  0:00:12s
epoch 64 | loss: 0.42921 | val_0_rmse: 0.65835 | val_1_rmse: 0.7187  |  0:00:13s
epoch 65 | loss: 0.43112 | val_0_rmse: 0.66014 | val_1_rmse: 0.70468 |  0:00:13s
epoch 66 | loss: 0.43686 | val_0_rmse: 0.66333 | val_1_rmse: 0.69017 |  0:00:13s
epoch 67 | loss: 0.44066 | val_0_rmse: 0.6637  | val_1_rmse: 0.68109 |  0:00:13s
epoch 68 | loss: 0.43153 | val_0_rmse: 0.65441 | val_1_rmse: 0.67556 |  0:00:13s
epoch 69 | loss: 0.43911 | val_0_rmse: 0.64777 | val_1_rmse: 0.67719 |  0:00:14s
epoch 70 | loss: 0.42018 | val_0_rmse: 0.6586  | val_1_rmse: 0.67097 |  0:00:14s
epoch 71 | loss: 0.42216 | val_0_rmse: 0.66693 | val_1_rmse: 0.67017 |  0:00:14s
epoch 72 | loss: 0.43668 | val_0_rmse: 0.64846 | val_1_rmse: 0.66318 |  0:00:14s
epoch 73 | loss: 0.40702 | val_0_rmse: 0.65416 | val_1_rmse: 0.69284 |  0:00:14s
epoch 74 | loss: 0.40527 | val_0_rmse: 0.63198 | val_1_rmse: 0.67574 |  0:00:15s
epoch 75 | loss: 0.40038 | val_0_rmse: 0.63287 | val_1_rmse: 0.67764 |  0:00:15s
epoch 76 | loss: 0.39512 | val_0_rmse: 0.62171 | val_1_rmse: 0.67837 |  0:00:15s
epoch 77 | loss: 0.38713 | val_0_rmse: 0.6177  | val_1_rmse: 0.67739 |  0:00:15s
epoch 78 | loss: 0.39385 | val_0_rmse: 0.61876 | val_1_rmse: 0.6777  |  0:00:15s
epoch 79 | loss: 0.40302 | val_0_rmse: 0.61883 | val_1_rmse: 0.67039 |  0:00:16s
epoch 80 | loss: 0.39995 | val_0_rmse: 0.6405  | val_1_rmse: 0.69933 |  0:00:16s
epoch 81 | loss: 0.39187 | val_0_rmse: 0.63764 | val_1_rmse: 0.69934 |  0:00:16s
epoch 82 | loss: 0.40906 | val_0_rmse: 0.60451 | val_1_rmse: 0.66254 |  0:00:16s
epoch 83 | loss: 0.38569 | val_0_rmse: 0.617   | val_1_rmse: 0.67345 |  0:00:16s
epoch 84 | loss: 0.38916 | val_0_rmse: 0.62783 | val_1_rmse: 0.68157 |  0:00:17s
epoch 85 | loss: 0.37875 | val_0_rmse: 0.65769 | val_1_rmse: 0.70899 |  0:00:17s
epoch 86 | loss: 0.39793 | val_0_rmse: 0.6406  | val_1_rmse: 0.69872 |  0:00:17s
epoch 87 | loss: 0.37214 | val_0_rmse: 0.61512 | val_1_rmse: 0.6692  |  0:00:17s
epoch 88 | loss: 0.37395 | val_0_rmse: 0.61237 | val_1_rmse: 0.66669 |  0:00:17s
epoch 89 | loss: 0.37737 | val_0_rmse: 0.61109 | val_1_rmse: 0.65936 |  0:00:18s
epoch 90 | loss: 0.37161 | val_0_rmse: 0.61448 | val_1_rmse: 0.66317 |  0:00:18s
epoch 91 | loss: 0.38176 | val_0_rmse: 0.64757 | val_1_rmse: 0.70395 |  0:00:18s
epoch 92 | loss: 0.3572  | val_0_rmse: 0.64124 | val_1_rmse: 0.71074 |  0:00:18s
epoch 93 | loss: 0.36523 | val_0_rmse: 0.58732 | val_1_rmse: 0.65632 |  0:00:18s
epoch 94 | loss: 0.36942 | val_0_rmse: 0.64317 | val_1_rmse: 0.70511 |  0:00:19s
epoch 95 | loss: 0.42632 | val_0_rmse: 0.69967 | val_1_rmse: 0.75433 |  0:00:19s
epoch 96 | loss: 0.40698 | val_0_rmse: 0.74007 | val_1_rmse: 0.79114 |  0:00:19s
epoch 97 | loss: 0.43488 | val_0_rmse: 0.69107 | val_1_rmse: 0.74546 |  0:00:19s
epoch 98 | loss: 0.42167 | val_0_rmse: 0.65328 | val_1_rmse: 0.71309 |  0:00:19s
epoch 99 | loss: 0.41718 | val_0_rmse: 0.664   | val_1_rmse: 0.71899 |  0:00:20s
epoch 100| loss: 0.40751 | val_0_rmse: 0.64486 | val_1_rmse: 0.69917 |  0:00:20s
epoch 101| loss: 0.38079 | val_0_rmse: 0.63637 | val_1_rmse: 0.70146 |  0:00:20s
epoch 102| loss: 0.39934 | val_0_rmse: 0.6343  | val_1_rmse: 0.69782 |  0:00:20s
epoch 103| loss: 0.36557 | val_0_rmse: 0.63107 | val_1_rmse: 0.69096 |  0:00:20s
epoch 104| loss: 0.37528 | val_0_rmse: 0.62755 | val_1_rmse: 0.68419 |  0:00:21s
epoch 105| loss: 0.36594 | val_0_rmse: 0.61561 | val_1_rmse: 0.66467 |  0:00:21s
epoch 106| loss: 0.36857 | val_0_rmse: 0.70462 | val_1_rmse: 0.75542 |  0:00:21s
epoch 107| loss: 0.37171 | val_0_rmse: 0.75714 | val_1_rmse: 0.80502 |  0:00:21s
epoch 108| loss: 0.37237 | val_0_rmse: 0.641   | val_1_rmse: 0.69924 |  0:00:22s
epoch 109| loss: 0.35398 | val_0_rmse: 0.59513 | val_1_rmse: 0.6438  |  0:00:22s
epoch 110| loss: 0.3509  | val_0_rmse: 0.59323 | val_1_rmse: 0.64634 |  0:00:22s
epoch 111| loss: 0.34486 | val_0_rmse: 0.58921 | val_1_rmse: 0.6536  |  0:00:22s
epoch 112| loss: 0.348   | val_0_rmse: 0.57888 | val_1_rmse: 0.63988 |  0:00:22s
epoch 113| loss: 0.34397 | val_0_rmse: 0.61301 | val_1_rmse: 0.66911 |  0:00:23s
epoch 114| loss: 0.35149 | val_0_rmse: 0.65624 | val_1_rmse: 0.71931 |  0:00:23s
epoch 115| loss: 0.33956 | val_0_rmse: 0.61566 | val_1_rmse: 0.66485 |  0:00:23s
epoch 116| loss: 0.33432 | val_0_rmse: 0.5671  | val_1_rmse: 0.61191 |  0:00:23s
epoch 117| loss: 0.3216  | val_0_rmse: 0.57703 | val_1_rmse: 0.62401 |  0:00:23s
epoch 118| loss: 0.33569 | val_0_rmse: 0.60725 | val_1_rmse: 0.65434 |  0:00:24s
epoch 119| loss: 0.33297 | val_0_rmse: 0.60401 | val_1_rmse: 0.64155 |  0:00:24s
epoch 120| loss: 0.35666 | val_0_rmse: 0.58485 | val_1_rmse: 0.6157  |  0:00:24s
epoch 121| loss: 0.32767 | val_0_rmse: 0.59533 | val_1_rmse: 0.63185 |  0:00:24s
epoch 122| loss: 0.34701 | val_0_rmse: 0.62146 | val_1_rmse: 0.6634  |  0:00:24s
epoch 123| loss: 0.33597 | val_0_rmse: 0.61795 | val_1_rmse: 0.65083 |  0:00:25s
epoch 124| loss: 0.34972 | val_0_rmse: 0.58533 | val_1_rmse: 0.61958 |  0:00:25s
epoch 125| loss: 0.33399 | val_0_rmse: 0.57436 | val_1_rmse: 0.6032  |  0:00:25s
epoch 126| loss: 0.34552 | val_0_rmse: 0.60127 | val_1_rmse: 0.62031 |  0:00:25s
epoch 127| loss: 0.32567 | val_0_rmse: 0.64634 | val_1_rmse: 0.66821 |  0:00:25s
epoch 128| loss: 0.34756 | val_0_rmse: 0.61563 | val_1_rmse: 0.63744 |  0:00:26s
epoch 129| loss: 0.33397 | val_0_rmse: 0.58403 | val_1_rmse: 0.60697 |  0:00:26s
epoch 130| loss: 0.33179 | val_0_rmse: 0.58988 | val_1_rmse: 0.61049 |  0:00:26s
epoch 131| loss: 0.32347 | val_0_rmse: 0.57227 | val_1_rmse: 0.5887  |  0:00:26s
epoch 132| loss: 0.32076 | val_0_rmse: 0.58011 | val_1_rmse: 0.59798 |  0:00:26s
epoch 133| loss: 0.32764 | val_0_rmse: 0.58143 | val_1_rmse: 0.60835 |  0:00:27s
epoch 134| loss: 0.32209 | val_0_rmse: 0.56227 | val_1_rmse: 0.59489 |  0:00:27s
epoch 135| loss: 0.32075 | val_0_rmse: 0.55655 | val_1_rmse: 0.58927 |  0:00:27s
epoch 136| loss: 0.33187 | val_0_rmse: 0.57206 | val_1_rmse: 0.60097 |  0:00:27s
epoch 137| loss: 0.31941 | val_0_rmse: 0.63856 | val_1_rmse: 0.67464 |  0:00:27s
epoch 138| loss: 0.32093 | val_0_rmse: 0.64626 | val_1_rmse: 0.67906 |  0:00:28s
epoch 139| loss: 0.31778 | val_0_rmse: 0.57127 | val_1_rmse: 0.59523 |  0:00:28s
epoch 140| loss: 0.30977 | val_0_rmse: 0.57747 | val_1_rmse: 0.59446 |  0:00:28s
epoch 141| loss: 0.31571 | val_0_rmse: 0.56557 | val_1_rmse: 0.5827  |  0:00:28s
epoch 142| loss: 0.31315 | val_0_rmse: 0.55607 | val_1_rmse: 0.57797 |  0:00:28s
epoch 143| loss: 0.331   | val_0_rmse: 0.55577 | val_1_rmse: 0.57424 |  0:00:29s
epoch 144| loss: 0.31625 | val_0_rmse: 0.56159 | val_1_rmse: 0.57522 |  0:00:29s
epoch 145| loss: 0.32967 | val_0_rmse: 0.55603 | val_1_rmse: 0.57274 |  0:00:29s
epoch 146| loss: 0.31948 | val_0_rmse: 0.56661 | val_1_rmse: 0.58476 |  0:00:29s
epoch 147| loss: 0.31077 | val_0_rmse: 0.57122 | val_1_rmse: 0.58531 |  0:00:29s
epoch 148| loss: 0.32485 | val_0_rmse: 0.55775 | val_1_rmse: 0.57731 |  0:00:30s
epoch 149| loss: 0.31539 | val_0_rmse: 0.57788 | val_1_rmse: 0.60537 |  0:00:30s
Stop training because you reached max_epochs = 150 with best_epoch = 145 and best_val_1_rmse = 0.57274
Best weights from best epoch are automatically used!
ended training at: 08:19:52
Feature importance:
Mean squared error is of 2375699073.067838
Mean absolute error:35615.72874107143
MAPE:0.32520221556995177
R2 score:0.6697105574051749
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:19:52
epoch 0  | loss: 2.97451 | val_0_rmse: 1.17581 | val_1_rmse: 1.17331 |  0:00:00s
epoch 1  | loss: 1.62134 | val_0_rmse: 1.04773 | val_1_rmse: 1.06002 |  0:00:00s
epoch 2  | loss: 1.25178 | val_0_rmse: 0.87249 | val_1_rmse: 0.91113 |  0:00:00s
epoch 3  | loss: 0.87241 | val_0_rmse: 0.88768 | val_1_rmse: 0.90659 |  0:00:00s
epoch 4  | loss: 0.68773 | val_0_rmse: 0.85881 | val_1_rmse: 0.85845 |  0:00:01s
epoch 5  | loss: 0.63882 | val_0_rmse: 0.80295 | val_1_rmse: 0.84056 |  0:00:01s
epoch 6  | loss: 0.55043 | val_0_rmse: 0.77989 | val_1_rmse: 0.87986 |  0:00:01s
epoch 7  | loss: 0.51496 | val_0_rmse: 0.76012 | val_1_rmse: 0.86181 |  0:00:01s
epoch 8  | loss: 0.5139  | val_0_rmse: 0.77633 | val_1_rmse: 0.81705 |  0:00:01s
epoch 9  | loss: 0.53459 | val_0_rmse: 0.7301  | val_1_rmse: 0.77217 |  0:00:02s
epoch 10 | loss: 0.48824 | val_0_rmse: 0.75697 | val_1_rmse: 0.80838 |  0:00:02s
epoch 11 | loss: 0.44969 | val_0_rmse: 0.79081 | val_1_rmse: 0.85683 |  0:00:02s
epoch 12 | loss: 0.4771  | val_0_rmse: 0.69084 | val_1_rmse: 0.75781 |  0:00:02s
epoch 13 | loss: 0.48347 | val_0_rmse: 0.69246 | val_1_rmse: 0.76578 |  0:00:02s
epoch 14 | loss: 0.46304 | val_0_rmse: 0.74745 | val_1_rmse: 0.82397 |  0:00:03s
epoch 15 | loss: 0.47112 | val_0_rmse: 0.78389 | val_1_rmse: 0.85274 |  0:00:03s
epoch 16 | loss: 0.46428 | val_0_rmse: 0.77424 | val_1_rmse: 0.83658 |  0:00:03s
epoch 17 | loss: 0.46015 | val_0_rmse: 0.79123 | val_1_rmse: 0.85131 |  0:00:03s
epoch 18 | loss: 0.46405 | val_0_rmse: 0.77063 | val_1_rmse: 0.82076 |  0:00:03s
epoch 19 | loss: 0.45104 | val_0_rmse: 0.7679  | val_1_rmse: 0.81445 |  0:00:04s
epoch 20 | loss: 0.45009 | val_0_rmse: 0.7898  | val_1_rmse: 0.83801 |  0:00:04s
epoch 21 | loss: 0.44517 | val_0_rmse: 0.76645 | val_1_rmse: 0.816   |  0:00:04s
epoch 22 | loss: 0.44271 | val_0_rmse: 0.74219 | val_1_rmse: 0.78782 |  0:00:04s
epoch 23 | loss: 0.45244 | val_0_rmse: 0.74532 | val_1_rmse: 0.79051 |  0:00:04s
epoch 24 | loss: 0.43136 | val_0_rmse: 0.75102 | val_1_rmse: 0.79803 |  0:00:05s
epoch 25 | loss: 0.41825 | val_0_rmse: 0.73848 | val_1_rmse: 0.78931 |  0:00:05s
epoch 26 | loss: 0.43084 | val_0_rmse: 0.71802 | val_1_rmse: 0.77019 |  0:00:05s
epoch 27 | loss: 0.42198 | val_0_rmse: 0.71223 | val_1_rmse: 0.76622 |  0:00:05s
epoch 28 | loss: 0.43519 | val_0_rmse: 0.69078 | val_1_rmse: 0.75352 |  0:00:05s
epoch 29 | loss: 0.42861 | val_0_rmse: 0.67208 | val_1_rmse: 0.74056 |  0:00:06s
epoch 30 | loss: 0.4368  | val_0_rmse: 0.67012 | val_1_rmse: 0.74468 |  0:00:06s
epoch 31 | loss: 0.42221 | val_0_rmse: 0.67206 | val_1_rmse: 0.74636 |  0:00:06s
epoch 32 | loss: 0.41685 | val_0_rmse: 0.6683  | val_1_rmse: 0.7385  |  0:00:06s
epoch 33 | loss: 0.4329  | val_0_rmse: 0.6644  | val_1_rmse: 0.73742 |  0:00:06s
epoch 34 | loss: 0.41753 | val_0_rmse: 0.67792 | val_1_rmse: 0.75869 |  0:00:07s
epoch 35 | loss: 0.43777 | val_0_rmse: 0.66954 | val_1_rmse: 0.74637 |  0:00:07s
epoch 36 | loss: 0.42812 | val_0_rmse: 0.68641 | val_1_rmse: 0.75961 |  0:00:07s
epoch 37 | loss: 0.42578 | val_0_rmse: 0.71141 | val_1_rmse: 0.7812  |  0:00:07s
epoch 38 | loss: 0.42545 | val_0_rmse: 0.70173 | val_1_rmse: 0.77828 |  0:00:07s
epoch 39 | loss: 0.41833 | val_0_rmse: 0.68749 | val_1_rmse: 0.77225 |  0:00:08s
epoch 40 | loss: 0.42187 | val_0_rmse: 0.68008 | val_1_rmse: 0.76812 |  0:00:08s
epoch 41 | loss: 0.41111 | val_0_rmse: 0.67872 | val_1_rmse: 0.76573 |  0:00:08s
epoch 42 | loss: 0.42243 | val_0_rmse: 0.66907 | val_1_rmse: 0.76174 |  0:00:08s
epoch 43 | loss: 0.41657 | val_0_rmse: 0.67268 | val_1_rmse: 0.77123 |  0:00:09s
epoch 44 | loss: 0.40756 | val_0_rmse: 0.6747  | val_1_rmse: 0.77741 |  0:00:09s
epoch 45 | loss: 0.41363 | val_0_rmse: 0.66541 | val_1_rmse: 0.76611 |  0:00:09s
epoch 46 | loss: 0.41472 | val_0_rmse: 0.66574 | val_1_rmse: 0.76465 |  0:00:09s
epoch 47 | loss: 0.41609 | val_0_rmse: 0.66458 | val_1_rmse: 0.76787 |  0:00:09s
epoch 48 | loss: 0.4152  | val_0_rmse: 0.65607 | val_1_rmse: 0.77073 |  0:00:10s
epoch 49 | loss: 0.41266 | val_0_rmse: 0.66395 | val_1_rmse: 0.78848 |  0:00:10s
epoch 50 | loss: 0.39981 | val_0_rmse: 0.67447 | val_1_rmse: 0.79935 |  0:00:10s
epoch 51 | loss: 0.41071 | val_0_rmse: 0.65632 | val_1_rmse: 0.76715 |  0:00:10s
epoch 52 | loss: 0.4055  | val_0_rmse: 0.65285 | val_1_rmse: 0.75937 |  0:00:10s
epoch 53 | loss: 0.40209 | val_0_rmse: 0.66473 | val_1_rmse: 0.7729  |  0:00:11s
epoch 54 | loss: 0.4111  | val_0_rmse: 0.66835 | val_1_rmse: 0.76659 |  0:00:11s
epoch 55 | loss: 0.41622 | val_0_rmse: 0.64602 | val_1_rmse: 0.72923 |  0:00:11s
epoch 56 | loss: 0.4044  | val_0_rmse: 0.64388 | val_1_rmse: 0.72515 |  0:00:11s
epoch 57 | loss: 0.39784 | val_0_rmse: 0.6584  | val_1_rmse: 0.75054 |  0:00:11s
epoch 58 | loss: 0.40934 | val_0_rmse: 0.65999 | val_1_rmse: 0.74833 |  0:00:12s
epoch 59 | loss: 0.40504 | val_0_rmse: 0.65001 | val_1_rmse: 0.72721 |  0:00:12s
epoch 60 | loss: 0.4175  | val_0_rmse: 0.65709 | val_1_rmse: 0.735   |  0:00:12s
epoch 61 | loss: 0.40097 | val_0_rmse: 0.72075 | val_1_rmse: 0.80413 |  0:00:12s
epoch 62 | loss: 0.41742 | val_0_rmse: 0.68795 | val_1_rmse: 0.77636 |  0:00:12s
epoch 63 | loss: 0.4019  | val_0_rmse: 0.67182 | val_1_rmse: 0.75476 |  0:00:13s
epoch 64 | loss: 0.41667 | val_0_rmse: 0.66965 | val_1_rmse: 0.75284 |  0:00:13s
epoch 65 | loss: 0.40833 | val_0_rmse: 0.6834  | val_1_rmse: 0.76362 |  0:00:13s
epoch 66 | loss: 0.40573 | val_0_rmse: 0.66121 | val_1_rmse: 0.74075 |  0:00:13s
epoch 67 | loss: 0.40137 | val_0_rmse: 0.65164 | val_1_rmse: 0.73241 |  0:00:13s
epoch 68 | loss: 0.40275 | val_0_rmse: 0.66249 | val_1_rmse: 0.74625 |  0:00:14s
epoch 69 | loss: 0.40479 | val_0_rmse: 0.66746 | val_1_rmse: 0.75235 |  0:00:14s
epoch 70 | loss: 0.39777 | val_0_rmse: 0.6617  | val_1_rmse: 0.75065 |  0:00:14s
epoch 71 | loss: 0.41158 | val_0_rmse: 0.65595 | val_1_rmse: 0.73886 |  0:00:14s
epoch 72 | loss: 0.40129 | val_0_rmse: 0.65373 | val_1_rmse: 0.7363  |  0:00:14s
epoch 73 | loss: 0.39643 | val_0_rmse: 0.66714 | val_1_rmse: 0.75824 |  0:00:15s
epoch 74 | loss: 0.40701 | val_0_rmse: 0.65799 | val_1_rmse: 0.75349 |  0:00:15s
epoch 75 | loss: 0.3946  | val_0_rmse: 0.64227 | val_1_rmse: 0.74278 |  0:00:15s
epoch 76 | loss: 0.40552 | val_0_rmse: 0.64303 | val_1_rmse: 0.75126 |  0:00:15s
epoch 77 | loss: 0.389   | val_0_rmse: 0.65548 | val_1_rmse: 0.76667 |  0:00:15s
epoch 78 | loss: 0.39779 | val_0_rmse: 0.64486 | val_1_rmse: 0.75871 |  0:00:16s
epoch 79 | loss: 0.40042 | val_0_rmse: 0.64141 | val_1_rmse: 0.74365 |  0:00:16s
epoch 80 | loss: 0.39809 | val_0_rmse: 0.64185 | val_1_rmse: 0.74402 |  0:00:16s
epoch 81 | loss: 0.40748 | val_0_rmse: 0.64767 | val_1_rmse: 0.75384 |  0:00:16s
epoch 82 | loss: 0.39125 | val_0_rmse: 0.65069 | val_1_rmse: 0.75706 |  0:00:16s
epoch 83 | loss: 0.39157 | val_0_rmse: 0.63864 | val_1_rmse: 0.73995 |  0:00:17s
epoch 84 | loss: 0.39872 | val_0_rmse: 0.63863 | val_1_rmse: 0.73733 |  0:00:17s
epoch 85 | loss: 0.3911  | val_0_rmse: 0.63958 | val_1_rmse: 0.74684 |  0:00:17s
epoch 86 | loss: 0.38642 | val_0_rmse: 0.64415 | val_1_rmse: 0.76082 |  0:00:17s

Early stopping occured at epoch 86 with best_epoch = 56 and best_val_1_rmse = 0.72515
Best weights from best epoch are automatically used!
ended training at: 08:20:10
Feature importance:
Mean squared error is of 3466365630.332837
Mean absolute error:43550.8979875
MAPE:0.44344644514058257
R2 score:0.47763959982738824
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:20:10
epoch 0  | loss: 3.07923 | val_0_rmse: 1.11143 | val_1_rmse: 1.15295 |  0:00:00s
epoch 1  | loss: 1.71954 | val_0_rmse: 0.98223 | val_1_rmse: 1.06852 |  0:00:00s
epoch 2  | loss: 1.08692 | val_0_rmse: 0.98928 | val_1_rmse: 1.07988 |  0:00:00s
epoch 3  | loss: 0.78095 | val_0_rmse: 0.84534 | val_1_rmse: 0.91157 |  0:00:00s
epoch 4  | loss: 0.72529 | val_0_rmse: 0.8362  | val_1_rmse: 0.87923 |  0:00:01s
epoch 5  | loss: 0.63728 | val_0_rmse: 0.85458 | val_1_rmse: 0.89757 |  0:00:01s
epoch 6  | loss: 0.6045  | val_0_rmse: 0.76298 | val_1_rmse: 0.81205 |  0:00:01s
epoch 7  | loss: 0.56089 | val_0_rmse: 0.76181 | val_1_rmse: 0.82544 |  0:00:01s
epoch 8  | loss: 0.59549 | val_0_rmse: 0.7549  | val_1_rmse: 0.80292 |  0:00:01s
epoch 9  | loss: 0.51522 | val_0_rmse: 0.792   | val_1_rmse: 0.82348 |  0:00:02s
epoch 10 | loss: 0.51235 | val_0_rmse: 0.79032 | val_1_rmse: 0.82358 |  0:00:02s
epoch 11 | loss: 0.53584 | val_0_rmse: 0.75972 | val_1_rmse: 0.80626 |  0:00:02s
epoch 12 | loss: 0.48125 | val_0_rmse: 0.74583 | val_1_rmse: 0.79334 |  0:00:02s
epoch 13 | loss: 0.49019 | val_0_rmse: 0.7376  | val_1_rmse: 0.80864 |  0:00:02s
epoch 14 | loss: 0.48912 | val_0_rmse: 0.75993 | val_1_rmse: 0.83426 |  0:00:03s
epoch 15 | loss: 0.46434 | val_0_rmse: 0.74685 | val_1_rmse: 0.81617 |  0:00:03s
epoch 16 | loss: 0.48407 | val_0_rmse: 0.74312 | val_1_rmse: 0.81686 |  0:00:03s
epoch 17 | loss: 0.45264 | val_0_rmse: 0.72523 | val_1_rmse: 0.80151 |  0:00:03s
epoch 18 | loss: 0.45399 | val_0_rmse: 0.72752 | val_1_rmse: 0.79834 |  0:00:03s
epoch 19 | loss: 0.46491 | val_0_rmse: 0.73719 | val_1_rmse: 0.81272 |  0:00:04s
epoch 20 | loss: 0.46231 | val_0_rmse: 0.74724 | val_1_rmse: 0.83095 |  0:00:04s
epoch 21 | loss: 0.45993 | val_0_rmse: 0.72985 | val_1_rmse: 0.79601 |  0:00:04s
epoch 22 | loss: 0.45576 | val_0_rmse: 0.72122 | val_1_rmse: 0.77751 |  0:00:04s
epoch 23 | loss: 0.45642 | val_0_rmse: 0.74104 | val_1_rmse: 0.79408 |  0:00:04s
epoch 24 | loss: 0.46386 | val_0_rmse: 0.73614 | val_1_rmse: 0.78465 |  0:00:05s
epoch 25 | loss: 0.44593 | val_0_rmse: 0.72809 | val_1_rmse: 0.78037 |  0:00:05s
epoch 26 | loss: 0.43717 | val_0_rmse: 0.73055 | val_1_rmse: 0.78104 |  0:00:05s
epoch 27 | loss: 0.44228 | val_0_rmse: 0.73444 | val_1_rmse: 0.78468 |  0:00:05s
epoch 28 | loss: 0.43762 | val_0_rmse: 0.74075 | val_1_rmse: 0.79798 |  0:00:05s
epoch 29 | loss: 0.4471  | val_0_rmse: 0.74017 | val_1_rmse: 0.80356 |  0:00:06s
epoch 30 | loss: 0.45151 | val_0_rmse: 0.73094 | val_1_rmse: 0.79719 |  0:00:06s
epoch 31 | loss: 0.44402 | val_0_rmse: 0.7214  | val_1_rmse: 0.77979 |  0:00:06s
epoch 32 | loss: 0.45057 | val_0_rmse: 0.71133 | val_1_rmse: 0.762   |  0:00:06s
epoch 33 | loss: 0.44709 | val_0_rmse: 0.71309 | val_1_rmse: 0.76331 |  0:00:06s
epoch 34 | loss: 0.44891 | val_0_rmse: 0.71027 | val_1_rmse: 0.75612 |  0:00:07s
epoch 35 | loss: 0.43929 | val_0_rmse: 0.70191 | val_1_rmse: 0.75537 |  0:00:07s
epoch 36 | loss: 0.43393 | val_0_rmse: 0.69477 | val_1_rmse: 0.75843 |  0:00:07s
epoch 37 | loss: 0.44336 | val_0_rmse: 0.68949 | val_1_rmse: 0.74661 |  0:00:07s
epoch 38 | loss: 0.43501 | val_0_rmse: 0.69916 | val_1_rmse: 0.74955 |  0:00:07s
epoch 39 | loss: 0.43563 | val_0_rmse: 0.70997 | val_1_rmse: 0.76541 |  0:00:08s
epoch 40 | loss: 0.43874 | val_0_rmse: 0.70595 | val_1_rmse: 0.7597  |  0:00:08s
epoch 41 | loss: 0.43666 | val_0_rmse: 0.70016 | val_1_rmse: 0.75316 |  0:00:08s
epoch 42 | loss: 0.4359  | val_0_rmse: 0.68747 | val_1_rmse: 0.753   |  0:00:08s
epoch 43 | loss: 0.43067 | val_0_rmse: 0.69591 | val_1_rmse: 0.76139 |  0:00:08s
epoch 44 | loss: 0.43533 | val_0_rmse: 0.71551 | val_1_rmse: 0.77719 |  0:00:09s
epoch 45 | loss: 0.43544 | val_0_rmse: 0.72254 | val_1_rmse: 0.77326 |  0:00:09s
epoch 46 | loss: 0.42313 | val_0_rmse: 0.71763 | val_1_rmse: 0.76683 |  0:00:09s
epoch 47 | loss: 0.43445 | val_0_rmse: 0.69321 | val_1_rmse: 0.76538 |  0:00:09s
epoch 48 | loss: 0.42181 | val_0_rmse: 0.68762 | val_1_rmse: 0.777   |  0:00:09s
epoch 49 | loss: 0.41986 | val_0_rmse: 0.6834  | val_1_rmse: 0.78558 |  0:00:10s
epoch 50 | loss: 0.42669 | val_0_rmse: 0.67159 | val_1_rmse: 0.77088 |  0:00:10s
epoch 51 | loss: 0.41505 | val_0_rmse: 0.66276 | val_1_rmse: 0.75272 |  0:00:10s
epoch 52 | loss: 0.42239 | val_0_rmse: 0.66483 | val_1_rmse: 0.74537 |  0:00:10s
epoch 53 | loss: 0.42227 | val_0_rmse: 0.65614 | val_1_rmse: 0.72835 |  0:00:10s
epoch 54 | loss: 0.42843 | val_0_rmse: 0.65967 | val_1_rmse: 0.73143 |  0:00:11s
epoch 55 | loss: 0.42394 | val_0_rmse: 0.67407 | val_1_rmse: 0.75763 |  0:00:11s
epoch 56 | loss: 0.42718 | val_0_rmse: 0.65921 | val_1_rmse: 0.75212 |  0:00:11s
epoch 57 | loss: 0.42817 | val_0_rmse: 0.65883 | val_1_rmse: 0.74763 |  0:00:11s
epoch 58 | loss: 0.43102 | val_0_rmse: 0.6719  | val_1_rmse: 0.75474 |  0:00:11s
epoch 59 | loss: 0.41701 | val_0_rmse: 0.68571 | val_1_rmse: 0.77245 |  0:00:12s
epoch 60 | loss: 0.42666 | val_0_rmse: 0.67159 | val_1_rmse: 0.75884 |  0:00:12s
epoch 61 | loss: 0.41392 | val_0_rmse: 0.66666 | val_1_rmse: 0.74739 |  0:00:12s
epoch 62 | loss: 0.41367 | val_0_rmse: 0.6725  | val_1_rmse: 0.76143 |  0:00:12s
epoch 63 | loss: 0.40895 | val_0_rmse: 0.67428 | val_1_rmse: 0.75682 |  0:00:12s
epoch 64 | loss: 0.42134 | val_0_rmse: 0.67046 | val_1_rmse: 0.75472 |  0:00:13s
epoch 65 | loss: 0.42087 | val_0_rmse: 0.67104 | val_1_rmse: 0.75691 |  0:00:13s
epoch 66 | loss: 0.42389 | val_0_rmse: 0.68131 | val_1_rmse: 0.77003 |  0:00:13s
epoch 67 | loss: 0.43944 | val_0_rmse: 0.68097 | val_1_rmse: 0.75971 |  0:00:13s
epoch 68 | loss: 0.43977 | val_0_rmse: 0.68794 | val_1_rmse: 0.76087 |  0:00:13s
epoch 69 | loss: 0.43457 | val_0_rmse: 0.68834 | val_1_rmse: 0.74708 |  0:00:14s
epoch 70 | loss: 0.42153 | val_0_rmse: 0.67412 | val_1_rmse: 0.73387 |  0:00:14s
epoch 71 | loss: 0.42534 | val_0_rmse: 0.67255 | val_1_rmse: 0.74353 |  0:00:14s
epoch 72 | loss: 0.42716 | val_0_rmse: 0.66595 | val_1_rmse: 0.76153 |  0:00:14s
epoch 73 | loss: 0.41498 | val_0_rmse: 0.65446 | val_1_rmse: 0.76202 |  0:00:14s
epoch 74 | loss: 0.41929 | val_0_rmse: 0.64915 | val_1_rmse: 0.76677 |  0:00:15s
epoch 75 | loss: 0.42804 | val_0_rmse: 0.65647 | val_1_rmse: 0.75351 |  0:00:15s
epoch 76 | loss: 0.43166 | val_0_rmse: 0.66323 | val_1_rmse: 0.75782 |  0:00:15s
epoch 77 | loss: 0.43453 | val_0_rmse: 0.66867 | val_1_rmse: 0.76375 |  0:00:15s
epoch 78 | loss: 0.44341 | val_0_rmse: 0.6817  | val_1_rmse: 0.76824 |  0:00:15s
epoch 79 | loss: 0.45661 | val_0_rmse: 0.67911 | val_1_rmse: 0.76704 |  0:00:16s
epoch 80 | loss: 0.44983 | val_0_rmse: 0.69599 | val_1_rmse: 0.77272 |  0:00:16s
epoch 81 | loss: 0.45707 | val_0_rmse: 0.70475 | val_1_rmse: 0.76849 |  0:00:16s
epoch 82 | loss: 0.44655 | val_0_rmse: 0.7124  | val_1_rmse: 0.77884 |  0:00:16s
epoch 83 | loss: 0.45622 | val_0_rmse: 0.69519 | val_1_rmse: 0.77414 |  0:00:16s

Early stopping occured at epoch 83 with best_epoch = 53 and best_val_1_rmse = 0.72835
Best weights from best epoch are automatically used!
ended training at: 08:20:27
Feature importance:
Mean squared error is of 3056236417.509193
Mean absolute error:42914.75386771979
MAPE:0.4414674197879204
R2 score:0.5915818874432526
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:20:27
epoch 0  | loss: 1.21062 | val_0_rmse: 0.91131 | val_1_rmse: 0.91009 |  0:00:00s
epoch 1  | loss: 0.83919 | val_0_rmse: 1.06924 | val_1_rmse: 1.05703 |  0:00:00s
epoch 2  | loss: 0.68004 | val_0_rmse: 1.18343 | val_1_rmse: 1.17843 |  0:00:01s
epoch 3  | loss: 0.61087 | val_0_rmse: 2.21648 | val_1_rmse: 2.24196 |  0:00:01s
epoch 4  | loss: 0.55262 | val_0_rmse: 2.22986 | val_1_rmse: 2.29417 |  0:00:02s
epoch 5  | loss: 0.52353 | val_0_rmse: 2.23536 | val_1_rmse: 2.28463 |  0:00:02s
epoch 6  | loss: 0.53    | val_0_rmse: 1.28217 | val_1_rmse: 1.30819 |  0:00:03s
epoch 7  | loss: 0.52163 | val_0_rmse: 1.25335 | val_1_rmse: 1.28281 |  0:00:03s
epoch 8  | loss: 0.48987 | val_0_rmse: 1.00472 | val_1_rmse: 1.02694 |  0:00:04s
epoch 9  | loss: 0.47054 | val_0_rmse: 0.94706 | val_1_rmse: 0.94234 |  0:00:04s
epoch 10 | loss: 0.45336 | val_0_rmse: 1.0265  | val_1_rmse: 1.01832 |  0:00:04s
epoch 11 | loss: 0.44525 | val_0_rmse: 1.23756 | val_1_rmse: 1.22563 |  0:00:05s
epoch 12 | loss: 0.41775 | val_0_rmse: 1.30566 | val_1_rmse: 1.29691 |  0:00:05s
epoch 13 | loss: 0.40453 | val_0_rmse: 1.34599 | val_1_rmse: 1.31983 |  0:00:06s
epoch 14 | loss: 0.40789 | val_0_rmse: 1.73043 | val_1_rmse: 1.73664 |  0:00:06s
epoch 15 | loss: 0.40158 | val_0_rmse: 1.7174  | val_1_rmse: 1.72618 |  0:00:07s
epoch 16 | loss: 0.39837 | val_0_rmse: 1.2755  | val_1_rmse: 1.27496 |  0:00:07s
epoch 17 | loss: 0.3951  | val_0_rmse: 1.63057 | val_1_rmse: 1.64609 |  0:00:07s
epoch 18 | loss: 0.39636 | val_0_rmse: 1.69655 | val_1_rmse: 1.71586 |  0:00:08s
epoch 19 | loss: 0.38952 | val_0_rmse: 1.29872 | val_1_rmse: 1.30122 |  0:00:08s
epoch 20 | loss: 0.38855 | val_0_rmse: 1.79172 | val_1_rmse: 1.80595 |  0:00:09s
epoch 21 | loss: 0.4134  | val_0_rmse: 1.30197 | val_1_rmse: 1.30254 |  0:00:09s
epoch 22 | loss: 0.38854 | val_0_rmse: 1.18019 | val_1_rmse: 1.17587 |  0:00:10s
epoch 23 | loss: 0.3822  | val_0_rmse: 1.05171 | val_1_rmse: 1.04451 |  0:00:10s
epoch 24 | loss: 0.40311 | val_0_rmse: 1.34821 | val_1_rmse: 1.33863 |  0:00:11s
epoch 25 | loss: 0.38985 | val_0_rmse: 1.423   | val_1_rmse: 1.44244 |  0:00:11s
epoch 26 | loss: 0.3883  | val_0_rmse: 1.15919 | val_1_rmse: 1.18006 |  0:00:11s
epoch 27 | loss: 0.37417 | val_0_rmse: 1.15072 | val_1_rmse: 1.16431 |  0:00:12s
epoch 28 | loss: 0.37672 | val_0_rmse: 1.24074 | val_1_rmse: 1.2623  |  0:00:12s
epoch 29 | loss: 0.38152 | val_0_rmse: 1.04609 | val_1_rmse: 1.06252 |  0:00:13s
epoch 30 | loss: 0.37565 | val_0_rmse: 0.84367 | val_1_rmse: 0.86393 |  0:00:13s
epoch 31 | loss: 0.38264 | val_0_rmse: 0.98099 | val_1_rmse: 0.99043 |  0:00:14s
epoch 32 | loss: 0.38386 | val_0_rmse: 0.86252 | val_1_rmse: 0.87386 |  0:00:14s
epoch 33 | loss: 0.37719 | val_0_rmse: 0.91879 | val_1_rmse: 0.93051 |  0:00:15s
epoch 34 | loss: 0.37273 | val_0_rmse: 0.97482 | val_1_rmse: 0.99336 |  0:00:15s
epoch 35 | loss: 0.38056 | val_0_rmse: 0.92226 | val_1_rmse: 0.94017 |  0:00:16s
epoch 36 | loss: 0.39185 | val_0_rmse: 0.86229 | val_1_rmse: 0.8634  |  0:00:16s
epoch 37 | loss: 0.37551 | val_0_rmse: 0.87242 | val_1_rmse: 0.88053 |  0:00:16s
epoch 38 | loss: 0.38332 | val_0_rmse: 0.98704 | val_1_rmse: 0.99818 |  0:00:17s
epoch 39 | loss: 0.37778 | val_0_rmse: 0.79585 | val_1_rmse: 0.81075 |  0:00:17s
epoch 40 | loss: 0.38206 | val_0_rmse: 0.8128  | val_1_rmse: 0.83118 |  0:00:18s
epoch 41 | loss: 0.35843 | val_0_rmse: 0.99298 | val_1_rmse: 1.00139 |  0:00:18s
epoch 42 | loss: 0.36928 | val_0_rmse: 1.04171 | val_1_rmse: 1.05747 |  0:00:19s
epoch 43 | loss: 0.37893 | val_0_rmse: 0.85217 | val_1_rmse: 0.86359 |  0:00:19s
epoch 44 | loss: 0.38345 | val_0_rmse: 0.77618 | val_1_rmse: 0.792   |  0:00:20s
epoch 45 | loss: 0.37036 | val_0_rmse: 0.90105 | val_1_rmse: 0.91896 |  0:00:20s
epoch 46 | loss: 0.36528 | val_0_rmse: 0.79724 | val_1_rmse: 0.8151  |  0:00:20s
epoch 47 | loss: 0.36647 | val_0_rmse: 0.703   | val_1_rmse: 0.72426 |  0:00:21s
epoch 48 | loss: 0.37393 | val_0_rmse: 0.98723 | val_1_rmse: 1.01096 |  0:00:21s
epoch 49 | loss: 0.36236 | val_0_rmse: 0.70156 | val_1_rmse: 0.72197 |  0:00:22s
epoch 50 | loss: 0.36772 | val_0_rmse: 0.65341 | val_1_rmse: 0.6826  |  0:00:22s
epoch 51 | loss: 0.36057 | val_0_rmse: 0.68401 | val_1_rmse: 0.71932 |  0:00:23s
epoch 52 | loss: 0.36415 | val_0_rmse: 0.69207 | val_1_rmse: 0.72866 |  0:00:23s
epoch 53 | loss: 0.35964 | val_0_rmse: 0.60373 | val_1_rmse: 0.63708 |  0:00:24s
epoch 54 | loss: 0.35518 | val_0_rmse: 0.71716 | val_1_rmse: 0.75546 |  0:00:24s
epoch 55 | loss: 0.3509  | val_0_rmse: 0.69922 | val_1_rmse: 0.73396 |  0:00:24s
epoch 56 | loss: 0.36567 | val_0_rmse: 0.66916 | val_1_rmse: 0.7016  |  0:00:25s
epoch 57 | loss: 0.3603  | val_0_rmse: 0.61136 | val_1_rmse: 0.64288 |  0:00:25s
epoch 58 | loss: 0.35103 | val_0_rmse: 0.61044 | val_1_rmse: 0.64352 |  0:00:26s
epoch 59 | loss: 0.34308 | val_0_rmse: 0.63371 | val_1_rmse: 0.66133 |  0:00:26s
epoch 60 | loss: 0.34389 | val_0_rmse: 0.66224 | val_1_rmse: 0.69008 |  0:00:27s
epoch 61 | loss: 0.34438 | val_0_rmse: 0.60713 | val_1_rmse: 0.63859 |  0:00:27s
epoch 62 | loss: 0.3626  | val_0_rmse: 0.65412 | val_1_rmse: 0.68185 |  0:00:28s
epoch 63 | loss: 0.34488 | val_0_rmse: 0.69544 | val_1_rmse: 0.73548 |  0:00:28s
epoch 64 | loss: 0.34434 | val_0_rmse: 0.64921 | val_1_rmse: 0.6926  |  0:00:28s
epoch 65 | loss: 0.34146 | val_0_rmse: 0.61753 | val_1_rmse: 0.65386 |  0:00:29s
epoch 66 | loss: 0.33946 | val_0_rmse: 0.60336 | val_1_rmse: 0.64008 |  0:00:29s
epoch 67 | loss: 0.34051 | val_0_rmse: 0.6541  | val_1_rmse: 0.68827 |  0:00:30s
epoch 68 | loss: 0.34246 | val_0_rmse: 0.68155 | val_1_rmse: 0.71786 |  0:00:30s
epoch 69 | loss: 0.35025 | val_0_rmse: 0.65067 | val_1_rmse: 0.68686 |  0:00:31s
epoch 70 | loss: 0.33519 | val_0_rmse: 0.60033 | val_1_rmse: 0.64023 |  0:00:31s
epoch 71 | loss: 0.33478 | val_0_rmse: 0.59311 | val_1_rmse: 0.63025 |  0:00:32s
epoch 72 | loss: 0.34469 | val_0_rmse: 0.59607 | val_1_rmse: 0.62632 |  0:00:32s
epoch 73 | loss: 0.34127 | val_0_rmse: 0.59941 | val_1_rmse: 0.63954 |  0:00:33s
epoch 74 | loss: 0.34136 | val_0_rmse: 0.62571 | val_1_rmse: 0.66498 |  0:00:33s
epoch 75 | loss: 0.33572 | val_0_rmse: 0.63705 | val_1_rmse: 0.67969 |  0:00:33s
epoch 76 | loss: 0.35764 | val_0_rmse: 0.74993 | val_1_rmse: 0.7821  |  0:00:34s
epoch 77 | loss: 0.35332 | val_0_rmse: 0.62836 | val_1_rmse: 0.6645  |  0:00:34s
epoch 78 | loss: 0.34781 | val_0_rmse: 0.60096 | val_1_rmse: 0.64351 |  0:00:35s
epoch 79 | loss: 0.34746 | val_0_rmse: 0.61145 | val_1_rmse: 0.64817 |  0:00:35s
epoch 80 | loss: 0.35379 | val_0_rmse: 0.58772 | val_1_rmse: 0.63043 |  0:00:36s
epoch 81 | loss: 0.34012 | val_0_rmse: 0.59396 | val_1_rmse: 0.63797 |  0:00:36s
epoch 82 | loss: 0.33577 | val_0_rmse: 0.58015 | val_1_rmse: 0.62077 |  0:00:37s
epoch 83 | loss: 0.33651 | val_0_rmse: 0.57956 | val_1_rmse: 0.62826 |  0:00:37s
epoch 84 | loss: 0.34129 | val_0_rmse: 0.59309 | val_1_rmse: 0.63408 |  0:00:37s
epoch 85 | loss: 0.33799 | val_0_rmse: 0.58543 | val_1_rmse: 0.62413 |  0:00:38s
epoch 86 | loss: 0.33497 | val_0_rmse: 0.58411 | val_1_rmse: 0.62675 |  0:00:38s
epoch 87 | loss: 0.32709 | val_0_rmse: 0.59088 | val_1_rmse: 0.64244 |  0:00:39s
epoch 88 | loss: 0.33664 | val_0_rmse: 0.59606 | val_1_rmse: 0.63896 |  0:00:39s
epoch 89 | loss: 0.32595 | val_0_rmse: 0.63554 | val_1_rmse: 0.67933 |  0:00:40s
epoch 90 | loss: 0.33546 | val_0_rmse: 0.59802 | val_1_rmse: 0.64445 |  0:00:40s
epoch 91 | loss: 0.33039 | val_0_rmse: 0.63187 | val_1_rmse: 0.68331 |  0:00:41s
epoch 92 | loss: 0.34067 | val_0_rmse: 0.59873 | val_1_rmse: 0.65834 |  0:00:41s
epoch 93 | loss: 0.32897 | val_0_rmse: 0.58867 | val_1_rmse: 0.63962 |  0:00:42s
epoch 94 | loss: 0.34029 | val_0_rmse: 0.61693 | val_1_rmse: 0.64587 |  0:00:42s
epoch 95 | loss: 0.32792 | val_0_rmse: 0.60227 | val_1_rmse: 0.64828 |  0:00:42s
epoch 96 | loss: 0.33496 | val_0_rmse: 0.62964 | val_1_rmse: 0.66833 |  0:00:43s
epoch 97 | loss: 0.35104 | val_0_rmse: 0.65027 | val_1_rmse: 0.69141 |  0:00:43s
epoch 98 | loss: 0.33196 | val_0_rmse: 0.6177  | val_1_rmse: 0.66691 |  0:00:44s
epoch 99 | loss: 0.33131 | val_0_rmse: 0.65773 | val_1_rmse: 0.69839 |  0:00:44s
epoch 100| loss: 0.33311 | val_0_rmse: 0.64234 | val_1_rmse: 0.68746 |  0:00:45s
epoch 101| loss: 0.34662 | val_0_rmse: 0.65809 | val_1_rmse: 0.70271 |  0:00:45s
epoch 102| loss: 0.35132 | val_0_rmse: 0.61058 | val_1_rmse: 0.65052 |  0:00:46s
epoch 103| loss: 0.34119 | val_0_rmse: 0.60992 | val_1_rmse: 0.63809 |  0:00:46s
epoch 104| loss: 0.34957 | val_0_rmse: 0.61248 | val_1_rmse: 0.64897 |  0:00:46s
epoch 105| loss: 0.34673 | val_0_rmse: 0.61282 | val_1_rmse: 0.65637 |  0:00:47s
epoch 106| loss: 0.33689 | val_0_rmse: 0.60282 | val_1_rmse: 0.64823 |  0:00:47s
epoch 107| loss: 0.33577 | val_0_rmse: 0.58659 | val_1_rmse: 0.6398  |  0:00:48s
epoch 108| loss: 0.33263 | val_0_rmse: 0.59146 | val_1_rmse: 0.63832 |  0:00:48s
epoch 109| loss: 0.33976 | val_0_rmse: 0.6029  | val_1_rmse: 0.64667 |  0:00:49s
epoch 110| loss: 0.3408  | val_0_rmse: 0.58469 | val_1_rmse: 0.63291 |  0:00:49s
epoch 111| loss: 0.3355  | val_0_rmse: 0.60649 | val_1_rmse: 0.65861 |  0:00:50s
epoch 112| loss: 0.33468 | val_0_rmse: 0.60124 | val_1_rmse: 0.65048 |  0:00:50s

Early stopping occured at epoch 112 with best_epoch = 82 and best_val_1_rmse = 0.62077
Best weights from best epoch are automatically used!
ended training at: 08:21:18
Feature importance:
Mean squared error is of 2864060651.4486775
Mean absolute error:36577.49454671583
MAPE:0.38062719723371935
R2 score:0.6274976826291745
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:21:18
epoch 0  | loss: 1.1312  | val_0_rmse: 0.90094 | val_1_rmse: 0.92325 |  0:00:00s
epoch 1  | loss: 0.7367  | val_0_rmse: 0.81974 | val_1_rmse: 0.8268  |  0:00:00s
epoch 2  | loss: 0.58832 | val_0_rmse: 0.77379 | val_1_rmse: 0.79016 |  0:00:01s
epoch 3  | loss: 0.54996 | val_0_rmse: 0.77558 | val_1_rmse: 0.80294 |  0:00:01s
epoch 4  | loss: 0.52724 | val_0_rmse: 0.7277  | val_1_rmse: 0.76259 |  0:00:02s
epoch 5  | loss: 0.50747 | val_0_rmse: 0.74986 | val_1_rmse: 0.78215 |  0:00:02s
epoch 6  | loss: 0.50073 | val_0_rmse: 0.70922 | val_1_rmse: 0.74172 |  0:00:03s
epoch 7  | loss: 0.49796 | val_0_rmse: 0.69862 | val_1_rmse: 0.73527 |  0:00:03s
epoch 8  | loss: 0.49165 | val_0_rmse: 0.69862 | val_1_rmse: 0.7289  |  0:00:04s
epoch 9  | loss: 0.47915 | val_0_rmse: 0.69962 | val_1_rmse: 0.73175 |  0:00:04s
epoch 10 | loss: 0.4741  | val_0_rmse: 0.69782 | val_1_rmse: 0.72679 |  0:00:05s
epoch 11 | loss: 0.48653 | val_0_rmse: 0.69441 | val_1_rmse: 0.73216 |  0:00:05s
epoch 12 | loss: 0.45775 | val_0_rmse: 0.68858 | val_1_rmse: 0.72568 |  0:00:05s
epoch 13 | loss: 0.45794 | val_0_rmse: 0.6775  | val_1_rmse: 0.71824 |  0:00:06s
epoch 14 | loss: 0.44401 | val_0_rmse: 0.67162 | val_1_rmse: 0.71574 |  0:00:06s
epoch 15 | loss: 0.43412 | val_0_rmse: 0.66869 | val_1_rmse: 0.71451 |  0:00:07s
epoch 16 | loss: 0.41821 | val_0_rmse: 0.66558 | val_1_rmse: 0.70791 |  0:00:07s
epoch 17 | loss: 0.42256 | val_0_rmse: 0.6687  | val_1_rmse: 0.7047  |  0:00:08s
epoch 18 | loss: 0.4141  | val_0_rmse: 0.65585 | val_1_rmse: 0.69459 |  0:00:08s
epoch 19 | loss: 0.41436 | val_0_rmse: 0.71475 | val_1_rmse: 0.73875 |  0:00:09s
epoch 20 | loss: 0.39867 | val_0_rmse: 0.67353 | val_1_rmse: 0.70992 |  0:00:09s
epoch 21 | loss: 0.39559 | val_0_rmse: 0.63803 | val_1_rmse: 0.6815  |  0:00:10s
epoch 22 | loss: 0.38744 | val_0_rmse: 0.63104 | val_1_rmse: 0.6758  |  0:00:10s
epoch 23 | loss: 0.3948  | val_0_rmse: 0.65076 | val_1_rmse: 0.69288 |  0:00:11s
epoch 24 | loss: 0.38261 | val_0_rmse: 0.61326 | val_1_rmse: 0.65824 |  0:00:11s
epoch 25 | loss: 0.37523 | val_0_rmse: 0.66902 | val_1_rmse: 0.71417 |  0:00:12s
epoch 26 | loss: 0.36971 | val_0_rmse: 0.59864 | val_1_rmse: 0.65241 |  0:00:12s
epoch 27 | loss: 0.35585 | val_0_rmse: 0.6941  | val_1_rmse: 0.74324 |  0:00:12s
epoch 28 | loss: 0.36812 | val_0_rmse: 0.63074 | val_1_rmse: 0.68262 |  0:00:13s
epoch 29 | loss: 0.36939 | val_0_rmse: 0.67319 | val_1_rmse: 0.72163 |  0:00:13s
epoch 30 | loss: 0.3574  | val_0_rmse: 0.62672 | val_1_rmse: 0.67229 |  0:00:14s
epoch 31 | loss: 0.36461 | val_0_rmse: 0.66128 | val_1_rmse: 0.707   |  0:00:14s
epoch 32 | loss: 0.35868 | val_0_rmse: 0.59985 | val_1_rmse: 0.64548 |  0:00:15s
epoch 33 | loss: 0.35764 | val_0_rmse: 0.61397 | val_1_rmse: 0.66077 |  0:00:15s
epoch 34 | loss: 0.34465 | val_0_rmse: 0.59914 | val_1_rmse: 0.64928 |  0:00:16s
epoch 35 | loss: 0.34341 | val_0_rmse: 0.6272  | val_1_rmse: 0.67318 |  0:00:16s
epoch 36 | loss: 0.35185 | val_0_rmse: 0.5901  | val_1_rmse: 0.63635 |  0:00:17s
epoch 37 | loss: 0.33532 | val_0_rmse: 0.61174 | val_1_rmse: 0.65207 |  0:00:17s
epoch 38 | loss: 0.33869 | val_0_rmse: 0.59256 | val_1_rmse: 0.63811 |  0:00:18s
epoch 39 | loss: 0.34543 | val_0_rmse: 0.64693 | val_1_rmse: 0.68777 |  0:00:18s
epoch 40 | loss: 0.34209 | val_0_rmse: 0.60719 | val_1_rmse: 0.65815 |  0:00:18s
epoch 41 | loss: 0.32678 | val_0_rmse: 0.58809 | val_1_rmse: 0.63937 |  0:00:19s
epoch 42 | loss: 0.32591 | val_0_rmse: 0.59956 | val_1_rmse: 0.65003 |  0:00:19s
epoch 43 | loss: 0.33771 | val_0_rmse: 0.5868  | val_1_rmse: 0.63526 |  0:00:20s
epoch 44 | loss: 0.33222 | val_0_rmse: 0.58792 | val_1_rmse: 0.63166 |  0:00:20s
epoch 45 | loss: 0.32633 | val_0_rmse: 0.59314 | val_1_rmse: 0.63666 |  0:00:21s
epoch 46 | loss: 0.31715 | val_0_rmse: 0.58413 | val_1_rmse: 0.63002 |  0:00:21s
epoch 47 | loss: 0.32548 | val_0_rmse: 0.58065 | val_1_rmse: 0.63076 |  0:00:22s
epoch 48 | loss: 0.32467 | val_0_rmse: 0.57474 | val_1_rmse: 0.62186 |  0:00:22s
epoch 49 | loss: 0.32238 | val_0_rmse: 0.60565 | val_1_rmse: 0.64822 |  0:00:22s
epoch 50 | loss: 0.32297 | val_0_rmse: 0.59399 | val_1_rmse: 0.63072 |  0:00:23s
epoch 51 | loss: 0.32244 | val_0_rmse: 0.58826 | val_1_rmse: 0.632   |  0:00:23s
epoch 52 | loss: 0.32597 | val_0_rmse: 0.56851 | val_1_rmse: 0.61413 |  0:00:24s
epoch 53 | loss: 0.32844 | val_0_rmse: 0.60088 | val_1_rmse: 0.64111 |  0:00:24s
epoch 54 | loss: 0.32486 | val_0_rmse: 0.59117 | val_1_rmse: 0.6325  |  0:00:25s
epoch 55 | loss: 0.32193 | val_0_rmse: 0.57871 | val_1_rmse: 0.62262 |  0:00:25s
epoch 56 | loss: 0.31527 | val_0_rmse: 0.59468 | val_1_rmse: 0.64367 |  0:00:26s
epoch 57 | loss: 0.3153  | val_0_rmse: 0.58634 | val_1_rmse: 0.63078 |  0:00:26s
epoch 58 | loss: 0.32605 | val_0_rmse: 0.61183 | val_1_rmse: 0.65047 |  0:00:26s
epoch 59 | loss: 0.31394 | val_0_rmse: 0.59549 | val_1_rmse: 0.64036 |  0:00:27s
epoch 60 | loss: 0.31202 | val_0_rmse: 0.59645 | val_1_rmse: 0.63797 |  0:00:27s
epoch 61 | loss: 0.30721 | val_0_rmse: 0.59722 | val_1_rmse: 0.63518 |  0:00:28s
epoch 62 | loss: 0.31876 | val_0_rmse: 0.58266 | val_1_rmse: 0.62841 |  0:00:28s
epoch 63 | loss: 0.32103 | val_0_rmse: 0.60508 | val_1_rmse: 0.64964 |  0:00:29s
epoch 64 | loss: 0.33354 | val_0_rmse: 0.59217 | val_1_rmse: 0.63589 |  0:00:29s
epoch 65 | loss: 0.32857 | val_0_rmse: 0.57893 | val_1_rmse: 0.62285 |  0:00:30s
epoch 66 | loss: 0.32643 | val_0_rmse: 0.60464 | val_1_rmse: 0.64266 |  0:00:30s
epoch 67 | loss: 0.31441 | val_0_rmse: 0.58605 | val_1_rmse: 0.63276 |  0:00:31s
epoch 68 | loss: 0.32617 | val_0_rmse: 0.59317 | val_1_rmse: 0.63704 |  0:00:31s
epoch 69 | loss: 0.31832 | val_0_rmse: 0.61428 | val_1_rmse: 0.65284 |  0:00:31s
epoch 70 | loss: 0.32301 | val_0_rmse: 0.64057 | val_1_rmse: 0.67429 |  0:00:32s
epoch 71 | loss: 0.32209 | val_0_rmse: 0.58829 | val_1_rmse: 0.64847 |  0:00:32s
epoch 72 | loss: 0.32887 | val_0_rmse: 0.59908 | val_1_rmse: 0.64899 |  0:00:33s
epoch 73 | loss: 0.32045 | val_0_rmse: 0.59207 | val_1_rmse: 0.63726 |  0:00:33s
epoch 74 | loss: 0.32156 | val_0_rmse: 0.57248 | val_1_rmse: 0.61864 |  0:00:34s
epoch 75 | loss: 0.31315 | val_0_rmse: 0.58752 | val_1_rmse: 0.63423 |  0:00:34s
epoch 76 | loss: 0.3152  | val_0_rmse: 0.62568 | val_1_rmse: 0.66456 |  0:00:35s
epoch 77 | loss: 0.31438 | val_0_rmse: 0.57614 | val_1_rmse: 0.62647 |  0:00:35s
epoch 78 | loss: 0.31068 | val_0_rmse: 0.63973 | val_1_rmse: 0.67217 |  0:00:35s
epoch 79 | loss: 0.32122 | val_0_rmse: 0.58934 | val_1_rmse: 0.64298 |  0:00:36s
epoch 80 | loss: 0.32772 | val_0_rmse: 0.60145 | val_1_rmse: 0.6463  |  0:00:36s
epoch 81 | loss: 0.32483 | val_0_rmse: 0.60131 | val_1_rmse: 0.64973 |  0:00:37s
epoch 82 | loss: 0.30641 | val_0_rmse: 0.58209 | val_1_rmse: 0.63429 |  0:00:37s

Early stopping occured at epoch 82 with best_epoch = 52 and best_val_1_rmse = 0.61413
Best weights from best epoch are automatically used!
ended training at: 08:21:56
Feature importance:
Mean squared error is of 3055468714.593131
Mean absolute error:37956.09455708828
MAPE:0.35473847243797496
R2 score:0.6272399946477618
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:21:56
epoch 0  | loss: 1.09615 | val_0_rmse: 0.98386 | val_1_rmse: 0.94151 |  0:00:00s
epoch 1  | loss: 0.79624 | val_0_rmse: 0.91503 | val_1_rmse: 0.89539 |  0:00:00s
epoch 2  | loss: 0.6502  | val_0_rmse: 0.84294 | val_1_rmse: 0.82301 |  0:00:01s
epoch 3  | loss: 0.595   | val_0_rmse: 0.80423 | val_1_rmse: 0.81329 |  0:00:01s
epoch 4  | loss: 0.51365 | val_0_rmse: 0.76402 | val_1_rmse: 0.77402 |  0:00:02s
epoch 5  | loss: 0.4886  | val_0_rmse: 0.72553 | val_1_rmse: 0.72137 |  0:00:02s
epoch 6  | loss: 0.48136 | val_0_rmse: 0.71748 | val_1_rmse: 0.72928 |  0:00:03s
epoch 7  | loss: 0.4746  | val_0_rmse: 0.69583 | val_1_rmse: 0.69657 |  0:00:03s
epoch 8  | loss: 0.47687 | val_0_rmse: 0.69257 | val_1_rmse: 0.70064 |  0:00:04s
epoch 9  | loss: 0.4601  | val_0_rmse: 0.67833 | val_1_rmse: 0.70028 |  0:00:04s
epoch 10 | loss: 0.45656 | val_0_rmse: 0.678   | val_1_rmse: 0.6903  |  0:00:05s
epoch 11 | loss: 0.44963 | val_0_rmse: 0.67005 | val_1_rmse: 0.70121 |  0:00:05s
epoch 12 | loss: 0.44725 | val_0_rmse: 0.67693 | val_1_rmse: 0.70397 |  0:00:05s
epoch 13 | loss: 0.45167 | val_0_rmse: 0.67283 | val_1_rmse: 0.71235 |  0:00:06s
epoch 14 | loss: 0.4421  | val_0_rmse: 0.67283 | val_1_rmse: 0.68667 |  0:00:06s
epoch 15 | loss: 0.44776 | val_0_rmse: 0.67069 | val_1_rmse: 0.69112 |  0:00:07s
epoch 16 | loss: 0.44553 | val_0_rmse: 0.67678 | val_1_rmse: 0.68607 |  0:00:07s
epoch 17 | loss: 0.45757 | val_0_rmse: 0.67402 | val_1_rmse: 0.69927 |  0:00:08s
epoch 18 | loss: 0.45857 | val_0_rmse: 0.67435 | val_1_rmse: 0.69321 |  0:00:08s
epoch 19 | loss: 0.44437 | val_0_rmse: 0.67643 | val_1_rmse: 0.69794 |  0:00:09s
epoch 20 | loss: 0.44323 | val_0_rmse: 0.67798 | val_1_rmse: 0.6961  |  0:00:09s
epoch 21 | loss: 0.44165 | val_0_rmse: 0.67956 | val_1_rmse: 0.70027 |  0:00:09s
epoch 22 | loss: 0.43714 | val_0_rmse: 0.6764  | val_1_rmse: 0.69825 |  0:00:10s
epoch 23 | loss: 0.4391  | val_0_rmse: 0.68215 | val_1_rmse: 0.69394 |  0:00:10s
epoch 24 | loss: 0.43637 | val_0_rmse: 0.67995 | val_1_rmse: 0.69684 |  0:00:11s
epoch 25 | loss: 0.43807 | val_0_rmse: 0.69916 | val_1_rmse: 0.70121 |  0:00:11s
epoch 26 | loss: 0.43093 | val_0_rmse: 0.67908 | val_1_rmse: 0.69485 |  0:00:12s
epoch 27 | loss: 0.43873 | val_0_rmse: 0.69052 | val_1_rmse: 0.68658 |  0:00:12s
epoch 28 | loss: 0.42942 | val_0_rmse: 0.68955 | val_1_rmse: 0.69706 |  0:00:13s
epoch 29 | loss: 0.43958 | val_0_rmse: 0.69845 | val_1_rmse: 0.69628 |  0:00:13s
epoch 30 | loss: 0.43741 | val_0_rmse: 0.68155 | val_1_rmse: 0.68429 |  0:00:13s
epoch 31 | loss: 0.44579 | val_0_rmse: 0.70031 | val_1_rmse: 0.69178 |  0:00:14s
epoch 32 | loss: 0.44192 | val_0_rmse: 0.68307 | val_1_rmse: 0.68661 |  0:00:14s
epoch 33 | loss: 0.43692 | val_0_rmse: 0.68347 | val_1_rmse: 0.67461 |  0:00:15s
epoch 34 | loss: 0.43541 | val_0_rmse: 0.68327 | val_1_rmse: 0.68831 |  0:00:15s
epoch 35 | loss: 0.43258 | val_0_rmse: 0.6875  | val_1_rmse: 0.68913 |  0:00:16s
epoch 36 | loss: 0.43687 | val_0_rmse: 0.68016 | val_1_rmse: 0.68851 |  0:00:16s
epoch 37 | loss: 0.44027 | val_0_rmse: 0.6932  | val_1_rmse: 0.68765 |  0:00:17s
epoch 38 | loss: 0.42749 | val_0_rmse: 0.68197 | val_1_rmse: 0.68347 |  0:00:17s
epoch 39 | loss: 0.43769 | val_0_rmse: 0.6813  | val_1_rmse: 0.67749 |  0:00:18s
epoch 40 | loss: 0.43295 | val_0_rmse: 0.68499 | val_1_rmse: 0.67918 |  0:00:18s
epoch 41 | loss: 0.42515 | val_0_rmse: 0.67622 | val_1_rmse: 0.67848 |  0:00:18s
epoch 42 | loss: 0.42651 | val_0_rmse: 0.67555 | val_1_rmse: 0.6779  |  0:00:19s
epoch 43 | loss: 0.4245  | val_0_rmse: 0.69165 | val_1_rmse: 0.69897 |  0:00:19s
epoch 44 | loss: 0.44024 | val_0_rmse: 0.68888 | val_1_rmse: 0.69054 |  0:00:20s
epoch 45 | loss: 0.43123 | val_0_rmse: 0.68572 | val_1_rmse: 0.68801 |  0:00:20s
epoch 46 | loss: 0.42665 | val_0_rmse: 0.6872  | val_1_rmse: 0.68892 |  0:00:21s
epoch 47 | loss: 0.43083 | val_0_rmse: 0.6859  | val_1_rmse: 0.69437 |  0:00:21s
epoch 48 | loss: 0.42893 | val_0_rmse: 0.68612 | val_1_rmse: 0.68724 |  0:00:22s
epoch 49 | loss: 0.42861 | val_0_rmse: 0.67336 | val_1_rmse: 0.6758  |  0:00:22s
epoch 50 | loss: 0.42505 | val_0_rmse: 0.67941 | val_1_rmse: 0.6771  |  0:00:22s
epoch 51 | loss: 0.42694 | val_0_rmse: 0.67959 | val_1_rmse: 0.68916 |  0:00:23s
epoch 52 | loss: 0.42912 | val_0_rmse: 0.69165 | val_1_rmse: 0.69336 |  0:00:23s
epoch 53 | loss: 0.4286  | val_0_rmse: 0.6813  | val_1_rmse: 0.68306 |  0:00:24s
epoch 54 | loss: 0.43295 | val_0_rmse: 0.68204 | val_1_rmse: 0.68142 |  0:00:24s
epoch 55 | loss: 0.42101 | val_0_rmse: 0.68568 | val_1_rmse: 0.68712 |  0:00:25s
epoch 56 | loss: 0.41566 | val_0_rmse: 0.68686 | val_1_rmse: 0.68694 |  0:00:25s
epoch 57 | loss: 0.41383 | val_0_rmse: 0.67639 | val_1_rmse: 0.68149 |  0:00:26s
epoch 58 | loss: 0.41701 | val_0_rmse: 0.68479 | val_1_rmse: 0.68197 |  0:00:26s
epoch 59 | loss: 0.42561 | val_0_rmse: 0.68039 | val_1_rmse: 0.68516 |  0:00:26s
epoch 60 | loss: 0.41811 | val_0_rmse: 0.68316 | val_1_rmse: 0.68324 |  0:00:27s
epoch 61 | loss: 0.41495 | val_0_rmse: 0.67414 | val_1_rmse: 0.67592 |  0:00:27s
epoch 62 | loss: 0.41797 | val_0_rmse: 0.67394 | val_1_rmse: 0.67509 |  0:00:28s
epoch 63 | loss: 0.41617 | val_0_rmse: 0.67783 | val_1_rmse: 0.66876 |  0:00:28s
epoch 64 | loss: 0.4124  | val_0_rmse: 0.67967 | val_1_rmse: 0.67758 |  0:00:29s
epoch 65 | loss: 0.40687 | val_0_rmse: 0.68208 | val_1_rmse: 0.67857 |  0:00:29s
epoch 66 | loss: 0.40506 | val_0_rmse: 0.67239 | val_1_rmse: 0.66718 |  0:00:30s
epoch 67 | loss: 0.41612 | val_0_rmse: 0.68337 | val_1_rmse: 0.67362 |  0:00:30s
epoch 68 | loss: 0.41466 | val_0_rmse: 0.67688 | val_1_rmse: 0.67448 |  0:00:31s
epoch 69 | loss: 0.40562 | val_0_rmse: 0.68066 | val_1_rmse: 0.68206 |  0:00:31s
epoch 70 | loss: 0.40544 | val_0_rmse: 0.67667 | val_1_rmse: 0.68208 |  0:00:31s
epoch 71 | loss: 0.39936 | val_0_rmse: 0.67557 | val_1_rmse: 0.67415 |  0:00:32s
epoch 72 | loss: 0.39696 | val_0_rmse: 0.67354 | val_1_rmse: 0.67289 |  0:00:32s
epoch 73 | loss: 0.39372 | val_0_rmse: 0.68837 | val_1_rmse: 0.68787 |  0:00:33s
epoch 74 | loss: 0.38262 | val_0_rmse: 0.70609 | val_1_rmse: 0.70989 |  0:00:33s
epoch 75 | loss: 0.37909 | val_0_rmse: 0.68956 | val_1_rmse: 0.68932 |  0:00:34s
epoch 76 | loss: 0.38136 | val_0_rmse: 0.68673 | val_1_rmse: 0.68312 |  0:00:34s
epoch 77 | loss: 0.36988 | val_0_rmse: 0.67663 | val_1_rmse: 0.67064 |  0:00:35s
epoch 78 | loss: 0.36324 | val_0_rmse: 0.7298  | val_1_rmse: 0.7179  |  0:00:35s
epoch 79 | loss: 0.36461 | val_0_rmse: 0.64317 | val_1_rmse: 0.64865 |  0:00:35s
epoch 80 | loss: 0.37859 | val_0_rmse: 0.75287 | val_1_rmse: 0.74475 |  0:00:36s
epoch 81 | loss: 0.35701 | val_0_rmse: 0.63914 | val_1_rmse: 0.64918 |  0:00:36s
epoch 82 | loss: 0.36781 | val_0_rmse: 0.67152 | val_1_rmse: 0.68003 |  0:00:37s
epoch 83 | loss: 0.35214 | val_0_rmse: 0.68673 | val_1_rmse: 0.70059 |  0:00:37s
epoch 84 | loss: 0.35175 | val_0_rmse: 0.61976 | val_1_rmse: 0.64308 |  0:00:38s
epoch 85 | loss: 0.34703 | val_0_rmse: 0.64363 | val_1_rmse: 0.65311 |  0:00:38s
epoch 86 | loss: 0.3498  | val_0_rmse: 0.60933 | val_1_rmse: 0.63006 |  0:00:39s
epoch 87 | loss: 0.34117 | val_0_rmse: 0.63127 | val_1_rmse: 0.64039 |  0:00:39s
epoch 88 | loss: 0.33964 | val_0_rmse: 0.60033 | val_1_rmse: 0.61905 |  0:00:40s
epoch 89 | loss: 0.34261 | val_0_rmse: 0.63078 | val_1_rmse: 0.64297 |  0:00:40s
epoch 90 | loss: 0.33704 | val_0_rmse: 0.62528 | val_1_rmse: 0.63216 |  0:00:40s
epoch 91 | loss: 0.33487 | val_0_rmse: 0.62384 | val_1_rmse: 0.62551 |  0:00:41s
epoch 92 | loss: 0.3289  | val_0_rmse: 0.59073 | val_1_rmse: 0.60612 |  0:00:41s
epoch 93 | loss: 0.32818 | val_0_rmse: 0.60211 | val_1_rmse: 0.61421 |  0:00:42s
epoch 94 | loss: 0.33009 | val_0_rmse: 0.58679 | val_1_rmse: 0.60982 |  0:00:42s
epoch 95 | loss: 0.32915 | val_0_rmse: 0.62716 | val_1_rmse: 0.63848 |  0:00:43s
epoch 96 | loss: 0.34187 | val_0_rmse: 0.60086 | val_1_rmse: 0.62067 |  0:00:43s
epoch 97 | loss: 0.33824 | val_0_rmse: 0.64614 | val_1_rmse: 0.65687 |  0:00:44s
epoch 98 | loss: 0.33882 | val_0_rmse: 0.61078 | val_1_rmse: 0.6315  |  0:00:44s
epoch 99 | loss: 0.34395 | val_0_rmse: 0.60092 | val_1_rmse: 0.61385 |  0:00:44s
epoch 100| loss: 0.3455  | val_0_rmse: 0.59253 | val_1_rmse: 0.60066 |  0:00:45s
epoch 101| loss: 0.33018 | val_0_rmse: 0.60326 | val_1_rmse: 0.60559 |  0:00:45s
epoch 102| loss: 0.33192 | val_0_rmse: 0.59181 | val_1_rmse: 0.60838 |  0:00:46s
epoch 103| loss: 0.32717 | val_0_rmse: 0.59338 | val_1_rmse: 0.60548 |  0:00:46s
epoch 104| loss: 0.32181 | val_0_rmse: 0.58269 | val_1_rmse: 0.59905 |  0:00:47s
epoch 105| loss: 0.31903 | val_0_rmse: 0.59376 | val_1_rmse: 0.60966 |  0:00:47s
epoch 106| loss: 0.31757 | val_0_rmse: 0.58749 | val_1_rmse: 0.60345 |  0:00:48s
epoch 107| loss: 0.31878 | val_0_rmse: 0.59818 | val_1_rmse: 0.61292 |  0:00:48s
epoch 108| loss: 0.32652 | val_0_rmse: 0.6035  | val_1_rmse: 0.61808 |  0:00:49s
epoch 109| loss: 0.31553 | val_0_rmse: 0.6051  | val_1_rmse: 0.6175  |  0:00:49s
epoch 110| loss: 0.31467 | val_0_rmse: 0.62805 | val_1_rmse: 0.63883 |  0:00:49s
epoch 111| loss: 0.3121  | val_0_rmse: 0.57777 | val_1_rmse: 0.60427 |  0:00:50s
epoch 112| loss: 0.31564 | val_0_rmse: 0.60066 | val_1_rmse: 0.62325 |  0:00:50s
epoch 113| loss: 0.32056 | val_0_rmse: 0.58067 | val_1_rmse: 0.60402 |  0:00:51s
epoch 114| loss: 0.30044 | val_0_rmse: 0.57762 | val_1_rmse: 0.60379 |  0:00:51s
epoch 115| loss: 0.31027 | val_0_rmse: 0.5682  | val_1_rmse: 0.59791 |  0:00:52s
epoch 116| loss: 0.3072  | val_0_rmse: 0.58663 | val_1_rmse: 0.60667 |  0:00:52s
epoch 117| loss: 0.3132  | val_0_rmse: 0.5768  | val_1_rmse: 0.60575 |  0:00:53s
epoch 118| loss: 0.30419 | val_0_rmse: 0.59701 | val_1_rmse: 0.61113 |  0:00:53s
epoch 119| loss: 0.30951 | val_0_rmse: 0.58607 | val_1_rmse: 0.60172 |  0:00:53s
epoch 120| loss: 0.30644 | val_0_rmse: 0.63338 | val_1_rmse: 0.63958 |  0:00:54s
epoch 121| loss: 0.31273 | val_0_rmse: 0.58693 | val_1_rmse: 0.61215 |  0:00:54s
epoch 122| loss: 0.31666 | val_0_rmse: 0.60333 | val_1_rmse: 0.61679 |  0:00:55s
epoch 123| loss: 0.31951 | val_0_rmse: 0.58159 | val_1_rmse: 0.60306 |  0:00:55s
epoch 124| loss: 0.30508 | val_0_rmse: 0.64731 | val_1_rmse: 0.65432 |  0:00:56s
epoch 125| loss: 0.31136 | val_0_rmse: 0.58751 | val_1_rmse: 0.60819 |  0:00:56s
epoch 126| loss: 0.30511 | val_0_rmse: 0.60481 | val_1_rmse: 0.63863 |  0:00:57s
epoch 127| loss: 0.30544 | val_0_rmse: 0.56995 | val_1_rmse: 0.59791 |  0:00:57s
epoch 128| loss: 0.32152 | val_0_rmse: 0.63509 | val_1_rmse: 0.63808 |  0:00:57s
epoch 129| loss: 0.31797 | val_0_rmse: 0.59208 | val_1_rmse: 0.61454 |  0:00:58s
epoch 130| loss: 0.3044  | val_0_rmse: 0.58841 | val_1_rmse: 0.6097  |  0:00:58s
epoch 131| loss: 0.30931 | val_0_rmse: 0.57805 | val_1_rmse: 0.60272 |  0:00:59s
epoch 132| loss: 0.30418 | val_0_rmse: 0.56892 | val_1_rmse: 0.59657 |  0:00:59s
epoch 133| loss: 0.30268 | val_0_rmse: 0.58948 | val_1_rmse: 0.61293 |  0:01:00s
epoch 134| loss: 0.30879 | val_0_rmse: 0.62664 | val_1_rmse: 0.64022 |  0:01:00s
epoch 135| loss: 0.30577 | val_0_rmse: 0.59595 | val_1_rmse: 0.61968 |  0:01:01s
epoch 136| loss: 0.2996  | val_0_rmse: 0.5697  | val_1_rmse: 0.59472 |  0:01:01s
epoch 137| loss: 0.30604 | val_0_rmse: 0.57324 | val_1_rmse: 0.59912 |  0:01:02s
epoch 138| loss: 0.30247 | val_0_rmse: 0.56514 | val_1_rmse: 0.59376 |  0:01:02s
epoch 139| loss: 0.29144 | val_0_rmse: 0.57517 | val_1_rmse: 0.60002 |  0:01:02s
epoch 140| loss: 0.29887 | val_0_rmse: 0.58777 | val_1_rmse: 0.60013 |  0:01:03s
epoch 141| loss: 0.30747 | val_0_rmse: 0.59259 | val_1_rmse: 0.60734 |  0:01:03s
epoch 142| loss: 0.29975 | val_0_rmse: 0.5783  | val_1_rmse: 0.5932  |  0:01:04s
epoch 143| loss: 0.29167 | val_0_rmse: 0.57575 | val_1_rmse: 0.59257 |  0:01:04s
epoch 144| loss: 0.30427 | val_0_rmse: 0.59746 | val_1_rmse: 0.61103 |  0:01:05s
epoch 145| loss: 0.31078 | val_0_rmse: 0.57456 | val_1_rmse: 0.60275 |  0:01:05s
epoch 146| loss: 0.31782 | val_0_rmse: 0.63345 | val_1_rmse: 0.65617 |  0:01:06s
epoch 147| loss: 0.31056 | val_0_rmse: 0.56259 | val_1_rmse: 0.59949 |  0:01:06s
epoch 148| loss: 0.30399 | val_0_rmse: 0.65499 | val_1_rmse: 0.66254 |  0:01:07s
epoch 149| loss: 0.31287 | val_0_rmse: 0.58679 | val_1_rmse: 0.59313 |  0:01:07s
Stop training because you reached max_epochs = 150 with best_epoch = 143 and best_val_1_rmse = 0.59257
Best weights from best epoch are automatically used!
ended training at: 08:23:04
Feature importance:
Mean squared error is of 2920902667.763817
Mean absolute error:36601.32213805178
MAPE:0.3059480872134674
R2 score:0.6451021001459298
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:23:04
epoch 0  | loss: 1.04707 | val_0_rmse: 0.98882 | val_1_rmse: 1.01532 |  0:00:00s
epoch 1  | loss: 0.81542 | val_0_rmse: 0.93704 | val_1_rmse: 0.94095 |  0:00:00s
epoch 2  | loss: 0.6819  | val_0_rmse: 0.80833 | val_1_rmse: 0.84531 |  0:00:01s
epoch 3  | loss: 0.59535 | val_0_rmse: 0.81962 | val_1_rmse: 0.82391 |  0:00:01s
epoch 4  | loss: 0.56686 | val_0_rmse: 0.76827 | val_1_rmse: 0.79837 |  0:00:02s
epoch 5  | loss: 0.54977 | val_0_rmse: 0.75903 | val_1_rmse: 0.77803 |  0:00:02s
epoch 6  | loss: 0.52889 | val_0_rmse: 0.71324 | val_1_rmse: 0.72947 |  0:00:03s
epoch 7  | loss: 0.51168 | val_0_rmse: 0.71339 | val_1_rmse: 0.73688 |  0:00:03s
epoch 8  | loss: 0.4997  | val_0_rmse: 0.70057 | val_1_rmse: 0.71784 |  0:00:04s
epoch 9  | loss: 0.49172 | val_0_rmse: 0.69531 | val_1_rmse: 0.69835 |  0:00:04s
epoch 10 | loss: 0.48554 | val_0_rmse: 0.68833 | val_1_rmse: 0.68939 |  0:00:05s
epoch 11 | loss: 0.47499 | val_0_rmse: 0.70218 | val_1_rmse: 0.70839 |  0:00:05s
epoch 12 | loss: 0.47937 | val_0_rmse: 0.6916  | val_1_rmse: 0.69566 |  0:00:05s
epoch 13 | loss: 0.46683 | val_0_rmse: 0.68889 | val_1_rmse: 0.69457 |  0:00:06s
epoch 14 | loss: 0.47363 | val_0_rmse: 0.68933 | val_1_rmse: 0.69238 |  0:00:06s
epoch 15 | loss: 0.47245 | val_0_rmse: 0.701   | val_1_rmse: 0.70359 |  0:00:07s
epoch 16 | loss: 0.46405 | val_0_rmse: 0.69814 | val_1_rmse: 0.70394 |  0:00:07s
epoch 17 | loss: 0.45869 | val_0_rmse: 0.69461 | val_1_rmse: 0.69605 |  0:00:08s
epoch 18 | loss: 0.46205 | val_0_rmse: 0.70969 | val_1_rmse: 0.72086 |  0:00:08s
epoch 19 | loss: 0.47834 | val_0_rmse: 0.70142 | val_1_rmse: 0.70843 |  0:00:09s
epoch 20 | loss: 0.45153 | val_0_rmse: 0.67663 | val_1_rmse: 0.67679 |  0:00:09s
epoch 21 | loss: 0.44895 | val_0_rmse: 0.66177 | val_1_rmse: 0.67712 |  0:00:09s
epoch 22 | loss: 0.43287 | val_0_rmse: 0.65822 | val_1_rmse: 0.65843 |  0:00:10s
epoch 23 | loss: 0.42758 | val_0_rmse: 0.66822 | val_1_rmse: 0.65306 |  0:00:10s
epoch 24 | loss: 0.40499 | val_0_rmse: 0.64139 | val_1_rmse: 0.63703 |  0:00:11s
epoch 25 | loss: 0.41434 | val_0_rmse: 0.69093 | val_1_rmse: 0.67977 |  0:00:11s
epoch 26 | loss: 0.40699 | val_0_rmse: 0.63829 | val_1_rmse: 0.63986 |  0:00:12s
epoch 27 | loss: 0.39965 | val_0_rmse: 0.63462 | val_1_rmse: 0.63283 |  0:00:12s
epoch 28 | loss: 0.38104 | val_0_rmse: 0.61415 | val_1_rmse: 0.61102 |  0:00:13s
epoch 29 | loss: 0.36676 | val_0_rmse: 0.6342  | val_1_rmse: 0.63329 |  0:00:13s
epoch 30 | loss: 0.37553 | val_0_rmse: 0.64795 | val_1_rmse: 0.65097 |  0:00:14s
epoch 31 | loss: 0.36089 | val_0_rmse: 0.6525  | val_1_rmse: 0.63448 |  0:00:14s
epoch 32 | loss: 0.38408 | val_0_rmse: 0.68289 | val_1_rmse: 0.67857 |  0:00:14s
epoch 33 | loss: 0.40831 | val_0_rmse: 0.66862 | val_1_rmse: 0.67287 |  0:00:15s
epoch 34 | loss: 0.4128  | val_0_rmse: 0.82359 | val_1_rmse: 0.84685 |  0:00:15s
epoch 35 | loss: 0.45273 | val_0_rmse: 0.72042 | val_1_rmse: 0.74406 |  0:00:16s
epoch 36 | loss: 0.40033 | val_0_rmse: 0.68231 | val_1_rmse: 0.70436 |  0:00:16s
epoch 37 | loss: 0.40596 | val_0_rmse: 0.69753 | val_1_rmse: 0.71575 |  0:00:17s
epoch 38 | loss: 0.39737 | val_0_rmse: 0.70233 | val_1_rmse: 0.72038 |  0:00:17s
epoch 39 | loss: 0.41521 | val_0_rmse: 0.66801 | val_1_rmse: 0.6896  |  0:00:18s
epoch 40 | loss: 0.39649 | val_0_rmse: 0.67401 | val_1_rmse: 0.69273 |  0:00:18s
epoch 41 | loss: 0.38673 | val_0_rmse: 0.6538  | val_1_rmse: 0.67521 |  0:00:18s
epoch 42 | loss: 0.39485 | val_0_rmse: 0.62467 | val_1_rmse: 0.64198 |  0:00:19s
epoch 43 | loss: 0.38609 | val_0_rmse: 0.62179 | val_1_rmse: 0.63343 |  0:00:19s
epoch 44 | loss: 0.38795 | val_0_rmse: 0.64386 | val_1_rmse: 0.64279 |  0:00:20s
epoch 45 | loss: 0.38962 | val_0_rmse: 0.62142 | val_1_rmse: 0.64018 |  0:00:20s
epoch 46 | loss: 0.37383 | val_0_rmse: 0.60399 | val_1_rmse: 0.6121  |  0:00:21s
epoch 47 | loss: 0.37374 | val_0_rmse: 0.60648 | val_1_rmse: 0.61578 |  0:00:21s
epoch 48 | loss: 0.38055 | val_0_rmse: 0.6167  | val_1_rmse: 0.62773 |  0:00:22s
epoch 49 | loss: 0.38543 | val_0_rmse: 0.61235 | val_1_rmse: 0.62498 |  0:00:22s
epoch 50 | loss: 0.37509 | val_0_rmse: 0.60713 | val_1_rmse: 0.61764 |  0:00:23s
epoch 51 | loss: 0.39084 | val_0_rmse: 0.60246 | val_1_rmse: 0.62306 |  0:00:23s
epoch 52 | loss: 0.40008 | val_0_rmse: 0.62285 | val_1_rmse: 0.64331 |  0:00:23s
epoch 53 | loss: 0.36883 | val_0_rmse: 0.61349 | val_1_rmse: 0.62126 |  0:00:24s
epoch 54 | loss: 0.36993 | val_0_rmse: 0.6374  | val_1_rmse: 0.65195 |  0:00:24s
epoch 55 | loss: 0.3772  | val_0_rmse: 0.61653 | val_1_rmse: 0.63428 |  0:00:25s
epoch 56 | loss: 0.36935 | val_0_rmse: 0.61639 | val_1_rmse: 0.63347 |  0:00:25s
epoch 57 | loss: 0.3581  | val_0_rmse: 0.60079 | val_1_rmse: 0.61721 |  0:00:26s
epoch 58 | loss: 0.35847 | val_0_rmse: 0.62125 | val_1_rmse: 0.63617 |  0:00:26s

Early stopping occured at epoch 58 with best_epoch = 28 and best_val_1_rmse = 0.61102
Best weights from best epoch are automatically used!
ended training at: 08:23:31
Feature importance:
Mean squared error is of 3590972784.4817934
Mean absolute error:40282.79016479202
MAPE:0.3986118332787961
R2 score:0.5957188986623214
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:23:31
epoch 0  | loss: 1.06485 | val_0_rmse: 0.98442 | val_1_rmse: 0.99844 |  0:00:00s
epoch 1  | loss: 0.78852 | val_0_rmse: 0.89673 | val_1_rmse: 0.90064 |  0:00:00s
epoch 2  | loss: 0.64464 | val_0_rmse: 0.80057 | val_1_rmse: 0.79261 |  0:00:01s
epoch 3  | loss: 0.57016 | val_0_rmse: 0.80884 | val_1_rmse: 0.82411 |  0:00:01s
epoch 4  | loss: 0.55531 | val_0_rmse: 0.77541 | val_1_rmse: 0.78403 |  0:00:02s
epoch 5  | loss: 0.52734 | val_0_rmse: 0.75399 | val_1_rmse: 0.74593 |  0:00:02s
epoch 6  | loss: 0.50952 | val_0_rmse: 0.85748 | val_1_rmse: 0.83284 |  0:00:03s
epoch 7  | loss: 0.50674 | val_0_rmse: 0.78927 | val_1_rmse: 0.76966 |  0:00:03s
epoch 8  | loss: 0.49305 | val_0_rmse: 0.72792 | val_1_rmse: 0.70371 |  0:00:04s
epoch 9  | loss: 0.48421 | val_0_rmse: 0.72415 | val_1_rmse: 0.69253 |  0:00:04s
epoch 10 | loss: 0.49226 | val_0_rmse: 0.71193 | val_1_rmse: 0.69246 |  0:00:04s
epoch 11 | loss: 0.48276 | val_0_rmse: 0.68909 | val_1_rmse: 0.68293 |  0:00:05s
epoch 12 | loss: 0.46433 | val_0_rmse: 0.69547 | val_1_rmse: 0.69499 |  0:00:05s
epoch 13 | loss: 0.47764 | val_0_rmse: 0.68574 | val_1_rmse: 0.69088 |  0:00:06s
epoch 14 | loss: 0.45483 | val_0_rmse: 0.68969 | val_1_rmse: 0.68723 |  0:00:06s
epoch 15 | loss: 0.44307 | val_0_rmse: 0.68768 | val_1_rmse: 0.67654 |  0:00:07s
epoch 16 | loss: 0.43052 | val_0_rmse: 0.69366 | val_1_rmse: 0.69357 |  0:00:07s
epoch 17 | loss: 0.4374  | val_0_rmse: 0.69985 | val_1_rmse: 0.70174 |  0:00:08s
epoch 18 | loss: 0.43491 | val_0_rmse: 0.70102 | val_1_rmse: 0.69749 |  0:00:08s
epoch 19 | loss: 0.42624 | val_0_rmse: 0.74969 | val_1_rmse: 0.74506 |  0:00:09s
epoch 20 | loss: 0.40864 | val_0_rmse: 0.73095 | val_1_rmse: 0.73433 |  0:00:09s
epoch 21 | loss: 0.41625 | val_0_rmse: 0.72741 | val_1_rmse: 0.72432 |  0:00:09s
epoch 22 | loss: 0.39256 | val_0_rmse: 0.7288  | val_1_rmse: 0.70834 |  0:00:10s
epoch 23 | loss: 0.39635 | val_0_rmse: 0.71264 | val_1_rmse: 0.70394 |  0:00:10s
epoch 24 | loss: 0.38453 | val_0_rmse: 0.69214 | val_1_rmse: 0.69047 |  0:00:11s
epoch 25 | loss: 0.37746 | val_0_rmse: 0.68639 | val_1_rmse: 0.67746 |  0:00:11s
epoch 26 | loss: 0.37699 | val_0_rmse: 0.65372 | val_1_rmse: 0.65209 |  0:00:12s
epoch 27 | loss: 0.37264 | val_0_rmse: 0.67093 | val_1_rmse: 0.66585 |  0:00:12s
epoch 28 | loss: 0.36891 | val_0_rmse: 0.69101 | val_1_rmse: 0.68673 |  0:00:13s
epoch 29 | loss: 0.38425 | val_0_rmse: 0.69074 | val_1_rmse: 0.68415 |  0:00:13s
epoch 30 | loss: 0.37789 | val_0_rmse: 0.63804 | val_1_rmse: 0.63505 |  0:00:13s
epoch 31 | loss: 0.38455 | val_0_rmse: 0.64658 | val_1_rmse: 0.63739 |  0:00:14s
epoch 32 | loss: 0.36766 | val_0_rmse: 0.6578  | val_1_rmse: 0.66205 |  0:00:14s
epoch 33 | loss: 0.36767 | val_0_rmse: 0.65794 | val_1_rmse: 0.63951 |  0:00:15s
epoch 34 | loss: 0.37835 | val_0_rmse: 0.63291 | val_1_rmse: 0.61846 |  0:00:15s
epoch 35 | loss: 0.37096 | val_0_rmse: 0.68974 | val_1_rmse: 0.6722  |  0:00:16s
epoch 36 | loss: 0.37959 | val_0_rmse: 0.63173 | val_1_rmse: 0.62088 |  0:00:16s
epoch 37 | loss: 0.37035 | val_0_rmse: 0.71117 | val_1_rmse: 0.6995  |  0:00:17s
epoch 38 | loss: 0.38844 | val_0_rmse: 0.71249 | val_1_rmse: 0.70514 |  0:00:17s
epoch 39 | loss: 0.36056 | val_0_rmse: 0.63786 | val_1_rmse: 0.62922 |  0:00:18s
epoch 40 | loss: 0.36794 | val_0_rmse: 0.67069 | val_1_rmse: 0.65617 |  0:00:18s
epoch 41 | loss: 0.366   | val_0_rmse: 0.69148 | val_1_rmse: 0.6814  |  0:00:19s
epoch 42 | loss: 0.36852 | val_0_rmse: 0.70623 | val_1_rmse: 0.69003 |  0:00:19s
epoch 43 | loss: 0.35863 | val_0_rmse: 0.62941 | val_1_rmse: 0.60905 |  0:00:19s
epoch 44 | loss: 0.36693 | val_0_rmse: 0.71083 | val_1_rmse: 0.69041 |  0:00:20s
epoch 45 | loss: 0.36847 | val_0_rmse: 0.67962 | val_1_rmse: 0.66012 |  0:00:20s
epoch 46 | loss: 0.36182 | val_0_rmse: 0.69062 | val_1_rmse: 0.67381 |  0:00:21s
epoch 47 | loss: 0.36387 | val_0_rmse: 0.68561 | val_1_rmse: 0.68009 |  0:00:21s
epoch 48 | loss: 0.36569 | val_0_rmse: 0.65973 | val_1_rmse: 0.65132 |  0:00:22s
epoch 49 | loss: 0.35991 | val_0_rmse: 0.66185 | val_1_rmse: 0.65482 |  0:00:22s
epoch 50 | loss: 0.36069 | val_0_rmse: 0.68141 | val_1_rmse: 0.66515 |  0:00:23s
epoch 51 | loss: 0.36744 | val_0_rmse: 0.66315 | val_1_rmse: 0.64964 |  0:00:23s
epoch 52 | loss: 0.3616  | val_0_rmse: 0.65374 | val_1_rmse: 0.64126 |  0:00:24s
epoch 53 | loss: 0.35562 | val_0_rmse: 0.67963 | val_1_rmse: 0.67944 |  0:00:24s
epoch 54 | loss: 0.35402 | val_0_rmse: 0.63975 | val_1_rmse: 0.63105 |  0:00:24s
epoch 55 | loss: 0.34861 | val_0_rmse: 0.63439 | val_1_rmse: 0.61769 |  0:00:25s
epoch 56 | loss: 0.34596 | val_0_rmse: 0.63553 | val_1_rmse: 0.6249  |  0:00:25s
epoch 57 | loss: 0.34828 | val_0_rmse: 0.64753 | val_1_rmse: 0.63443 |  0:00:26s
epoch 58 | loss: 0.34533 | val_0_rmse: 0.66245 | val_1_rmse: 0.65713 |  0:00:26s
epoch 59 | loss: 0.35159 | val_0_rmse: 0.62333 | val_1_rmse: 0.61362 |  0:00:27s
epoch 60 | loss: 0.35151 | val_0_rmse: 0.61591 | val_1_rmse: 0.60294 |  0:00:27s
epoch 61 | loss: 0.34345 | val_0_rmse: 0.60456 | val_1_rmse: 0.59808 |  0:00:28s
epoch 62 | loss: 0.34597 | val_0_rmse: 0.66799 | val_1_rmse: 0.6607  |  0:00:28s
epoch 63 | loss: 0.35339 | val_0_rmse: 0.6718  | val_1_rmse: 0.66743 |  0:00:28s
epoch 64 | loss: 0.35262 | val_0_rmse: 0.66721 | val_1_rmse: 0.6562  |  0:00:29s
epoch 65 | loss: 0.35667 | val_0_rmse: 0.69106 | val_1_rmse: 0.68664 |  0:00:29s
epoch 66 | loss: 0.34311 | val_0_rmse: 0.67848 | val_1_rmse: 0.68646 |  0:00:30s
epoch 67 | loss: 0.34969 | val_0_rmse: 0.67604 | val_1_rmse: 0.68031 |  0:00:30s
epoch 68 | loss: 0.35088 | val_0_rmse: 0.70768 | val_1_rmse: 0.70727 |  0:00:31s
epoch 69 | loss: 0.34424 | val_0_rmse: 0.61264 | val_1_rmse: 0.6034  |  0:00:31s
epoch 70 | loss: 0.34489 | val_0_rmse: 0.63091 | val_1_rmse: 0.6221  |  0:00:32s
epoch 71 | loss: 0.3414  | val_0_rmse: 0.62696 | val_1_rmse: 0.62927 |  0:00:32s
epoch 72 | loss: 0.34107 | val_0_rmse: 0.64398 | val_1_rmse: 0.64045 |  0:00:32s
epoch 73 | loss: 0.34272 | val_0_rmse: 0.6488  | val_1_rmse: 0.64648 |  0:00:33s
epoch 74 | loss: 0.34595 | val_0_rmse: 0.62487 | val_1_rmse: 0.62568 |  0:00:33s
epoch 75 | loss: 0.33778 | val_0_rmse: 0.63076 | val_1_rmse: 0.63215 |  0:00:34s
epoch 76 | loss: 0.34587 | val_0_rmse: 0.59491 | val_1_rmse: 0.59967 |  0:00:34s
epoch 77 | loss: 0.33377 | val_0_rmse: 0.63888 | val_1_rmse: 0.64244 |  0:00:35s
epoch 78 | loss: 0.33197 | val_0_rmse: 0.58087 | val_1_rmse: 0.58206 |  0:00:35s
epoch 79 | loss: 0.34202 | val_0_rmse: 0.60629 | val_1_rmse: 0.61327 |  0:00:36s
epoch 80 | loss: 0.3435  | val_0_rmse: 0.60446 | val_1_rmse: 0.59758 |  0:00:36s
epoch 81 | loss: 0.34167 | val_0_rmse: 0.63852 | val_1_rmse: 0.62382 |  0:00:36s
epoch 82 | loss: 0.33261 | val_0_rmse: 0.59965 | val_1_rmse: 0.58337 |  0:00:37s
epoch 83 | loss: 0.33338 | val_0_rmse: 0.64073 | val_1_rmse: 0.62366 |  0:00:37s
epoch 84 | loss: 0.32415 | val_0_rmse: 0.61441 | val_1_rmse: 0.59699 |  0:00:38s
epoch 85 | loss: 0.32861 | val_0_rmse: 0.60275 | val_1_rmse: 0.58294 |  0:00:38s
epoch 86 | loss: 0.33652 | val_0_rmse: 0.63799 | val_1_rmse: 0.61407 |  0:00:39s
epoch 87 | loss: 0.33289 | val_0_rmse: 0.61334 | val_1_rmse: 0.59192 |  0:00:39s
epoch 88 | loss: 0.3444  | val_0_rmse: 0.65286 | val_1_rmse: 0.63629 |  0:00:40s
epoch 89 | loss: 0.33106 | val_0_rmse: 0.60593 | val_1_rmse: 0.61004 |  0:00:40s
epoch 90 | loss: 0.33466 | val_0_rmse: 0.58996 | val_1_rmse: 0.58471 |  0:00:41s
epoch 91 | loss: 0.33576 | val_0_rmse: 0.58595 | val_1_rmse: 0.57997 |  0:00:41s
epoch 92 | loss: 0.33018 | val_0_rmse: 0.62977 | val_1_rmse: 0.62361 |  0:00:41s
epoch 93 | loss: 0.32926 | val_0_rmse: 0.59896 | val_1_rmse: 0.59492 |  0:00:42s
epoch 94 | loss: 0.33258 | val_0_rmse: 0.61179 | val_1_rmse: 0.60361 |  0:00:42s
epoch 95 | loss: 0.33082 | val_0_rmse: 0.57345 | val_1_rmse: 0.56764 |  0:00:43s
epoch 96 | loss: 0.33016 | val_0_rmse: 0.63146 | val_1_rmse: 0.62262 |  0:00:43s
epoch 97 | loss: 0.33132 | val_0_rmse: 0.5951  | val_1_rmse: 0.58861 |  0:00:44s
epoch 98 | loss: 0.33839 | val_0_rmse: 0.58684 | val_1_rmse: 0.57635 |  0:00:44s
epoch 99 | loss: 0.33424 | val_0_rmse: 0.62927 | val_1_rmse: 0.62185 |  0:00:45s
epoch 100| loss: 0.33858 | val_0_rmse: 0.59334 | val_1_rmse: 0.58616 |  0:00:45s
epoch 101| loss: 0.3312  | val_0_rmse: 0.58655 | val_1_rmse: 0.5716  |  0:00:46s
epoch 102| loss: 0.32881 | val_0_rmse: 0.60207 | val_1_rmse: 0.58827 |  0:00:46s
epoch 103| loss: 0.33318 | val_0_rmse: 0.61364 | val_1_rmse: 0.61127 |  0:00:46s
epoch 104| loss: 0.33463 | val_0_rmse: 0.62132 | val_1_rmse: 0.62067 |  0:00:47s
epoch 105| loss: 0.33216 | val_0_rmse: 0.59029 | val_1_rmse: 0.5874  |  0:00:47s
epoch 106| loss: 0.33768 | val_0_rmse: 0.59246 | val_1_rmse: 0.60242 |  0:00:48s
epoch 107| loss: 0.33959 | val_0_rmse: 0.63672 | val_1_rmse: 0.65659 |  0:00:48s
epoch 108| loss: 0.33588 | val_0_rmse: 0.57671 | val_1_rmse: 0.59169 |  0:00:49s
epoch 109| loss: 0.34227 | val_0_rmse: 0.61579 | val_1_rmse: 0.61696 |  0:00:49s
epoch 110| loss: 0.34019 | val_0_rmse: 0.5929  | val_1_rmse: 0.60608 |  0:00:49s
epoch 111| loss: 0.33356 | val_0_rmse: 0.58087 | val_1_rmse: 0.59494 |  0:00:50s
epoch 112| loss: 0.32747 | val_0_rmse: 0.63126 | val_1_rmse: 0.63179 |  0:00:50s
epoch 113| loss: 0.3372  | val_0_rmse: 0.59474 | val_1_rmse: 0.58744 |  0:00:51s
epoch 114| loss: 0.33396 | val_0_rmse: 0.62672 | val_1_rmse: 0.61975 |  0:00:51s
epoch 115| loss: 0.32293 | val_0_rmse: 0.6294  | val_1_rmse: 0.61257 |  0:00:52s
epoch 116| loss: 0.33641 | val_0_rmse: 0.59451 | val_1_rmse: 0.58675 |  0:00:52s
epoch 117| loss: 0.33393 | val_0_rmse: 0.62746 | val_1_rmse: 0.62022 |  0:00:53s
epoch 118| loss: 0.33475 | val_0_rmse: 0.60494 | val_1_rmse: 0.5948  |  0:00:53s
epoch 119| loss: 0.32759 | val_0_rmse: 0.59749 | val_1_rmse: 0.58758 |  0:00:53s
epoch 120| loss: 0.3253  | val_0_rmse: 0.60535 | val_1_rmse: 0.5988  |  0:00:54s
epoch 121| loss: 0.32353 | val_0_rmse: 0.59143 | val_1_rmse: 0.57694 |  0:00:54s
epoch 122| loss: 0.32379 | val_0_rmse: 0.57226 | val_1_rmse: 0.56106 |  0:00:55s
epoch 123| loss: 0.32107 | val_0_rmse: 0.57169 | val_1_rmse: 0.56126 |  0:00:55s
epoch 124| loss: 0.32057 | val_0_rmse: 0.57552 | val_1_rmse: 0.5633  |  0:00:56s
epoch 125| loss: 0.31879 | val_0_rmse: 0.59561 | val_1_rmse: 0.58421 |  0:00:56s
epoch 126| loss: 0.33267 | val_0_rmse: 0.57176 | val_1_rmse: 0.56563 |  0:00:57s
epoch 127| loss: 0.31674 | val_0_rmse: 0.59321 | val_1_rmse: 0.58192 |  0:00:57s
epoch 128| loss: 0.33787 | val_0_rmse: 0.59252 | val_1_rmse: 0.58467 |  0:00:58s
epoch 129| loss: 0.32757 | val_0_rmse: 0.59973 | val_1_rmse: 0.59022 |  0:00:58s
epoch 130| loss: 0.32644 | val_0_rmse: 0.62467 | val_1_rmse: 0.61227 |  0:00:58s
epoch 131| loss: 0.33604 | val_0_rmse: 0.576   | val_1_rmse: 0.5657  |  0:00:59s
epoch 132| loss: 0.3371  | val_0_rmse: 0.59785 | val_1_rmse: 0.58934 |  0:00:59s
epoch 133| loss: 0.33679 | val_0_rmse: 0.63864 | val_1_rmse: 0.63064 |  0:01:00s
epoch 134| loss: 0.33165 | val_0_rmse: 0.62396 | val_1_rmse: 0.61262 |  0:01:00s
epoch 135| loss: 0.32607 | val_0_rmse: 0.57804 | val_1_rmse: 0.56412 |  0:01:01s
epoch 136| loss: 0.32298 | val_0_rmse: 0.64366 | val_1_rmse: 0.62919 |  0:01:01s
epoch 137| loss: 0.32258 | val_0_rmse: 0.58142 | val_1_rmse: 0.57364 |  0:01:01s
epoch 138| loss: 0.33097 | val_0_rmse: 0.57378 | val_1_rmse: 0.56747 |  0:01:02s
epoch 139| loss: 0.33027 | val_0_rmse: 0.56864 | val_1_rmse: 0.55928 |  0:01:02s
epoch 140| loss: 0.32413 | val_0_rmse: 0.56785 | val_1_rmse: 0.56687 |  0:01:03s
epoch 141| loss: 0.3278  | val_0_rmse: 0.59789 | val_1_rmse: 0.59722 |  0:01:03s
epoch 142| loss: 0.31195 | val_0_rmse: 0.60814 | val_1_rmse: 0.59824 |  0:01:04s
epoch 143| loss: 0.32402 | val_0_rmse: 0.58276 | val_1_rmse: 0.57419 |  0:01:04s
epoch 144| loss: 0.31315 | val_0_rmse: 0.57718 | val_1_rmse: 0.57475 |  0:01:05s
epoch 145| loss: 0.31735 | val_0_rmse: 0.59381 | val_1_rmse: 0.5847  |  0:01:05s
epoch 146| loss: 0.31788 | val_0_rmse: 0.58857 | val_1_rmse: 0.57951 |  0:01:05s
epoch 147| loss: 0.32072 | val_0_rmse: 0.59575 | val_1_rmse: 0.59362 |  0:01:06s
epoch 148| loss: 0.32287 | val_0_rmse: 0.58079 | val_1_rmse: 0.57069 |  0:01:06s
epoch 149| loss: 0.32177 | val_0_rmse: 0.57488 | val_1_rmse: 0.56666 |  0:01:07s
Stop training because you reached max_epochs = 150 with best_epoch = 139 and best_val_1_rmse = 0.55928
Best weights from best epoch are automatically used!
ended training at: 08:24:38
Feature importance:
Mean squared error is of 2981991363.897701
Mean absolute error:36717.76470259444
MAPE:0.3489205096670509
R2 score:0.6137000141039264
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:24:38
epoch 0  | loss: 1.01334 | val_0_rmse: 0.91785 | val_1_rmse: 0.9005  |  0:00:00s
epoch 1  | loss: 0.7411  | val_0_rmse: 0.79346 | val_1_rmse: 0.82027 |  0:00:00s
epoch 2  | loss: 0.60917 | val_0_rmse: 0.81851 | val_1_rmse: 0.82536 |  0:00:01s
epoch 3  | loss: 0.52518 | val_0_rmse: 0.78166 | val_1_rmse: 0.79372 |  0:00:01s
epoch 4  | loss: 0.53574 | val_0_rmse: 0.78835 | val_1_rmse: 0.80008 |  0:00:02s
epoch 5  | loss: 0.52348 | val_0_rmse: 0.78818 | val_1_rmse: 0.80388 |  0:00:02s
epoch 6  | loss: 0.51716 | val_0_rmse: 0.77262 | val_1_rmse: 0.7847  |  0:00:03s
epoch 7  | loss: 0.5112  | val_0_rmse: 0.79278 | val_1_rmse: 0.81266 |  0:00:03s
epoch 8  | loss: 0.4968  | val_0_rmse: 0.78708 | val_1_rmse: 0.80026 |  0:00:04s
epoch 9  | loss: 0.49012 | val_0_rmse: 0.816   | val_1_rmse: 0.83615 |  0:00:04s
epoch 10 | loss: 0.48185 | val_0_rmse: 0.81676 | val_1_rmse: 0.82837 |  0:00:05s
epoch 11 | loss: 0.47043 | val_0_rmse: 0.7903  | val_1_rmse: 0.8099  |  0:00:05s
epoch 12 | loss: 0.47031 | val_0_rmse: 0.7866  | val_1_rmse: 0.81014 |  0:00:05s
epoch 13 | loss: 0.49369 | val_0_rmse: 0.78382 | val_1_rmse: 0.79985 |  0:00:06s
epoch 14 | loss: 0.47471 | val_0_rmse: 0.70141 | val_1_rmse: 0.71074 |  0:00:06s
epoch 15 | loss: 0.46402 | val_0_rmse: 0.68889 | val_1_rmse: 0.70214 |  0:00:07s
epoch 16 | loss: 0.45241 | val_0_rmse: 0.67953 | val_1_rmse: 0.70347 |  0:00:07s
epoch 17 | loss: 0.44316 | val_0_rmse: 0.67175 | val_1_rmse: 0.68755 |  0:00:08s
epoch 18 | loss: 0.42804 | val_0_rmse: 0.64939 | val_1_rmse: 0.6633  |  0:00:08s
epoch 19 | loss: 0.43346 | val_0_rmse: 0.64908 | val_1_rmse: 0.65716 |  0:00:09s
epoch 20 | loss: 0.44661 | val_0_rmse: 0.71407 | val_1_rmse: 0.74265 |  0:00:09s
epoch 21 | loss: 0.43511 | val_0_rmse: 0.65286 | val_1_rmse: 0.69345 |  0:00:10s
epoch 22 | loss: 0.40399 | val_0_rmse: 0.64996 | val_1_rmse: 0.67986 |  0:00:10s
epoch 23 | loss: 0.39748 | val_0_rmse: 0.6467  | val_1_rmse: 0.66739 |  0:00:10s
epoch 24 | loss: 0.39381 | val_0_rmse: 0.65552 | val_1_rmse: 0.6746  |  0:00:11s
epoch 25 | loss: 0.41147 | val_0_rmse: 0.68231 | val_1_rmse: 0.71776 |  0:00:11s
epoch 26 | loss: 0.40129 | val_0_rmse: 0.65667 | val_1_rmse: 0.68783 |  0:00:12s
epoch 27 | loss: 0.4005  | val_0_rmse: 0.6651  | val_1_rmse: 0.69111 |  0:00:12s
epoch 28 | loss: 0.39711 | val_0_rmse: 0.63579 | val_1_rmse: 0.65618 |  0:00:13s
epoch 29 | loss: 0.39302 | val_0_rmse: 0.64778 | val_1_rmse: 0.67167 |  0:00:13s
epoch 30 | loss: 0.38418 | val_0_rmse: 0.64407 | val_1_rmse: 0.67947 |  0:00:14s
epoch 31 | loss: 0.39149 | val_0_rmse: 0.63669 | val_1_rmse: 0.66166 |  0:00:14s
epoch 32 | loss: 0.40672 | val_0_rmse: 0.63314 | val_1_rmse: 0.66062 |  0:00:14s
epoch 33 | loss: 0.38294 | val_0_rmse: 0.646   | val_1_rmse: 0.63751 |  0:00:15s
epoch 34 | loss: 0.38583 | val_0_rmse: 0.61266 | val_1_rmse: 0.61397 |  0:00:15s
epoch 35 | loss: 0.37166 | val_0_rmse: 0.61975 | val_1_rmse: 0.61532 |  0:00:16s
epoch 36 | loss: 0.38499 | val_0_rmse: 0.6061  | val_1_rmse: 0.61138 |  0:00:16s
epoch 37 | loss: 0.38263 | val_0_rmse: 0.63103 | val_1_rmse: 0.62923 |  0:00:17s
epoch 38 | loss: 0.37462 | val_0_rmse: 0.60805 | val_1_rmse: 0.60782 |  0:00:17s
epoch 39 | loss: 0.38826 | val_0_rmse: 0.61986 | val_1_rmse: 0.61229 |  0:00:18s
epoch 40 | loss: 0.36624 | val_0_rmse: 0.62814 | val_1_rmse: 0.63021 |  0:00:18s
epoch 41 | loss: 0.37308 | val_0_rmse: 0.62199 | val_1_rmse: 0.63826 |  0:00:18s
epoch 42 | loss: 0.36985 | val_0_rmse: 0.60928 | val_1_rmse: 0.63048 |  0:00:19s
epoch 43 | loss: 0.38844 | val_0_rmse: 0.62671 | val_1_rmse: 0.61164 |  0:00:19s
epoch 44 | loss: 0.36628 | val_0_rmse: 0.61979 | val_1_rmse: 0.62312 |  0:00:20s
epoch 45 | loss: 0.35458 | val_0_rmse: 0.62053 | val_1_rmse: 0.6217  |  0:00:20s
epoch 46 | loss: 0.35188 | val_0_rmse: 0.63431 | val_1_rmse: 0.63007 |  0:00:21s
epoch 47 | loss: 0.35198 | val_0_rmse: 0.61349 | val_1_rmse: 0.60653 |  0:00:21s
epoch 48 | loss: 0.37075 | val_0_rmse: 0.61738 | val_1_rmse: 0.64077 |  0:00:22s
epoch 49 | loss: 0.34691 | val_0_rmse: 0.61879 | val_1_rmse: 0.62364 |  0:00:22s
epoch 50 | loss: 0.34625 | val_0_rmse: 0.60719 | val_1_rmse: 0.62156 |  0:00:22s
epoch 51 | loss: 0.33665 | val_0_rmse: 0.61428 | val_1_rmse: 0.63517 |  0:00:23s
epoch 52 | loss: 0.34402 | val_0_rmse: 0.59887 | val_1_rmse: 0.61796 |  0:00:23s
epoch 53 | loss: 0.36618 | val_0_rmse: 0.66523 | val_1_rmse: 0.68302 |  0:00:24s
epoch 54 | loss: 0.35596 | val_0_rmse: 0.61434 | val_1_rmse: 0.63156 |  0:00:24s
epoch 55 | loss: 0.37443 | val_0_rmse: 0.61771 | val_1_rmse: 0.63102 |  0:00:25s
epoch 56 | loss: 0.35081 | val_0_rmse: 0.62024 | val_1_rmse: 0.64295 |  0:00:25s
epoch 57 | loss: 0.35304 | val_0_rmse: 0.63441 | val_1_rmse: 0.69083 |  0:00:26s
epoch 58 | loss: 0.35134 | val_0_rmse: 0.6168  | val_1_rmse: 0.64318 |  0:00:26s
epoch 59 | loss: 0.34093 | val_0_rmse: 0.63883 | val_1_rmse: 0.67108 |  0:00:27s
epoch 60 | loss: 0.34991 | val_0_rmse: 0.60379 | val_1_rmse: 0.62007 |  0:00:27s
epoch 61 | loss: 0.33031 | val_0_rmse: 0.60689 | val_1_rmse: 0.63413 |  0:00:27s
epoch 62 | loss: 0.33709 | val_0_rmse: 0.60052 | val_1_rmse: 0.61713 |  0:00:28s
epoch 63 | loss: 0.34194 | val_0_rmse: 0.6021  | val_1_rmse: 0.61989 |  0:00:28s
epoch 64 | loss: 0.33836 | val_0_rmse: 0.59936 | val_1_rmse: 0.61773 |  0:00:29s
epoch 65 | loss: 0.33595 | val_0_rmse: 0.59872 | val_1_rmse: 0.60828 |  0:00:29s
epoch 66 | loss: 0.3415  | val_0_rmse: 0.59699 | val_1_rmse: 0.60665 |  0:00:30s
epoch 67 | loss: 0.32384 | val_0_rmse: 0.60305 | val_1_rmse: 0.60533 |  0:00:30s
epoch 68 | loss: 0.3274  | val_0_rmse: 0.61695 | val_1_rmse: 0.61905 |  0:00:31s
epoch 69 | loss: 0.32919 | val_0_rmse: 0.59658 | val_1_rmse: 0.5974  |  0:00:31s
epoch 70 | loss: 0.32522 | val_0_rmse: 0.59823 | val_1_rmse: 0.59776 |  0:00:31s
epoch 71 | loss: 0.32899 | val_0_rmse: 0.61079 | val_1_rmse: 0.63633 |  0:00:32s
epoch 72 | loss: 0.32689 | val_0_rmse: 0.62521 | val_1_rmse: 0.65732 |  0:00:32s
epoch 73 | loss: 0.33436 | val_0_rmse: 0.62875 | val_1_rmse: 0.66023 |  0:00:33s
epoch 74 | loss: 0.33268 | val_0_rmse: 0.60775 | val_1_rmse: 0.63432 |  0:00:33s
epoch 75 | loss: 0.33551 | val_0_rmse: 0.59634 | val_1_rmse: 0.61463 |  0:00:34s
epoch 76 | loss: 0.32976 | val_0_rmse: 0.58557 | val_1_rmse: 0.59916 |  0:00:34s
epoch 77 | loss: 0.32959 | val_0_rmse: 0.58489 | val_1_rmse: 0.58575 |  0:00:35s
epoch 78 | loss: 0.3288  | val_0_rmse: 0.59316 | val_1_rmse: 0.6041  |  0:00:35s
epoch 79 | loss: 0.33725 | val_0_rmse: 0.59903 | val_1_rmse: 0.60903 |  0:00:36s
epoch 80 | loss: 0.32859 | val_0_rmse: 0.60572 | val_1_rmse: 0.61692 |  0:00:36s
epoch 81 | loss: 0.33272 | val_0_rmse: 0.64469 | val_1_rmse: 0.64544 |  0:00:36s
epoch 82 | loss: 0.32989 | val_0_rmse: 0.59425 | val_1_rmse: 0.6068  |  0:00:37s
epoch 83 | loss: 0.32935 | val_0_rmse: 0.59622 | val_1_rmse: 0.61192 |  0:00:37s
epoch 84 | loss: 0.32333 | val_0_rmse: 0.59387 | val_1_rmse: 0.62031 |  0:00:38s
epoch 85 | loss: 0.3244  | val_0_rmse: 0.61172 | val_1_rmse: 0.65216 |  0:00:38s
epoch 86 | loss: 0.32688 | val_0_rmse: 0.61874 | val_1_rmse: 0.63669 |  0:00:39s
epoch 87 | loss: 0.31811 | val_0_rmse: 0.59502 | val_1_rmse: 0.614   |  0:00:39s
epoch 88 | loss: 0.31664 | val_0_rmse: 0.59291 | val_1_rmse: 0.61437 |  0:00:40s
epoch 89 | loss: 0.31627 | val_0_rmse: 0.58393 | val_1_rmse: 0.60204 |  0:00:40s
epoch 90 | loss: 0.32121 | val_0_rmse: 0.58134 | val_1_rmse: 0.5976  |  0:00:40s
epoch 91 | loss: 0.32255 | val_0_rmse: 0.57416 | val_1_rmse: 0.59182 |  0:00:41s
epoch 92 | loss: 0.32913 | val_0_rmse: 0.58948 | val_1_rmse: 0.59792 |  0:00:41s
epoch 93 | loss: 0.31994 | val_0_rmse: 0.61576 | val_1_rmse: 0.63007 |  0:00:42s
epoch 94 | loss: 0.31882 | val_0_rmse: 0.59733 | val_1_rmse: 0.62556 |  0:00:42s
epoch 95 | loss: 0.32119 | val_0_rmse: 0.60886 | val_1_rmse: 0.62698 |  0:00:43s
epoch 96 | loss: 0.32223 | val_0_rmse: 0.63101 | val_1_rmse: 0.64614 |  0:00:43s
epoch 97 | loss: 0.32449 | val_0_rmse: 0.59354 | val_1_rmse: 0.61002 |  0:00:44s
epoch 98 | loss: 0.33375 | val_0_rmse: 0.60705 | val_1_rmse: 0.60585 |  0:00:44s
epoch 99 | loss: 0.31962 | val_0_rmse: 0.62061 | val_1_rmse: 0.63933 |  0:00:44s
epoch 100| loss: 0.32466 | val_0_rmse: 0.63608 | val_1_rmse: 0.64288 |  0:00:45s
epoch 101| loss: 0.32201 | val_0_rmse: 0.61394 | val_1_rmse: 0.62527 |  0:00:45s
epoch 102| loss: 0.32011 | val_0_rmse: 0.61847 | val_1_rmse: 0.62331 |  0:00:46s
epoch 103| loss: 0.31262 | val_0_rmse: 0.62604 | val_1_rmse: 0.6286  |  0:00:46s
epoch 104| loss: 0.30849 | val_0_rmse: 0.61725 | val_1_rmse: 0.6175  |  0:00:47s
epoch 105| loss: 0.30832 | val_0_rmse: 0.60953 | val_1_rmse: 0.61546 |  0:00:47s
epoch 106| loss: 0.3109  | val_0_rmse: 0.60011 | val_1_rmse: 0.61083 |  0:00:48s
epoch 107| loss: 0.31306 | val_0_rmse: 0.61137 | val_1_rmse: 0.62615 |  0:00:48s

Early stopping occured at epoch 107 with best_epoch = 77 and best_val_1_rmse = 0.58575
Best weights from best epoch are automatically used!
ended training at: 08:25:27
Feature importance:
Mean squared error is of 3301569309.1386957
Mean absolute error:39021.705583828516
MAPE:0.37025696818990783
R2 score:0.6339129226880418
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:25:27
epoch 0  | loss: 1.1093  | val_0_rmse: 0.89299 | val_1_rmse: 0.87271 |  0:00:00s
epoch 1  | loss: 0.70625 | val_0_rmse: 0.92774 | val_1_rmse: 0.98562 |  0:00:00s
epoch 2  | loss: 0.56522 | val_0_rmse: 0.78752 | val_1_rmse: 0.81486 |  0:00:01s
epoch 3  | loss: 0.51514 | val_0_rmse: 0.7595  | val_1_rmse: 0.79258 |  0:00:01s
epoch 4  | loss: 0.50791 | val_0_rmse: 0.79886 | val_1_rmse: 0.85456 |  0:00:02s
epoch 5  | loss: 0.49445 | val_0_rmse: 0.70492 | val_1_rmse: 0.74853 |  0:00:02s
epoch 6  | loss: 0.47699 | val_0_rmse: 0.76102 | val_1_rmse: 0.77539 |  0:00:03s
epoch 7  | loss: 0.47671 | val_0_rmse: 0.67554 | val_1_rmse: 0.70929 |  0:00:03s
epoch 8  | loss: 0.46648 | val_0_rmse: 0.69339 | val_1_rmse: 0.72684 |  0:00:04s
epoch 9  | loss: 0.46778 | val_0_rmse: 0.66806 | val_1_rmse: 0.69924 |  0:00:04s
epoch 10 | loss: 0.46204 | val_0_rmse: 0.66517 | val_1_rmse: 0.68719 |  0:00:04s
epoch 11 | loss: 0.45632 | val_0_rmse: 0.67197 | val_1_rmse: 0.71043 |  0:00:05s
epoch 12 | loss: 0.44268 | val_0_rmse: 0.68056 | val_1_rmse: 0.71269 |  0:00:05s
epoch 13 | loss: 0.43708 | val_0_rmse: 0.66971 | val_1_rmse: 0.70188 |  0:00:06s
epoch 14 | loss: 0.43022 | val_0_rmse: 0.651   | val_1_rmse: 0.68558 |  0:00:06s
epoch 15 | loss: 0.43355 | val_0_rmse: 0.65038 | val_1_rmse: 0.67853 |  0:00:07s
epoch 16 | loss: 0.43976 | val_0_rmse: 0.64952 | val_1_rmse: 0.68856 |  0:00:07s
epoch 17 | loss: 0.41755 | val_0_rmse: 0.6403  | val_1_rmse: 0.67886 |  0:00:08s
epoch 18 | loss: 0.41154 | val_0_rmse: 0.64931 | val_1_rmse: 0.69243 |  0:00:08s
epoch 19 | loss: 0.40766 | val_0_rmse: 0.66687 | val_1_rmse: 0.69884 |  0:00:09s
epoch 20 | loss: 0.40961 | val_0_rmse: 0.67056 | val_1_rmse: 0.69456 |  0:00:09s
epoch 21 | loss: 0.40663 | val_0_rmse: 0.65728 | val_1_rmse: 0.67253 |  0:00:09s
epoch 22 | loss: 0.41475 | val_0_rmse: 0.65157 | val_1_rmse: 0.66705 |  0:00:10s
epoch 23 | loss: 0.41781 | val_0_rmse: 0.63953 | val_1_rmse: 0.65915 |  0:00:10s
epoch 24 | loss: 0.42369 | val_0_rmse: 0.67638 | val_1_rmse: 0.70536 |  0:00:11s
epoch 25 | loss: 0.4278  | val_0_rmse: 0.65037 | val_1_rmse: 0.6696  |  0:00:11s
epoch 26 | loss: 0.41603 | val_0_rmse: 0.64408 | val_1_rmse: 0.66096 |  0:00:12s
epoch 27 | loss: 0.40644 | val_0_rmse: 0.6409  | val_1_rmse: 0.65814 |  0:00:12s
epoch 28 | loss: 0.3992  | val_0_rmse: 0.65141 | val_1_rmse: 0.68845 |  0:00:13s
epoch 29 | loss: 0.40147 | val_0_rmse: 0.64387 | val_1_rmse: 0.66629 |  0:00:13s
epoch 30 | loss: 0.40004 | val_0_rmse: 0.62906 | val_1_rmse: 0.64633 |  0:00:14s
epoch 31 | loss: 0.37906 | val_0_rmse: 0.63787 | val_1_rmse: 0.64235 |  0:00:14s
epoch 32 | loss: 0.37682 | val_0_rmse: 0.68824 | val_1_rmse: 0.66692 |  0:00:14s
epoch 33 | loss: 0.38705 | val_0_rmse: 0.67617 | val_1_rmse: 0.67724 |  0:00:15s
epoch 34 | loss: 0.38361 | val_0_rmse: 0.61588 | val_1_rmse: 0.62638 |  0:00:15s
epoch 35 | loss: 0.37355 | val_0_rmse: 0.60918 | val_1_rmse: 0.6189  |  0:00:16s
epoch 36 | loss: 0.37806 | val_0_rmse: 0.6088  | val_1_rmse: 0.62088 |  0:00:16s
epoch 37 | loss: 0.36861 | val_0_rmse: 0.6099  | val_1_rmse: 0.61723 |  0:00:17s
epoch 38 | loss: 0.36787 | val_0_rmse: 0.60215 | val_1_rmse: 0.62003 |  0:00:17s
epoch 39 | loss: 0.3693  | val_0_rmse: 0.61806 | val_1_rmse: 0.65404 |  0:00:18s
epoch 40 | loss: 0.36641 | val_0_rmse: 0.62076 | val_1_rmse: 0.65989 |  0:00:18s
epoch 41 | loss: 0.35112 | val_0_rmse: 0.61243 | val_1_rmse: 0.64755 |  0:00:19s
epoch 42 | loss: 0.34296 | val_0_rmse: 0.5961  | val_1_rmse: 0.6209  |  0:00:19s
epoch 43 | loss: 0.34523 | val_0_rmse: 0.60975 | val_1_rmse: 0.6465  |  0:00:19s
epoch 44 | loss: 0.34689 | val_0_rmse: 0.59347 | val_1_rmse: 0.6271  |  0:00:20s
epoch 45 | loss: 0.33795 | val_0_rmse: 0.59534 | val_1_rmse: 0.63153 |  0:00:20s
epoch 46 | loss: 0.36059 | val_0_rmse: 0.59801 | val_1_rmse: 0.63152 |  0:00:21s
epoch 47 | loss: 0.36003 | val_0_rmse: 0.6215  | val_1_rmse: 0.63842 |  0:00:21s
epoch 48 | loss: 0.35219 | val_0_rmse: 0.59537 | val_1_rmse: 0.61901 |  0:00:22s
epoch 49 | loss: 0.35474 | val_0_rmse: 0.58738 | val_1_rmse: 0.61055 |  0:00:22s
epoch 50 | loss: 0.33766 | val_0_rmse: 0.57649 | val_1_rmse: 0.60788 |  0:00:23s
epoch 51 | loss: 0.33254 | val_0_rmse: 0.60075 | val_1_rmse: 0.61784 |  0:00:23s
epoch 52 | loss: 0.36157 | val_0_rmse: 0.62385 | val_1_rmse: 0.66058 |  0:00:23s
epoch 53 | loss: 0.36236 | val_0_rmse: 0.59708 | val_1_rmse: 0.62245 |  0:00:24s
epoch 54 | loss: 0.34826 | val_0_rmse: 0.62002 | val_1_rmse: 0.62663 |  0:00:24s
epoch 55 | loss: 0.33852 | val_0_rmse: 0.61983 | val_1_rmse: 0.64961 |  0:00:25s
epoch 56 | loss: 0.34581 | val_0_rmse: 0.60102 | val_1_rmse: 0.63034 |  0:00:25s
epoch 57 | loss: 0.33749 | val_0_rmse: 0.59617 | val_1_rmse: 0.62028 |  0:00:26s
epoch 58 | loss: 0.33694 | val_0_rmse: 0.58928 | val_1_rmse: 0.61606 |  0:00:26s
epoch 59 | loss: 0.33286 | val_0_rmse: 0.59136 | val_1_rmse: 0.61758 |  0:00:27s
epoch 60 | loss: 0.33527 | val_0_rmse: 0.63114 | val_1_rmse: 0.64934 |  0:00:27s
epoch 61 | loss: 0.34605 | val_0_rmse: 0.61074 | val_1_rmse: 0.62574 |  0:00:28s
epoch 62 | loss: 0.33271 | val_0_rmse: 0.63392 | val_1_rmse: 0.63881 |  0:00:28s
epoch 63 | loss: 0.3573  | val_0_rmse: 0.66731 | val_1_rmse: 0.69924 |  0:00:28s
epoch 64 | loss: 0.35043 | val_0_rmse: 0.6615  | val_1_rmse: 0.69667 |  0:00:29s
epoch 65 | loss: 0.3711  | val_0_rmse: 0.78437 | val_1_rmse: 0.83344 |  0:00:29s
epoch 66 | loss: 0.36262 | val_0_rmse: 0.65044 | val_1_rmse: 0.70134 |  0:00:30s
epoch 67 | loss: 0.35089 | val_0_rmse: 0.65248 | val_1_rmse: 0.6948  |  0:00:30s
epoch 68 | loss: 0.34743 | val_0_rmse: 0.67665 | val_1_rmse: 0.71107 |  0:00:31s
epoch 69 | loss: 0.34512 | val_0_rmse: 0.60484 | val_1_rmse: 0.64408 |  0:00:31s
epoch 70 | loss: 0.33155 | val_0_rmse: 0.58683 | val_1_rmse: 0.62189 |  0:00:32s
epoch 71 | loss: 0.34729 | val_0_rmse: 0.59321 | val_1_rmse: 0.61456 |  0:00:32s
epoch 72 | loss: 0.33488 | val_0_rmse: 0.59161 | val_1_rmse: 0.6287  |  0:00:32s
epoch 73 | loss: 0.34208 | val_0_rmse: 0.60209 | val_1_rmse: 0.64267 |  0:00:33s
epoch 74 | loss: 0.33628 | val_0_rmse: 0.6186  | val_1_rmse: 0.64968 |  0:00:33s
epoch 75 | loss: 0.33832 | val_0_rmse: 0.59511 | val_1_rmse: 0.62555 |  0:00:34s
epoch 76 | loss: 0.33909 | val_0_rmse: 0.60416 | val_1_rmse: 0.63436 |  0:00:34s
epoch 77 | loss: 0.32979 | val_0_rmse: 0.60031 | val_1_rmse: 0.63099 |  0:00:35s
epoch 78 | loss: 0.32474 | val_0_rmse: 0.63785 | val_1_rmse: 0.65412 |  0:00:35s
epoch 79 | loss: 0.33108 | val_0_rmse: 0.64327 | val_1_rmse: 0.66842 |  0:00:36s
epoch 80 | loss: 0.33021 | val_0_rmse: 0.60425 | val_1_rmse: 0.62954 |  0:00:36s

Early stopping occured at epoch 80 with best_epoch = 50 and best_val_1_rmse = 0.60788
Best weights from best epoch are automatically used!
ended training at: 08:26:04
Feature importance:
Mean squared error is of 3225245855.0204206
Mean absolute error:38632.58584524883
MAPE:0.3380156859385367
R2 score:0.6346873276576709
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:26:04
epoch 0  | loss: 1.05652 | val_0_rmse: 0.96231 | val_1_rmse: 0.98222 |  0:00:00s
epoch 1  | loss: 0.84386 | val_0_rmse: 0.92988 | val_1_rmse: 0.9549  |  0:00:00s
epoch 2  | loss: 0.67217 | val_0_rmse: 0.89214 | val_1_rmse: 0.90374 |  0:00:01s
epoch 3  | loss: 0.5782  | val_0_rmse: 0.79512 | val_1_rmse: 0.79309 |  0:00:01s
epoch 4  | loss: 0.56631 | val_0_rmse: 0.76048 | val_1_rmse: 0.75859 |  0:00:02s
epoch 5  | loss: 0.5237  | val_0_rmse: 0.73765 | val_1_rmse: 0.7478  |  0:00:02s
epoch 6  | loss: 0.48978 | val_0_rmse: 0.76471 | val_1_rmse: 0.79706 |  0:00:03s
epoch 7  | loss: 0.50013 | val_0_rmse: 0.75297 | val_1_rmse: 0.76227 |  0:00:03s
epoch 8  | loss: 0.46927 | val_0_rmse: 0.75305 | val_1_rmse: 0.76852 |  0:00:04s
epoch 9  | loss: 0.47453 | val_0_rmse: 0.73201 | val_1_rmse: 0.72985 |  0:00:04s
epoch 10 | loss: 0.46854 | val_0_rmse: 0.71971 | val_1_rmse: 0.71825 |  0:00:05s
epoch 11 | loss: 0.46877 | val_0_rmse: 0.72083 | val_1_rmse: 0.71281 |  0:00:05s
epoch 12 | loss: 0.45721 | val_0_rmse: 0.71243 | val_1_rmse: 0.707   |  0:00:05s
epoch 13 | loss: 0.45384 | val_0_rmse: 0.71026 | val_1_rmse: 0.70705 |  0:00:06s
epoch 14 | loss: 0.44402 | val_0_rmse: 0.70399 | val_1_rmse: 0.71092 |  0:00:06s
epoch 15 | loss: 0.45061 | val_0_rmse: 0.72184 | val_1_rmse: 0.71945 |  0:00:07s
epoch 16 | loss: 0.45511 | val_0_rmse: 0.71342 | val_1_rmse: 0.71214 |  0:00:07s
epoch 17 | loss: 0.44202 | val_0_rmse: 0.72867 | val_1_rmse: 0.73004 |  0:00:08s
epoch 18 | loss: 0.4432  | val_0_rmse: 0.69588 | val_1_rmse: 0.71995 |  0:00:08s
epoch 19 | loss: 0.43262 | val_0_rmse: 0.70191 | val_1_rmse: 0.73272 |  0:00:09s
epoch 20 | loss: 0.42524 | val_0_rmse: 0.68457 | val_1_rmse: 0.69072 |  0:00:09s
epoch 21 | loss: 0.43046 | val_0_rmse: 0.69704 | val_1_rmse: 0.7017  |  0:00:09s
epoch 22 | loss: 0.41848 | val_0_rmse: 0.70496 | val_1_rmse: 0.70128 |  0:00:10s
epoch 23 | loss: 0.41503 | val_0_rmse: 0.70577 | val_1_rmse: 0.71349 |  0:00:10s
epoch 24 | loss: 0.41748 | val_0_rmse: 0.74441 | val_1_rmse: 0.74638 |  0:00:11s
epoch 25 | loss: 0.40354 | val_0_rmse: 0.77734 | val_1_rmse: 0.78343 |  0:00:11s
epoch 26 | loss: 0.39583 | val_0_rmse: 0.73375 | val_1_rmse: 0.73868 |  0:00:12s
epoch 27 | loss: 0.39518 | val_0_rmse: 0.75933 | val_1_rmse: 0.7716  |  0:00:12s
epoch 28 | loss: 0.37297 | val_0_rmse: 0.70926 | val_1_rmse: 0.72013 |  0:00:13s
epoch 29 | loss: 0.38236 | val_0_rmse: 0.63852 | val_1_rmse: 0.66564 |  0:00:13s
epoch 30 | loss: 0.36848 | val_0_rmse: 0.6375  | val_1_rmse: 0.65065 |  0:00:13s
epoch 31 | loss: 0.35647 | val_0_rmse: 0.72952 | val_1_rmse: 0.75574 |  0:00:14s
epoch 32 | loss: 0.36882 | val_0_rmse: 0.6446  | val_1_rmse: 0.69316 |  0:00:14s
epoch 33 | loss: 0.35466 | val_0_rmse: 0.68463 | val_1_rmse: 0.75459 |  0:00:15s
epoch 34 | loss: 0.34295 | val_0_rmse: 0.68605 | val_1_rmse: 0.76821 |  0:00:15s
epoch 35 | loss: 0.35226 | val_0_rmse: 0.70239 | val_1_rmse: 0.81359 |  0:00:16s
epoch 36 | loss: 0.3491  | val_0_rmse: 0.65751 | val_1_rmse: 0.74493 |  0:00:16s
epoch 37 | loss: 0.33661 | val_0_rmse: 0.60807 | val_1_rmse: 0.66013 |  0:00:17s
epoch 38 | loss: 0.34198 | val_0_rmse: 0.62035 | val_1_rmse: 0.67366 |  0:00:17s
epoch 39 | loss: 0.33785 | val_0_rmse: 0.61305 | val_1_rmse: 0.66441 |  0:00:18s
epoch 40 | loss: 0.34572 | val_0_rmse: 0.68186 | val_1_rmse: 0.74252 |  0:00:18s
epoch 41 | loss: 0.34048 | val_0_rmse: 0.65459 | val_1_rmse: 0.70585 |  0:00:18s
epoch 42 | loss: 0.34138 | val_0_rmse: 0.63981 | val_1_rmse: 0.68216 |  0:00:19s
epoch 43 | loss: 0.33468 | val_0_rmse: 0.62572 | val_1_rmse: 0.67406 |  0:00:19s
epoch 44 | loss: 0.33097 | val_0_rmse: 0.61024 | val_1_rmse: 0.65216 |  0:00:20s
epoch 45 | loss: 0.32355 | val_0_rmse: 0.60311 | val_1_rmse: 0.63987 |  0:00:20s
epoch 46 | loss: 0.33271 | val_0_rmse: 0.59106 | val_1_rmse: 0.63124 |  0:00:21s
epoch 47 | loss: 0.33189 | val_0_rmse: 0.6208  | val_1_rmse: 0.66047 |  0:00:21s
epoch 48 | loss: 0.34305 | val_0_rmse: 0.59488 | val_1_rmse: 0.63769 |  0:00:22s
epoch 49 | loss: 0.34149 | val_0_rmse: 0.65452 | val_1_rmse: 0.68448 |  0:00:22s
epoch 50 | loss: 0.35289 | val_0_rmse: 0.67808 | val_1_rmse: 0.7149  |  0:00:22s
epoch 51 | loss: 0.34704 | val_0_rmse: 0.63946 | val_1_rmse: 0.66832 |  0:00:23s
epoch 52 | loss: 0.33601 | val_0_rmse: 0.63996 | val_1_rmse: 0.66215 |  0:00:23s
epoch 53 | loss: 0.34247 | val_0_rmse: 0.63076 | val_1_rmse: 0.65592 |  0:00:24s
epoch 54 | loss: 0.32978 | val_0_rmse: 0.63301 | val_1_rmse: 0.65508 |  0:00:24s
epoch 55 | loss: 0.33354 | val_0_rmse: 0.58114 | val_1_rmse: 0.58967 |  0:00:25s
epoch 56 | loss: 0.32277 | val_0_rmse: 0.58561 | val_1_rmse: 0.59468 |  0:00:25s
epoch 57 | loss: 0.32731 | val_0_rmse: 0.59562 | val_1_rmse: 0.61533 |  0:00:26s
epoch 58 | loss: 0.31702 | val_0_rmse: 0.62293 | val_1_rmse: 0.64594 |  0:00:26s
epoch 59 | loss: 0.31779 | val_0_rmse: 0.59298 | val_1_rmse: 0.61164 |  0:00:26s
epoch 60 | loss: 0.31955 | val_0_rmse: 0.58623 | val_1_rmse: 0.60618 |  0:00:27s
epoch 61 | loss: 0.31785 | val_0_rmse: 0.66032 | val_1_rmse: 0.69184 |  0:00:27s
epoch 62 | loss: 0.32254 | val_0_rmse: 0.62089 | val_1_rmse: 0.64216 |  0:00:28s
epoch 63 | loss: 0.33919 | val_0_rmse: 0.69195 | val_1_rmse: 0.7061  |  0:00:28s
epoch 64 | loss: 0.34435 | val_0_rmse: 0.64302 | val_1_rmse: 0.65759 |  0:00:29s
epoch 65 | loss: 0.33512 | val_0_rmse: 0.60765 | val_1_rmse: 0.62241 |  0:00:29s
epoch 66 | loss: 0.3321  | val_0_rmse: 0.59311 | val_1_rmse: 0.61253 |  0:00:30s
epoch 67 | loss: 0.34564 | val_0_rmse: 0.59472 | val_1_rmse: 0.61017 |  0:00:30s
epoch 68 | loss: 0.32969 | val_0_rmse: 0.59416 | val_1_rmse: 0.60724 |  0:00:30s
epoch 69 | loss: 0.33359 | val_0_rmse: 0.60396 | val_1_rmse: 0.62038 |  0:00:31s
epoch 70 | loss: 0.32914 | val_0_rmse: 0.59085 | val_1_rmse: 0.6077  |  0:00:31s
epoch 71 | loss: 0.33697 | val_0_rmse: 0.64109 | val_1_rmse: 0.65742 |  0:00:32s
epoch 72 | loss: 0.351   | val_0_rmse: 0.64517 | val_1_rmse: 0.66722 |  0:00:32s
epoch 73 | loss: 0.33754 | val_0_rmse: 0.65199 | val_1_rmse: 0.66768 |  0:00:33s
epoch 74 | loss: 0.33789 | val_0_rmse: 0.6515  | val_1_rmse: 0.67011 |  0:00:33s
epoch 75 | loss: 0.33324 | val_0_rmse: 0.59835 | val_1_rmse: 0.60668 |  0:00:34s
epoch 76 | loss: 0.34369 | val_0_rmse: 0.6239  | val_1_rmse: 0.63593 |  0:00:34s
epoch 77 | loss: 0.32986 | val_0_rmse: 0.60904 | val_1_rmse: 0.61645 |  0:00:34s
epoch 78 | loss: 0.32596 | val_0_rmse: 0.57536 | val_1_rmse: 0.58186 |  0:00:35s
epoch 79 | loss: 0.34149 | val_0_rmse: 0.58812 | val_1_rmse: 0.58606 |  0:00:35s
epoch 80 | loss: 0.33395 | val_0_rmse: 0.59127 | val_1_rmse: 0.59044 |  0:00:36s
epoch 81 | loss: 0.33297 | val_0_rmse: 0.59352 | val_1_rmse: 0.60208 |  0:00:36s
epoch 82 | loss: 0.35558 | val_0_rmse: 0.59289 | val_1_rmse: 0.59703 |  0:00:37s
epoch 83 | loss: 0.33923 | val_0_rmse: 0.62655 | val_1_rmse: 0.63885 |  0:00:37s
epoch 84 | loss: 0.32394 | val_0_rmse: 0.59888 | val_1_rmse: 0.61217 |  0:00:38s
epoch 85 | loss: 0.3325  | val_0_rmse: 0.65658 | val_1_rmse: 0.66175 |  0:00:38s
epoch 86 | loss: 0.33013 | val_0_rmse: 0.62583 | val_1_rmse: 0.63484 |  0:00:39s
epoch 87 | loss: 0.32895 | val_0_rmse: 0.62871 | val_1_rmse: 0.63107 |  0:00:39s
epoch 88 | loss: 0.33264 | val_0_rmse: 0.59777 | val_1_rmse: 0.60199 |  0:00:39s
epoch 89 | loss: 0.3461  | val_0_rmse: 0.61553 | val_1_rmse: 0.61747 |  0:00:40s
epoch 90 | loss: 0.34404 | val_0_rmse: 0.64999 | val_1_rmse: 0.6612  |  0:00:40s
epoch 91 | loss: 0.33267 | val_0_rmse: 0.5988  | val_1_rmse: 0.60364 |  0:00:41s
epoch 92 | loss: 0.33756 | val_0_rmse: 0.6271  | val_1_rmse: 0.63267 |  0:00:41s
epoch 93 | loss: 0.33956 | val_0_rmse: 0.64758 | val_1_rmse: 0.65638 |  0:00:42s
epoch 94 | loss: 0.32432 | val_0_rmse: 0.61622 | val_1_rmse: 0.62965 |  0:00:42s
epoch 95 | loss: 0.32105 | val_0_rmse: 0.64171 | val_1_rmse: 0.65759 |  0:00:43s
epoch 96 | loss: 0.32708 | val_0_rmse: 0.63631 | val_1_rmse: 0.6578  |  0:00:43s
epoch 97 | loss: 0.33119 | val_0_rmse: 0.62767 | val_1_rmse: 0.65007 |  0:00:44s
epoch 98 | loss: 0.32907 | val_0_rmse: 0.56937 | val_1_rmse: 0.60036 |  0:00:44s
epoch 99 | loss: 0.32809 | val_0_rmse: 0.55491 | val_1_rmse: 0.58522 |  0:00:44s
epoch 100| loss: 0.32223 | val_0_rmse: 0.60841 | val_1_rmse: 0.6479  |  0:00:45s
epoch 101| loss: 0.31555 | val_0_rmse: 0.58222 | val_1_rmse: 0.59308 |  0:00:45s
epoch 102| loss: 0.30934 | val_0_rmse: 0.55477 | val_1_rmse: 0.56211 |  0:00:46s
epoch 103| loss: 0.3199  | val_0_rmse: 0.59699 | val_1_rmse: 0.60435 |  0:00:46s
epoch 104| loss: 0.32028 | val_0_rmse: 0.64083 | val_1_rmse: 0.65773 |  0:00:47s
epoch 105| loss: 0.31482 | val_0_rmse: 0.69774 | val_1_rmse: 0.72064 |  0:00:47s
epoch 106| loss: 0.31577 | val_0_rmse: 0.61901 | val_1_rmse: 0.6207  |  0:00:48s
epoch 107| loss: 0.31971 | val_0_rmse: 0.6851  | val_1_rmse: 0.70288 |  0:00:48s
epoch 108| loss: 0.32599 | val_0_rmse: 0.65223 | val_1_rmse: 0.66258 |  0:00:49s
epoch 109| loss: 0.32095 | val_0_rmse: 0.5936  | val_1_rmse: 0.597   |  0:00:49s
epoch 110| loss: 0.33327 | val_0_rmse: 0.58591 | val_1_rmse: 0.58968 |  0:00:49s
epoch 111| loss: 0.32495 | val_0_rmse: 0.6214  | val_1_rmse: 0.63132 |  0:00:50s
epoch 112| loss: 0.32917 | val_0_rmse: 0.56595 | val_1_rmse: 0.56482 |  0:00:50s
epoch 113| loss: 0.32046 | val_0_rmse: 0.56793 | val_1_rmse: 0.56346 |  0:00:51s
epoch 114| loss: 0.32615 | val_0_rmse: 0.57481 | val_1_rmse: 0.58048 |  0:00:51s
epoch 115| loss: 0.32095 | val_0_rmse: 0.58212 | val_1_rmse: 0.58458 |  0:00:52s
epoch 116| loss: 0.31416 | val_0_rmse: 0.58874 | val_1_rmse: 0.59171 |  0:00:52s
epoch 117| loss: 0.31311 | val_0_rmse: 0.60523 | val_1_rmse: 0.61435 |  0:00:53s
epoch 118| loss: 0.31249 | val_0_rmse: 0.58624 | val_1_rmse: 0.59609 |  0:00:53s
epoch 119| loss: 0.31288 | val_0_rmse: 0.57087 | val_1_rmse: 0.57946 |  0:00:53s
epoch 120| loss: 0.29985 | val_0_rmse: 0.57721 | val_1_rmse: 0.58821 |  0:00:54s
epoch 121| loss: 0.31387 | val_0_rmse: 0.64679 | val_1_rmse: 0.65466 |  0:00:54s
epoch 122| loss: 0.3031  | val_0_rmse: 0.59727 | val_1_rmse: 0.59848 |  0:00:55s
epoch 123| loss: 0.30846 | val_0_rmse: 0.59505 | val_1_rmse: 0.59777 |  0:00:55s
epoch 124| loss: 0.30799 | val_0_rmse: 0.64235 | val_1_rmse: 0.65815 |  0:00:56s
epoch 125| loss: 0.30139 | val_0_rmse: 0.60841 | val_1_rmse: 0.61366 |  0:00:56s
epoch 126| loss: 0.2959  | val_0_rmse: 0.58074 | val_1_rmse: 0.58203 |  0:00:57s
epoch 127| loss: 0.29734 | val_0_rmse: 0.64804 | val_1_rmse: 0.65996 |  0:00:57s
epoch 128| loss: 0.30202 | val_0_rmse: 0.5866  | val_1_rmse: 0.59164 |  0:00:57s
epoch 129| loss: 0.3014  | val_0_rmse: 0.6451  | val_1_rmse: 0.6564  |  0:00:58s
epoch 130| loss: 0.3027  | val_0_rmse: 0.58274 | val_1_rmse: 0.58932 |  0:00:58s
epoch 131| loss: 0.30548 | val_0_rmse: 0.58397 | val_1_rmse: 0.59106 |  0:00:59s
epoch 132| loss: 0.30429 | val_0_rmse: 0.61192 | val_1_rmse: 0.62527 |  0:00:59s

Early stopping occured at epoch 132 with best_epoch = 102 and best_val_1_rmse = 0.56211
Best weights from best epoch are automatically used!
ended training at: 08:27:04
Feature importance:
Mean squared error is of 3308126498.4946513
Mean absolute error:38136.41823127122
MAPE:0.38727513410303144
R2 score:0.6199137552390317
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:27:04
epoch 0  | loss: 0.97242 | val_0_rmse: 0.91485 | val_1_rmse: 0.87121 |  0:00:00s
epoch 1  | loss: 0.78008 | val_0_rmse: 0.92225 | val_1_rmse: 0.93265 |  0:00:00s
epoch 2  | loss: 0.67452 | val_0_rmse: 0.80552 | val_1_rmse: 0.8471  |  0:00:01s
epoch 3  | loss: 0.6314  | val_0_rmse: 0.82838 | val_1_rmse: 0.83071 |  0:00:01s
epoch 4  | loss: 0.61166 | val_0_rmse: 0.84666 | val_1_rmse: 0.86716 |  0:00:02s
epoch 5  | loss: 0.6023  | val_0_rmse: 0.81627 | val_1_rmse: 0.81404 |  0:00:02s
epoch 6  | loss: 0.56923 | val_0_rmse: 0.8529  | val_1_rmse: 0.88807 |  0:00:03s
epoch 7  | loss: 0.52647 | val_0_rmse: 0.80173 | val_1_rmse: 0.79854 |  0:00:03s
epoch 8  | loss: 0.53377 | val_0_rmse: 0.86217 | val_1_rmse: 0.88465 |  0:00:04s
epoch 9  | loss: 0.5253  | val_0_rmse: 0.73606 | val_1_rmse: 0.7765  |  0:00:04s
epoch 10 | loss: 0.49622 | val_0_rmse: 0.78233 | val_1_rmse: 0.8084  |  0:00:04s
epoch 11 | loss: 0.49156 | val_0_rmse: 0.71234 | val_1_rmse: 0.76212 |  0:00:05s
epoch 12 | loss: 0.47889 | val_0_rmse: 0.73176 | val_1_rmse: 0.78065 |  0:00:05s
epoch 13 | loss: 0.46993 | val_0_rmse: 0.73411 | val_1_rmse: 0.81139 |  0:00:06s
epoch 14 | loss: 0.45615 | val_0_rmse: 0.72068 | val_1_rmse: 0.77373 |  0:00:06s
epoch 15 | loss: 0.45859 | val_0_rmse: 0.68757 | val_1_rmse: 0.71533 |  0:00:07s
epoch 16 | loss: 0.43639 | val_0_rmse: 0.69569 | val_1_rmse: 0.71801 |  0:00:07s
epoch 17 | loss: 0.44032 | val_0_rmse: 0.68258 | val_1_rmse: 0.70353 |  0:00:08s
epoch 18 | loss: 0.40824 | val_0_rmse: 0.66004 | val_1_rmse: 0.68311 |  0:00:08s
epoch 19 | loss: 0.40404 | val_0_rmse: 0.66168 | val_1_rmse: 0.68616 |  0:00:09s
epoch 20 | loss: 0.38618 | val_0_rmse: 0.64147 | val_1_rmse: 0.66389 |  0:00:09s
epoch 21 | loss: 0.38225 | val_0_rmse: 0.67845 | val_1_rmse: 0.69091 |  0:00:09s
epoch 22 | loss: 0.39264 | val_0_rmse: 0.65225 | val_1_rmse: 0.66629 |  0:00:10s
epoch 23 | loss: 0.41233 | val_0_rmse: 0.72887 | val_1_rmse: 0.75176 |  0:00:10s
epoch 24 | loss: 0.4162  | val_0_rmse: 0.64749 | val_1_rmse: 0.66569 |  0:00:11s
epoch 25 | loss: 0.39264 | val_0_rmse: 0.62824 | val_1_rmse: 0.65075 |  0:00:11s
epoch 26 | loss: 0.38389 | val_0_rmse: 0.63597 | val_1_rmse: 0.66003 |  0:00:12s
epoch 27 | loss: 0.36944 | val_0_rmse: 0.62625 | val_1_rmse: 0.6529  |  0:00:12s
epoch 28 | loss: 0.36756 | val_0_rmse: 0.6253  | val_1_rmse: 0.65807 |  0:00:13s
epoch 29 | loss: 0.35017 | val_0_rmse: 0.65524 | val_1_rmse: 0.68037 |  0:00:13s
epoch 30 | loss: 0.35897 | val_0_rmse: 0.64558 | val_1_rmse: 0.67658 |  0:00:13s
epoch 31 | loss: 0.3601  | val_0_rmse: 0.63873 | val_1_rmse: 0.66769 |  0:00:14s
epoch 32 | loss: 0.35257 | val_0_rmse: 0.62223 | val_1_rmse: 0.64628 |  0:00:14s
epoch 33 | loss: 0.33917 | val_0_rmse: 0.64616 | val_1_rmse: 0.66456 |  0:00:15s
epoch 34 | loss: 0.37274 | val_0_rmse: 0.63589 | val_1_rmse: 0.66228 |  0:00:15s
epoch 35 | loss: 0.35647 | val_0_rmse: 0.62905 | val_1_rmse: 0.65832 |  0:00:16s
epoch 36 | loss: 0.36003 | val_0_rmse: 0.6708  | val_1_rmse: 0.70357 |  0:00:16s
epoch 37 | loss: 0.34792 | val_0_rmse: 0.6004  | val_1_rmse: 0.62688 |  0:00:17s
epoch 38 | loss: 0.35792 | val_0_rmse: 0.64513 | val_1_rmse: 0.67237 |  0:00:17s
epoch 39 | loss: 0.35458 | val_0_rmse: 0.71064 | val_1_rmse: 0.76252 |  0:00:17s
epoch 40 | loss: 0.36773 | val_0_rmse: 0.73654 | val_1_rmse: 0.76415 |  0:00:18s
epoch 41 | loss: 0.34147 | val_0_rmse: 0.66636 | val_1_rmse: 0.70046 |  0:00:18s
epoch 42 | loss: 0.34462 | val_0_rmse: 0.64709 | val_1_rmse: 0.6806  |  0:00:19s
epoch 43 | loss: 0.35411 | val_0_rmse: 0.67555 | val_1_rmse: 0.69759 |  0:00:19s
epoch 44 | loss: 0.35821 | val_0_rmse: 0.63715 | val_1_rmse: 0.66419 |  0:00:20s
epoch 45 | loss: 0.3506  | val_0_rmse: 0.64287 | val_1_rmse: 0.677   |  0:00:20s
epoch 46 | loss: 0.36229 | val_0_rmse: 0.60325 | val_1_rmse: 0.63831 |  0:00:21s
epoch 47 | loss: 0.34686 | val_0_rmse: 0.63393 | val_1_rmse: 0.67435 |  0:00:21s
epoch 48 | loss: 0.34561 | val_0_rmse: 0.61289 | val_1_rmse: 0.65112 |  0:00:21s
epoch 49 | loss: 0.33655 | val_0_rmse: 0.67411 | val_1_rmse: 0.70655 |  0:00:22s
epoch 50 | loss: 0.34141 | val_0_rmse: 0.61661 | val_1_rmse: 0.66566 |  0:00:22s
epoch 51 | loss: 0.33178 | val_0_rmse: 0.65188 | val_1_rmse: 0.70097 |  0:00:23s
epoch 52 | loss: 0.33258 | val_0_rmse: 0.59891 | val_1_rmse: 0.65355 |  0:00:23s
epoch 53 | loss: 0.33801 | val_0_rmse: 0.59747 | val_1_rmse: 0.64936 |  0:00:24s
epoch 54 | loss: 0.33653 | val_0_rmse: 0.62087 | val_1_rmse: 0.66833 |  0:00:24s
epoch 55 | loss: 0.3362  | val_0_rmse: 0.64291 | val_1_rmse: 0.68953 |  0:00:25s
epoch 56 | loss: 0.33057 | val_0_rmse: 0.59696 | val_1_rmse: 0.66085 |  0:00:25s
epoch 57 | loss: 0.3352  | val_0_rmse: 0.68546 | val_1_rmse: 0.73158 |  0:00:25s
epoch 58 | loss: 0.35193 | val_0_rmse: 0.64454 | val_1_rmse: 0.69087 |  0:00:26s
epoch 59 | loss: 0.35947 | val_0_rmse: 0.74458 | val_1_rmse: 0.7825  |  0:00:26s
epoch 60 | loss: 0.35424 | val_0_rmse: 0.63876 | val_1_rmse: 0.6896  |  0:00:27s
epoch 61 | loss: 0.35618 | val_0_rmse: 0.85993 | val_1_rmse: 0.88668 |  0:00:27s
epoch 62 | loss: 0.40276 | val_0_rmse: 0.76135 | val_1_rmse: 0.79569 |  0:00:28s
epoch 63 | loss: 0.41492 | val_0_rmse: 0.82058 | val_1_rmse: 0.85621 |  0:00:28s
epoch 64 | loss: 0.37991 | val_0_rmse: 0.79379 | val_1_rmse: 0.83906 |  0:00:28s
epoch 65 | loss: 0.35118 | val_0_rmse: 0.68049 | val_1_rmse: 0.72549 |  0:00:29s
epoch 66 | loss: 0.35744 | val_0_rmse: 0.6726  | val_1_rmse: 0.71639 |  0:00:29s
epoch 67 | loss: 0.37355 | val_0_rmse: 0.66039 | val_1_rmse: 0.69375 |  0:00:30s

Early stopping occured at epoch 67 with best_epoch = 37 and best_val_1_rmse = 0.62688
Best weights from best epoch are automatically used!
ended training at: 08:27:34
Feature importance:
Mean squared error is of 3120593612.008538
Mean absolute error:37830.86159786185
MAPE:0.3194038880206819
R2 score:0.6518639855015218
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:27:34
epoch 0  | loss: 1.06995 | val_0_rmse: 0.9352  | val_1_rmse: 0.89538 |  0:00:00s
epoch 1  | loss: 0.79745 | val_0_rmse: 0.95751 | val_1_rmse: 0.89892 |  0:00:00s
epoch 2  | loss: 0.63334 | val_0_rmse: 0.89921 | val_1_rmse: 0.85127 |  0:00:01s
epoch 3  | loss: 0.57655 | val_0_rmse: 0.95797 | val_1_rmse: 0.92122 |  0:00:01s
epoch 4  | loss: 0.53829 | val_0_rmse: 0.90572 | val_1_rmse: 0.8792  |  0:00:02s
epoch 5  | loss: 0.50856 | val_0_rmse: 0.84825 | val_1_rmse: 0.81725 |  0:00:02s
epoch 6  | loss: 0.50226 | val_0_rmse: 0.83055 | val_1_rmse: 0.79371 |  0:00:03s
epoch 7  | loss: 0.48617 | val_0_rmse: 0.81851 | val_1_rmse: 0.79085 |  0:00:03s
epoch 8  | loss: 0.47899 | val_0_rmse: 0.80718 | val_1_rmse: 0.78694 |  0:00:04s
epoch 9  | loss: 0.47619 | val_0_rmse: 0.84588 | val_1_rmse: 0.81542 |  0:00:04s
epoch 10 | loss: 0.4733  | val_0_rmse: 0.79235 | val_1_rmse: 0.75635 |  0:00:05s
epoch 11 | loss: 0.47013 | val_0_rmse: 0.73935 | val_1_rmse: 0.71542 |  0:00:05s
epoch 12 | loss: 0.46763 | val_0_rmse: 0.75021 | val_1_rmse: 0.73231 |  0:00:05s
epoch 13 | loss: 0.46569 | val_0_rmse: 0.72617 | val_1_rmse: 0.70639 |  0:00:06s
epoch 14 | loss: 0.45637 | val_0_rmse: 0.69845 | val_1_rmse: 0.68646 |  0:00:06s
epoch 15 | loss: 0.44971 | val_0_rmse: 0.69675 | val_1_rmse: 0.67623 |  0:00:07s
epoch 16 | loss: 0.44933 | val_0_rmse: 0.7011  | val_1_rmse: 0.68285 |  0:00:07s
epoch 17 | loss: 0.45757 | val_0_rmse: 0.695   | val_1_rmse: 0.67617 |  0:00:08s
epoch 18 | loss: 0.45575 | val_0_rmse: 0.67846 | val_1_rmse: 0.66866 |  0:00:08s
epoch 19 | loss: 0.45962 | val_0_rmse: 0.67621 | val_1_rmse: 0.66651 |  0:00:09s
epoch 20 | loss: 0.45394 | val_0_rmse: 0.6847  | val_1_rmse: 0.67188 |  0:00:09s
epoch 21 | loss: 0.45883 | val_0_rmse: 0.67849 | val_1_rmse: 0.66659 |  0:00:10s
epoch 22 | loss: 0.45002 | val_0_rmse: 0.67551 | val_1_rmse: 0.66417 |  0:00:10s
epoch 23 | loss: 0.44991 | val_0_rmse: 0.6757  | val_1_rmse: 0.66704 |  0:00:10s
epoch 24 | loss: 0.44723 | val_0_rmse: 0.67696 | val_1_rmse: 0.66133 |  0:00:11s
epoch 25 | loss: 0.4456  | val_0_rmse: 0.67096 | val_1_rmse: 0.65641 |  0:00:11s
epoch 26 | loss: 0.44707 | val_0_rmse: 0.67767 | val_1_rmse: 0.65949 |  0:00:12s
epoch 27 | loss: 0.44038 | val_0_rmse: 0.6716  | val_1_rmse: 0.65659 |  0:00:12s
epoch 28 | loss: 0.44978 | val_0_rmse: 0.67081 | val_1_rmse: 0.6503  |  0:00:13s
epoch 29 | loss: 0.44817 | val_0_rmse: 0.68033 | val_1_rmse: 0.66707 |  0:00:13s
epoch 30 | loss: 0.44062 | val_0_rmse: 0.66959 | val_1_rmse: 0.65669 |  0:00:14s
epoch 31 | loss: 0.44411 | val_0_rmse: 0.67335 | val_1_rmse: 0.65724 |  0:00:14s
epoch 32 | loss: 0.4362  | val_0_rmse: 0.66926 | val_1_rmse: 0.6548  |  0:00:14s
epoch 33 | loss: 0.44251 | val_0_rmse: 0.67774 | val_1_rmse: 0.66069 |  0:00:15s
epoch 34 | loss: 0.43817 | val_0_rmse: 0.66524 | val_1_rmse: 0.6527  |  0:00:15s
epoch 35 | loss: 0.43473 | val_0_rmse: 0.6747  | val_1_rmse: 0.65818 |  0:00:16s
epoch 36 | loss: 0.43946 | val_0_rmse: 0.66746 | val_1_rmse: 0.65153 |  0:00:16s
epoch 37 | loss: 0.43503 | val_0_rmse: 0.67724 | val_1_rmse: 0.6656  |  0:00:17s
epoch 38 | loss: 0.43261 | val_0_rmse: 0.67234 | val_1_rmse: 0.65804 |  0:00:17s
epoch 39 | loss: 0.43917 | val_0_rmse: 0.67115 | val_1_rmse: 0.65306 |  0:00:18s
epoch 40 | loss: 0.44551 | val_0_rmse: 0.67114 | val_1_rmse: 0.65099 |  0:00:18s
epoch 41 | loss: 0.43387 | val_0_rmse: 0.66264 | val_1_rmse: 0.64575 |  0:00:19s
epoch 42 | loss: 0.43301 | val_0_rmse: 0.6602  | val_1_rmse: 0.64958 |  0:00:19s
epoch 43 | loss: 0.43529 | val_0_rmse: 0.66173 | val_1_rmse: 0.64956 |  0:00:19s
epoch 44 | loss: 0.43536 | val_0_rmse: 0.69003 | val_1_rmse: 0.67797 |  0:00:20s
epoch 45 | loss: 0.44702 | val_0_rmse: 0.67029 | val_1_rmse: 0.65825 |  0:00:20s
epoch 46 | loss: 0.4453  | val_0_rmse: 0.69258 | val_1_rmse: 0.67869 |  0:00:21s
epoch 47 | loss: 0.4394  | val_0_rmse: 0.66303 | val_1_rmse: 0.65459 |  0:00:21s
epoch 48 | loss: 0.4336  | val_0_rmse: 0.67247 | val_1_rmse: 0.66348 |  0:00:22s
epoch 49 | loss: 0.43615 | val_0_rmse: 0.66917 | val_1_rmse: 0.6552  |  0:00:22s
epoch 50 | loss: 0.43202 | val_0_rmse: 0.6691  | val_1_rmse: 0.6539  |  0:00:23s
epoch 51 | loss: 0.42563 | val_0_rmse: 0.67293 | val_1_rmse: 0.65879 |  0:00:23s
epoch 52 | loss: 0.43047 | val_0_rmse: 0.67081 | val_1_rmse: 0.6569  |  0:00:24s
epoch 53 | loss: 0.44126 | val_0_rmse: 0.66259 | val_1_rmse: 0.65003 |  0:00:24s
epoch 54 | loss: 0.43533 | val_0_rmse: 0.69224 | val_1_rmse: 0.67248 |  0:00:24s
epoch 55 | loss: 0.43131 | val_0_rmse: 0.66608 | val_1_rmse: 0.64938 |  0:00:25s
epoch 56 | loss: 0.43262 | val_0_rmse: 0.69635 | val_1_rmse: 0.67836 |  0:00:25s
epoch 57 | loss: 0.44062 | val_0_rmse: 0.66559 | val_1_rmse: 0.64707 |  0:00:26s
epoch 58 | loss: 0.43084 | val_0_rmse: 0.67091 | val_1_rmse: 0.65397 |  0:00:26s
epoch 59 | loss: 0.42507 | val_0_rmse: 0.66477 | val_1_rmse: 0.65046 |  0:00:27s
epoch 60 | loss: 0.41847 | val_0_rmse: 0.66618 | val_1_rmse: 0.64827 |  0:00:27s
epoch 61 | loss: 0.42239 | val_0_rmse: 0.66963 | val_1_rmse: 0.65511 |  0:00:28s
epoch 62 | loss: 0.42205 | val_0_rmse: 0.66075 | val_1_rmse: 0.65239 |  0:00:28s
epoch 63 | loss: 0.41908 | val_0_rmse: 0.66515 | val_1_rmse: 0.65206 |  0:00:29s
epoch 64 | loss: 0.41843 | val_0_rmse: 0.66576 | val_1_rmse: 0.65335 |  0:00:29s
epoch 65 | loss: 0.41746 | val_0_rmse: 0.66291 | val_1_rmse: 0.65442 |  0:00:29s
epoch 66 | loss: 0.41961 | val_0_rmse: 0.66713 | val_1_rmse: 0.65634 |  0:00:30s
epoch 67 | loss: 0.42032 | val_0_rmse: 0.66389 | val_1_rmse: 0.65224 |  0:00:30s
epoch 68 | loss: 0.42057 | val_0_rmse: 0.66296 | val_1_rmse: 0.65285 |  0:00:31s
epoch 69 | loss: 0.42214 | val_0_rmse: 0.6694  | val_1_rmse: 0.65396 |  0:00:31s
epoch 70 | loss: 0.4229  | val_0_rmse: 0.67091 | val_1_rmse: 0.65434 |  0:00:32s
epoch 71 | loss: 0.41679 | val_0_rmse: 0.66419 | val_1_rmse: 0.65439 |  0:00:32s

Early stopping occured at epoch 71 with best_epoch = 41 and best_val_1_rmse = 0.64575
Best weights from best epoch are automatically used!
ended training at: 08:28:07
Feature importance:
Mean squared error is of 4317479572.94979
Mean absolute error:45212.65129178162
MAPE:0.4106615248071227
R2 score:0.47816854321384683
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:28:08
epoch 0  | loss: 0.71869 | val_0_rmse: 0.69036 | val_1_rmse: 0.69427 |  0:00:12s
epoch 1  | loss: 0.41666 | val_0_rmse: 0.64408 | val_1_rmse: 0.64691 |  0:00:24s
epoch 2  | loss: 0.40422 | val_0_rmse: 0.63284 | val_1_rmse: 0.63371 |  0:00:37s
epoch 3  | loss: 0.39993 | val_0_rmse: 0.65113 | val_1_rmse: 0.65383 |  0:00:49s
epoch 4  | loss: 0.39317 | val_0_rmse: 0.61079 | val_1_rmse: 0.61144 |  0:01:01s
epoch 5  | loss: 0.38598 | val_0_rmse: 0.60902 | val_1_rmse: 0.60862 |  0:01:14s
epoch 6  | loss: 0.3825  | val_0_rmse: 0.61175 | val_1_rmse: 0.61067 |  0:01:26s
epoch 7  | loss: 0.37817 | val_0_rmse: 0.61007 | val_1_rmse: 0.60929 |  0:01:38s
epoch 8  | loss: 0.38373 | val_0_rmse: 0.61435 | val_1_rmse: 0.61251 |  0:01:51s
epoch 9  | loss: 0.41165 | val_0_rmse: 0.72065 | val_1_rmse: 0.71444 |  0:02:03s
epoch 10 | loss: 0.44695 | val_0_rmse: 0.63715 | val_1_rmse: 0.63612 |  0:02:16s
epoch 11 | loss: 0.43205 | val_0_rmse: 0.64038 | val_1_rmse: 0.63977 |  0:02:29s
epoch 12 | loss: 0.39991 | val_0_rmse: 0.6456  | val_1_rmse: 0.64335 |  0:02:42s
epoch 13 | loss: 0.38829 | val_0_rmse: 0.60952 | val_1_rmse: 0.60906 |  0:02:55s
epoch 14 | loss: 0.37858 | val_0_rmse: 0.60959 | val_1_rmse: 0.60684 |  0:03:07s
epoch 15 | loss: 0.37766 | val_0_rmse: 0.63513 | val_1_rmse: 0.63589 |  0:03:20s
epoch 16 | loss: 0.37684 | val_0_rmse: 0.61391 | val_1_rmse: 0.61105 |  0:03:33s
epoch 17 | loss: 0.37561 | val_0_rmse: 0.61902 | val_1_rmse: 0.6146  |  0:03:46s
epoch 18 | loss: 0.37492 | val_0_rmse: 0.60276 | val_1_rmse: 0.60161 |  0:03:59s
epoch 19 | loss: 0.37093 | val_0_rmse: 0.60247 | val_1_rmse: 0.6028  |  0:04:12s
epoch 20 | loss: 0.37045 | val_0_rmse: 0.60512 | val_1_rmse: 0.60377 |  0:04:25s
epoch 21 | loss: 0.37005 | val_0_rmse: 0.60595 | val_1_rmse: 0.60311 |  0:04:37s
epoch 22 | loss: 0.36842 | val_0_rmse: 0.59969 | val_1_rmse: 0.59892 |  0:04:50s
epoch 23 | loss: 0.36715 | val_0_rmse: 0.60219 | val_1_rmse: 0.60086 |  0:05:03s
epoch 24 | loss: 0.36819 | val_0_rmse: 0.61416 | val_1_rmse: 0.61491 |  0:05:16s
epoch 25 | loss: 0.3786  | val_0_rmse: 0.64034 | val_1_rmse: 0.63768 |  0:05:29s
epoch 26 | loss: 0.38037 | val_0_rmse: 0.60596 | val_1_rmse: 0.60548 |  0:05:41s
epoch 27 | loss: 0.36928 | val_0_rmse: 0.60106 | val_1_rmse: 0.60039 |  0:05:54s
epoch 28 | loss: 0.36873 | val_0_rmse: 0.60077 | val_1_rmse: 0.60046 |  0:06:07s
epoch 29 | loss: 0.36571 | val_0_rmse: 0.5996  | val_1_rmse: 0.60012 |  0:06:20s
epoch 30 | loss: 0.36661 | val_0_rmse: 0.60639 | val_1_rmse: 0.60508 |  0:06:32s
epoch 31 | loss: 0.36573 | val_0_rmse: 0.59893 | val_1_rmse: 0.59824 |  0:06:45s
epoch 32 | loss: 0.36311 | val_0_rmse: 0.59689 | val_1_rmse: 0.59793 |  0:06:58s
epoch 33 | loss: 0.36726 | val_0_rmse: 0.59718 | val_1_rmse: 0.59863 |  0:07:11s
epoch 34 | loss: 0.3648  | val_0_rmse: 0.59673 | val_1_rmse: 0.59653 |  0:07:24s
epoch 35 | loss: 0.36377 | val_0_rmse: 0.59799 | val_1_rmse: 0.59892 |  0:07:36s
epoch 36 | loss: 0.36286 | val_0_rmse: 0.60023 | val_1_rmse: 0.59881 |  0:07:49s
epoch 37 | loss: 0.36237 | val_0_rmse: 0.5962  | val_1_rmse: 0.59642 |  0:08:02s
epoch 38 | loss: 0.3626  | val_0_rmse: 0.60407 | val_1_rmse: 0.60569 |  0:08:15s
epoch 39 | loss: 0.36313 | val_0_rmse: 0.60812 | val_1_rmse: 0.60981 |  0:08:28s
epoch 40 | loss: 0.3616  | val_0_rmse: 0.5962  | val_1_rmse: 0.5965  |  0:08:40s
epoch 41 | loss: 0.36272 | val_0_rmse: 0.5954  | val_1_rmse: 0.59625 |  0:08:53s
epoch 42 | loss: 0.36164 | val_0_rmse: 0.60111 | val_1_rmse: 0.6007  |  0:09:06s
epoch 43 | loss: 0.36114 | val_0_rmse: 0.60648 | val_1_rmse: 0.60714 |  0:09:19s
epoch 44 | loss: 0.36382 | val_0_rmse: 0.60092 | val_1_rmse: 0.6013  |  0:09:32s
epoch 45 | loss: 0.36078 | val_0_rmse: 0.59607 | val_1_rmse: 0.59633 |  0:09:44s
epoch 46 | loss: 0.35982 | val_0_rmse: 0.59157 | val_1_rmse: 0.59277 |  0:09:57s
epoch 47 | loss: 0.35958 | val_0_rmse: 0.60181 | val_1_rmse: 0.6018  |  0:10:10s
epoch 48 | loss: 0.35953 | val_0_rmse: 0.60309 | val_1_rmse: 0.60596 |  0:10:23s
epoch 49 | loss: 0.35977 | val_0_rmse: 0.59657 | val_1_rmse: 0.59834 |  0:10:35s
epoch 50 | loss: 0.36057 | val_0_rmse: 0.79801 | val_1_rmse: 0.8083  |  0:10:48s
epoch 51 | loss: 0.35556 | val_0_rmse: 0.59717 | val_1_rmse: 0.59955 |  0:11:01s
epoch 52 | loss: 0.35724 | val_0_rmse: 0.62839 | val_1_rmse: 0.63175 |  0:11:14s
epoch 53 | loss: 0.35585 | val_0_rmse: 0.59233 | val_1_rmse: 0.59495 |  0:11:27s
epoch 54 | loss: 0.35307 | val_0_rmse: 0.58966 | val_1_rmse: 0.58969 |  0:11:40s
epoch 55 | loss: 0.35182 | val_0_rmse: 0.59929 | val_1_rmse: 0.60125 |  0:11:52s
epoch 56 | loss: 0.35027 | val_0_rmse: 0.59125 | val_1_rmse: 0.5944  |  0:12:05s
epoch 57 | loss: 0.34967 | val_0_rmse: 0.60116 | val_1_rmse: 0.60337 |  0:12:18s
epoch 58 | loss: 0.34822 | val_0_rmse: 0.59222 | val_1_rmse: 0.59593 |  0:12:31s
epoch 59 | loss: 0.34655 | val_0_rmse: 0.63963 | val_1_rmse: 0.64514 |  0:12:44s
epoch 60 | loss: 0.34553 | val_0_rmse: 0.62131 | val_1_rmse: 0.62089 |  0:12:56s
epoch 61 | loss: 0.34485 | val_0_rmse: 0.58048 | val_1_rmse: 0.58298 |  0:13:09s
epoch 62 | loss: 0.34316 | val_0_rmse: 0.58821 | val_1_rmse: 0.59171 |  0:13:22s
epoch 63 | loss: 0.34425 | val_0_rmse: 0.62288 | val_1_rmse: 0.62874 |  0:13:35s
epoch 64 | loss: 0.34402 | val_0_rmse: 0.67361 | val_1_rmse: 0.6723  |  0:13:48s
epoch 65 | loss: 0.34208 | val_0_rmse: 0.58122 | val_1_rmse: 0.58528 |  0:14:00s
epoch 66 | loss: 0.34174 | val_0_rmse: 0.58084 | val_1_rmse: 0.58488 |  0:14:13s
epoch 67 | loss: 0.34149 | val_0_rmse: 0.60603 | val_1_rmse: 0.60704 |  0:14:26s
epoch 68 | loss: 0.34043 | val_0_rmse: 0.64781 | val_1_rmse: 0.65436 |  0:14:38s
epoch 69 | loss: 0.34079 | val_0_rmse: 0.58913 | val_1_rmse: 0.594   |  0:14:51s
epoch 70 | loss: 0.33871 | val_0_rmse: 0.59864 | val_1_rmse: 0.6008  |  0:15:04s
epoch 71 | loss: 0.33877 | val_0_rmse: 0.5767  | val_1_rmse: 0.5804  |  0:15:17s
epoch 72 | loss: 0.33821 | val_0_rmse: 0.57151 | val_1_rmse: 0.57477 |  0:15:29s
epoch 73 | loss: 0.33606 | val_0_rmse: 0.62451 | val_1_rmse: 0.6491  |  0:15:42s
epoch 74 | loss: 0.33788 | val_0_rmse: 0.61503 | val_1_rmse: 0.621   |  0:15:55s
epoch 75 | loss: 0.3363  | val_0_rmse: 0.58428 | val_1_rmse: 0.58925 |  0:16:08s
epoch 76 | loss: 0.33592 | val_0_rmse: 0.65635 | val_1_rmse: 0.66293 |  0:16:20s
epoch 77 | loss: 0.33396 | val_0_rmse: 0.58715 | val_1_rmse: 0.59243 |  0:16:33s
epoch 78 | loss: 0.33455 | val_0_rmse: 0.58257 | val_1_rmse: 0.58607 |  0:16:46s
epoch 79 | loss: 0.33289 | val_0_rmse: 0.6324  | val_1_rmse: 0.63846 |  0:16:59s
epoch 80 | loss: 0.33268 | val_0_rmse: 0.5877  | val_1_rmse: 0.59185 |  0:17:11s
epoch 81 | loss: 0.3343  | val_0_rmse: 0.62859 | val_1_rmse: 0.63487 |  0:17:24s
epoch 82 | loss: 0.33349 | val_0_rmse: 0.57535 | val_1_rmse: 0.57967 |  0:17:37s
epoch 83 | loss: 0.33303 | val_0_rmse: 0.66184 | val_1_rmse: 0.66791 |  0:17:49s
epoch 84 | loss: 0.3341  | val_0_rmse: 0.60497 | val_1_rmse: 0.61108 |  0:18:02s
epoch 85 | loss: 0.33356 | val_0_rmse: 0.65156 | val_1_rmse: 0.65854 |  0:18:15s
epoch 86 | loss: 0.33186 | val_0_rmse: 0.59545 | val_1_rmse: 0.6019  |  0:18:28s
epoch 87 | loss: 0.33251 | val_0_rmse: 0.58546 | val_1_rmse: 0.58927 |  0:18:40s
epoch 88 | loss: 0.33109 | val_0_rmse: 0.59395 | val_1_rmse: 0.59811 |  0:18:53s
epoch 89 | loss: 0.33106 | val_0_rmse: 0.62209 | val_1_rmse: 0.6279  |  0:19:06s
epoch 90 | loss: 0.33048 | val_0_rmse: 0.57036 | val_1_rmse: 0.57443 |  0:19:19s
epoch 91 | loss: 0.33042 | val_0_rmse: 0.57452 | val_1_rmse: 0.57937 |  0:19:31s
epoch 92 | loss: 0.3302  | val_0_rmse: 0.67667 | val_1_rmse: 0.68386 |  0:19:44s
epoch 93 | loss: 0.33097 | val_0_rmse: 0.58076 | val_1_rmse: 0.58654 |  0:19:57s
epoch 94 | loss: 0.33182 | val_0_rmse: 0.58212 | val_1_rmse: 0.58831 |  0:20:10s
epoch 95 | loss: 0.32965 | val_0_rmse: 0.57688 | val_1_rmse: 0.58192 |  0:20:22s
epoch 96 | loss: 0.32882 | val_0_rmse: 0.57177 | val_1_rmse: 0.57726 |  0:20:35s
epoch 97 | loss: 0.32824 | val_0_rmse: 0.57141 | val_1_rmse: 0.57708 |  0:20:48s
epoch 98 | loss: 0.3291  | val_0_rmse: 0.59117 | val_1_rmse: 0.59827 |  0:21:00s
epoch 99 | loss: 0.32669 | val_0_rmse: 0.60892 | val_1_rmse: 0.61506 |  0:21:13s
epoch 100| loss: 0.32857 | val_0_rmse: 0.58221 | val_1_rmse: 0.58751 |  0:21:26s
epoch 101| loss: 0.32808 | val_0_rmse: 0.68089 | val_1_rmse: 0.68091 |  0:21:38s
epoch 102| loss: 0.32777 | val_0_rmse: 0.58582 | val_1_rmse: 0.59157 |  0:21:51s
epoch 103| loss: 0.34144 | val_0_rmse: 0.61256 | val_1_rmse: 0.61454 |  0:22:04s
epoch 104| loss: 0.33408 | val_0_rmse: 0.71824 | val_1_rmse: 0.71676 |  0:22:17s
epoch 105| loss: 0.33581 | val_0_rmse: 0.59976 | val_1_rmse: 0.6067  |  0:22:30s
epoch 106| loss: 0.33244 | val_0_rmse: 0.66961 | val_1_rmse: 0.67692 |  0:22:42s
epoch 107| loss: 0.33489 | val_0_rmse: 0.66884 | val_1_rmse: 0.8288  |  0:22:55s
epoch 108| loss: 0.33352 | val_0_rmse: 0.58279 | val_1_rmse: 0.66324 |  0:23:08s
epoch 109| loss: 0.33163 | val_0_rmse: 0.58767 | val_1_rmse: 0.59431 |  0:23:21s
epoch 110| loss: 0.33288 | val_0_rmse: 0.57862 | val_1_rmse: 0.58293 |  0:23:34s
epoch 111| loss: 0.33269 | val_0_rmse: 0.68959 | val_1_rmse: 0.68783 |  0:23:46s
epoch 112| loss: 0.34553 | val_0_rmse: 0.69747 | val_1_rmse: 0.6909  |  0:23:59s
epoch 113| loss: 0.35846 | val_0_rmse: 0.58418 | val_1_rmse: 0.58683 |  0:24:12s
epoch 114| loss: 0.33992 | val_0_rmse: 0.59771 | val_1_rmse: 0.59701 |  0:24:24s
epoch 115| loss: 0.34365 | val_0_rmse: 0.63407 | val_1_rmse: 0.63942 |  0:24:37s
epoch 116| loss: 0.344   | val_0_rmse: 0.57226 | val_1_rmse: 0.57422 |  0:24:50s
epoch 117| loss: 0.33738 | val_0_rmse: 0.57622 | val_1_rmse: 0.58076 |  0:25:03s
epoch 118| loss: 0.33166 | val_0_rmse: 0.57688 | val_1_rmse: 0.58    |  0:25:16s
epoch 119| loss: 0.32977 | val_0_rmse: 0.58315 | val_1_rmse: 0.58737 |  0:25:28s
epoch 120| loss: 0.32809 | val_0_rmse: 0.67052 | val_1_rmse: 0.67659 |  0:25:41s
epoch 121| loss: 0.33166 | val_0_rmse: 0.65074 | val_1_rmse: 0.65009 |  0:25:54s
epoch 122| loss: 0.32846 | val_0_rmse: 0.64131 | val_1_rmse: 0.64469 |  0:26:06s
epoch 123| loss: 0.32937 | val_0_rmse: 0.60201 | val_1_rmse: 0.58321 |  0:26:19s
epoch 124| loss: 0.32749 | val_0_rmse: 0.57628 | val_1_rmse: 0.57937 |  0:26:31s
epoch 125| loss: 0.32676 | val_0_rmse: 0.60993 | val_1_rmse: 0.61032 |  0:26:44s
epoch 126| loss: 0.3486  | val_0_rmse: 0.58673 | val_1_rmse: 0.58912 |  0:26:57s
epoch 127| loss: 0.33057 | val_0_rmse: 0.7165  | val_1_rmse: 0.71235 |  0:27:10s
epoch 128| loss: 0.32883 | val_0_rmse: 0.56692 | val_1_rmse: 0.57251 |  0:27:22s
epoch 129| loss: 0.32699 | val_0_rmse: 0.60075 | val_1_rmse: 0.60651 |  0:27:35s
epoch 130| loss: 0.3263  | val_0_rmse: 0.57556 | val_1_rmse: 0.57799 |  0:27:48s
epoch 131| loss: 0.32384 | val_0_rmse: 0.70995 | val_1_rmse: 0.70813 |  0:28:01s
epoch 132| loss: 0.32355 | val_0_rmse: 0.64917 | val_1_rmse: 0.64894 |  0:28:14s
epoch 133| loss: 0.32372 | val_0_rmse: 0.7003  | val_1_rmse: 0.69972 |  0:28:26s
epoch 134| loss: 0.32269 | val_0_rmse: 0.57313 | val_1_rmse: 0.57721 |  0:28:39s
epoch 135| loss: 0.32672 | val_0_rmse: 0.66974 | val_1_rmse: 0.66888 |  0:28:52s
epoch 136| loss: 0.32705 | val_0_rmse: 0.56941 | val_1_rmse: 0.57227 |  0:29:05s
epoch 137| loss: 0.33116 | val_0_rmse: 0.57028 | val_1_rmse: 0.57469 |  0:29:17s
epoch 138| loss: 0.3257  | val_0_rmse: 0.55756 | val_1_rmse: 0.56775 |  0:29:30s
epoch 139| loss: 0.3243  | val_0_rmse: 0.71115 | val_1_rmse: 0.70926 |  0:29:43s
epoch 140| loss: 0.327   | val_0_rmse: 0.56296 | val_1_rmse: 0.56765 |  0:29:55s
epoch 141| loss: 0.32313 | val_0_rmse: 0.67872 | val_1_rmse: 0.67609 |  0:30:08s
epoch 142| loss: 0.32402 | val_0_rmse: 0.63605 | val_1_rmse: 0.64224 |  0:30:21s
epoch 143| loss: 0.32429 | val_0_rmse: 0.55752 | val_1_rmse: 0.56282 |  0:30:33s
epoch 144| loss: 0.32617 | val_0_rmse: 0.7644  | val_1_rmse: 0.7616  |  0:30:46s
epoch 145| loss: 0.32489 | val_0_rmse: 0.56764 | val_1_rmse: 0.57321 |  0:30:59s
epoch 146| loss: 0.32192 | val_0_rmse: 0.65274 | val_1_rmse: 0.65444 |  0:31:11s
epoch 147| loss: 0.33569 | val_0_rmse: 0.72453 | val_1_rmse: 0.73227 |  0:31:24s
epoch 148| loss: 0.3279  | val_0_rmse: 0.6624  | val_1_rmse: 0.65937 |  0:31:37s
epoch 149| loss: 0.32285 | val_0_rmse: 0.58671 | val_1_rmse: 0.58764 |  0:31:50s
Stop training because you reached max_epochs = 150 with best_epoch = 143 and best_val_1_rmse = 0.56282
Best weights from best epoch are automatically used!
ended training at: 09:00:02
Feature importance:
Mean squared error is of 2098213808.3820372
Mean absolute error:32320.08827500356
MAPE:0.3319433113847993
R2 score:0.6790882886061133
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:00:04
epoch 0  | loss: 0.65237 | val_0_rmse: 0.68621 | val_1_rmse: 0.68516 |  0:00:12s
epoch 1  | loss: 0.40094 | val_0_rmse: 0.63572 | val_1_rmse: 0.63436 |  0:00:25s
epoch 2  | loss: 0.39032 | val_0_rmse: 0.62145 | val_1_rmse: 0.62037 |  0:00:38s
epoch 3  | loss: 0.38932 | val_0_rmse: 0.62674 | val_1_rmse: 0.62474 |  0:00:50s
epoch 4  | loss: 0.39116 | val_0_rmse: 0.6251  | val_1_rmse: 0.62533 |  0:01:03s
epoch 5  | loss: 0.38197 | val_0_rmse: 0.60219 | val_1_rmse: 0.60242 |  0:01:16s
epoch 6  | loss: 0.37573 | val_0_rmse: 0.60341 | val_1_rmse: 0.60376 |  0:01:29s
epoch 7  | loss: 0.37316 | val_0_rmse: 0.60478 | val_1_rmse: 0.60405 |  0:01:42s
epoch 8  | loss: 0.37159 | val_0_rmse: 0.60515 | val_1_rmse: 0.60726 |  0:01:54s
epoch 9  | loss: 0.36991 | val_0_rmse: 0.62765 | val_1_rmse: 0.6309  |  0:02:07s
epoch 10 | loss: 0.36919 | val_0_rmse: 0.60973 | val_1_rmse: 0.60855 |  0:02:20s
epoch 11 | loss: 0.36955 | val_0_rmse: 0.59692 | val_1_rmse: 0.59692 |  0:02:33s
epoch 12 | loss: 0.36762 | val_0_rmse: 0.59732 | val_1_rmse: 0.59827 |  0:02:45s
epoch 13 | loss: 0.36854 | val_0_rmse: 0.60104 | val_1_rmse: 0.602   |  0:02:58s
epoch 14 | loss: 0.36515 | val_0_rmse: 0.60125 | val_1_rmse: 0.60271 |  0:03:11s
epoch 15 | loss: 0.3666  | val_0_rmse: 0.60257 | val_1_rmse: 0.60401 |  0:03:24s
epoch 16 | loss: 0.36585 | val_0_rmse: 0.60791 | val_1_rmse: 0.60844 |  0:03:36s
epoch 17 | loss: 0.36494 | val_0_rmse: 0.60714 | val_1_rmse: 0.60736 |  0:03:49s
epoch 18 | loss: 0.36542 | val_0_rmse: 0.59404 | val_1_rmse: 0.59589 |  0:04:02s
epoch 19 | loss: 0.36372 | val_0_rmse: 0.59696 | val_1_rmse: 0.59771 |  0:04:14s
epoch 20 | loss: 0.36119 | val_0_rmse: 0.60073 | val_1_rmse: 0.60327 |  0:04:27s
epoch 21 | loss: 0.36245 | val_0_rmse: 0.5953  | val_1_rmse: 0.59697 |  0:04:40s
epoch 22 | loss: 0.36088 | val_0_rmse: 0.60023 | val_1_rmse: 0.60189 |  0:04:53s
epoch 23 | loss: 0.3609  | val_0_rmse: 0.60787 | val_1_rmse: 0.63064 |  0:05:06s
epoch 24 | loss: 0.3605  | val_0_rmse: 0.60431 | val_1_rmse: 0.62167 |  0:05:18s
epoch 25 | loss: 0.36069 | val_0_rmse: 0.59658 | val_1_rmse: 0.60427 |  0:05:31s
epoch 26 | loss: 0.36129 | val_0_rmse: 0.59718 | val_1_rmse: 0.61888 |  0:05:44s
epoch 27 | loss: 0.35567 | val_0_rmse: 0.61426 | val_1_rmse: 0.61856 |  0:05:57s
epoch 28 | loss: 0.3534  | val_0_rmse: 0.69595 | val_1_rmse: 0.69366 |  0:06:09s
epoch 29 | loss: 0.35212 | val_0_rmse: 0.58963 | val_1_rmse: 0.59178 |  0:06:22s
epoch 30 | loss: 0.34775 | val_0_rmse: 0.61261 | val_1_rmse: 0.61204 |  0:06:35s
epoch 31 | loss: 0.35039 | val_0_rmse: 0.58907 | val_1_rmse: 0.59091 |  0:06:48s
epoch 32 | loss: 0.3454  | val_0_rmse: 0.58371 | val_1_rmse: 0.58697 |  0:07:01s
epoch 33 | loss: 0.34361 | val_0_rmse: 0.59823 | val_1_rmse: 0.59849 |  0:07:13s
epoch 34 | loss: 0.3438  | val_0_rmse: 0.58911 | val_1_rmse: 0.59002 |  0:07:26s
epoch 35 | loss: 0.35114 | val_0_rmse: 0.60138 | val_1_rmse: 0.60208 |  0:07:39s
epoch 36 | loss: 0.34432 | val_0_rmse: 0.59701 | val_1_rmse: 0.59775 |  0:07:52s
epoch 37 | loss: 0.34155 | val_0_rmse: 0.63796 | val_1_rmse: 0.63542 |  0:08:05s
epoch 38 | loss: 0.34017 | val_0_rmse: 0.58765 | val_1_rmse: 0.59114 |  0:08:18s
epoch 39 | loss: 0.34027 | val_0_rmse: 0.59559 | val_1_rmse: 0.59908 |  0:08:30s
epoch 40 | loss: 0.33904 | val_0_rmse: 0.58244 | val_1_rmse: 0.58697 |  0:08:43s
epoch 41 | loss: 0.33925 | val_0_rmse: 0.63421 | val_1_rmse: 0.63312 |  0:08:56s
epoch 42 | loss: 0.34034 | val_0_rmse: 0.57728 | val_1_rmse: 0.58232 |  0:09:09s
epoch 43 | loss: 0.33868 | val_0_rmse: 0.60629 | val_1_rmse: 0.60621 |  0:09:22s
epoch 44 | loss: 0.33805 | val_0_rmse: 0.57372 | val_1_rmse: 0.57687 |  0:09:35s
epoch 45 | loss: 0.3361  | val_0_rmse: 0.63423 | val_1_rmse: 0.63767 |  0:09:48s
epoch 46 | loss: 0.33666 | val_0_rmse: 0.57877 | val_1_rmse: 0.58286 |  0:10:01s
epoch 47 | loss: 0.33487 | val_0_rmse: 0.58351 | val_1_rmse: 0.58568 |  0:10:13s
epoch 48 | loss: 0.33623 | val_0_rmse: 0.58443 | val_1_rmse: 0.58763 |  0:10:26s
epoch 49 | loss: 0.33713 | val_0_rmse: 0.60492 | val_1_rmse: 0.60915 |  0:10:39s
epoch 50 | loss: 0.33458 | val_0_rmse: 0.6377  | val_1_rmse: 0.63756 |  0:10:52s
epoch 51 | loss: 0.33511 | val_0_rmse: 0.71633 | val_1_rmse: 0.71975 |  0:11:05s
epoch 52 | loss: 0.33501 | val_0_rmse: 0.593   | val_1_rmse: 0.59597 |  0:11:17s
epoch 53 | loss: 0.33457 | val_0_rmse: 0.57357 | val_1_rmse: 0.57591 |  0:11:30s
epoch 54 | loss: 0.33397 | val_0_rmse: 0.61602 | val_1_rmse: 0.61722 |  0:11:43s
epoch 55 | loss: 0.3327  | val_0_rmse: 0.6347  | val_1_rmse: 0.63477 |  0:11:56s
epoch 56 | loss: 0.33204 | val_0_rmse: 0.64781 | val_1_rmse: 0.64705 |  0:12:09s
epoch 57 | loss: 0.3333  | val_0_rmse: 0.58514 | val_1_rmse: 0.58952 |  0:12:21s
epoch 58 | loss: 0.33257 | val_0_rmse: 0.58871 | val_1_rmse: 0.59085 |  0:12:34s
epoch 59 | loss: 0.33148 | val_0_rmse: 0.64337 | val_1_rmse: 0.64159 |  0:12:47s
epoch 60 | loss: 0.33032 | val_0_rmse: 0.59056 | val_1_rmse: 0.5995  |  0:13:00s
epoch 61 | loss: 0.32798 | val_0_rmse: 0.57099 | val_1_rmse: 0.57556 |  0:13:12s
epoch 62 | loss: 0.33087 | val_0_rmse: 0.5734  | val_1_rmse: 0.58032 |  0:13:25s
epoch 63 | loss: 0.32898 | val_0_rmse: 0.59338 | val_1_rmse: 0.58023 |  0:13:38s
epoch 64 | loss: 0.32992 | val_0_rmse: 0.73984 | val_1_rmse: 0.7369  |  0:13:51s
epoch 65 | loss: 0.32831 | val_0_rmse: 0.64023 | val_1_rmse: 0.61484 |  0:14:04s
epoch 66 | loss: 0.32877 | val_0_rmse: 0.61704 | val_1_rmse: 0.62018 |  0:14:17s
epoch 67 | loss: 0.328   | val_0_rmse: 0.65497 | val_1_rmse: 0.65274 |  0:14:30s
epoch 68 | loss: 0.32581 | val_0_rmse: 0.62282 | val_1_rmse: 0.62988 |  0:14:43s
epoch 69 | loss: 0.32561 | val_0_rmse: 0.60849 | val_1_rmse: 0.61009 |  0:14:56s
epoch 70 | loss: 0.32625 | val_0_rmse: 0.57141 | val_1_rmse: 0.57674 |  0:15:08s
epoch 71 | loss: 0.32497 | val_0_rmse: 0.81668 | val_1_rmse: 0.69401 |  0:15:21s
epoch 72 | loss: 0.32529 | val_0_rmse: 1.16593 | val_1_rmse: 0.70705 |  0:15:34s
epoch 73 | loss: 0.32479 | val_0_rmse: 2.07284 | val_1_rmse: 0.67592 |  0:15:47s
epoch 74 | loss: 0.32487 | val_0_rmse: 0.69826 | val_1_rmse: 0.69738 |  0:16:00s
epoch 75 | loss: 0.32371 | val_0_rmse: 0.57205 | val_1_rmse: 0.57778 |  0:16:13s
epoch 76 | loss: 0.32434 | val_0_rmse: 0.7847  | val_1_rmse: 0.78816 |  0:16:26s
epoch 77 | loss: 0.32347 | val_0_rmse: 1.92103 | val_1_rmse: 0.66665 |  0:16:39s
epoch 78 | loss: 0.32249 | val_0_rmse: 1.2642  | val_1_rmse: 0.65217 |  0:16:52s
epoch 79 | loss: 0.32379 | val_0_rmse: 0.63894 | val_1_rmse: 0.63829 |  0:17:05s
epoch 80 | loss: 0.32202 | val_0_rmse: 0.9779  | val_1_rmse: 0.78889 |  0:17:18s
epoch 81 | loss: 0.32254 | val_0_rmse: 0.71737 | val_1_rmse: 0.71516 |  0:17:30s
epoch 82 | loss: 0.322   | val_0_rmse: 0.67299 | val_1_rmse: 0.6711  |  0:17:43s
epoch 83 | loss: 0.3244  | val_0_rmse: 0.56723 | val_1_rmse: 0.6173  |  0:17:56s
epoch 84 | loss: 0.32282 | val_0_rmse: 0.61323 | val_1_rmse: 0.61588 |  0:18:09s
epoch 85 | loss: 0.32737 | val_0_rmse: 0.62365 | val_1_rmse: 0.73198 |  0:18:22s
epoch 86 | loss: 0.34015 | val_0_rmse: 0.70759 | val_1_rmse: 0.70958 |  0:18:35s
epoch 87 | loss: 0.34128 | val_0_rmse: 0.64426 | val_1_rmse: 0.64233 |  0:18:48s
epoch 88 | loss: 0.32884 | val_0_rmse: 0.70792 | val_1_rmse: 0.70478 |  0:19:01s
epoch 89 | loss: 0.32367 | val_0_rmse: 0.67271 | val_1_rmse: 0.66974 |  0:19:13s
epoch 90 | loss: 0.32302 | val_0_rmse: 0.59214 | val_1_rmse: 0.59463 |  0:19:26s
epoch 91 | loss: 0.32285 | val_0_rmse: 0.77737 | val_1_rmse: 0.77899 |  0:19:39s

Early stopping occured at epoch 91 with best_epoch = 61 and best_val_1_rmse = 0.57556
Best weights from best epoch are automatically used!
ended training at: 09:19:47
Feature importance:
Mean squared error is of 2237709984.3510914
Mean absolute error:33885.702781172244
MAPE:0.35518142578934664
R2 score:0.6649659501489518
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:19:49
epoch 0  | loss: 0.64179 | val_0_rmse: 0.72079 | val_1_rmse: 0.71301 |  0:00:12s
epoch 1  | loss: 0.41425 | val_0_rmse: 0.66044 | val_1_rmse: 0.65058 |  0:00:25s
epoch 2  | loss: 0.39663 | val_0_rmse: 0.62762 | val_1_rmse: 0.62095 |  0:00:38s
epoch 3  | loss: 0.38627 | val_0_rmse: 0.6212  | val_1_rmse: 0.61229 |  0:00:51s
epoch 4  | loss: 0.3838  | val_0_rmse: 0.64532 | val_1_rmse: 0.63829 |  0:01:04s
epoch 5  | loss: 0.38433 | val_0_rmse: 0.60944 | val_1_rmse: 0.60389 |  0:01:17s
epoch 6  | loss: 0.38174 | val_0_rmse: 0.60492 | val_1_rmse: 0.60024 |  0:01:30s
epoch 7  | loss: 0.3759  | val_0_rmse: 0.6046  | val_1_rmse: 0.60047 |  0:01:43s
epoch 8  | loss: 0.37407 | val_0_rmse: 0.60595 | val_1_rmse: 0.60131 |  0:01:56s
epoch 9  | loss: 0.3728  | val_0_rmse: 0.60623 | val_1_rmse: 0.60227 |  0:02:08s
epoch 10 | loss: 0.37083 | val_0_rmse: 0.60906 | val_1_rmse: 0.59897 |  0:02:21s
epoch 11 | loss: 0.3717  | val_0_rmse: 0.60499 | val_1_rmse: 0.60297 |  0:02:34s
epoch 12 | loss: 0.36988 | val_0_rmse: 0.61239 | val_1_rmse: 0.5954  |  0:02:47s
epoch 13 | loss: 0.36887 | val_0_rmse: 0.60456 | val_1_rmse: 0.6013  |  0:03:00s
epoch 14 | loss: 0.36756 | val_0_rmse: 0.59967 | val_1_rmse: 0.59709 |  0:03:13s
epoch 15 | loss: 0.37045 | val_0_rmse: 0.60079 | val_1_rmse: 0.59671 |  0:03:25s
epoch 16 | loss: 0.36679 | val_0_rmse: 0.60001 | val_1_rmse: 0.59756 |  0:03:38s
epoch 17 | loss: 0.36699 | val_0_rmse: 0.59865 | val_1_rmse: 0.59471 |  0:03:51s
epoch 18 | loss: 0.36758 | val_0_rmse: 0.60123 | val_1_rmse: 0.59709 |  0:04:04s
epoch 19 | loss: 0.36614 | val_0_rmse: 0.6062  | val_1_rmse: 0.60401 |  0:04:17s
epoch 20 | loss: 0.36528 | val_0_rmse: 0.60034 | val_1_rmse: 0.59791 |  0:04:30s
epoch 21 | loss: 0.36546 | val_0_rmse: 4.60889 | val_1_rmse: 0.6029  |  0:04:43s
epoch 22 | loss: 0.36602 | val_0_rmse: 2.74103 | val_1_rmse: 0.60831 |  0:04:55s
epoch 23 | loss: 0.36337 | val_0_rmse: 1.71962 | val_1_rmse: 0.60648 |  0:05:08s
epoch 24 | loss: 0.36392 | val_0_rmse: 2.17547 | val_1_rmse: 0.59278 |  0:05:21s
epoch 25 | loss: 0.36218 | val_0_rmse: 1.44357 | val_1_rmse: 0.6058  |  0:05:34s
epoch 26 | loss: 0.36288 | val_0_rmse: 1.62298 | val_1_rmse: 0.60067 |  0:05:47s
epoch 27 | loss: 0.36395 | val_0_rmse: 1.60393 | val_1_rmse: 0.59345 |  0:06:00s
epoch 28 | loss: 0.36366 | val_0_rmse: 1.34541 | val_1_rmse: 0.59208 |  0:06:13s
epoch 29 | loss: 0.35934 | val_0_rmse: 1.48425 | val_1_rmse: 0.58744 |  0:06:26s
epoch 30 | loss: 0.35939 | val_0_rmse: 0.80652 | val_1_rmse: 0.60903 |  0:06:39s
epoch 31 | loss: 0.35744 | val_0_rmse: 0.79552 | val_1_rmse: 0.75086 |  0:06:52s
epoch 32 | loss: 0.35591 | val_0_rmse: 0.64485 | val_1_rmse: 0.63639 |  0:07:04s
epoch 33 | loss: 0.35132 | val_0_rmse: 0.6181  | val_1_rmse: 0.60868 |  0:07:17s
epoch 34 | loss: 0.34841 | val_0_rmse: 0.76403 | val_1_rmse: 0.60049 |  0:07:30s
epoch 35 | loss: 0.34793 | val_0_rmse: 0.66693 | val_1_rmse: 0.6386  |  0:07:43s
epoch 36 | loss: 0.34628 | val_0_rmse: 0.67303 | val_1_rmse: 0.58121 |  0:07:56s
epoch 37 | loss: 0.34637 | val_0_rmse: 0.61279 | val_1_rmse: 0.60192 |  0:08:09s
epoch 38 | loss: 0.34584 | val_0_rmse: 0.64943 | val_1_rmse: 0.64756 |  0:08:22s
epoch 39 | loss: 0.35373 | val_0_rmse: 0.60154 | val_1_rmse: 0.59814 |  0:08:35s
epoch 40 | loss: 0.3699  | val_0_rmse: 0.60127 | val_1_rmse: 0.59633 |  0:08:48s
epoch 41 | loss: 0.34945 | val_0_rmse: 0.5876  | val_1_rmse: 0.58557 |  0:09:01s
epoch 42 | loss: 0.34528 | val_0_rmse: 0.617   | val_1_rmse: 0.61513 |  0:09:14s
epoch 43 | loss: 0.34424 | val_0_rmse: 0.59961 | val_1_rmse: 0.59629 |  0:09:27s
epoch 44 | loss: 0.34217 | val_0_rmse: 0.59101 | val_1_rmse: 0.59045 |  0:09:40s
epoch 45 | loss: 0.34057 | val_0_rmse: 0.58094 | val_1_rmse: 0.58185 |  0:09:53s
epoch 46 | loss: 0.34219 | val_0_rmse: 0.57906 | val_1_rmse: 0.57803 |  0:10:05s
epoch 47 | loss: 0.34019 | val_0_rmse: 0.60941 | val_1_rmse: 0.61283 |  0:10:18s
epoch 48 | loss: 0.33901 | val_0_rmse: 0.64344 | val_1_rmse: 0.64339 |  0:10:31s
epoch 49 | loss: 0.34047 | val_0_rmse: 0.6363  | val_1_rmse: 0.63726 |  0:10:44s
epoch 50 | loss: 0.33821 | val_0_rmse: 0.64538 | val_1_rmse: 0.64805 |  0:10:57s
epoch 51 | loss: 0.33636 | val_0_rmse: 0.57341 | val_1_rmse: 0.57343 |  0:11:10s
epoch 52 | loss: 0.3357  | val_0_rmse: 0.59315 | val_1_rmse: 0.59526 |  0:11:23s
epoch 53 | loss: 0.33595 | val_0_rmse: 0.57109 | val_1_rmse: 0.57262 |  0:11:36s
epoch 54 | loss: 0.33561 | val_0_rmse: 0.59435 | val_1_rmse: 0.59466 |  0:11:49s
epoch 55 | loss: 0.33788 | val_0_rmse: 0.58549 | val_1_rmse: 0.58491 |  0:12:01s
epoch 56 | loss: 0.33619 | val_0_rmse: 0.57759 | val_1_rmse: 0.57952 |  0:12:14s
epoch 57 | loss: 0.33303 | val_0_rmse: 0.57258 | val_1_rmse: 0.57439 |  0:12:27s
epoch 58 | loss: 0.33551 | val_0_rmse: 0.59746 | val_1_rmse: 0.60218 |  0:12:39s
epoch 59 | loss: 0.33404 | val_0_rmse: 0.61358 | val_1_rmse: 0.61966 |  0:12:53s
epoch 60 | loss: 0.33373 | val_0_rmse: 0.58633 | val_1_rmse: 0.58874 |  0:13:05s
epoch 61 | loss: 0.33373 | val_0_rmse: 0.59561 | val_1_rmse: 0.59782 |  0:13:18s
epoch 62 | loss: 0.33188 | val_0_rmse: 0.57004 | val_1_rmse: 0.57321 |  0:13:31s
epoch 63 | loss: 0.33241 | val_0_rmse: 0.57513 | val_1_rmse: 0.57735 |  0:13:44s
epoch 64 | loss: 0.33141 | val_0_rmse: 0.67321 | val_1_rmse: 0.67875 |  0:13:57s
epoch 65 | loss: 0.33009 | val_0_rmse: 0.57371 | val_1_rmse: 0.57622 |  0:14:10s
epoch 66 | loss: 0.3298  | val_0_rmse: 0.58104 | val_1_rmse: 0.58127 |  0:14:22s
epoch 67 | loss: 0.32802 | val_0_rmse: 0.63562 | val_1_rmse: 0.63673 |  0:14:35s
epoch 68 | loss: 0.32784 | val_0_rmse: 0.60185 | val_1_rmse: 0.60707 |  0:14:48s
epoch 69 | loss: 0.32823 | val_0_rmse: 0.56741 | val_1_rmse: 0.56929 |  0:15:01s
epoch 70 | loss: 0.32894 | val_0_rmse: 0.60682 | val_1_rmse: 0.60721 |  0:15:14s
epoch 71 | loss: 0.32714 | val_0_rmse: 0.59059 | val_1_rmse: 0.59335 |  0:15:26s
epoch 72 | loss: 0.32737 | val_0_rmse: 0.57132 | val_1_rmse: 0.57268 |  0:15:39s
epoch 73 | loss: 0.32704 | val_0_rmse: 0.62535 | val_1_rmse: 0.62681 |  0:15:52s
epoch 74 | loss: 0.32694 | val_0_rmse: 0.57917 | val_1_rmse: 0.58303 |  0:16:05s
epoch 75 | loss: 0.32695 | val_0_rmse: 0.65755 | val_1_rmse: 0.66134 |  0:16:18s
epoch 76 | loss: 0.32539 | val_0_rmse: 0.56774 | val_1_rmse: 0.5727  |  0:16:31s
epoch 77 | loss: 0.32532 | val_0_rmse: 0.58871 | val_1_rmse: 0.59254 |  0:16:44s
epoch 78 | loss: 0.3264  | val_0_rmse: 0.57254 | val_1_rmse: 0.57679 |  0:16:56s
epoch 79 | loss: 0.32413 | val_0_rmse: 0.60416 | val_1_rmse: 0.60911 |  0:17:09s
epoch 80 | loss: 0.32559 | val_0_rmse: 0.57077 | val_1_rmse: 0.57384 |  0:17:22s
epoch 81 | loss: 0.32394 | val_0_rmse: 0.61973 | val_1_rmse: 0.62328 |  0:17:35s
epoch 82 | loss: 0.32376 | val_0_rmse: 0.63644 | val_1_rmse: 0.63672 |  0:17:48s
epoch 83 | loss: 0.32507 | val_0_rmse: 0.66967 | val_1_rmse: 0.67269 |  0:18:00s
epoch 84 | loss: 0.32378 | val_0_rmse: 0.67023 | val_1_rmse: 0.67499 |  0:18:13s
epoch 85 | loss: 0.32307 | val_0_rmse: 0.74648 | val_1_rmse: 0.75308 |  0:18:26s
epoch 86 | loss: 0.32305 | val_0_rmse: 0.68584 | val_1_rmse: 0.69024 |  0:18:39s
epoch 87 | loss: 0.32373 | val_0_rmse: 0.8142  | val_1_rmse: 0.82218 |  0:18:52s
epoch 88 | loss: 0.32375 | val_0_rmse: 0.58106 | val_1_rmse: 0.5976  |  0:19:05s
epoch 89 | loss: 0.32348 | val_0_rmse: 0.68547 | val_1_rmse: 0.69082 |  0:19:18s
epoch 90 | loss: 0.32245 | val_0_rmse: 0.56447 | val_1_rmse: 0.56971 |  0:19:31s
epoch 91 | loss: 0.32209 | val_0_rmse: 0.73104 | val_1_rmse: 0.73471 |  0:19:44s
epoch 92 | loss: 0.32125 | val_0_rmse: 0.64513 | val_1_rmse: 0.64944 |  0:19:56s
epoch 93 | loss: 0.32242 | val_0_rmse: 0.66888 | val_1_rmse: 0.68921 |  0:20:09s
epoch 94 | loss: 0.32113 | val_0_rmse: 0.70438 | val_1_rmse: 0.71723 |  0:20:22s
epoch 95 | loss: 0.31907 | val_0_rmse: 0.70312 | val_1_rmse: 0.70667 |  0:20:35s
epoch 96 | loss: 0.3193  | val_0_rmse: 0.76122 | val_1_rmse: 0.76224 |  0:20:48s
epoch 97 | loss: 0.31991 | val_0_rmse: 0.66106 | val_1_rmse: 0.65926 |  0:21:01s
epoch 98 | loss: 0.31761 | val_0_rmse: 0.66379 | val_1_rmse: 0.66725 |  0:21:14s
epoch 99 | loss: 0.32015 | val_0_rmse: 0.64151 | val_1_rmse: 0.62598 |  0:21:26s

Early stopping occured at epoch 99 with best_epoch = 69 and best_val_1_rmse = 0.56929
Best weights from best epoch are automatically used!
ended training at: 09:41:20
Feature importance:
Mean squared error is of 2221639578.7795377
Mean absolute error:34212.557672193165
MAPE:0.3681431599901211
R2 score:0.6686755344026973
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:41:21
epoch 0  | loss: 0.61621 | val_0_rmse: 0.73339 | val_1_rmse: 0.73413 |  0:00:12s
epoch 1  | loss: 0.39859 | val_0_rmse: 0.63131 | val_1_rmse: 0.63292 |  0:00:25s
epoch 2  | loss: 0.39171 | val_0_rmse: 0.61737 | val_1_rmse: 0.62154 |  0:00:38s
epoch 3  | loss: 0.39031 | val_0_rmse: 0.63157 | val_1_rmse: 0.63601 |  0:00:50s
epoch 4  | loss: 0.38348 | val_0_rmse: 0.62159 | val_1_rmse: 0.62643 |  0:01:03s
epoch 5  | loss: 0.38354 | val_0_rmse: 0.61357 | val_1_rmse: 0.62145 |  0:01:16s
epoch 6  | loss: 0.37805 | val_0_rmse: 0.62764 | val_1_rmse: 0.63452 |  0:01:29s
epoch 7  | loss: 0.37701 | val_0_rmse: 0.60558 | val_1_rmse: 0.61071 |  0:01:41s
epoch 8  | loss: 0.38489 | val_0_rmse: 0.61633 | val_1_rmse: 0.62288 |  0:01:54s
epoch 9  | loss: 0.38488 | val_0_rmse: 0.61085 | val_1_rmse: 0.61768 |  0:02:07s
epoch 10 | loss: 0.37374 | val_0_rmse: 0.60524 | val_1_rmse: 0.61047 |  0:02:20s
epoch 11 | loss: 0.37977 | val_0_rmse: 0.6116  | val_1_rmse: 0.61774 |  0:02:33s
epoch 12 | loss: 0.3773  | val_0_rmse: 0.62185 | val_1_rmse: 0.6294  |  0:02:45s
epoch 13 | loss: 0.37722 | val_0_rmse: 0.61483 | val_1_rmse: 0.62056 |  0:02:58s
epoch 14 | loss: 0.37278 | val_0_rmse: 0.60801 | val_1_rmse: 0.6133  |  0:03:11s
epoch 15 | loss: 0.37207 | val_0_rmse: 0.60399 | val_1_rmse: 0.60846 |  0:03:24s
epoch 16 | loss: 0.37007 | val_0_rmse: 0.60052 | val_1_rmse: 0.6074  |  0:03:36s
epoch 17 | loss: 0.36794 | val_0_rmse: 0.60305 | val_1_rmse: 0.60916 |  0:03:49s
epoch 18 | loss: 0.37277 | val_0_rmse: 0.59872 | val_1_rmse: 0.60474 |  0:04:02s
epoch 19 | loss: 0.36495 | val_0_rmse: 0.60013 | val_1_rmse: 0.60678 |  0:04:15s
epoch 20 | loss: 0.36621 | val_0_rmse: 0.59896 | val_1_rmse: 0.60483 |  0:04:28s
epoch 21 | loss: 0.37955 | val_0_rmse: 0.61384 | val_1_rmse: 0.61765 |  0:04:40s
epoch 22 | loss: 0.36946 | val_0_rmse: 0.60352 | val_1_rmse: 0.60961 |  0:04:53s
epoch 23 | loss: 0.37275 | val_0_rmse: 0.60794 | val_1_rmse: 0.61213 |  0:05:06s
epoch 24 | loss: 0.37519 | val_0_rmse: 0.60069 | val_1_rmse: 0.60632 |  0:05:19s
epoch 25 | loss: 0.36768 | val_0_rmse: 0.59737 | val_1_rmse: 0.60391 |  0:05:32s
epoch 26 | loss: 0.3646  | val_0_rmse: 0.59594 | val_1_rmse: 0.60351 |  0:05:45s
epoch 27 | loss: 0.36352 | val_0_rmse: 0.63152 | val_1_rmse: 0.64011 |  0:05:57s
epoch 28 | loss: 0.36178 | val_0_rmse: 0.59647 | val_1_rmse: 0.60367 |  0:06:10s
epoch 29 | loss: 0.36245 | val_0_rmse: 0.59839 | val_1_rmse: 0.60552 |  0:06:23s
epoch 30 | loss: 0.38091 | val_0_rmse: 0.86725 | val_1_rmse: 0.8771  |  0:06:35s
epoch 31 | loss: 0.36858 | val_0_rmse: 0.59766 | val_1_rmse: 0.60514 |  0:06:48s
epoch 32 | loss: 0.3645  | val_0_rmse: 0.59552 | val_1_rmse: 0.60273 |  0:07:01s
epoch 33 | loss: 0.36175 | val_0_rmse: 0.60174 | val_1_rmse: 0.6083  |  0:07:14s
epoch 34 | loss: 0.36641 | val_0_rmse: 0.67479 | val_1_rmse: 0.68014 |  0:07:26s
epoch 35 | loss: 0.36676 | val_0_rmse: 0.60585 | val_1_rmse: 0.61213 |  0:07:39s
epoch 36 | loss: 0.37205 | val_0_rmse: 0.60783 | val_1_rmse: 0.61261 |  0:07:52s
epoch 37 | loss: 0.37025 | val_0_rmse: 0.60442 | val_1_rmse: 0.61075 |  0:08:05s
epoch 38 | loss: 0.36678 | val_0_rmse: 0.60024 | val_1_rmse: 0.60576 |  0:08:18s
epoch 39 | loss: 0.36158 | val_0_rmse: 0.59469 | val_1_rmse: 0.60042 |  0:08:31s
epoch 40 | loss: 0.35932 | val_0_rmse: 0.61581 | val_1_rmse: 0.6218  |  0:08:44s
epoch 41 | loss: 0.35637 | val_0_rmse: 0.60904 | val_1_rmse: 0.61751 |  0:08:56s
epoch 42 | loss: 0.35456 | val_0_rmse: 0.58678 | val_1_rmse: 0.59342 |  0:09:09s
epoch 43 | loss: 0.35087 | val_0_rmse: 0.58257 | val_1_rmse: 0.58925 |  0:09:22s
epoch 44 | loss: 0.34862 | val_0_rmse: 0.60599 | val_1_rmse: 0.61352 |  0:09:35s
epoch 45 | loss: 0.34606 | val_0_rmse: 0.61394 | val_1_rmse: 0.62198 |  0:09:47s
epoch 46 | loss: 0.34347 | val_0_rmse: 0.61231 | val_1_rmse: 0.6206  |  0:10:00s
epoch 47 | loss: 0.34329 | val_0_rmse: 0.61491 | val_1_rmse: 0.62327 |  0:10:13s
epoch 48 | loss: 0.34323 | val_0_rmse: 0.61006 | val_1_rmse: 0.61711 |  0:10:25s
epoch 49 | loss: 0.34528 | val_0_rmse: 0.76298 | val_1_rmse: 0.77178 |  0:10:38s
epoch 50 | loss: 0.34393 | val_0_rmse: 0.59016 | val_1_rmse: 0.60114 |  0:10:51s
epoch 51 | loss: 0.34309 | val_0_rmse: 0.59665 | val_1_rmse: 0.6043  |  0:11:03s
epoch 52 | loss: 0.34018 | val_0_rmse: 0.60952 | val_1_rmse: 0.61949 |  0:11:16s
epoch 53 | loss: 0.3404  | val_0_rmse: 0.59946 | val_1_rmse: 0.60912 |  0:11:29s
epoch 54 | loss: 0.33915 | val_0_rmse: 0.60783 | val_1_rmse: 0.61663 |  0:11:42s
epoch 55 | loss: 0.34021 | val_0_rmse: 0.58468 | val_1_rmse: 0.59414 |  0:11:55s
epoch 56 | loss: 0.34289 | val_0_rmse: 0.63354 | val_1_rmse: 0.64167 |  0:12:07s
epoch 57 | loss: 0.35798 | val_0_rmse: 1.02529 | val_1_rmse: 1.03387 |  0:12:20s
epoch 58 | loss: 0.36019 | val_0_rmse: 0.59327 | val_1_rmse: 0.60107 |  0:12:32s
epoch 59 | loss: 0.34425 | val_0_rmse: 0.59647 | val_1_rmse: 0.60519 |  0:12:45s
epoch 60 | loss: 0.34158 | val_0_rmse: 0.59898 | val_1_rmse: 0.60679 |  0:12:58s
epoch 61 | loss: 0.34087 | val_0_rmse: 0.60478 | val_1_rmse: 0.61202 |  0:13:11s
epoch 62 | loss: 0.34193 | val_0_rmse: 0.57614 | val_1_rmse: 0.58546 |  0:13:24s
epoch 63 | loss: 0.34753 | val_0_rmse: 0.57709 | val_1_rmse: 0.58685 |  0:13:36s
epoch 64 | loss: 0.33749 | val_0_rmse: 0.58277 | val_1_rmse: 0.59332 |  0:13:49s
epoch 65 | loss: 0.33502 | val_0_rmse: 0.6077  | val_1_rmse: 0.6176  |  0:14:02s
epoch 66 | loss: 0.33561 | val_0_rmse: 0.59833 | val_1_rmse: 0.60741 |  0:14:15s
epoch 67 | loss: 0.33449 | val_0_rmse: 0.59384 | val_1_rmse: 0.6032  |  0:14:27s
epoch 68 | loss: 0.33316 | val_0_rmse: 0.57528 | val_1_rmse: 0.58579 |  0:14:40s
epoch 69 | loss: 0.3333  | val_0_rmse: 0.61404 | val_1_rmse: 0.62459 |  0:14:53s
epoch 70 | loss: 0.33165 | val_0_rmse: 0.56873 | val_1_rmse: 0.5804  |  0:15:06s
epoch 71 | loss: 0.33323 | val_0_rmse: 0.62587 | val_1_rmse: 0.63639 |  0:15:19s
epoch 72 | loss: 0.33213 | val_0_rmse: 0.5812  | val_1_rmse: 0.59189 |  0:15:31s
epoch 73 | loss: 0.3318  | val_0_rmse: 0.58507 | val_1_rmse: 0.59483 |  0:15:44s
epoch 74 | loss: 0.33066 | val_0_rmse: 0.62548 | val_1_rmse: 0.63541 |  0:15:57s
epoch 75 | loss: 0.33058 | val_0_rmse: 0.57002 | val_1_rmse: 0.58107 |  0:16:09s
epoch 76 | loss: 0.3299  | val_0_rmse: 0.58948 | val_1_rmse: 0.59887 |  0:16:22s
epoch 77 | loss: 0.33082 | val_0_rmse: 0.59645 | val_1_rmse: 0.60731 |  0:16:35s
epoch 78 | loss: 0.33077 | val_0_rmse: 0.67501 | val_1_rmse: 0.68386 |  0:16:48s
epoch 79 | loss: 0.32856 | val_0_rmse: 0.58689 | val_1_rmse: 0.5955  |  0:17:00s
epoch 80 | loss: 0.32927 | val_0_rmse: 0.64897 | val_1_rmse: 0.65859 |  0:17:13s
epoch 81 | loss: 0.32721 | val_0_rmse: 0.57004 | val_1_rmse: 0.57711 |  0:17:26s
epoch 82 | loss: 0.32714 | val_0_rmse: 0.60577 | val_1_rmse: 0.61468 |  0:17:39s
epoch 83 | loss: 0.32691 | val_0_rmse: 0.60145 | val_1_rmse: 0.61093 |  0:17:51s
epoch 84 | loss: 0.32597 | val_0_rmse: 0.59178 | val_1_rmse: 0.6022  |  0:18:04s
epoch 85 | loss: 0.32595 | val_0_rmse: 0.61887 | val_1_rmse: 0.62906 |  0:18:17s
epoch 86 | loss: 0.32433 | val_0_rmse: 0.63073 | val_1_rmse: 0.64019 |  0:18:29s
epoch 87 | loss: 0.32434 | val_0_rmse: 0.57986 | val_1_rmse: 0.59092 |  0:18:42s
epoch 88 | loss: 0.32287 | val_0_rmse: 0.5877  | val_1_rmse: 0.60068 |  0:18:54s
epoch 89 | loss: 0.32542 | val_0_rmse: 0.60054 | val_1_rmse: 0.61305 |  0:19:07s
epoch 90 | loss: 0.32404 | val_0_rmse: 0.58289 | val_1_rmse: 0.59457 |  0:19:20s
epoch 91 | loss: 0.32372 | val_0_rmse: 0.57191 | val_1_rmse: 0.58536 |  0:19:33s
epoch 92 | loss: 0.32106 | val_0_rmse: 0.58361 | val_1_rmse: 0.5946  |  0:19:45s
epoch 93 | loss: 0.32562 | val_0_rmse: 0.66487 | val_1_rmse: 0.67588 |  0:19:58s
epoch 94 | loss: 0.3262  | val_0_rmse: 0.59351 | val_1_rmse: 0.60394 |  0:20:11s
epoch 95 | loss: 0.32161 | val_0_rmse: 0.58843 | val_1_rmse: 0.60052 |  0:20:24s
epoch 96 | loss: 0.32295 | val_0_rmse: 0.67266 | val_1_rmse: 0.68363 |  0:20:36s
epoch 97 | loss: 0.32017 | val_0_rmse: 0.59167 | val_1_rmse: 0.60229 |  0:20:49s
epoch 98 | loss: 0.32023 | val_0_rmse: 0.60031 | val_1_rmse: 0.61138 |  0:21:02s
epoch 99 | loss: 0.31831 | val_0_rmse: 0.60125 | val_1_rmse: 0.61216 |  0:21:15s
epoch 100| loss: 0.3165  | val_0_rmse: 0.5705  | val_1_rmse: 0.58287 |  0:21:27s
epoch 101| loss: 0.31586 | val_0_rmse: 0.66479 | val_1_rmse: 0.6779  |  0:21:40s
epoch 102| loss: 0.31625 | val_0_rmse: 0.62418 | val_1_rmse: 0.63526 |  0:21:53s
epoch 103| loss: 0.31586 | val_0_rmse: 0.72159 | val_1_rmse: 0.73226 |  0:22:05s
epoch 104| loss: 0.31545 | val_0_rmse: 0.60768 | val_1_rmse: 0.61802 |  0:22:18s
epoch 105| loss: 0.31454 | val_0_rmse: 0.64847 | val_1_rmse: 0.65856 |  0:22:31s
epoch 106| loss: 0.31412 | val_0_rmse: 0.65509 | val_1_rmse: 0.66565 |  0:22:43s
epoch 107| loss: 0.31351 | val_0_rmse: 0.59084 | val_1_rmse: 0.60352 |  0:22:56s
epoch 108| loss: 0.31589 | val_0_rmse: 0.58555 | val_1_rmse: 0.59522 |  0:23:09s
epoch 109| loss: 0.31436 | val_0_rmse: 0.60765 | val_1_rmse: 0.61818 |  0:23:22s
epoch 110| loss: 0.31382 | val_0_rmse: 0.60022 | val_1_rmse: 0.61355 |  0:23:35s
epoch 111| loss: 0.31228 | val_0_rmse: 0.59962 | val_1_rmse: 0.61403 |  0:23:47s

Early stopping occured at epoch 111 with best_epoch = 81 and best_val_1_rmse = 0.57711
Best weights from best epoch are automatically used!
ended training at: 10:05:12
Feature importance:
Mean squared error is of 2208950616.6060987
Mean absolute error:32977.843685897504
MAPE:0.31874832394078256
R2 score:0.6683676977040793
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:05:14
epoch 0  | loss: 0.61884 | val_0_rmse: 0.72299 | val_1_rmse: 0.72095 |  0:00:12s
epoch 1  | loss: 0.40004 | val_0_rmse: 0.64271 | val_1_rmse: 0.63947 |  0:00:25s
epoch 2  | loss: 0.38668 | val_0_rmse: 0.62523 | val_1_rmse: 0.62193 |  0:00:38s
epoch 3  | loss: 0.38533 | val_0_rmse: 0.60961 | val_1_rmse: 0.60803 |  0:00:51s
epoch 4  | loss: 0.37939 | val_0_rmse: 0.61059 | val_1_rmse: 0.60876 |  0:01:04s
epoch 5  | loss: 0.37642 | val_0_rmse: 0.61231 | val_1_rmse: 0.60959 |  0:01:16s
epoch 6  | loss: 0.37322 | val_0_rmse: 0.61039 | val_1_rmse: 0.60855 |  0:01:29s
epoch 7  | loss: 0.37337 | val_0_rmse: 0.61962 | val_1_rmse: 0.61899 |  0:01:42s
epoch 8  | loss: 0.37221 | val_0_rmse: 0.60745 | val_1_rmse: 0.60429 |  0:01:55s
epoch 9  | loss: 0.37084 | val_0_rmse: 0.59708 | val_1_rmse: 0.59554 |  0:02:08s
epoch 10 | loss: 0.37002 | val_0_rmse: 0.60147 | val_1_rmse: 0.59989 |  0:02:21s
epoch 11 | loss: 0.36763 | val_0_rmse: 0.65727 | val_1_rmse: 0.79293 |  0:02:34s
epoch 12 | loss: 0.36706 | val_0_rmse: 0.60326 | val_1_rmse: 0.60828 |  0:02:46s
epoch 13 | loss: 0.36464 | val_0_rmse: 0.66087 | val_1_rmse: 0.81567 |  0:02:59s
epoch 14 | loss: 0.36725 | val_0_rmse: 0.70427 | val_1_rmse: 0.95346 |  0:03:12s
epoch 15 | loss: 0.36384 | val_0_rmse: 0.70189 | val_1_rmse: 0.77343 |  0:03:25s
epoch 16 | loss: 0.36296 | val_0_rmse: 0.89178 | val_1_rmse: 1.44638 |  0:03:37s
epoch 17 | loss: 0.36234 | val_0_rmse: 0.79501 | val_1_rmse: 1.20135 |  0:03:50s
epoch 18 | loss: 0.36238 | val_0_rmse: 0.85456 | val_1_rmse: 1.15255 |  0:04:03s
epoch 19 | loss: 0.36089 | val_0_rmse: 0.59885 | val_1_rmse: 0.60259 |  0:04:16s
epoch 20 | loss: 0.36043 | val_0_rmse: 0.67479 | val_1_rmse: 0.77507 |  0:04:28s
epoch 21 | loss: 0.36216 | val_0_rmse: 0.64331 | val_1_rmse: 0.75643 |  0:04:41s
epoch 22 | loss: 0.35934 | val_0_rmse: 0.62599 | val_1_rmse: 0.65982 |  0:04:54s
epoch 23 | loss: 0.36059 | val_0_rmse: 0.73947 | val_1_rmse: 0.93007 |  0:05:07s
epoch 24 | loss: 0.35877 | val_0_rmse: 0.85552 | val_1_rmse: 1.16662 |  0:05:19s
epoch 25 | loss: 0.35741 | val_0_rmse: 0.70622 | val_1_rmse: 0.96315 |  0:05:32s
epoch 26 | loss: 0.35697 | val_0_rmse: 0.76744 | val_1_rmse: 1.13196 |  0:05:45s
epoch 27 | loss: 0.3585  | val_0_rmse: 0.86667 | val_1_rmse: 1.38625 |  0:05:58s
epoch 28 | loss: 0.361   | val_0_rmse: 0.63126 | val_1_rmse: 0.60657 |  0:06:11s
epoch 29 | loss: 0.35657 | val_0_rmse: 0.60281 | val_1_rmse: 0.60229 |  0:06:23s
epoch 30 | loss: 0.34952 | val_0_rmse: 0.63948 | val_1_rmse: 0.63428 |  0:06:36s
epoch 31 | loss: 0.34899 | val_0_rmse: 0.58181 | val_1_rmse: 0.58258 |  0:06:49s
epoch 32 | loss: 0.34573 | val_0_rmse: 0.60707 | val_1_rmse: 0.60688 |  0:07:01s
epoch 33 | loss: 0.3446  | val_0_rmse: 0.59825 | val_1_rmse: 0.5983  |  0:07:14s
epoch 34 | loss: 0.34203 | val_0_rmse: 0.59956 | val_1_rmse: 0.59858 |  0:07:27s
epoch 35 | loss: 0.34352 | val_0_rmse: 0.60319 | val_1_rmse: 0.60486 |  0:07:40s
epoch 36 | loss: 0.34026 | val_0_rmse: 0.59309 | val_1_rmse: 0.59357 |  0:07:53s
epoch 37 | loss: 0.34037 | val_0_rmse: 0.60601 | val_1_rmse: 0.60712 |  0:08:05s
epoch 38 | loss: 0.33752 | val_0_rmse: 0.62572 | val_1_rmse: 0.62621 |  0:08:18s
epoch 39 | loss: 0.33996 | val_0_rmse: 0.58924 | val_1_rmse: 0.5892  |  0:08:31s
epoch 40 | loss: 0.3383  | val_0_rmse: 0.57675 | val_1_rmse: 0.57726 |  0:08:44s
epoch 41 | loss: 0.33598 | val_0_rmse: 0.57655 | val_1_rmse: 0.5772  |  0:08:57s
epoch 42 | loss: 0.33534 | val_0_rmse: 0.61583 | val_1_rmse: 0.61697 |  0:09:09s
epoch 43 | loss: 0.33381 | val_0_rmse: 0.58236 | val_1_rmse: 0.58307 |  0:09:22s
epoch 44 | loss: 0.33711 | val_0_rmse: 0.57817 | val_1_rmse: 0.57763 |  0:09:35s
epoch 45 | loss: 0.33338 | val_0_rmse: 0.62167 | val_1_rmse: 0.62244 |  0:09:48s
epoch 46 | loss: 0.331   | val_0_rmse: 0.58628 | val_1_rmse: 0.58556 |  0:10:01s
epoch 47 | loss: 0.33184 | val_0_rmse: 0.59013 | val_1_rmse: 0.59191 |  0:10:13s
epoch 48 | loss: 0.32954 | val_0_rmse: 0.61576 | val_1_rmse: 0.61748 |  0:10:26s
epoch 49 | loss: 0.33099 | val_0_rmse: 0.57649 | val_1_rmse: 0.58041 |  0:10:39s
epoch 50 | loss: 0.33031 | val_0_rmse: 0.58218 | val_1_rmse: 0.58575 |  0:10:52s
epoch 51 | loss: 0.32924 | val_0_rmse: 0.59309 | val_1_rmse: 0.59303 |  0:11:04s
epoch 52 | loss: 0.32936 | val_0_rmse: 0.67938 | val_1_rmse: 0.68351 |  0:11:17s
epoch 53 | loss: 0.33174 | val_0_rmse: 0.59306 | val_1_rmse: 0.59744 |  0:11:30s
epoch 54 | loss: 0.32695 | val_0_rmse: 0.65451 | val_1_rmse: 0.64594 |  0:11:43s
epoch 55 | loss: 0.32729 | val_0_rmse: 0.59182 | val_1_rmse: 0.58674 |  0:11:56s
epoch 56 | loss: 0.3279  | val_0_rmse: 0.59184 | val_1_rmse: 0.58794 |  0:12:09s
epoch 57 | loss: 0.32814 | val_0_rmse: 0.69813 | val_1_rmse: 0.70199 |  0:12:21s
epoch 58 | loss: 0.32873 | val_0_rmse: 0.62895 | val_1_rmse: 0.63097 |  0:12:34s
epoch 59 | loss: 0.32858 | val_0_rmse: 0.56772 | val_1_rmse: 0.5729  |  0:12:47s
epoch 60 | loss: 0.32883 | val_0_rmse: 0.65684 | val_1_rmse: 0.66206 |  0:13:00s
epoch 61 | loss: 0.32617 | val_0_rmse: 0.62347 | val_1_rmse: 0.62743 |  0:13:13s
epoch 62 | loss: 0.327   | val_0_rmse: 0.5806  | val_1_rmse: 0.58615 |  0:13:25s
epoch 63 | loss: 0.32592 | val_0_rmse: 0.62761 | val_1_rmse: 0.62966 |  0:13:38s
epoch 64 | loss: 0.32616 | val_0_rmse: 0.61932 | val_1_rmse: 0.62141 |  0:13:51s
epoch 65 | loss: 0.32718 | val_0_rmse: 0.57622 | val_1_rmse: 0.57916 |  0:14:04s
epoch 66 | loss: 0.32474 | val_0_rmse: 0.59099 | val_1_rmse: 0.59779 |  0:14:16s
epoch 67 | loss: 0.32439 | val_0_rmse: 0.5671  | val_1_rmse: 0.57085 |  0:14:29s
epoch 68 | loss: 0.32896 | val_0_rmse: 0.57167 | val_1_rmse: 0.5786  |  0:14:42s
epoch 69 | loss: 0.32384 | val_0_rmse: 0.62762 | val_1_rmse: 0.63126 |  0:14:55s
epoch 70 | loss: 0.32269 | val_0_rmse: 0.57512 | val_1_rmse: 0.57999 |  0:15:08s
epoch 71 | loss: 0.3246  | val_0_rmse: 0.57629 | val_1_rmse: 0.58198 |  0:15:20s
epoch 72 | loss: 0.32257 | val_0_rmse: 0.56493 | val_1_rmse: 0.57239 |  0:15:33s
epoch 73 | loss: 0.32508 | val_0_rmse: 0.7108  | val_1_rmse: 0.71934 |  0:15:46s
epoch 74 | loss: 0.32916 | val_0_rmse: 0.62689 | val_1_rmse: 0.62522 |  0:15:59s
epoch 75 | loss: 0.32399 | val_0_rmse: 0.62986 | val_1_rmse: 0.62996 |  0:16:11s
epoch 76 | loss: 0.32477 | val_0_rmse: 0.56722 | val_1_rmse: 0.57144 |  0:16:24s
epoch 77 | loss: 0.32532 | val_0_rmse: 0.60359 | val_1_rmse: 0.61707 |  0:16:37s
epoch 78 | loss: 0.32273 | val_0_rmse: 0.56979 | val_1_rmse: 0.57557 |  0:16:50s
epoch 79 | loss: 0.31905 | val_0_rmse: 0.62653 | val_1_rmse: 0.62989 |  0:17:03s
epoch 80 | loss: 0.32068 | val_0_rmse: 0.56492 | val_1_rmse: 0.57601 |  0:17:16s
epoch 81 | loss: 0.31999 | val_0_rmse: 0.60156 | val_1_rmse: 0.60305 |  0:17:28s
epoch 82 | loss: 0.31913 | val_0_rmse: 0.75403 | val_1_rmse: 0.76184 |  0:17:41s
epoch 83 | loss: 0.32133 | val_0_rmse: 0.56822 | val_1_rmse: 0.57342 |  0:17:54s
epoch 84 | loss: 0.32201 | val_0_rmse: 0.69409 | val_1_rmse: 0.70192 |  0:18:07s
epoch 85 | loss: 0.32071 | val_0_rmse: 0.69014 | val_1_rmse: 0.69317 |  0:18:20s
epoch 86 | loss: 0.3195  | val_0_rmse: 0.63355 | val_1_rmse: 0.63657 |  0:18:33s
epoch 87 | loss: 0.3187  | val_0_rmse: 0.69675 | val_1_rmse: 0.66129 |  0:18:45s
epoch 88 | loss: 0.31948 | val_0_rmse: 0.5758  | val_1_rmse: 0.57966 |  0:18:58s
epoch 89 | loss: 0.31738 | val_0_rmse: 0.62941 | val_1_rmse: 0.63553 |  0:19:11s
epoch 90 | loss: 0.31611 | val_0_rmse: 0.80107 | val_1_rmse: 0.80916 |  0:19:24s
epoch 91 | loss: 0.31808 | val_0_rmse: 0.60443 | val_1_rmse: 0.60839 |  0:19:37s
epoch 92 | loss: 0.31788 | val_0_rmse: 0.67364 | val_1_rmse: 0.67745 |  0:19:49s
epoch 93 | loss: 0.31988 | val_0_rmse: 0.69494 | val_1_rmse: 0.69945 |  0:20:02s
epoch 94 | loss: 0.31778 | val_0_rmse: 0.58373 | val_1_rmse: 0.59052 |  0:20:15s
epoch 95 | loss: 0.31533 | val_0_rmse: 0.67078 | val_1_rmse: 0.67579 |  0:20:28s
epoch 96 | loss: 0.31569 | val_0_rmse: 0.65473 | val_1_rmse: 0.6563  |  0:20:41s
epoch 97 | loss: 0.31549 | val_0_rmse: 0.65825 | val_1_rmse: 0.66196 |  0:20:53s

Early stopping occured at epoch 97 with best_epoch = 67 and best_val_1_rmse = 0.57085
Best weights from best epoch are automatically used!
ended training at: 10:26:11
Feature importance:
Mean squared error is of 2319391042.8368487
Mean absolute error:33321.08120060091
MAPE:0.32855994908652786
R2 score:0.653838585907271
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:26:13
epoch 0  | loss: 0.66318 | val_0_rmse: 0.69229 | val_1_rmse: 0.69641 |  0:00:12s
epoch 1  | loss: 0.41301 | val_0_rmse: 0.65433 | val_1_rmse: 0.65975 |  0:00:25s
epoch 2  | loss: 0.39863 | val_0_rmse: 0.62573 | val_1_rmse: 0.63179 |  0:00:38s
epoch 3  | loss: 0.38966 | val_0_rmse: 0.61884 | val_1_rmse: 0.62601 |  0:00:51s
epoch 4  | loss: 0.38401 | val_0_rmse: 0.61492 | val_1_rmse: 0.6229  |  0:01:04s
epoch 5  | loss: 0.37756 | val_0_rmse: 0.63626 | val_1_rmse: 0.64471 |  0:01:16s
epoch 6  | loss: 0.37918 | val_0_rmse: 0.60228 | val_1_rmse: 0.61058 |  0:01:29s
epoch 7  | loss: 0.37544 | val_0_rmse: 0.62418 | val_1_rmse: 0.63158 |  0:01:42s
epoch 8  | loss: 0.37328 | val_0_rmse: 0.62037 | val_1_rmse: 0.62902 |  0:01:55s
epoch 9  | loss: 0.37611 | val_0_rmse: 0.60238 | val_1_rmse: 0.61064 |  0:02:08s
epoch 10 | loss: 0.3777  | val_0_rmse: 0.61055 | val_1_rmse: 0.61777 |  0:02:21s
epoch 11 | loss: 0.37099 | val_0_rmse: 0.59943 | val_1_rmse: 0.60836 |  0:02:34s
epoch 12 | loss: 0.37417 | val_0_rmse: 0.61036 | val_1_rmse: 0.6198  |  0:02:47s
epoch 13 | loss: 0.37606 | val_0_rmse: 0.62009 | val_1_rmse: 0.62644 |  0:02:59s
epoch 14 | loss: 0.37455 | val_0_rmse: 0.6048  | val_1_rmse: 0.61355 |  0:03:12s
epoch 15 | loss: 0.37455 | val_0_rmse: 0.66016 | val_1_rmse: 0.66954 |  0:03:25s
epoch 16 | loss: 0.38714 | val_0_rmse: 0.66965 | val_1_rmse: 0.68087 |  0:03:38s
epoch 17 | loss: 0.3787  | val_0_rmse: 0.62451 | val_1_rmse: 0.6344  |  0:03:51s
epoch 18 | loss: 0.3694  | val_0_rmse: 0.68589 | val_1_rmse: 0.69572 |  0:04:04s
epoch 19 | loss: 0.37066 | val_0_rmse: 0.64249 | val_1_rmse: 0.65337 |  0:04:17s
epoch 20 | loss: 0.36854 | val_0_rmse: 0.61674 | val_1_rmse: 0.62509 |  0:04:29s
epoch 21 | loss: 0.38112 | val_0_rmse: 0.60665 | val_1_rmse: 0.61704 |  0:04:42s
epoch 22 | loss: 0.36815 | val_0_rmse: 0.60722 | val_1_rmse: 0.61782 |  0:04:55s
epoch 23 | loss: 0.37037 | val_0_rmse: 0.59963 | val_1_rmse: 0.61015 |  0:05:08s
epoch 24 | loss: 0.36772 | val_0_rmse: 0.61371 | val_1_rmse: 0.6244  |  0:05:21s
epoch 25 | loss: 0.37598 | val_0_rmse: 0.59993 | val_1_rmse: 0.60896 |  0:05:34s
epoch 26 | loss: 0.36835 | val_0_rmse: 0.60253 | val_1_rmse: 0.61319 |  0:05:47s
epoch 27 | loss: 0.36711 | val_0_rmse: 0.6131  | val_1_rmse: 0.62133 |  0:06:00s
epoch 28 | loss: 0.37655 | val_0_rmse: 0.60996 | val_1_rmse: 0.61821 |  0:06:12s
epoch 29 | loss: 0.36669 | val_0_rmse: 0.60616 | val_1_rmse: 0.61489 |  0:06:25s
epoch 30 | loss: 0.36709 | val_0_rmse: 0.60194 | val_1_rmse: 0.61157 |  0:06:38s
epoch 31 | loss: 0.36478 | val_0_rmse: 0.60045 | val_1_rmse: 0.61095 |  0:06:51s
epoch 32 | loss: 0.37559 | val_0_rmse: 0.64572 | val_1_rmse: 0.65417 |  0:07:04s
epoch 33 | loss: 0.37293 | val_0_rmse: 0.60085 | val_1_rmse: 0.60835 |  0:07:17s
epoch 34 | loss: 0.37617 | val_0_rmse: 0.60613 | val_1_rmse: 0.61555 |  0:07:29s
epoch 35 | loss: 0.36969 | val_0_rmse: 0.61369 | val_1_rmse: 0.62242 |  0:07:42s
epoch 36 | loss: 0.3658  | val_0_rmse: 0.60315 | val_1_rmse: 0.61266 |  0:07:55s
epoch 37 | loss: 0.36776 | val_0_rmse: 0.59807 | val_1_rmse: 0.60844 |  0:08:08s
epoch 38 | loss: 0.36759 | val_0_rmse: 0.60328 | val_1_rmse: 0.61212 |  0:08:20s
epoch 39 | loss: 0.36863 | val_0_rmse: 0.60173 | val_1_rmse: 0.61232 |  0:08:33s
epoch 40 | loss: 0.36918 | val_0_rmse: 0.61538 | val_1_rmse: 0.62508 |  0:08:46s
epoch 41 | loss: 0.37417 | val_0_rmse: 0.60042 | val_1_rmse: 0.61025 |  0:08:59s
epoch 42 | loss: 0.37702 | val_0_rmse: 0.63472 | val_1_rmse: 0.6449  |  0:09:12s
epoch 43 | loss: 0.36601 | val_0_rmse: 0.59916 | val_1_rmse: 0.60859 |  0:09:24s
epoch 44 | loss: 0.36282 | val_0_rmse: 0.59493 | val_1_rmse: 0.60674 |  0:09:37s
epoch 45 | loss: 0.36366 | val_0_rmse: 0.59391 | val_1_rmse: 0.60489 |  0:09:50s
epoch 46 | loss: 0.36283 | val_0_rmse: 0.6063  | val_1_rmse: 0.61666 |  0:10:03s
epoch 47 | loss: 0.36196 | val_0_rmse: 0.59515 | val_1_rmse: 0.60618 |  0:10:16s
epoch 48 | loss: 0.36365 | val_0_rmse: 0.59449 | val_1_rmse: 0.6064  |  0:10:28s
epoch 49 | loss: 0.36295 | val_0_rmse: 0.59171 | val_1_rmse: 0.60241 |  0:10:41s
epoch 50 | loss: 0.36195 | val_0_rmse: 0.59534 | val_1_rmse: 0.6069  |  0:10:54s
epoch 51 | loss: 0.36263 | val_0_rmse: 0.60118 | val_1_rmse: 0.61182 |  0:11:07s
epoch 52 | loss: 0.36381 | val_0_rmse: 0.60087 | val_1_rmse: 0.61264 |  0:11:19s
epoch 53 | loss: 0.3612  | val_0_rmse: 0.5982  | val_1_rmse: 0.61008 |  0:11:32s
epoch 54 | loss: 0.35956 | val_0_rmse: 0.60194 | val_1_rmse: 0.61448 |  0:11:45s
epoch 55 | loss: 0.36068 | val_0_rmse: 0.5952  | val_1_rmse: 0.60744 |  0:11:58s
epoch 56 | loss: 0.35984 | val_0_rmse: 0.59424 | val_1_rmse: 0.60655 |  0:12:11s
epoch 57 | loss: 0.35897 | val_0_rmse: 0.62205 | val_1_rmse: 0.60972 |  0:12:23s
epoch 58 | loss: 0.36004 | val_0_rmse: 0.60244 | val_1_rmse: 0.60593 |  0:12:36s
epoch 59 | loss: 0.36089 | val_0_rmse: 0.74981 | val_1_rmse: 0.61863 |  0:12:49s
epoch 60 | loss: 0.3583  | val_0_rmse: 0.66586 | val_1_rmse: 0.62252 |  0:13:02s
epoch 61 | loss: 0.35756 | val_0_rmse: 0.6695  | val_1_rmse: 0.61868 |  0:13:15s
epoch 62 | loss: 0.35653 | val_0_rmse: 0.68461 | val_1_rmse: 0.60906 |  0:13:27s
epoch 63 | loss: 0.35781 | val_0_rmse: 0.74945 | val_1_rmse: 0.67852 |  0:13:40s
epoch 64 | loss: 0.35451 | val_0_rmse: 0.69173 | val_1_rmse: 0.60838 |  0:13:53s
epoch 65 | loss: 0.35515 | val_0_rmse: 0.74823 | val_1_rmse: 0.60307 |  0:14:06s
epoch 66 | loss: 0.35261 | val_0_rmse: 0.65306 | val_1_rmse: 0.66411 |  0:14:19s
epoch 67 | loss: 0.35275 | val_0_rmse: 0.59933 | val_1_rmse: 0.5893  |  0:14:32s
epoch 68 | loss: 0.34858 | val_0_rmse: 0.67452 | val_1_rmse: 0.68822 |  0:14:45s
epoch 69 | loss: 0.34788 | val_0_rmse: 0.73553 | val_1_rmse: 0.60328 |  0:14:57s
epoch 70 | loss: 0.34654 | val_0_rmse: 0.67867 | val_1_rmse: 0.59357 |  0:15:10s
epoch 71 | loss: 0.34469 | val_0_rmse: 0.66733 | val_1_rmse: 0.59116 |  0:15:23s
epoch 72 | loss: 0.34366 | val_0_rmse: 0.686   | val_1_rmse: 0.5899  |  0:15:36s
epoch 73 | loss: 0.34454 | val_0_rmse: 0.5834  | val_1_rmse: 0.59419 |  0:15:49s
epoch 74 | loss: 0.34268 | val_0_rmse: 0.576   | val_1_rmse: 0.58683 |  0:16:01s
epoch 75 | loss: 0.34319 | val_0_rmse: 0.59603 | val_1_rmse: 0.60692 |  0:16:14s
epoch 76 | loss: 0.34209 | val_0_rmse: 0.57959 | val_1_rmse: 0.59104 |  0:16:27s
epoch 77 | loss: 0.33986 | val_0_rmse: 0.58943 | val_1_rmse: 0.6033  |  0:16:40s
epoch 78 | loss: 0.33901 | val_0_rmse: 0.60946 | val_1_rmse: 0.6207  |  0:16:53s
epoch 79 | loss: 0.33985 | val_0_rmse: 0.57188 | val_1_rmse: 0.5845  |  0:17:05s
epoch 80 | loss: 0.33741 | val_0_rmse: 0.6496  | val_1_rmse: 0.65989 |  0:17:18s
epoch 81 | loss: 0.33794 | val_0_rmse: 0.57344 | val_1_rmse: 0.58607 |  0:17:31s
epoch 82 | loss: 0.34245 | val_0_rmse: 0.67157 | val_1_rmse: 0.68157 |  0:17:44s
epoch 83 | loss: 0.33785 | val_0_rmse: 0.61668 | val_1_rmse: 0.63081 |  0:17:57s
epoch 84 | loss: 0.33754 | val_0_rmse: 0.5779  | val_1_rmse: 0.58994 |  0:18:10s
epoch 85 | loss: 0.33684 | val_0_rmse: 0.57665 | val_1_rmse: 0.58943 |  0:18:22s
epoch 86 | loss: 0.35209 | val_0_rmse: 0.60402 | val_1_rmse: 0.61264 |  0:18:35s
epoch 87 | loss: 0.34302 | val_0_rmse: 0.59157 | val_1_rmse: 0.60214 |  0:18:48s
epoch 88 | loss: 0.33955 | val_0_rmse: 0.61204 | val_1_rmse: 0.61258 |  0:19:01s
epoch 89 | loss: 0.33682 | val_0_rmse: 0.59121 | val_1_rmse: 0.58505 |  0:19:14s
epoch 90 | loss: 0.34737 | val_0_rmse: 0.60204 | val_1_rmse: 0.6101  |  0:19:26s
epoch 91 | loss: 0.34018 | val_0_rmse: 0.57982 | val_1_rmse: 0.59    |  0:19:39s
epoch 92 | loss: 0.33823 | val_0_rmse: 0.59802 | val_1_rmse: 0.60779 |  0:19:52s
epoch 93 | loss: 0.33504 | val_0_rmse: 0.59833 | val_1_rmse: 0.59494 |  0:20:05s
epoch 94 | loss: 0.33493 | val_0_rmse: 0.67731 | val_1_rmse: 0.68304 |  0:20:18s
epoch 95 | loss: 0.33602 | val_0_rmse: 0.62508 | val_1_rmse: 0.63605 |  0:20:31s
epoch 96 | loss: 0.34194 | val_0_rmse: 0.67963 | val_1_rmse: 0.69365 |  0:20:43s
epoch 97 | loss: 0.34403 | val_0_rmse: 0.61017 | val_1_rmse: 0.62047 |  0:20:56s
epoch 98 | loss: 0.3356  | val_0_rmse: 0.61278 | val_1_rmse: 0.62668 |  0:21:09s
epoch 99 | loss: 0.33426 | val_0_rmse: 0.58382 | val_1_rmse: 0.59428 |  0:21:21s
epoch 100| loss: 0.335   | val_0_rmse: 0.62449 | val_1_rmse: 0.63571 |  0:21:34s
epoch 101| loss: 0.33234 | val_0_rmse: 0.85954 | val_1_rmse: 0.88976 |  0:21:47s
epoch 102| loss: 0.33744 | val_0_rmse: 0.59056 | val_1_rmse: 0.60265 |  0:22:00s
epoch 103| loss: 0.33175 | val_0_rmse: 0.60152 | val_1_rmse: 0.6141  |  0:22:13s
epoch 104| loss: 0.3325  | val_0_rmse: 0.58611 | val_1_rmse: 0.5965  |  0:22:25s
epoch 105| loss: 0.33278 | val_0_rmse: 0.57457 | val_1_rmse: 0.58395 |  0:22:38s
epoch 106| loss: 0.32931 | val_0_rmse: 0.5722  | val_1_rmse: 0.58722 |  0:22:51s
epoch 107| loss: 0.33142 | val_0_rmse: 0.58814 | val_1_rmse: 0.60219 |  0:23:04s
epoch 108| loss: 0.32967 | val_0_rmse: 0.56693 | val_1_rmse: 0.58131 |  0:23:17s
epoch 109| loss: 0.33038 | val_0_rmse: 0.70803 | val_1_rmse: 0.71915 |  0:23:30s
epoch 110| loss: 0.3306  | val_0_rmse: 0.56216 | val_1_rmse: 0.57701 |  0:23:42s
epoch 111| loss: 0.32927 | val_0_rmse: 0.5726  | val_1_rmse: 0.58279 |  0:23:55s
epoch 112| loss: 0.33021 | val_0_rmse: 0.59425 | val_1_rmse: 0.60751 |  0:24:08s
epoch 113| loss: 0.32977 | val_0_rmse: 0.61686 | val_1_rmse: 0.62986 |  0:24:21s
epoch 114| loss: 0.32932 | val_0_rmse: 0.62864 | val_1_rmse: 0.63884 |  0:24:33s
epoch 115| loss: 0.32931 | val_0_rmse: 0.5793  | val_1_rmse: 0.59338 |  0:24:46s
epoch 116| loss: 0.3284  | val_0_rmse: 0.60422 | val_1_rmse: 0.62039 |  0:24:59s
epoch 117| loss: 0.3281  | val_0_rmse: 0.57634 | val_1_rmse: 0.59166 |  0:25:12s
epoch 118| loss: 0.32789 | val_0_rmse: 0.58394 | val_1_rmse: 0.59623 |  0:25:25s
epoch 119| loss: 0.32844 | val_0_rmse: 0.57364 | val_1_rmse: 0.5871  |  0:25:38s
epoch 120| loss: 0.32734 | val_0_rmse: 0.65239 | val_1_rmse: 0.6687  |  0:25:50s
epoch 121| loss: 0.32711 | val_0_rmse: 0.61259 | val_1_rmse: 0.62537 |  0:26:03s
epoch 122| loss: 0.32898 | val_0_rmse: 0.60741 | val_1_rmse: 0.6202  |  0:26:16s
epoch 123| loss: 0.32543 | val_0_rmse: 0.62476 | val_1_rmse: 0.64421 |  0:26:29s
epoch 124| loss: 0.32568 | val_0_rmse: 0.58257 | val_1_rmse: 0.59943 |  0:26:42s
epoch 125| loss: 0.32608 | val_0_rmse: 0.57726 | val_1_rmse: 0.59287 |  0:26:55s
epoch 126| loss: 0.32592 | val_0_rmse: 0.5784  | val_1_rmse: 0.59367 |  0:27:07s
epoch 127| loss: 0.32553 | val_0_rmse: 0.55807 | val_1_rmse: 0.57383 |  0:27:20s
epoch 128| loss: 0.32417 | val_0_rmse: 0.7051  | val_1_rmse: 0.72021 |  0:27:33s
epoch 129| loss: 0.3265  | val_0_rmse: 0.55915 | val_1_rmse: 0.57634 |  0:27:46s
epoch 130| loss: 0.32542 | val_0_rmse: 3.36115 | val_1_rmse: 3.36202 |  0:27:59s
epoch 131| loss: 0.64791 | val_0_rmse: 0.69978 | val_1_rmse: 1.08875 |  0:28:12s
epoch 132| loss: 0.42742 | val_0_rmse: 0.63558 | val_1_rmse: 1.33837 |  0:28:24s
epoch 133| loss: 0.3872  | val_0_rmse: 0.63399 | val_1_rmse: 1.16071 |  0:28:37s
epoch 134| loss: 0.36884 | val_0_rmse: 0.59251 | val_1_rmse: 1.14143 |  0:28:50s
epoch 135| loss: 0.3546  | val_0_rmse: 0.6577  | val_1_rmse: 1.38491 |  0:29:02s
epoch 136| loss: 0.35125 | val_0_rmse: 0.58335 | val_1_rmse: 1.01253 |  0:29:15s
epoch 137| loss: 0.34347 | val_0_rmse: 0.58293 | val_1_rmse: 1.06333 |  0:29:28s
epoch 138| loss: 0.3416  | val_0_rmse: 0.60257 | val_1_rmse: 1.09607 |  0:29:41s
epoch 139| loss: 0.33803 | val_0_rmse: 0.58113 | val_1_rmse: 1.09635 |  0:29:54s
epoch 140| loss: 0.33889 | val_0_rmse: 0.61462 | val_1_rmse: 1.17498 |  0:30:06s
epoch 141| loss: 0.33536 | val_0_rmse: 0.59705 | val_1_rmse: 1.21467 |  0:30:19s
epoch 142| loss: 0.3365  | val_0_rmse: 0.6327  | val_1_rmse: 1.33579 |  0:30:32s
epoch 143| loss: 0.33673 | val_0_rmse: 0.66162 | val_1_rmse: 1.16511 |  0:30:45s
epoch 144| loss: 0.33505 | val_0_rmse: 0.69202 | val_1_rmse: 1.22535 |  0:30:58s
epoch 145| loss: 0.33303 | val_0_rmse: 0.98412 | val_1_rmse: 1.35596 |  0:31:11s
epoch 146| loss: 0.33339 | val_0_rmse: 0.60179 | val_1_rmse: 1.1295  |  0:31:24s
epoch 147| loss: 0.33293 | val_0_rmse: 0.5701  | val_1_rmse: 1.26916 |  0:31:36s
epoch 148| loss: 0.32949 | val_0_rmse: 0.6081  | val_1_rmse: 0.99765 |  0:31:49s
epoch 149| loss: 0.32922 | val_0_rmse: 0.72988 | val_1_rmse: 1.0719  |  0:32:02s
Stop training because you reached max_epochs = 150 with best_epoch = 127 and best_val_1_rmse = 0.57383
Best weights from best epoch are automatically used!
ended training at: 10:58:19
Feature importance:
Mean squared error is of 2119544221.2620015
Mean absolute error:32570.91972503592
MAPE:0.33436669559638044
R2 score:0.6772857250399618
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:58:20
epoch 0  | loss: 0.67118 | val_0_rmse: 0.71062 | val_1_rmse: 0.71204 |  0:00:12s
epoch 1  | loss: 0.40717 | val_0_rmse: 0.64562 | val_1_rmse: 0.644   |  0:00:25s
epoch 2  | loss: 0.3969  | val_0_rmse: 0.62623 | val_1_rmse: 0.62536 |  0:00:38s
epoch 3  | loss: 0.39213 | val_0_rmse: 0.6182  | val_1_rmse: 0.61946 |  0:00:51s
epoch 4  | loss: 0.38597 | val_0_rmse: 0.61795 | val_1_rmse: 0.61767 |  0:01:04s
epoch 5  | loss: 0.3855  | val_0_rmse: 0.61155 | val_1_rmse: 0.61239 |  0:01:17s
epoch 6  | loss: 0.37986 | val_0_rmse: 0.60709 | val_1_rmse: 0.60861 |  0:01:30s
epoch 7  | loss: 0.37655 | val_0_rmse: 0.60987 | val_1_rmse: 0.61284 |  0:01:43s
epoch 8  | loss: 0.37838 | val_0_rmse: 0.64797 | val_1_rmse: 0.65027 |  0:01:55s
epoch 9  | loss: 0.37423 | val_0_rmse: 0.60988 | val_1_rmse: 0.61157 |  0:02:08s
epoch 10 | loss: 0.37422 | val_0_rmse: 0.61758 | val_1_rmse: 0.61781 |  0:02:21s
epoch 11 | loss: 0.37391 | val_0_rmse: 0.62694 | val_1_rmse: 0.62949 |  0:02:34s
epoch 12 | loss: 0.37252 | val_0_rmse: 0.60454 | val_1_rmse: 0.60643 |  0:02:47s
epoch 13 | loss: 0.37186 | val_0_rmse: 0.60056 | val_1_rmse: 0.60129 |  0:03:00s
epoch 14 | loss: 0.37049 | val_0_rmse: 0.59943 | val_1_rmse: 0.60137 |  0:03:12s
epoch 15 | loss: 0.36981 | val_0_rmse: 0.60096 | val_1_rmse: 0.60174 |  0:03:25s
epoch 16 | loss: 0.3701  | val_0_rmse: 0.6022  | val_1_rmse: 0.60302 |  0:03:38s
epoch 17 | loss: 0.36873 | val_0_rmse: 0.60869 | val_1_rmse: 0.61024 |  0:03:51s
epoch 18 | loss: 0.36751 | val_0_rmse: 0.59749 | val_1_rmse: 0.59898 |  0:04:04s
epoch 19 | loss: 0.36696 | val_0_rmse: 0.6136  | val_1_rmse: 0.61666 |  0:04:16s
epoch 20 | loss: 0.3684  | val_0_rmse: 0.61141 | val_1_rmse: 0.61415 |  0:04:29s
epoch 21 | loss: 0.37029 | val_0_rmse: 0.60604 | val_1_rmse: 0.60913 |  0:04:42s
epoch 22 | loss: 0.3676  | val_0_rmse: 0.602   | val_1_rmse: 0.60172 |  0:04:55s
epoch 23 | loss: 0.36682 | val_0_rmse: 0.59752 | val_1_rmse: 0.59957 |  0:05:07s
epoch 24 | loss: 0.36403 | val_0_rmse: 0.59798 | val_1_rmse: 0.59917 |  0:05:20s
epoch 25 | loss: 0.36511 | val_0_rmse: 0.59386 | val_1_rmse: 0.59636 |  0:05:33s
epoch 26 | loss: 0.36452 | val_0_rmse: 0.62545 | val_1_rmse: 0.62741 |  0:05:46s
epoch 27 | loss: 0.37002 | val_0_rmse: 0.65754 | val_1_rmse: 0.66122 |  0:05:59s
epoch 28 | loss: 0.37659 | val_0_rmse: 0.62391 | val_1_rmse: 0.59988 |  0:06:12s
epoch 29 | loss: 0.36679 | val_0_rmse: 0.59949 | val_1_rmse: 0.60373 |  0:06:24s
epoch 30 | loss: 0.36464 | val_0_rmse: 0.60051 | val_1_rmse: 0.60409 |  0:06:37s
epoch 31 | loss: 0.36401 | val_0_rmse: 0.6062  | val_1_rmse: 0.61096 |  0:06:50s
epoch 32 | loss: 0.36262 | val_0_rmse: 0.59607 | val_1_rmse: 0.59813 |  0:07:03s
epoch 33 | loss: 0.36211 | val_0_rmse: 0.59429 | val_1_rmse: 0.59658 |  0:07:16s
epoch 34 | loss: 0.3633  | val_0_rmse: 0.5956  | val_1_rmse: 0.5992  |  0:07:28s
epoch 35 | loss: 0.3637  | val_0_rmse: 0.59639 | val_1_rmse: 0.59974 |  0:07:41s
epoch 36 | loss: 0.36286 | val_0_rmse: 0.60682 | val_1_rmse: 0.61128 |  0:07:54s
epoch 37 | loss: 0.36045 | val_0_rmse: 0.5994  | val_1_rmse: 0.60403 |  0:08:07s
epoch 38 | loss: 0.36102 | val_0_rmse: 0.5959  | val_1_rmse: 0.59892 |  0:08:20s
epoch 39 | loss: 0.36138 | val_0_rmse: 0.5973  | val_1_rmse: 0.60088 |  0:08:33s
epoch 40 | loss: 0.35874 | val_0_rmse: 0.59355 | val_1_rmse: 0.59657 |  0:08:46s
epoch 41 | loss: 0.35964 | val_0_rmse: 0.59185 | val_1_rmse: 0.59544 |  0:08:59s
epoch 42 | loss: 0.36147 | val_0_rmse: 0.6289  | val_1_rmse: 0.63262 |  0:09:11s
epoch 43 | loss: 0.35683 | val_0_rmse: 0.58807 | val_1_rmse: 0.59129 |  0:09:24s
epoch 44 | loss: 0.35509 | val_0_rmse: 0.60544 | val_1_rmse: 0.60808 |  0:09:37s
epoch 45 | loss: 0.35271 | val_0_rmse: 0.6131  | val_1_rmse: 0.61686 |  0:09:50s
epoch 46 | loss: 0.35163 | val_0_rmse: 0.60503 | val_1_rmse: 0.6095  |  0:10:03s
epoch 47 | loss: 0.35106 | val_0_rmse: 0.59266 | val_1_rmse: 0.59584 |  0:10:15s
epoch 48 | loss: 0.35052 | val_0_rmse: 0.61318 | val_1_rmse: 0.61674 |  0:10:28s
epoch 49 | loss: 0.34779 | val_0_rmse: 0.59545 | val_1_rmse: 0.60075 |  0:10:41s
epoch 50 | loss: 0.34674 | val_0_rmse: 0.57762 | val_1_rmse: 0.58262 |  0:10:54s
epoch 51 | loss: 0.34514 | val_0_rmse: 0.57891 | val_1_rmse: 0.58328 |  0:11:07s
epoch 52 | loss: 0.34449 | val_0_rmse: 0.59163 | val_1_rmse: 0.59629 |  0:11:19s
epoch 53 | loss: 0.34568 | val_0_rmse: 0.6322  | val_1_rmse: 0.63539 |  0:11:32s
epoch 54 | loss: 0.34485 | val_0_rmse: 0.60163 | val_1_rmse: 0.60621 |  0:11:45s
epoch 55 | loss: 0.34356 | val_0_rmse: 0.57882 | val_1_rmse: 0.58243 |  0:11:58s
epoch 56 | loss: 0.34517 | val_0_rmse: 0.57922 | val_1_rmse: 0.58318 |  0:12:11s
epoch 57 | loss: 0.34109 | val_0_rmse: 0.59416 | val_1_rmse: 0.5922  |  0:12:23s
epoch 58 | loss: 0.34023 | val_0_rmse: 0.6208  | val_1_rmse: 0.62471 |  0:12:36s
epoch 59 | loss: 0.34228 | val_0_rmse: 0.6025  | val_1_rmse: 0.60799 |  0:12:49s
epoch 60 | loss: 0.34026 | val_0_rmse: 0.58106 | val_1_rmse: 0.58663 |  0:13:02s
epoch 61 | loss: 0.33826 | val_0_rmse: 0.61572 | val_1_rmse: 0.61993 |  0:13:15s
epoch 62 | loss: 0.33809 | val_0_rmse: 0.63396 | val_1_rmse: 0.63488 |  0:13:28s
epoch 63 | loss: 0.33872 | val_0_rmse: 0.59026 | val_1_rmse: 0.59597 |  0:13:40s
epoch 64 | loss: 0.33704 | val_0_rmse: 0.61974 | val_1_rmse: 0.62236 |  0:13:53s
epoch 65 | loss: 0.33744 | val_0_rmse: 0.58836 | val_1_rmse: 0.59306 |  0:14:06s
epoch 66 | loss: 0.33591 | val_0_rmse: 0.61064 | val_1_rmse: 0.61609 |  0:14:19s
epoch 67 | loss: 0.33743 | val_0_rmse: 0.65048 | val_1_rmse: 0.65198 |  0:14:32s
epoch 68 | loss: 0.33539 | val_0_rmse: 0.58953 | val_1_rmse: 0.59316 |  0:14:44s
epoch 69 | loss: 0.33371 | val_0_rmse: 0.57475 | val_1_rmse: 0.57968 |  0:14:57s
epoch 70 | loss: 0.33352 | val_0_rmse: 0.59054 | val_1_rmse: 0.59615 |  0:15:10s
epoch 71 | loss: 0.33514 | val_0_rmse: 0.6165  | val_1_rmse: 0.62152 |  0:15:23s
epoch 72 | loss: 0.33712 | val_0_rmse: 0.70556 | val_1_rmse: 0.7088  |  0:15:36s
epoch 73 | loss: 0.33563 | val_0_rmse: 0.59174 | val_1_rmse: 0.5958  |  0:15:49s
epoch 74 | loss: 0.33406 | val_0_rmse: 0.61419 | val_1_rmse: 0.61884 |  0:16:02s
epoch 75 | loss: 0.3336  | val_0_rmse: 0.63883 | val_1_rmse: 0.64469 |  0:16:14s
epoch 76 | loss: 0.33216 | val_0_rmse: 0.56809 | val_1_rmse: 0.57295 |  0:16:27s
epoch 77 | loss: 0.33302 | val_0_rmse: 0.57707 | val_1_rmse: 0.58275 |  0:16:40s
epoch 78 | loss: 0.33426 | val_0_rmse: 0.58217 | val_1_rmse: 0.58903 |  0:16:53s
epoch 79 | loss: 0.3312  | val_0_rmse: 0.57321 | val_1_rmse: 0.58022 |  0:17:06s
epoch 80 | loss: 0.33385 | val_0_rmse: 0.62403 | val_1_rmse: 0.62838 |  0:17:18s
epoch 81 | loss: 0.33098 | val_0_rmse: 0.60787 | val_1_rmse: 0.61307 |  0:17:31s
epoch 82 | loss: 0.33255 | val_0_rmse: 0.60942 | val_1_rmse: 0.61047 |  0:17:44s
epoch 83 | loss: 0.33056 | val_0_rmse: 0.57709 | val_1_rmse: 0.58248 |  0:17:57s
epoch 84 | loss: 0.33053 | val_0_rmse: 0.66784 | val_1_rmse: 0.6733  |  0:18:10s
epoch 85 | loss: 0.33005 | val_0_rmse: 0.57995 | val_1_rmse: 0.58509 |  0:18:23s
epoch 86 | loss: 0.33033 | val_0_rmse: 0.59412 | val_1_rmse: 0.60011 |  0:18:35s
epoch 87 | loss: 0.33121 | val_0_rmse: 0.61876 | val_1_rmse: 0.62167 |  0:18:48s
epoch 88 | loss: 0.33    | val_0_rmse: 0.57094 | val_1_rmse: 0.5769  |  0:19:01s
epoch 89 | loss: 0.32975 | val_0_rmse: 0.564   | val_1_rmse: 0.57072 |  0:19:14s
epoch 90 | loss: 0.33002 | val_0_rmse: 0.57033 | val_1_rmse: 0.57549 |  0:19:27s
epoch 91 | loss: 0.32716 | val_0_rmse: 0.56695 | val_1_rmse: 0.57458 |  0:19:40s
epoch 92 | loss: 0.32822 | val_0_rmse: 0.59461 | val_1_rmse: 0.60094 |  0:19:52s
epoch 93 | loss: 0.32681 | val_0_rmse: 0.60537 | val_1_rmse: 0.61109 |  0:20:05s
epoch 94 | loss: 0.3278  | val_0_rmse: 0.5863  | val_1_rmse: 0.59103 |  0:20:18s
epoch 95 | loss: 0.32868 | val_0_rmse: 0.57531 | val_1_rmse: 0.58131 |  0:20:31s
epoch 96 | loss: 0.32805 | val_0_rmse: 0.62697 | val_1_rmse: 0.6329  |  0:20:44s
epoch 97 | loss: 0.32745 | val_0_rmse: 0.65339 | val_1_rmse: 0.6572  |  0:20:56s
epoch 98 | loss: 0.32828 | val_0_rmse: 0.6023  | val_1_rmse: 0.60762 |  0:21:09s
epoch 99 | loss: 0.33005 | val_0_rmse: 0.5755  | val_1_rmse: 0.58211 |  0:21:22s
epoch 100| loss: 0.33004 | val_0_rmse: 0.56788 | val_1_rmse: 0.57643 |  0:21:35s
epoch 101| loss: 0.3294  | val_0_rmse: 0.67012 | val_1_rmse: 0.67221 |  0:21:48s
epoch 102| loss: 0.3281  | val_0_rmse: 0.64273 | val_1_rmse: 0.648   |  0:22:01s
epoch 103| loss: 0.32596 | val_0_rmse: 0.62471 | val_1_rmse: 0.63091 |  0:22:13s
epoch 104| loss: 0.32792 | val_0_rmse: 0.61754 | val_1_rmse: 0.62107 |  0:22:26s
epoch 105| loss: 0.33087 | val_0_rmse: 0.58177 | val_1_rmse: 0.58859 |  0:22:39s
epoch 106| loss: 0.32992 | val_0_rmse: 0.60936 | val_1_rmse: 0.61441 |  0:22:52s
epoch 107| loss: 0.32718 | val_0_rmse: 0.59786 | val_1_rmse: 0.60492 |  0:23:05s
epoch 108| loss: 0.32679 | val_0_rmse: 0.57463 | val_1_rmse: 0.58127 |  0:23:17s
epoch 109| loss: 0.3256  | val_0_rmse: 0.5984  | val_1_rmse: 0.60367 |  0:23:30s
epoch 110| loss: 0.32492 | val_0_rmse: 0.56324 | val_1_rmse: 0.57099 |  0:23:43s
epoch 111| loss: 0.33479 | val_0_rmse: 0.65431 | val_1_rmse: 0.65588 |  0:23:56s
epoch 112| loss: 0.32797 | val_0_rmse: 0.56582 | val_1_rmse: 0.57318 |  0:24:08s
epoch 113| loss: 0.32661 | val_0_rmse: 0.63992 | val_1_rmse: 0.64802 |  0:24:21s
epoch 114| loss: 0.32752 | val_0_rmse: 0.62745 | val_1_rmse: 0.63065 |  0:24:34s
epoch 115| loss: 0.32582 | val_0_rmse: 0.57674 | val_1_rmse: 0.58387 |  0:24:47s
epoch 116| loss: 0.32449 | val_0_rmse: 0.56865 | val_1_rmse: 0.57665 |  0:25:00s
epoch 117| loss: 0.3266  | val_0_rmse: 0.5704  | val_1_rmse: 0.57652 |  0:25:13s
epoch 118| loss: 0.32554 | val_0_rmse: 0.57049 | val_1_rmse: 0.57833 |  0:25:25s
epoch 119| loss: 0.32286 | val_0_rmse: 0.67059 | val_1_rmse: 0.6738  |  0:25:38s

Early stopping occured at epoch 119 with best_epoch = 89 and best_val_1_rmse = 0.57072
Best weights from best epoch are automatically used!
ended training at: 11:24:03
Feature importance:
Mean squared error is of 2145964146.52976
Mean absolute error:32632.25449125793
MAPE:0.3242140681192506
R2 score:0.6738711524510124
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 11:24:05
epoch 0  | loss: 0.63998 | val_0_rmse: 0.70621 | val_1_rmse: 0.70402 |  0:00:12s
epoch 1  | loss: 0.40147 | val_0_rmse: 0.64607 | val_1_rmse: 0.64409 |  0:00:25s
epoch 2  | loss: 0.38722 | val_0_rmse: 0.62067 | val_1_rmse: 0.61971 |  0:00:38s
epoch 3  | loss: 0.38291 | val_0_rmse: 0.60525 | val_1_rmse: 0.60541 |  0:00:50s
epoch 4  | loss: 0.3804  | val_0_rmse: 0.60581 | val_1_rmse: 0.60557 |  0:01:03s
epoch 5  | loss: 0.37595 | val_0_rmse: 0.61121 | val_1_rmse: 0.61127 |  0:01:16s
epoch 6  | loss: 0.37731 | val_0_rmse: 0.6043  | val_1_rmse: 0.6043  |  0:01:29s
epoch 7  | loss: 0.37202 | val_0_rmse: 0.61384 | val_1_rmse: 0.61346 |  0:01:42s
epoch 8  | loss: 0.37219 | val_0_rmse: 0.60389 | val_1_rmse: 0.605   |  0:01:54s
epoch 9  | loss: 0.3728  | val_0_rmse: 0.60696 | val_1_rmse: 0.60742 |  0:02:07s
epoch 10 | loss: 0.37128 | val_0_rmse: 0.60022 | val_1_rmse: 0.60098 |  0:02:20s
epoch 11 | loss: 0.37079 | val_0_rmse: 0.62326 | val_1_rmse: 0.64349 |  0:02:32s
epoch 12 | loss: 0.3689  | val_0_rmse: 0.69721 | val_1_rmse: 0.74688 |  0:02:45s
epoch 13 | loss: 0.36868 | val_0_rmse: 0.60132 | val_1_rmse: 0.6024  |  0:02:58s
epoch 14 | loss: 0.36731 | val_0_rmse: 0.60518 | val_1_rmse: 0.60691 |  0:03:11s
epoch 15 | loss: 0.36685 | val_0_rmse: 0.59892 | val_1_rmse: 0.5985  |  0:03:23s
epoch 16 | loss: 0.36692 | val_0_rmse: 0.59891 | val_1_rmse: 0.60135 |  0:03:36s
epoch 17 | loss: 0.36806 | val_0_rmse: 0.61742 | val_1_rmse: 0.61579 |  0:03:49s
epoch 18 | loss: 0.36668 | val_0_rmse: 0.61432 | val_1_rmse: 0.61606 |  0:04:02s
epoch 19 | loss: 0.36537 | val_0_rmse: 0.59567 | val_1_rmse: 0.59775 |  0:04:15s
epoch 20 | loss: 0.36396 | val_0_rmse: 0.60085 | val_1_rmse: 0.60164 |  0:04:27s
epoch 21 | loss: 0.36367 | val_0_rmse: 0.60001 | val_1_rmse: 0.60178 |  0:04:40s
epoch 22 | loss: 0.36199 | val_0_rmse: 0.60169 | val_1_rmse: 0.60344 |  0:04:53s
epoch 23 | loss: 0.3631  | val_0_rmse: 0.59873 | val_1_rmse: 0.59995 |  0:05:06s
epoch 24 | loss: 0.3642  | val_0_rmse: 0.59744 | val_1_rmse: 0.59904 |  0:05:18s
epoch 25 | loss: 0.36331 | val_0_rmse: 0.59707 | val_1_rmse: 0.59886 |  0:05:31s
epoch 26 | loss: 0.36144 | val_0_rmse: 0.591   | val_1_rmse: 0.59374 |  0:05:44s
epoch 27 | loss: 0.35931 | val_0_rmse: 0.5943  | val_1_rmse: 0.59691 |  0:05:57s
epoch 28 | loss: 0.35799 | val_0_rmse: 0.59621 | val_1_rmse: 0.59936 |  0:06:09s
epoch 29 | loss: 0.35735 | val_0_rmse: 0.60257 | val_1_rmse: 0.60473 |  0:06:22s
epoch 30 | loss: 0.35775 | val_0_rmse: 0.60647 | val_1_rmse: 0.60717 |  0:06:35s
epoch 31 | loss: 0.35492 | val_0_rmse: 0.62591 | val_1_rmse: 0.62481 |  0:06:48s
epoch 32 | loss: 0.35374 | val_0_rmse: 0.584   | val_1_rmse: 0.58452 |  0:07:00s
epoch 33 | loss: 0.34862 | val_0_rmse: 0.59349 | val_1_rmse: 0.59533 |  0:07:13s
epoch 34 | loss: 0.3478  | val_0_rmse: 0.60814 | val_1_rmse: 0.6109  |  0:07:26s
epoch 35 | loss: 0.34652 | val_0_rmse: 0.59261 | val_1_rmse: 0.5944  |  0:07:38s
epoch 36 | loss: 0.3463  | val_0_rmse: 0.57835 | val_1_rmse: 0.57982 |  0:07:51s
epoch 37 | loss: 0.3445  | val_0_rmse: 0.59286 | val_1_rmse: 0.59514 |  0:08:04s
epoch 38 | loss: 0.3428  | val_0_rmse: 0.71609 | val_1_rmse: 0.71468 |  0:08:17s
epoch 39 | loss: 0.34246 | val_0_rmse: 0.63302 | val_1_rmse: 0.63242 |  0:08:30s
epoch 40 | loss: 0.34344 | val_0_rmse: 0.64774 | val_1_rmse: 0.64778 |  0:08:42s
epoch 41 | loss: 0.34263 | val_0_rmse: 0.61477 | val_1_rmse: 0.61648 |  0:08:55s
epoch 42 | loss: 0.35112 | val_0_rmse: 0.65542 | val_1_rmse: 0.65594 |  0:09:08s
epoch 43 | loss: 0.34245 | val_0_rmse: 0.65947 | val_1_rmse: 0.65916 |  0:09:21s
epoch 44 | loss: 0.34792 | val_0_rmse: 0.58227 | val_1_rmse: 0.58437 |  0:09:33s
epoch 45 | loss: 0.35054 | val_0_rmse: 0.64174 | val_1_rmse: 0.64212 |  0:09:46s
epoch 46 | loss: 0.35277 | val_0_rmse: 0.64533 | val_1_rmse: 0.64744 |  0:09:59s
epoch 47 | loss: 0.35487 | val_0_rmse: 0.60871 | val_1_rmse: 0.61039 |  0:10:12s
epoch 48 | loss: 0.35029 | val_0_rmse: 0.60339 | val_1_rmse: 0.60718 |  0:10:25s
epoch 49 | loss: 0.34604 | val_0_rmse: 0.61066 | val_1_rmse: 0.61101 |  0:10:37s
epoch 50 | loss: 0.34704 | val_0_rmse: 0.62951 | val_1_rmse: 0.62944 |  0:10:50s
epoch 51 | loss: 0.34472 | val_0_rmse: 0.57604 | val_1_rmse: 0.57674 |  0:11:03s
epoch 52 | loss: 0.33756 | val_0_rmse: 0.57712 | val_1_rmse: 0.58092 |  0:11:16s
epoch 53 | loss: 0.33585 | val_0_rmse: 0.57548 | val_1_rmse: 0.57885 |  0:11:29s
epoch 54 | loss: 0.33615 | val_0_rmse: 0.59653 | val_1_rmse: 0.60033 |  0:11:41s
epoch 55 | loss: 0.33584 | val_0_rmse: 0.63021 | val_1_rmse: 0.63312 |  0:11:54s
epoch 56 | loss: 0.33696 | val_0_rmse: 0.59683 | val_1_rmse: 0.60001 |  0:12:07s
epoch 57 | loss: 0.33369 | val_0_rmse: 0.57984 | val_1_rmse: 0.58334 |  0:12:19s
epoch 58 | loss: 0.33336 | val_0_rmse: 0.61854 | val_1_rmse: 0.61905 |  0:12:32s
epoch 59 | loss: 0.335   | val_0_rmse: 0.56967 | val_1_rmse: 0.57202 |  0:12:45s
epoch 60 | loss: 0.33394 | val_0_rmse: 0.59169 | val_1_rmse: 0.59585 |  0:12:58s
epoch 61 | loss: 0.33084 | val_0_rmse: 0.62993 | val_1_rmse: 0.63306 |  0:13:11s
epoch 62 | loss: 0.33146 | val_0_rmse: 0.58419 | val_1_rmse: 0.58877 |  0:13:23s
epoch 63 | loss: 0.33211 | val_0_rmse: 0.5888  | val_1_rmse: 0.58972 |  0:13:36s
epoch 64 | loss: 0.33045 | val_0_rmse: 0.69852 | val_1_rmse: 0.69858 |  0:13:49s
epoch 65 | loss: 0.33077 | val_0_rmse: 0.74427 | val_1_rmse: 0.74656 |  0:14:02s
epoch 66 | loss: 0.33251 | val_0_rmse: 0.56839 | val_1_rmse: 0.57201 |  0:14:15s
epoch 67 | loss: 0.33027 | val_0_rmse: 0.62337 | val_1_rmse: 0.62643 |  0:14:27s
epoch 68 | loss: 0.32943 | val_0_rmse: 0.67516 | val_1_rmse: 0.67766 |  0:14:40s
epoch 69 | loss: 0.32852 | val_0_rmse: 0.56805 | val_1_rmse: 0.57109 |  0:14:53s
epoch 70 | loss: 0.33196 | val_0_rmse: 0.82457 | val_1_rmse: 0.59649 |  0:15:06s
epoch 71 | loss: 0.33527 | val_0_rmse: 0.64294 | val_1_rmse: 0.64629 |  0:15:18s
epoch 72 | loss: 0.32821 | val_0_rmse: 0.83233 | val_1_rmse: 0.83684 |  0:15:31s
epoch 73 | loss: 0.32527 | val_0_rmse: 0.65578 | val_1_rmse: 0.65585 |  0:15:44s
epoch 74 | loss: 0.32471 | val_0_rmse: 0.76014 | val_1_rmse: 0.72539 |  0:15:57s
epoch 75 | loss: 0.32667 | val_0_rmse: 0.59391 | val_1_rmse: 0.59847 |  0:16:10s
epoch 76 | loss: 0.32369 | val_0_rmse: 0.65541 | val_1_rmse: 0.65936 |  0:16:22s
epoch 77 | loss: 0.32418 | val_0_rmse: 0.68922 | val_1_rmse: 0.69035 |  0:16:35s
epoch 78 | loss: 0.32402 | val_0_rmse: 0.7223  | val_1_rmse: 0.72483 |  0:16:48s
epoch 79 | loss: 0.32278 | val_0_rmse: 0.64566 | val_1_rmse: 0.64572 |  0:17:01s
epoch 80 | loss: 0.32223 | val_0_rmse: 0.73368 | val_1_rmse: 0.73722 |  0:17:13s
epoch 81 | loss: 0.32038 | val_0_rmse: 0.59255 | val_1_rmse: 0.59538 |  0:17:26s
epoch 82 | loss: 0.32215 | val_0_rmse: 0.62878 | val_1_rmse: 0.63233 |  0:17:39s
epoch 83 | loss: 0.32331 | val_0_rmse: 0.6291  | val_1_rmse: 0.63247 |  0:17:52s
epoch 84 | loss: 0.32362 | val_0_rmse: 0.5607  | val_1_rmse: 0.56602 |  0:18:05s
epoch 85 | loss: 0.32163 | val_0_rmse: 0.70916 | val_1_rmse: 0.71271 |  0:18:17s
epoch 86 | loss: 0.31859 | val_0_rmse: 0.58081 | val_1_rmse: 0.585   |  0:18:30s
epoch 87 | loss: 0.3211  | val_0_rmse: 0.56266 | val_1_rmse: 0.56556 |  0:18:43s
epoch 88 | loss: 0.31933 | val_0_rmse: 0.6325  | val_1_rmse: 0.63546 |  0:18:56s
epoch 89 | loss: 0.31863 | val_0_rmse: 0.63085 | val_1_rmse: 0.63335 |  0:19:09s
epoch 90 | loss: 0.32085 | val_0_rmse: 0.60001 | val_1_rmse: 0.60227 |  0:19:21s
epoch 91 | loss: 0.31881 | val_0_rmse: 0.55812 | val_1_rmse: 0.56259 |  0:19:33s
epoch 92 | loss: 0.31932 | val_0_rmse: 0.63169 | val_1_rmse: 0.63806 |  0:19:44s
epoch 93 | loss: 0.31797 | val_0_rmse: 0.7265  | val_1_rmse: 0.72707 |  0:19:55s
epoch 94 | loss: 0.31835 | val_0_rmse: 0.61445 | val_1_rmse: 0.61722 |  0:20:06s
epoch 95 | loss: 0.31775 | val_0_rmse: 0.69618 | val_1_rmse: 0.69741 |  0:20:17s
epoch 96 | loss: 0.31477 | val_0_rmse: 0.57628 | val_1_rmse: 0.58575 |  0:20:29s
epoch 97 | loss: 0.31707 | val_0_rmse: 0.67015 | val_1_rmse: 0.67013 |  0:20:40s
epoch 98 | loss: 0.31582 | val_0_rmse: 0.70801 | val_1_rmse: 0.70788 |  0:20:51s
epoch 99 | loss: 0.31598 | val_0_rmse: 0.58737 | val_1_rmse: 0.59232 |  0:21:02s
epoch 100| loss: 0.31675 | val_0_rmse: 0.70494 | val_1_rmse: 0.70837 |  0:21:13s
epoch 101| loss: 0.31594 | val_0_rmse: 0.67401 | val_1_rmse: 0.67984 |  0:21:24s
epoch 102| loss: 0.31454 | val_0_rmse: 0.70419 | val_1_rmse: 0.6853  |  0:21:35s
epoch 103| loss: 0.31536 | val_0_rmse: 0.69662 | val_1_rmse: 0.69879 |  0:21:46s
epoch 104| loss: 0.31433 | val_0_rmse: 0.67395 | val_1_rmse: 0.57937 |  0:21:57s
epoch 105| loss: 0.31699 | val_0_rmse: 0.67197 | val_1_rmse: 0.67629 |  0:22:08s
epoch 106| loss: 0.31466 | val_0_rmse: 0.77707 | val_1_rmse: 0.70727 |  0:22:19s
epoch 107| loss: 0.31314 | val_0_rmse: 0.72362 | val_1_rmse: 0.72603 |  0:22:30s
epoch 108| loss: 0.31474 | val_0_rmse: 0.65147 | val_1_rmse: 0.6553  |  0:22:42s
epoch 109| loss: 0.31333 | val_0_rmse: 0.5992  | val_1_rmse: 0.60463 |  0:22:53s
epoch 110| loss: 0.31554 | val_0_rmse: 0.58656 | val_1_rmse: 0.59146 |  0:23:04s
epoch 111| loss: 0.31421 | val_0_rmse: 0.6831  | val_1_rmse: 0.68686 |  0:23:15s
epoch 112| loss: 0.3159  | val_0_rmse: 0.59819 | val_1_rmse: 0.57553 |  0:23:26s
epoch 113| loss: 0.31365 | val_0_rmse: 0.61865 | val_1_rmse: 0.6053  |  0:23:37s
epoch 114| loss: 0.31486 | val_0_rmse: 0.59691 | val_1_rmse: 0.6021  |  0:23:49s
epoch 115| loss: 0.31545 | val_0_rmse: 0.70864 | val_1_rmse: 0.71164 |  0:24:00s
epoch 116| loss: 0.31257 | val_0_rmse: 0.63132 | val_1_rmse: 0.63403 |  0:24:11s
epoch 117| loss: 0.31274 | val_0_rmse: 0.70544 | val_1_rmse: 0.70976 |  0:24:22s
epoch 118| loss: 0.31466 | val_0_rmse: 0.69228 | val_1_rmse: 0.69813 |  0:24:33s
epoch 119| loss: 0.31455 | val_0_rmse: 0.55676 | val_1_rmse: 0.5639  |  0:24:44s
epoch 120| loss: 0.31316 | val_0_rmse: 0.68041 | val_1_rmse: 0.68706 |  0:24:55s
epoch 121| loss: 0.31071 | val_0_rmse: 0.64765 | val_1_rmse: 0.65332 |  0:25:06s

Early stopping occured at epoch 121 with best_epoch = 91 and best_val_1_rmse = 0.56259
Best weights from best epoch are automatically used!
ended training at: 11:49:15
Feature importance:
Mean squared error is of 2115317257.1367035
Mean absolute error:32812.717768002905
MAPE:0.3297277691655688
R2 score:0.6797746463502177
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 11:49:16
epoch 0  | loss: 0.62292 | val_0_rmse: 0.73842 | val_1_rmse: 0.74824 |  0:00:11s
epoch 1  | loss: 0.42259 | val_0_rmse: 0.67235 | val_1_rmse: 0.68275 |  0:00:22s
epoch 2  | loss: 0.40015 | val_0_rmse: 0.63993 | val_1_rmse: 0.65048 |  0:00:33s
epoch 3  | loss: 0.39774 | val_0_rmse: 0.62896 | val_1_rmse: 0.6373  |  0:00:44s
epoch 4  | loss: 0.3921  | val_0_rmse: 0.61356 | val_1_rmse: 0.62049 |  0:00:55s
epoch 5  | loss: 0.39105 | val_0_rmse: 0.63062 | val_1_rmse: 0.63827 |  0:01:07s
epoch 6  | loss: 0.38856 | val_0_rmse: 0.60847 | val_1_rmse: 0.61626 |  0:01:18s
epoch 7  | loss: 0.38082 | val_0_rmse: 0.60838 | val_1_rmse: 0.61552 |  0:01:29s
epoch 8  | loss: 0.38124 | val_0_rmse: 0.61961 | val_1_rmse: 0.62633 |  0:01:40s
epoch 9  | loss: 0.37841 | val_0_rmse: 0.6105  | val_1_rmse: 0.61796 |  0:01:51s
epoch 10 | loss: 0.37609 | val_0_rmse: 0.61055 | val_1_rmse: 0.61791 |  0:02:03s
epoch 11 | loss: 0.37572 | val_0_rmse: 0.65195 | val_1_rmse: 0.65478 |  0:02:14s
epoch 12 | loss: 0.37204 | val_0_rmse: 0.61814 | val_1_rmse: 0.62894 |  0:02:25s
epoch 13 | loss: 0.37128 | val_0_rmse: 0.60003 | val_1_rmse: 0.60753 |  0:02:36s
epoch 14 | loss: 0.3694  | val_0_rmse: 0.6038  | val_1_rmse: 0.6122  |  0:02:47s
epoch 15 | loss: 0.36827 | val_0_rmse: 0.60619 | val_1_rmse: 0.61627 |  0:02:58s
epoch 16 | loss: 0.36575 | val_0_rmse: 0.60227 | val_1_rmse: 0.61162 |  0:03:09s
epoch 17 | loss: 0.36526 | val_0_rmse: 0.60171 | val_1_rmse: 0.61739 |  0:03:20s
epoch 18 | loss: 0.36649 | val_0_rmse: 0.60427 | val_1_rmse: 0.61288 |  0:03:32s
epoch 19 | loss: 0.36439 | val_0_rmse: 0.60912 | val_1_rmse: 0.61663 |  0:03:43s
epoch 20 | loss: 0.36344 | val_0_rmse: 0.599   | val_1_rmse: 0.60677 |  0:03:54s
epoch 21 | loss: 0.36169 | val_0_rmse: 0.59958 | val_1_rmse: 0.60973 |  0:04:05s
epoch 22 | loss: 0.36259 | val_0_rmse: 0.59932 | val_1_rmse: 0.60869 |  0:04:16s
epoch 23 | loss: 0.35904 | val_0_rmse: 0.60654 | val_1_rmse: 0.61454 |  0:04:28s
epoch 24 | loss: 0.36083 | val_0_rmse: 0.60428 | val_1_rmse: 0.60817 |  0:04:39s
epoch 25 | loss: 0.35928 | val_0_rmse: 0.61335 | val_1_rmse: 0.60845 |  0:04:50s
epoch 26 | loss: 0.3602  | val_0_rmse: 0.59542 | val_1_rmse: 0.60299 |  0:05:01s
epoch 27 | loss: 0.35841 | val_0_rmse: 0.60315 | val_1_rmse: 0.60568 |  0:05:12s
epoch 28 | loss: 0.35776 | val_0_rmse: 0.59428 | val_1_rmse: 0.60133 |  0:05:24s
epoch 29 | loss: 0.35587 | val_0_rmse: 0.59504 | val_1_rmse: 0.60593 |  0:05:35s
epoch 30 | loss: 0.35695 | val_0_rmse: 0.60537 | val_1_rmse: 0.61747 |  0:05:46s
epoch 31 | loss: 0.35812 | val_0_rmse: 0.61203 | val_1_rmse: 0.67203 |  0:05:57s
epoch 32 | loss: 0.35654 | val_0_rmse: 0.61478 | val_1_rmse: 0.6818  |  0:06:09s
epoch 33 | loss: 0.3549  | val_0_rmse: 0.6304  | val_1_rmse: 0.80468 |  0:06:20s
epoch 34 | loss: 0.35412 | val_0_rmse: 0.591   | val_1_rmse: 0.60211 |  0:06:31s
epoch 35 | loss: 0.35396 | val_0_rmse: 0.64926 | val_1_rmse: 0.88458 |  0:06:42s
epoch 36 | loss: 0.35516 | val_0_rmse: 0.60304 | val_1_rmse: 0.65095 |  0:06:54s
epoch 37 | loss: 0.35571 | val_0_rmse: 0.59745 | val_1_rmse: 0.64182 |  0:07:05s
epoch 38 | loss: 0.35449 | val_0_rmse: 0.60241 | val_1_rmse: 0.66046 |  0:07:16s
epoch 39 | loss: 0.35485 | val_0_rmse: 0.6266  | val_1_rmse: 0.64416 |  0:07:27s
epoch 40 | loss: 0.35554 | val_0_rmse: 4.38451 | val_1_rmse: 9.31525 |  0:07:38s
epoch 41 | loss: 0.35503 | val_0_rmse: 0.6277  | val_1_rmse: 0.69104 |  0:07:50s
epoch 42 | loss: 0.3548  | val_0_rmse: 2.30161 | val_1_rmse: 3.40341 |  0:08:01s
epoch 43 | loss: 0.35163 | val_0_rmse: 0.59024 | val_1_rmse: 0.60216 |  0:08:12s
epoch 44 | loss: 0.352   | val_0_rmse: 0.66813 | val_1_rmse: 0.90493 |  0:08:23s
epoch 45 | loss: 0.35209 | val_0_rmse: 0.6428  | val_1_rmse: 0.86867 |  0:08:35s
epoch 46 | loss: 0.35218 | val_0_rmse: 0.62643 | val_1_rmse: 0.75312 |  0:08:46s
epoch 47 | loss: 0.35092 | val_0_rmse: 0.63136 | val_1_rmse: 0.81866 |  0:08:57s
epoch 48 | loss: 0.34966 | val_0_rmse: 11.4624 | val_1_rmse: 27.72405|  0:09:09s
epoch 49 | loss: 0.34619 | val_0_rmse: 1.61043 | val_1_rmse: 3.0582  |  0:09:20s
epoch 50 | loss: 0.34463 | val_0_rmse: 0.6169  | val_1_rmse: 0.64486 |  0:09:31s
epoch 51 | loss: 0.34363 | val_0_rmse: 0.61313 | val_1_rmse: 0.71057 |  0:09:42s
epoch 52 | loss: 0.34219 | val_0_rmse: 0.61177 | val_1_rmse: 0.62484 |  0:09:54s
epoch 53 | loss: 0.33957 | val_0_rmse: 0.61059 | val_1_rmse: 0.62378 |  0:10:05s
epoch 54 | loss: 0.33912 | val_0_rmse: 0.61897 | val_1_rmse: 0.63106 |  0:10:16s
epoch 55 | loss: 0.33711 | val_0_rmse: 0.5909  | val_1_rmse: 0.64306 |  0:10:28s
epoch 56 | loss: 0.33619 | val_0_rmse: 0.60317 | val_1_rmse: 0.60654 |  0:10:39s
epoch 57 | loss: 0.33667 | val_0_rmse: 0.58077 | val_1_rmse: 0.58832 |  0:10:50s
epoch 58 | loss: 0.33694 | val_0_rmse: 0.57776 | val_1_rmse: 0.58999 |  0:11:01s
epoch 59 | loss: 0.33297 | val_0_rmse: 0.62792 | val_1_rmse: 0.64014 |  0:11:13s
epoch 60 | loss: 0.33351 | val_0_rmse: 0.5846  | val_1_rmse: 0.59549 |  0:11:24s
epoch 61 | loss: 0.33374 | val_0_rmse: 0.58678 | val_1_rmse: 0.59557 |  0:11:35s
epoch 62 | loss: 0.33394 | val_0_rmse: 0.59585 | val_1_rmse: 0.60783 |  0:11:46s
epoch 63 | loss: 0.33059 | val_0_rmse: 0.57615 | val_1_rmse: 0.58918 |  0:11:58s
epoch 64 | loss: 0.3312  | val_0_rmse: 0.57875 | val_1_rmse: 0.5916  |  0:12:09s
epoch 65 | loss: 0.3299  | val_0_rmse: 0.57174 | val_1_rmse: 0.58449 |  0:12:20s
epoch 66 | loss: 0.33071 | val_0_rmse: 0.57524 | val_1_rmse: 0.58851 |  0:12:31s
epoch 67 | loss: 0.32964 | val_0_rmse: 0.62396 | val_1_rmse: 0.63649 |  0:12:42s
epoch 68 | loss: 0.32992 | val_0_rmse: 0.57964 | val_1_rmse: 0.64141 |  0:12:54s
epoch 69 | loss: 0.32777 | val_0_rmse: 0.57433 | val_1_rmse: 0.58729 |  0:13:05s
epoch 70 | loss: 0.32949 | val_0_rmse: 0.60038 | val_1_rmse: 0.61342 |  0:13:16s
epoch 71 | loss: 0.32755 | val_0_rmse: 0.58262 | val_1_rmse: 0.59741 |  0:13:27s
epoch 72 | loss: 0.32716 | val_0_rmse: 0.62866 | val_1_rmse: 0.63885 |  0:13:39s
epoch 73 | loss: 0.32924 | val_0_rmse: 0.59538 | val_1_rmse: 0.61225 |  0:13:50s
epoch 74 | loss: 0.32933 | val_0_rmse: 0.59164 | val_1_rmse: 0.64113 |  0:14:01s
epoch 75 | loss: 0.32594 | val_0_rmse: 0.61886 | val_1_rmse: 0.63273 |  0:14:13s
epoch 76 | loss: 0.32387 | val_0_rmse: 0.63348 | val_1_rmse: 0.64841 |  0:14:24s
epoch 77 | loss: 0.32389 | val_0_rmse: 0.61321 | val_1_rmse: 0.62608 |  0:14:35s
epoch 78 | loss: 0.32484 | val_0_rmse: 0.58854 | val_1_rmse: 0.59901 |  0:14:46s
epoch 79 | loss: 0.32414 | val_0_rmse: 0.58393 | val_1_rmse: 0.59837 |  0:14:58s
epoch 80 | loss: 0.32567 | val_0_rmse: 0.60588 | val_1_rmse: 0.61925 |  0:15:09s
epoch 81 | loss: 0.32409 | val_0_rmse: 0.66466 | val_1_rmse: 0.67861 |  0:15:20s
epoch 82 | loss: 0.32221 | val_0_rmse: 0.59195 | val_1_rmse: 0.60584 |  0:15:31s
epoch 83 | loss: 0.32528 | val_0_rmse: 0.62604 | val_1_rmse: 0.64102 |  0:15:43s
epoch 84 | loss: 0.32355 | val_0_rmse: 0.6024  | val_1_rmse: 0.61772 |  0:15:54s
epoch 85 | loss: 0.32205 | val_0_rmse: 0.62495 | val_1_rmse: 0.63993 |  0:16:05s
epoch 86 | loss: 0.32721 | val_0_rmse: 1.07397 | val_1_rmse: 1.0331  |  0:16:17s
epoch 87 | loss: 0.33794 | val_0_rmse: 0.64788 | val_1_rmse: 0.65181 |  0:16:28s
epoch 88 | loss: 0.33154 | val_0_rmse: 0.60619 | val_1_rmse: 0.61529 |  0:16:39s
epoch 89 | loss: 0.32835 | val_0_rmse: 0.56805 | val_1_rmse: 0.58112 |  0:16:50s
epoch 90 | loss: 0.32486 | val_0_rmse: 0.56999 | val_1_rmse: 0.58394 |  0:17:02s
epoch 91 | loss: 0.32681 | val_0_rmse: 0.56401 | val_1_rmse: 0.57774 |  0:17:13s
epoch 92 | loss: 0.32364 | val_0_rmse: 0.56707 | val_1_rmse: 0.58135 |  0:17:24s
epoch 93 | loss: 0.3223  | val_0_rmse: 0.7394  | val_1_rmse: 0.75257 |  0:17:35s
epoch 94 | loss: 0.32373 | val_0_rmse: 0.64197 | val_1_rmse: 0.64913 |  0:17:47s
epoch 95 | loss: 0.32385 | val_0_rmse: 0.57965 | val_1_rmse: 0.5926  |  0:17:58s
epoch 96 | loss: 0.32324 | val_0_rmse: 0.67383 | val_1_rmse: 0.6861  |  0:18:09s
epoch 97 | loss: 0.32159 | val_0_rmse: 0.61347 | val_1_rmse: 0.62642 |  0:18:20s
epoch 98 | loss: 0.32114 | val_0_rmse: 0.63768 | val_1_rmse: 0.65188 |  0:18:31s
epoch 99 | loss: 0.32155 | val_0_rmse: 0.55583 | val_1_rmse: 0.57006 |  0:18:43s
epoch 100| loss: 0.32277 | val_0_rmse: 0.65123 | val_1_rmse: 0.66431 |  0:18:54s
epoch 101| loss: 0.31978 | val_0_rmse: 0.59681 | val_1_rmse: 0.60134 |  0:19:05s
epoch 102| loss: 0.32272 | val_0_rmse: 0.58633 | val_1_rmse: 0.60021 |  0:19:17s
epoch 103| loss: 0.31937 | val_0_rmse: 0.57705 | val_1_rmse: 0.58978 |  0:19:28s
epoch 104| loss: 0.32128 | val_0_rmse: 0.66423 | val_1_rmse: 0.67824 |  0:19:39s
epoch 105| loss: 0.31987 | val_0_rmse: 0.56878 | val_1_rmse: 0.58101 |  0:19:50s
epoch 106| loss: 0.31829 | val_0_rmse: 0.56187 | val_1_rmse: 0.57299 |  0:20:02s
epoch 107| loss: 0.3172  | val_0_rmse: 0.59769 | val_1_rmse: 0.61041 |  0:20:13s
epoch 108| loss: 0.3162  | val_0_rmse: 0.5598  | val_1_rmse: 0.57252 |  0:20:24s
epoch 109| loss: 0.3175  | val_0_rmse: 0.58389 | val_1_rmse: 0.59483 |  0:20:35s
epoch 110| loss: 0.31753 | val_0_rmse: 0.55773 | val_1_rmse: 0.57131 |  0:20:47s
epoch 111| loss: 0.31675 | val_0_rmse: 0.57678 | val_1_rmse: 0.5881  |  0:20:58s
epoch 112| loss: 0.31751 | val_0_rmse: 0.63202 | val_1_rmse: 0.64212 |  0:21:09s
epoch 113| loss: 0.31889 | val_0_rmse: 0.62318 | val_1_rmse: 0.6355  |  0:21:21s
epoch 114| loss: 0.31809 | val_0_rmse: 0.7986  | val_1_rmse: 0.7623  |  0:21:32s
epoch 115| loss: 0.31924 | val_0_rmse: 0.93624 | val_1_rmse: 0.68867 |  0:21:43s
epoch 116| loss: 0.31399 | val_0_rmse: 1.04005 | val_1_rmse: 0.79186 |  0:21:54s
epoch 117| loss: 0.31507 | val_0_rmse: 0.67946 | val_1_rmse: 0.61017 |  0:22:06s
epoch 118| loss: 0.31387 | val_0_rmse: 0.97709 | val_1_rmse: 0.74317 |  0:22:17s
epoch 119| loss: 0.31416 | val_0_rmse: 1.11522 | val_1_rmse: 0.84195 |  0:22:28s
epoch 120| loss: 0.31454 | val_0_rmse: 1.20638 | val_1_rmse: 0.81179 |  0:22:39s
epoch 121| loss: 0.31392 | val_0_rmse: 0.80714 | val_1_rmse: 0.6388  |  0:22:51s
epoch 122| loss: 0.31312 | val_0_rmse: 0.64231 | val_1_rmse: 0.60702 |  0:23:02s
epoch 123| loss: 0.31433 | val_0_rmse: 0.65238 | val_1_rmse: 0.6679  |  0:23:13s
epoch 124| loss: 0.3143  | val_0_rmse: 0.62787 | val_1_rmse: 0.68317 |  0:23:25s
epoch 125| loss: 0.31388 | val_0_rmse: 0.59019 | val_1_rmse: 0.65402 |  0:23:36s
epoch 126| loss: 0.31282 | val_0_rmse: 0.72326 | val_1_rmse: 0.73165 |  0:23:47s
epoch 127| loss: 0.32281 | val_0_rmse: 0.64053 | val_1_rmse: 0.65429 |  0:23:58s
epoch 128| loss: 0.3361  | val_0_rmse: 0.60528 | val_1_rmse: 0.61651 |  0:24:10s
epoch 129| loss: 0.31726 | val_0_rmse: 0.67088 | val_1_rmse: 0.68573 |  0:24:21s

Early stopping occured at epoch 129 with best_epoch = 99 and best_val_1_rmse = 0.57006
Best weights from best epoch are automatically used!
ended training at: 12:13:41
Feature importance:
Mean squared error is of 2166129775.64118
Mean absolute error:32721.238986647688
MAPE:0.33119516963602535
R2 score:0.6754645199730871
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 12:13:42
epoch 0  | loss: 0.67203 | val_0_rmse: 0.69846 | val_1_rmse: 0.69661 |  0:00:11s
epoch 1  | loss: 0.41567 | val_0_rmse: 0.6827  | val_1_rmse: 0.68141 |  0:00:22s
epoch 2  | loss: 0.40011 | val_0_rmse: 0.64418 | val_1_rmse: 0.64479 |  0:00:33s
epoch 3  | loss: 0.39387 | val_0_rmse: 0.62147 | val_1_rmse: 0.62178 |  0:00:45s
epoch 4  | loss: 0.38717 | val_0_rmse: 0.60726 | val_1_rmse: 0.6089  |  0:00:56s
epoch 5  | loss: 0.38413 | val_0_rmse: 0.61918 | val_1_rmse: 0.62231 |  0:01:07s
epoch 6  | loss: 0.38194 | val_0_rmse: 0.6141  | val_1_rmse: 0.62761 |  0:01:18s
epoch 7  | loss: 0.3846  | val_0_rmse: 0.63724 | val_1_rmse: 0.63978 |  0:01:30s
epoch 8  | loss: 0.38319 | val_0_rmse: 0.61395 | val_1_rmse: 0.66696 |  0:01:41s
epoch 9  | loss: 0.37666 | val_0_rmse: 0.60698 | val_1_rmse: 0.60832 |  0:01:52s
epoch 10 | loss: 0.37422 | val_0_rmse: 0.60786 | val_1_rmse: 0.98216 |  0:02:03s
epoch 11 | loss: 0.3728  | val_0_rmse: 0.61186 | val_1_rmse: 0.615   |  0:02:15s
epoch 12 | loss: 0.37086 | val_0_rmse: 0.60178 | val_1_rmse: 0.60587 |  0:02:26s
epoch 13 | loss: 0.36852 | val_0_rmse: 0.60165 | val_1_rmse: 0.60454 |  0:02:37s
epoch 14 | loss: 0.36898 | val_0_rmse: 0.62203 | val_1_rmse: 0.6297  |  0:02:48s
epoch 15 | loss: 0.36701 | val_0_rmse: 0.61094 | val_1_rmse: 0.61446 |  0:02:59s
epoch 16 | loss: 0.36613 | val_0_rmse: 0.60288 | val_1_rmse: 0.60537 |  0:03:11s
epoch 17 | loss: 0.36833 | val_0_rmse: 0.60385 | val_1_rmse: 0.609   |  0:03:22s
epoch 18 | loss: 0.36729 | val_0_rmse: 0.59654 | val_1_rmse: 0.60131 |  0:03:33s
epoch 19 | loss: 0.36601 | val_0_rmse: 0.59957 | val_1_rmse: 0.60659 |  0:03:45s
epoch 20 | loss: 0.36325 | val_0_rmse: 0.59738 | val_1_rmse: 0.60183 |  0:03:56s
epoch 21 | loss: 0.3802  | val_0_rmse: 0.63694 | val_1_rmse: 0.6409  |  0:04:07s
epoch 22 | loss: 0.37341 | val_0_rmse: 0.59941 | val_1_rmse: 0.6036  |  0:04:18s
epoch 23 | loss: 0.36843 | val_0_rmse: 0.6086  | val_1_rmse: 0.61381 |  0:04:30s
epoch 24 | loss: 0.36806 | val_0_rmse: 0.60481 | val_1_rmse: 0.60941 |  0:04:41s
epoch 25 | loss: 0.36606 | val_0_rmse: 0.60011 | val_1_rmse: 0.60548 |  0:04:52s
epoch 26 | loss: 0.36533 | val_0_rmse: 0.5932  | val_1_rmse: 0.59891 |  0:05:04s
epoch 27 | loss: 0.36683 | val_0_rmse: 0.62965 | val_1_rmse: 0.63206 |  0:05:15s
epoch 28 | loss: 0.36615 | val_0_rmse: 0.59711 | val_1_rmse: 0.60154 |  0:05:26s
epoch 29 | loss: 0.36521 | val_0_rmse: 0.60137 | val_1_rmse: 0.60709 |  0:05:37s
epoch 30 | loss: 0.36474 | val_0_rmse: 0.60432 | val_1_rmse: 0.6096  |  0:05:49s
epoch 31 | loss: 0.36384 | val_0_rmse: 0.61907 | val_1_rmse: 0.62481 |  0:06:00s
epoch 32 | loss: 0.36437 | val_0_rmse: 0.59262 | val_1_rmse: 0.59804 |  0:06:11s
epoch 33 | loss: 0.36364 | val_0_rmse: 0.60098 | val_1_rmse: 0.60831 |  0:06:23s
epoch 34 | loss: 0.37116 | val_0_rmse: 0.60273 | val_1_rmse: 0.60919 |  0:06:34s
epoch 35 | loss: 0.36299 | val_0_rmse: 0.59595 | val_1_rmse: 0.60112 |  0:06:45s
epoch 36 | loss: 0.36157 | val_0_rmse: 0.59696 | val_1_rmse: 0.6042  |  0:06:56s
epoch 37 | loss: 0.35934 | val_0_rmse: 0.59488 | val_1_rmse: 0.60285 |  0:07:08s
epoch 38 | loss: 0.36049 | val_0_rmse: 0.59703 | val_1_rmse: 0.60351 |  0:07:19s
epoch 39 | loss: 0.36154 | val_0_rmse: 0.59487 | val_1_rmse: 0.60151 |  0:07:30s
epoch 40 | loss: 0.36055 | val_0_rmse: 0.59696 | val_1_rmse: 0.60353 |  0:07:42s
epoch 41 | loss: 0.3603  | val_0_rmse: 0.5912  | val_1_rmse: 0.59813 |  0:07:53s
epoch 42 | loss: 0.35713 | val_0_rmse: 0.59145 | val_1_rmse: 0.60047 |  0:08:04s
epoch 43 | loss: 0.35818 | val_0_rmse: 0.61182 | val_1_rmse: 0.6205  |  0:08:15s
epoch 44 | loss: 0.35949 | val_0_rmse: 0.59107 | val_1_rmse: 0.60078 |  0:08:27s
epoch 45 | loss: 0.35883 | val_0_rmse: 0.59268 | val_1_rmse: 0.61126 |  0:08:38s
epoch 46 | loss: 0.35772 | val_0_rmse: 0.5903  | val_1_rmse: 0.60099 |  0:08:49s
epoch 47 | loss: 0.35788 | val_0_rmse: 0.58891 | val_1_rmse: 0.59676 |  0:09:00s
epoch 48 | loss: 0.35714 | val_0_rmse: 0.60787 | val_1_rmse: 0.61111 |  0:09:11s
epoch 49 | loss: 0.35512 | val_0_rmse: 0.61855 | val_1_rmse: 0.62525 |  0:09:22s
epoch 50 | loss: 0.36324 | val_0_rmse: 0.58582 | val_1_rmse: 0.59267 |  0:09:34s
epoch 51 | loss: 0.35407 | val_0_rmse: 0.58473 | val_1_rmse: 0.58999 |  0:09:45s
epoch 52 | loss: 0.35697 | val_0_rmse: 0.59657 | val_1_rmse: 0.60197 |  0:09:56s
epoch 53 | loss: 0.3472  | val_0_rmse: 0.57861 | val_1_rmse: 0.58371 |  0:10:07s
epoch 54 | loss: 0.35312 | val_0_rmse: 0.62288 | val_1_rmse: 0.63345 |  0:10:19s
epoch 55 | loss: 0.35712 | val_0_rmse: 0.61718 | val_1_rmse: 0.638   |  0:10:30s
epoch 56 | loss: 0.34786 | val_0_rmse: 0.60523 | val_1_rmse: 0.61438 |  0:10:41s
epoch 57 | loss: 0.3465  | val_0_rmse: 0.60231 | val_1_rmse: 0.6126  |  0:10:52s
epoch 58 | loss: 0.34433 | val_0_rmse: 0.63108 | val_1_rmse: 0.63391 |  0:11:03s
epoch 59 | loss: 0.34474 | val_0_rmse: 0.59348 | val_1_rmse: 0.60415 |  0:11:14s
epoch 60 | loss: 0.34586 | val_0_rmse: 0.63223 | val_1_rmse: 0.64277 |  0:11:26s
epoch 61 | loss: 0.34248 | val_0_rmse: 0.66203 | val_1_rmse: 0.66996 |  0:11:37s
epoch 62 | loss: 0.34159 | val_0_rmse: 0.57718 | val_1_rmse: 0.59018 |  0:11:48s
epoch 63 | loss: 0.3412  | val_0_rmse: 0.61354 | val_1_rmse: 0.61963 |  0:11:59s
epoch 64 | loss: 0.34953 | val_0_rmse: 0.60217 | val_1_rmse: 0.60969 |  0:12:11s
epoch 65 | loss: 0.34245 | val_0_rmse: 0.62971 | val_1_rmse: 0.63633 |  0:12:22s
epoch 66 | loss: 0.34116 | val_0_rmse: 0.57346 | val_1_rmse: 0.58209 |  0:12:33s
epoch 67 | loss: 0.34181 | val_0_rmse: 0.60426 | val_1_rmse: 0.61589 |  0:12:44s
epoch 68 | loss: 0.338   | val_0_rmse: 0.59526 | val_1_rmse: 0.60313 |  0:12:55s
epoch 69 | loss: 0.34884 | val_0_rmse: 0.5996  | val_1_rmse: 0.61658 |  0:13:07s
epoch 70 | loss: 0.34787 | val_0_rmse: 0.59575 | val_1_rmse: 0.61491 |  0:13:18s
epoch 71 | loss: 0.34402 | val_0_rmse: 0.64427 | val_1_rmse: 0.65802 |  0:13:29s
epoch 72 | loss: 0.34631 | val_0_rmse: 0.58618 | val_1_rmse: 0.5949  |  0:13:40s
epoch 73 | loss: 0.35737 | val_0_rmse: 0.60204 | val_1_rmse: 0.62777 |  0:13:51s
epoch 74 | loss: 0.35201 | val_0_rmse: 0.60868 | val_1_rmse: 0.62305 |  0:14:02s
epoch 75 | loss: 0.36635 | val_0_rmse: 0.61932 | val_1_rmse: 0.62076 |  0:14:14s
epoch 76 | loss: 0.36364 | val_0_rmse: 0.58408 | val_1_rmse: 0.58973 |  0:14:25s
epoch 77 | loss: 0.34264 | val_0_rmse: 0.58553 | val_1_rmse: 0.59095 |  0:14:36s
epoch 78 | loss: 0.33924 | val_0_rmse: 0.58344 | val_1_rmse: 0.58982 |  0:14:48s
epoch 79 | loss: 0.33771 | val_0_rmse: 0.58279 | val_1_rmse: 0.59084 |  0:14:59s
epoch 80 | loss: 0.33764 | val_0_rmse: 0.57079 | val_1_rmse: 0.57683 |  0:15:10s
epoch 81 | loss: 0.33595 | val_0_rmse: 0.59237 | val_1_rmse: 0.60023 |  0:15:21s
epoch 82 | loss: 0.33654 | val_0_rmse: 0.5849  | val_1_rmse: 0.59266 |  0:15:32s
epoch 83 | loss: 0.33411 | val_0_rmse: 0.57597 | val_1_rmse: 0.58388 |  0:15:44s
epoch 84 | loss: 0.33455 | val_0_rmse: 0.57076 | val_1_rmse: 0.57712 |  0:15:55s
epoch 85 | loss: 0.33255 | val_0_rmse: 0.61268 | val_1_rmse: 0.62239 |  0:16:06s
epoch 86 | loss: 0.3338  | val_0_rmse: 0.56854 | val_1_rmse: 0.5762  |  0:16:17s
epoch 87 | loss: 0.33437 | val_0_rmse: 0.62772 | val_1_rmse: 0.63066 |  0:16:29s
epoch 88 | loss: 0.335   | val_0_rmse: 0.59782 | val_1_rmse: 0.60638 |  0:16:40s
epoch 89 | loss: 0.33281 | val_0_rmse: 0.58653 | val_1_rmse: 0.59366 |  0:16:51s
epoch 90 | loss: 0.3338  | val_0_rmse: 0.58923 | val_1_rmse: 0.59363 |  0:17:02s
epoch 91 | loss: 0.33194 | val_0_rmse: 0.63556 | val_1_rmse: 0.64441 |  0:17:14s
epoch 92 | loss: 0.33232 | val_0_rmse: 0.6621  | val_1_rmse: 0.67148 |  0:17:25s
epoch 93 | loss: 0.33415 | val_0_rmse: 0.57402 | val_1_rmse: 0.58072 |  0:17:37s
epoch 94 | loss: 0.33079 | val_0_rmse: 0.56979 | val_1_rmse: 0.57623 |  0:17:48s
epoch 95 | loss: 0.33171 | val_0_rmse: 0.63088 | val_1_rmse: 0.63733 |  0:17:59s
epoch 96 | loss: 0.32981 | val_0_rmse: 0.57935 | val_1_rmse: 0.58453 |  0:18:11s
epoch 97 | loss: 0.33098 | val_0_rmse: 0.60752 | val_1_rmse: 0.60958 |  0:18:22s
epoch 98 | loss: 0.33016 | val_0_rmse: 0.57509 | val_1_rmse: 0.58586 |  0:18:33s
epoch 99 | loss: 0.33156 | val_0_rmse: 0.56497 | val_1_rmse: 0.57254 |  0:18:44s
epoch 100| loss: 0.33331 | val_0_rmse: 0.9301  | val_1_rmse: 0.63276 |  0:18:56s
epoch 101| loss: 0.33212 | val_0_rmse: 0.63726 | val_1_rmse: 0.64482 |  0:19:07s
epoch 102| loss: 0.33835 | val_0_rmse: 0.56572 | val_1_rmse: 0.57279 |  0:19:18s
epoch 103| loss: 0.33193 | val_0_rmse: 0.5834  | val_1_rmse: 0.59149 |  0:19:29s
epoch 104| loss: 0.32914 | val_0_rmse: 0.62994 | val_1_rmse: 0.63864 |  0:19:41s
epoch 105| loss: 0.32843 | val_0_rmse: 0.56162 | val_1_rmse: 0.56894 |  0:19:52s
epoch 106| loss: 0.34379 | val_0_rmse: 0.61477 | val_1_rmse: 0.61901 |  0:20:03s
epoch 107| loss: 0.33664 | val_0_rmse: 0.64457 | val_1_rmse: 0.65283 |  0:20:14s
epoch 108| loss: 0.33062 | val_0_rmse: 0.60791 | val_1_rmse: 0.61209 |  0:20:25s
epoch 109| loss: 0.32981 | val_0_rmse: 0.6446  | val_1_rmse: 0.65446 |  0:20:37s
epoch 110| loss: 0.33007 | val_0_rmse: 0.59188 | val_1_rmse: 0.60137 |  0:20:48s
epoch 111| loss: 0.32744 | val_0_rmse: 1.6231  | val_1_rmse: 0.6165  |  0:20:59s
epoch 112| loss: 0.32814 | val_0_rmse: 0.6659  | val_1_rmse: 0.63538 |  0:21:10s
epoch 113| loss: 0.33015 | val_0_rmse: 0.59174 | val_1_rmse: 0.60153 |  0:21:21s
epoch 114| loss: 0.32711 | val_0_rmse: 0.57318 | val_1_rmse: 0.57791 |  0:21:33s
epoch 115| loss: 0.32749 | val_0_rmse: 0.58724 | val_1_rmse: 0.60137 |  0:21:44s
epoch 116| loss: 0.32769 | val_0_rmse: 0.56647 | val_1_rmse: 0.57513 |  0:21:55s
epoch 117| loss: 0.32762 | val_0_rmse: 0.76986 | val_1_rmse: 0.77142 |  0:22:07s
epoch 118| loss: 0.32684 | val_0_rmse: 0.56295 | val_1_rmse: 0.57816 |  0:22:18s
epoch 119| loss: 0.32648 | val_0_rmse: 0.59885 | val_1_rmse: 0.61138 |  0:22:29s
epoch 120| loss: 0.32747 | val_0_rmse: 0.57429 | val_1_rmse: 0.58735 |  0:22:41s
epoch 121| loss: 0.32613 | val_0_rmse: 0.63054 | val_1_rmse: 0.64183 |  0:22:52s
epoch 122| loss: 0.32605 | val_0_rmse: 0.66804 | val_1_rmse: 0.63746 |  0:23:03s
epoch 123| loss: 0.32773 | val_0_rmse: 0.58971 | val_1_rmse: 0.60225 |  0:23:14s
epoch 124| loss: 0.32589 | val_0_rmse: 0.5844  | val_1_rmse: 0.59307 |  0:23:26s
epoch 125| loss: 0.32531 | val_0_rmse: 0.56901 | val_1_rmse: 0.57959 |  0:23:37s
epoch 126| loss: 0.32552 | val_0_rmse: 0.63821 | val_1_rmse: 0.64689 |  0:23:48s
epoch 127| loss: 0.32562 | val_0_rmse: 0.59772 | val_1_rmse: 0.60523 |  0:24:00s
epoch 128| loss: 0.32504 | val_0_rmse: 0.55736 | val_1_rmse: 0.56772 |  0:24:11s
epoch 129| loss: 0.32443 | val_0_rmse: 0.584   | val_1_rmse: 0.59289 |  0:24:22s
epoch 130| loss: 0.32378 | val_0_rmse: 0.61099 | val_1_rmse: 0.58725 |  0:24:33s
epoch 131| loss: 0.32449 | val_0_rmse: 0.59456 | val_1_rmse: 0.60426 |  0:24:45s
epoch 132| loss: 0.32477 | val_0_rmse: 0.57938 | val_1_rmse: 0.58924 |  0:24:56s
epoch 133| loss: 0.3258  | val_0_rmse: 0.60342 | val_1_rmse: 0.57445 |  0:25:07s
epoch 134| loss: 0.32297 | val_0_rmse: 0.81842 | val_1_rmse: 0.81916 |  0:25:18s
epoch 135| loss: 0.32304 | val_0_rmse: 0.58048 | val_1_rmse: 0.5876  |  0:25:30s
epoch 136| loss: 0.32349 | val_0_rmse: 0.61431 | val_1_rmse: 0.62604 |  0:25:41s
epoch 137| loss: 0.32445 | val_0_rmse: 0.80618 | val_1_rmse: 0.81214 |  0:25:52s
epoch 138| loss: 0.32473 | val_0_rmse: 0.56705 | val_1_rmse: 0.57507 |  0:26:03s
epoch 139| loss: 0.32495 | val_0_rmse: 0.57374 | val_1_rmse: 0.58533 |  0:26:15s
epoch 140| loss: 0.32489 | val_0_rmse: 0.64511 | val_1_rmse: 0.65743 |  0:26:26s
epoch 141| loss: 0.32281 | val_0_rmse: 0.7359  | val_1_rmse: 0.74629 |  0:26:37s
epoch 142| loss: 0.32411 | val_0_rmse: 0.57928 | val_1_rmse: 0.59485 |  0:26:48s
epoch 143| loss: 0.32429 | val_0_rmse: 0.56428 | val_1_rmse: 0.57216 |  0:27:00s
epoch 144| loss: 0.32299 | val_0_rmse: 0.61455 | val_1_rmse: 0.6277  |  0:27:11s
epoch 145| loss: 0.32226 | val_0_rmse: 0.58139 | val_1_rmse: 0.58983 |  0:27:22s
epoch 146| loss: 0.32329 | val_0_rmse: 0.79298 | val_1_rmse: 0.79605 |  0:27:34s
epoch 147| loss: 0.32152 | val_0_rmse: 0.78328 | val_1_rmse: 0.84301 |  0:27:45s
epoch 148| loss: 0.32626 | val_0_rmse: 0.58772 | val_1_rmse: 0.59995 |  0:27:56s
epoch 149| loss: 0.32263 | val_0_rmse: 0.60354 | val_1_rmse: 0.88449 |  0:28:07s
Stop training because you reached max_epochs = 150 with best_epoch = 128 and best_val_1_rmse = 0.56772
Best weights from best epoch are automatically used!
ended training at: 12:41:53
Feature importance:
Mean squared error is of 2183340327.424879
Mean absolute error:32949.38621047342
MAPE:0.32807075896525145
R2 score:0.6730946750236751
------------------------------------------------------------------
